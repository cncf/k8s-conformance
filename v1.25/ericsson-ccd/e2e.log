I1130 03:20:06.982952      20 e2e.go:116] Starting e2e run "39215a65-baa7-45d5-8b0e-6ad21501392e" on Ginkgo node 1
Nov 30 03:20:06.994: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1669778406 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Nov 30 03:20:07.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:20:07.095: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E1130 03:20:07.095854      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E1130 03:20:07.095854      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Nov 30 03:20:07.112: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 30 03:20:07.185: INFO: 79 / 79 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 30 03:20:07.185: INFO: expected 17 pod replicas in namespace 'kube-system', 17 are Running and Ready.
Nov 30 03:20:07.185: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 30 03:20:07.193: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Nov 30 03:20:07.193: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Nov 30 03:20:07.193: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'eric-tm-external-connectivity-frontend-speaker' (0 seconds elapsed)
Nov 30 03:20:07.193: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
Nov 30 03:20:07.193: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Nov 30 03:20:07.193: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kucero' (0 seconds elapsed)
Nov 30 03:20:07.193: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Nov 30 03:20:07.193: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Nov 30 03:20:07.193: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'subport-controller' (0 seconds elapsed)
Nov 30 03:20:07.193: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'subport-manager' (0 seconds elapsed)
Nov 30 03:20:07.193: INFO: e2e test version: v1.25.3
Nov 30 03:20:07.195: INFO: kube-apiserver version: v1.25.3
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Nov 30 03:20:07.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:20:07.198: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.103 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Nov 30 03:20:07.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:20:07.095: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E1130 03:20:07.095854      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Nov 30 03:20:07.112: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Nov 30 03:20:07.185: INFO: 79 / 79 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Nov 30 03:20:07.185: INFO: expected 17 pod replicas in namespace 'kube-system', 17 are Running and Ready.
    Nov 30 03:20:07.185: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Nov 30 03:20:07.193: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Nov 30 03:20:07.193: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
    Nov 30 03:20:07.193: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'eric-tm-external-connectivity-frontend-speaker' (0 seconds elapsed)
    Nov 30 03:20:07.193: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
    Nov 30 03:20:07.193: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Nov 30 03:20:07.193: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kucero' (0 seconds elapsed)
    Nov 30 03:20:07.193: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Nov 30 03:20:07.193: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
    Nov 30 03:20:07.193: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'subport-controller' (0 seconds elapsed)
    Nov 30 03:20:07.193: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'subport-manager' (0 seconds elapsed)
    Nov 30 03:20:07.193: INFO: e2e test version: v1.25.3
    Nov 30 03:20:07.195: INFO: kube-apiserver version: v1.25.3
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Nov 30 03:20:07.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:20:07.198: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:20:07.222
Nov 30 03:20:07.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 03:20:07.223
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:07.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:07.258
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 11/30/22 03:20:07.261
Nov 30 03:20:07.323: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-d26bff1a-da25-465a-87b5-d0c45290fe5e" in namespace "emptydir-3352" to be "running"
Nov 30 03:20:07.328: INFO: Pod "pod-sharedvolume-d26bff1a-da25-465a-87b5-d0c45290fe5e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.112054ms
Nov 30 03:20:09.333: INFO: Pod "pod-sharedvolume-d26bff1a-da25-465a-87b5-d0c45290fe5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010104224s
Nov 30 03:20:09.333: INFO: Pod "pod-sharedvolume-d26bff1a-da25-465a-87b5-d0c45290fe5e" satisfied condition "running"
STEP: Reading file content from the nginx-container 11/30/22 03:20:09.333
Nov 30 03:20:09.334: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3352 PodName:pod-sharedvolume-d26bff1a-da25-465a-87b5-d0c45290fe5e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:20:09.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:20:09.334: INFO: ExecWithOptions: Clientset creation
Nov 30 03:20:09.334: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-3352/pods/pod-sharedvolume-d26bff1a-da25-465a-87b5-d0c45290fe5e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Nov 30 03:20:09.404: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 03:20:09.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3352" for this suite. 11/30/22 03:20:09.408
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":1,"skipped":9,"failed":0}
------------------------------
• [2.194 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:20:07.222
    Nov 30 03:20:07.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 03:20:07.223
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:07.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:07.258
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 11/30/22 03:20:07.261
    Nov 30 03:20:07.323: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-d26bff1a-da25-465a-87b5-d0c45290fe5e" in namespace "emptydir-3352" to be "running"
    Nov 30 03:20:07.328: INFO: Pod "pod-sharedvolume-d26bff1a-da25-465a-87b5-d0c45290fe5e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.112054ms
    Nov 30 03:20:09.333: INFO: Pod "pod-sharedvolume-d26bff1a-da25-465a-87b5-d0c45290fe5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010104224s
    Nov 30 03:20:09.333: INFO: Pod "pod-sharedvolume-d26bff1a-da25-465a-87b5-d0c45290fe5e" satisfied condition "running"
    STEP: Reading file content from the nginx-container 11/30/22 03:20:09.333
    Nov 30 03:20:09.334: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3352 PodName:pod-sharedvolume-d26bff1a-da25-465a-87b5-d0c45290fe5e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:20:09.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:20:09.334: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:20:09.334: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-3352/pods/pod-sharedvolume-d26bff1a-da25-465a-87b5-d0c45290fe5e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Nov 30 03:20:09.404: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 03:20:09.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3352" for this suite. 11/30/22 03:20:09.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:20:09.417
Nov 30 03:20:09.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pods 11/30/22 03:20:09.418
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:09.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:09.442
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Nov 30 03:20:09.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: creating the pod 11/30/22 03:20:09.444
STEP: submitting the pod to kubernetes 11/30/22 03:20:09.444
Nov 30 03:20:09.460: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896" in namespace "pods-5920" to be "running and ready"
Nov 30 03:20:09.468: INFO: Pod "pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896": Phase="Pending", Reason="", readiness=false. Elapsed: 7.798309ms
Nov 30 03:20:09.468: INFO: The phase of Pod pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:20:11.471: INFO: Pod "pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896": Phase="Running", Reason="", readiness=true. Elapsed: 2.010831566s
Nov 30 03:20:11.471: INFO: The phase of Pod pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896 is Running (Ready = true)
Nov 30 03:20:11.471: INFO: Pod "pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Nov 30 03:20:11.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5920" for this suite. 11/30/22 03:20:11.582
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":2,"skipped":26,"failed":0}
------------------------------
• [2.171 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:20:09.417
    Nov 30 03:20:09.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pods 11/30/22 03:20:09.418
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:09.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:09.442
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Nov 30 03:20:09.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: creating the pod 11/30/22 03:20:09.444
    STEP: submitting the pod to kubernetes 11/30/22 03:20:09.444
    Nov 30 03:20:09.460: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896" in namespace "pods-5920" to be "running and ready"
    Nov 30 03:20:09.468: INFO: Pod "pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896": Phase="Pending", Reason="", readiness=false. Elapsed: 7.798309ms
    Nov 30 03:20:09.468: INFO: The phase of Pod pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:20:11.471: INFO: Pod "pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896": Phase="Running", Reason="", readiness=true. Elapsed: 2.010831566s
    Nov 30 03:20:11.471: INFO: The phase of Pod pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896 is Running (Ready = true)
    Nov 30 03:20:11.471: INFO: Pod "pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Nov 30 03:20:11.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5920" for this suite. 11/30/22 03:20:11.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:20:11.589
Nov 30 03:20:11.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:20:11.59
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:11.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:11.617
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 11/30/22 03:20:11.619
Nov 30 03:20:11.635: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e" in namespace "projected-9698" to be "Succeeded or Failed"
Nov 30 03:20:11.638: INFO: Pod "downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.899525ms
Nov 30 03:20:13.641: INFO: Pod "downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006135183s
Nov 30 03:20:15.656: INFO: Pod "downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021188252s
Nov 30 03:20:17.642: INFO: Pod "downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006869016s
STEP: Saw pod success 11/30/22 03:20:17.642
Nov 30 03:20:17.642: INFO: Pod "downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e" satisfied condition "Succeeded or Failed"
Nov 30 03:20:17.644: INFO: Trying to get logs from node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins pod downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e container client-container: <nil>
STEP: delete the pod 11/30/22 03:20:17.658
Nov 30 03:20:17.673: INFO: Waiting for pod downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e to disappear
Nov 30 03:20:17.678: INFO: Pod downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Nov 30 03:20:17.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9698" for this suite. 11/30/22 03:20:17.682
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":3,"skipped":65,"failed":0}
------------------------------
• [SLOW TEST] [6.103 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:20:11.589
    Nov 30 03:20:11.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:20:11.59
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:11.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:11.617
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 11/30/22 03:20:11.619
    Nov 30 03:20:11.635: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e" in namespace "projected-9698" to be "Succeeded or Failed"
    Nov 30 03:20:11.638: INFO: Pod "downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.899525ms
    Nov 30 03:20:13.641: INFO: Pod "downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006135183s
    Nov 30 03:20:15.656: INFO: Pod "downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021188252s
    Nov 30 03:20:17.642: INFO: Pod "downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006869016s
    STEP: Saw pod success 11/30/22 03:20:17.642
    Nov 30 03:20:17.642: INFO: Pod "downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e" satisfied condition "Succeeded or Failed"
    Nov 30 03:20:17.644: INFO: Trying to get logs from node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins pod downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e container client-container: <nil>
    STEP: delete the pod 11/30/22 03:20:17.658
    Nov 30 03:20:17.673: INFO: Waiting for pod downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e to disappear
    Nov 30 03:20:17.678: INFO: Pod downwardapi-volume-06c516aa-d495-4147-b565-09f32968337e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Nov 30 03:20:17.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9698" for this suite. 11/30/22 03:20:17.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:20:17.693
Nov 30 03:20:17.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename gc 11/30/22 03:20:17.694
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:17.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:17.717
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 11/30/22 03:20:17.725
STEP: delete the rc 11/30/22 03:20:22.737
STEP: wait for the rc to be deleted 11/30/22 03:20:22.746
Nov 30 03:20:23.769: INFO: 80 pods remaining
Nov 30 03:20:23.769: INFO: 80 pods has nil DeletionTimestamp
Nov 30 03:20:23.769: INFO: 
Nov 30 03:20:24.769: INFO: 72 pods remaining
Nov 30 03:20:24.769: INFO: 71 pods has nil DeletionTimestamp
Nov 30 03:20:24.769: INFO: 
Nov 30 03:20:25.772: INFO: 60 pods remaining
Nov 30 03:20:25.772: INFO: 59 pods has nil DeletionTimestamp
Nov 30 03:20:25.772: INFO: 
Nov 30 03:20:26.763: INFO: 40 pods remaining
Nov 30 03:20:26.763: INFO: 40 pods has nil DeletionTimestamp
Nov 30 03:20:26.763: INFO: 
Nov 30 03:20:27.777: INFO: 32 pods remaining
Nov 30 03:20:27.777: INFO: 32 pods has nil DeletionTimestamp
Nov 30 03:20:27.777: INFO: 
Nov 30 03:20:28.761: INFO: 19 pods remaining
Nov 30 03:20:28.761: INFO: 19 pods has nil DeletionTimestamp
Nov 30 03:20:28.761: INFO: 
STEP: Gathering metrics 11/30/22 03:20:29.755
Nov 30 03:20:29.787: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
Nov 30 03:20:29.792: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 4.616597ms
Nov 30 03:20:29.792: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
Nov 30 03:20:29.792: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
Nov 30 03:20:30.108: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Nov 30 03:20:30.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7131" for this suite. 11/30/22 03:20:30.112
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":4,"skipped":102,"failed":0}
------------------------------
• [SLOW TEST] [12.433 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:20:17.693
    Nov 30 03:20:17.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename gc 11/30/22 03:20:17.694
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:17.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:17.717
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 11/30/22 03:20:17.725
    STEP: delete the rc 11/30/22 03:20:22.737
    STEP: wait for the rc to be deleted 11/30/22 03:20:22.746
    Nov 30 03:20:23.769: INFO: 80 pods remaining
    Nov 30 03:20:23.769: INFO: 80 pods has nil DeletionTimestamp
    Nov 30 03:20:23.769: INFO: 
    Nov 30 03:20:24.769: INFO: 72 pods remaining
    Nov 30 03:20:24.769: INFO: 71 pods has nil DeletionTimestamp
    Nov 30 03:20:24.769: INFO: 
    Nov 30 03:20:25.772: INFO: 60 pods remaining
    Nov 30 03:20:25.772: INFO: 59 pods has nil DeletionTimestamp
    Nov 30 03:20:25.772: INFO: 
    Nov 30 03:20:26.763: INFO: 40 pods remaining
    Nov 30 03:20:26.763: INFO: 40 pods has nil DeletionTimestamp
    Nov 30 03:20:26.763: INFO: 
    Nov 30 03:20:27.777: INFO: 32 pods remaining
    Nov 30 03:20:27.777: INFO: 32 pods has nil DeletionTimestamp
    Nov 30 03:20:27.777: INFO: 
    Nov 30 03:20:28.761: INFO: 19 pods remaining
    Nov 30 03:20:28.761: INFO: 19 pods has nil DeletionTimestamp
    Nov 30 03:20:28.761: INFO: 
    STEP: Gathering metrics 11/30/22 03:20:29.755
    Nov 30 03:20:29.787: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
    Nov 30 03:20:29.792: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 4.616597ms
    Nov 30 03:20:29.792: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
    Nov 30 03:20:29.792: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
    Nov 30 03:20:30.108: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Nov 30 03:20:30.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7131" for this suite. 11/30/22 03:20:30.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:20:30.127
Nov 30 03:20:30.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 03:20:30.128
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:30.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:30.159
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-fc462676-f5ed-40b6-a0a3-086d5dc2b831 11/30/22 03:20:30.161
STEP: Creating a pod to test consume secrets 11/30/22 03:20:30.167
Nov 30 03:20:30.220: INFO: Waiting up to 5m0s for pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2" in namespace "secrets-5274" to be "Succeeded or Failed"
Nov 30 03:20:30.230: INFO: Pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.936478ms
Nov 30 03:20:32.234: INFO: Pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014012276s
Nov 30 03:20:34.234: INFO: Pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014331605s
Nov 30 03:20:36.234: INFO: Pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013986425s
Nov 30 03:20:38.234: INFO: Pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014341434s
STEP: Saw pod success 11/30/22 03:20:38.234
Nov 30 03:20:38.234: INFO: Pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2" satisfied condition "Succeeded or Failed"
Nov 30 03:20:38.237: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2 container secret-volume-test: <nil>
STEP: delete the pod 11/30/22 03:20:38.246
Nov 30 03:20:38.264: INFO: Waiting for pod pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2 to disappear
Nov 30 03:20:38.271: INFO: Pod pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Nov 30 03:20:38.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5274" for this suite. 11/30/22 03:20:38.274
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":5,"skipped":147,"failed":0}
------------------------------
• [SLOW TEST] [8.152 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:20:30.127
    Nov 30 03:20:30.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 03:20:30.128
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:30.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:30.159
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-fc462676-f5ed-40b6-a0a3-086d5dc2b831 11/30/22 03:20:30.161
    STEP: Creating a pod to test consume secrets 11/30/22 03:20:30.167
    Nov 30 03:20:30.220: INFO: Waiting up to 5m0s for pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2" in namespace "secrets-5274" to be "Succeeded or Failed"
    Nov 30 03:20:30.230: INFO: Pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.936478ms
    Nov 30 03:20:32.234: INFO: Pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014012276s
    Nov 30 03:20:34.234: INFO: Pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014331605s
    Nov 30 03:20:36.234: INFO: Pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013986425s
    Nov 30 03:20:38.234: INFO: Pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014341434s
    STEP: Saw pod success 11/30/22 03:20:38.234
    Nov 30 03:20:38.234: INFO: Pod "pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2" satisfied condition "Succeeded or Failed"
    Nov 30 03:20:38.237: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2 container secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 03:20:38.246
    Nov 30 03:20:38.264: INFO: Waiting for pod pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2 to disappear
    Nov 30 03:20:38.271: INFO: Pod pod-secrets-c11f34cc-fe6e-483f-b407-11b33f1d20d2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 03:20:38.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5274" for this suite. 11/30/22 03:20:38.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:20:38.28
Nov 30 03:20:38.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename sched-pred 11/30/22 03:20:38.281
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:38.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:38.3
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Nov 30 03:20:38.302: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 30 03:20:38.311: INFO: Waiting for terminating namespaces to be deleted...
Nov 30 03:20:38.315: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins before test
Nov 30 03:20:38.329: INFO: calico-node-9bcg6 from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 03:20:38.329: INFO: csi-cinder-nodeplugin-hnpf8 from kube-system started at 2022-11-30 02:52:37 +0000 UTC (3 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 03:20:38.329: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 03:20:38.329: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 03:20:38.329: INFO: eric-lm-combined-server-license-consumer-handler-75bfcf7bcsqh2w from kube-system started at 2022-11-30 03:04:36 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Nov 30 03:20:38.329: INFO: eric-tm-external-connectivity-frontend-controller-689dd9fddrvwp from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container controller ready: true, restart count 0
Nov 30 03:20:38.329: INFO: eric-tm-external-connectivity-frontend-speaker-wdz6h from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container speaker ready: true, restart count 0
Nov 30 03:20:38.329: INFO: kube-multus-ds-amd64-nw2qp from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 03:20:38.329: INFO: kube-proxy-stdnr from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 30 03:20:38.329: INFO: kucero-qz2ld from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container kucero ready: true, restart count 0
Nov 30 03:20:38.329: INFO: node-local-dns-wvqkc from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container node-cache ready: true, restart count 1
Nov 30 03:20:38.329: INFO: eric-pm-alertmanager-76c454d9f7-knmfv from monitoring started at 2022-11-30 02:59:03 +0000 UTC (2 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
Nov 30 03:20:38.329: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
Nov 30 03:20:38.329: INFO: eric-pm-node-exporter-rlfnb from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 03:20:38.329: INFO: eric-pm-server-utils-7585bb6b5d-b7sll from monitoring started at 2022-11-30 02:59:52 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
Nov 30 03:20:38.329: INFO: eric-victoria-metrics-alert-server-54c5c5474c-6djgg from monitoring started at 2022-11-30 02:58:01 +0000 UTC (2 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
Nov 30 03:20:38.329: INFO: 	Container vmalert-config-reload ready: true, restart count 0
Nov 30 03:20:38.329: INFO: eric-victoria-metrics-cluster-vminsert-6c76f96c54-cg7hw from monitoring started at 2022-11-30 02:57:04 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
Nov 30 03:20:38.329: INFO: eric-victoria-metrics-cluster-vmselect-55c4b48dfb-m4tk8 from monitoring started at 2022-11-30 02:57:04 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
Nov 30 03:20:38.329: INFO: node-cert-exporter-hn5jk from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 03:20:38.329: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-6bvhh from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 03:20:38.329: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 03:20:38.329: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 30 03:20:38.329: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins before test
Nov 30 03:20:38.357: INFO: default-http-backend-6f4f64db57-dnh7m from ingress-nginx started at 2022-11-30 02:55:20 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container default-http-backend ready: true, restart count 0
Nov 30 03:20:38.357: INFO: nginx-ingress-controller-844dff9bc5-684fv from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Nov 30 03:20:38.357: INFO: calico-node-thlq4 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 03:20:38.357: INFO: calicoctl-c47c68f5-42nfq from kube-system started at 2022-11-30 03:07:12 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container calicoctl ready: true, restart count 0
Nov 30 03:20:38.357: INFO: csi-cinder-nodeplugin-5hx92 from kube-system started at 2022-11-30 02:52:16 +0000 UTC (3 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 03:20:38.357: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 03:20:38.357: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 03:20:38.357: INFO: eric-app-sys-info-handler-754d6dcc6b-5xw88 from kube-system started at 2022-11-30 03:02:36 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
Nov 30 03:20:38.357: INFO: eric-data-document-database-pg-0 from kube-system started at 2022-11-30 03:03:48 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
Nov 30 03:20:38.357: INFO: eric-tm-external-connectivity-frontend-speaker-vkmgp from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container speaker ready: true, restart count 0
Nov 30 03:20:38.357: INFO: kube-multus-ds-amd64-lhrwf from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 03:20:38.357: INFO: kube-proxy-g68qp from kube-system started at 2022-11-30 02:51:41 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container kube-proxy ready: true, restart count 1
Nov 30 03:20:38.357: INFO: kucero-29zhp from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container kucero ready: true, restart count 0
Nov 30 03:20:38.357: INFO: network-resources-injector-6dfc58d4f-rm8m2 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container webhook-server ready: true, restart count 0
Nov 30 03:20:38.357: INFO: node-local-dns-4fvm6 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container node-cache ready: true, restart count 0
Nov 30 03:20:38.357: INFO: eric-pm-kube-state-metrics-6696677569-fnnhj from monitoring started at 2022-11-30 02:58:51 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
Nov 30 03:20:38.357: INFO: eric-pm-node-exporter-dhtts from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 03:20:38.357: INFO: eric-pm-pushgateway-6db47dc9d7-fbx4n from monitoring started at 2022-11-30 02:59:19 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container pushgateway ready: true, restart count 0
Nov 30 03:20:38.357: INFO: eric-victoria-metrics-agent-c9f978858-cmz9r from monitoring started at 2022-11-30 02:57:48 +0000 UTC (2 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
Nov 30 03:20:38.357: INFO: 	Container vmagent-config-reload ready: true, restart count 0
Nov 30 03:20:38.357: INFO: node-cert-exporter-jbl7k from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 03:20:38.357: INFO: sonobuoy-e2e-job-681d55e1c5a74dc7 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 03:20:38.357: INFO: 	Container e2e ready: true, restart count 0
Nov 30 03:20:38.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 03:20:38.358: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-nbjs4 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 03:20:38.358: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 03:20:38.358: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 30 03:20:38.358: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins before test
Nov 30 03:20:38.371: INFO: nginx-ingress-controller-844dff9bc5-7std7 from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Nov 30 03:20:38.371: INFO: calico-node-c6ckn from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 03:20:38.371: INFO: csi-cinder-nodeplugin-xvtjm from kube-system started at 2022-11-30 02:52:53 +0000 UTC (3 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 03:20:38.371: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 03:20:38.371: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 03:20:38.371: INFO: eric-lm-combined-server-license-server-client-d9b78bd6b-5r5rz from kube-system started at 2022-11-30 03:04:36 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Nov 30 03:20:38.371: INFO: eric-tm-external-connectivity-frontend-speaker-9k8gj from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container speaker ready: true, restart count 0
Nov 30 03:20:38.371: INFO: kube-multus-ds-amd64-rk7dk from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 03:20:38.371: INFO: kube-proxy-2zzdw from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 30 03:20:38.371: INFO: kucero-nds94 from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container kucero ready: true, restart count 0
Nov 30 03:20:38.371: INFO: node-local-dns-4bsnx from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container node-cache ready: true, restart count 1
Nov 30 03:20:38.371: INFO: eric-pm-node-exporter-xs7lt from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 03:20:38.371: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2022-11-30 02:57:08 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
Nov 30 03:20:38.371: INFO: node-cert-exporter-l77s4 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 03:20:38.371: INFO: pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896 from pods-5920 started at 2022-11-30 03:20:09 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container main ready: true, restart count 0
Nov 30 03:20:38.371: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-88nb5 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 03:20:38.371: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 03:20:38.371: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 30 03:20:38.371: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins before test
Nov 30 03:20:38.383: INFO: calico-node-klj9m from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 03:20:38.383: INFO: ccd-license-consumer-5949d66498-mjgr8 from kube-system started at 2022-11-30 03:05:25 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Nov 30 03:20:38.383: INFO: csi-cinder-nodeplugin-sw4wz from kube-system started at 2022-11-30 02:52:31 +0000 UTC (3 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 03:20:38.383: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 03:20:38.383: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 03:20:38.383: INFO: eric-lcm-container-registry-registry-7f7856985-4rfb5 from kube-system started at 2022-11-30 02:56:13 +0000 UTC (3 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
Nov 30 03:20:38.383: INFO: 	Container registry ready: true, restart count 0
Nov 30 03:20:38.383: INFO: 	Container sidecar ready: true, restart count 0
Nov 30 03:20:38.383: INFO: eric-tm-external-connectivity-frontend-speaker-vmfpl from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container speaker ready: true, restart count 0
Nov 30 03:20:38.383: INFO: kube-multus-ds-amd64-cf4v4 from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 03:20:38.383: INFO: kube-proxy-6s7bb from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 30 03:20:38.383: INFO: kucero-5bwbh from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container kucero ready: true, restart count 0
Nov 30 03:20:38.383: INFO: metrics-server-7567d6784b-dw2c8 from kube-system started at 2022-11-30 03:01:29 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container metrics-server ready: true, restart count 0
Nov 30 03:20:38.383: INFO: network-resources-injector-6dfc58d4f-nnqj7 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container webhook-server ready: true, restart count 0
Nov 30 03:20:38.383: INFO: node-local-dns-4hp9c from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container node-cache ready: true, restart count 1
Nov 30 03:20:38.383: INFO: eric-pm-node-exporter-lmw6v from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 03:20:38.383: INFO: isp-logger-5b4d57f796-5ncwr from monitoring started at 2022-11-30 03:06:55 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container isp-logger ready: true, restart count 0
Nov 30 03:20:38.383: INFO: node-cert-exporter-v97f6 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 03:20:38.383: INFO: sonobuoy from sonobuoy started at 2022-11-30 03:19:54 +0000 UTC (1 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 30 03:20:38.383: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-z79zw from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 03:20:38.383: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 03:20:38.383: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 11/30/22 03:20:38.383
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.172c3ea495704460], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 7 node(s) didn't match Pod's node affinity/selector. preemption: 0/7 nodes are available: 7 Preemption is not helpful for scheduling.] 11/30/22 03:20:38.454
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Nov 30 03:20:39.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3390" for this suite. 11/30/22 03:20:39.456
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":6,"skipped":181,"failed":0}
------------------------------
• [1.181 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:20:38.28
    Nov 30 03:20:38.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename sched-pred 11/30/22 03:20:38.281
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:38.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:38.3
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Nov 30 03:20:38.302: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 30 03:20:38.311: INFO: Waiting for terminating namespaces to be deleted...
    Nov 30 03:20:38.315: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins before test
    Nov 30 03:20:38.329: INFO: calico-node-9bcg6 from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: csi-cinder-nodeplugin-hnpf8 from kube-system started at 2022-11-30 02:52:37 +0000 UTC (3 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: eric-lm-combined-server-license-consumer-handler-75bfcf7bcsqh2w from kube-system started at 2022-11-30 03:04:36 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: eric-tm-external-connectivity-frontend-controller-689dd9fddrvwp from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container controller ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: eric-tm-external-connectivity-frontend-speaker-wdz6h from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: kube-multus-ds-amd64-nw2qp from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: kube-proxy-stdnr from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: kucero-qz2ld from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: node-local-dns-wvqkc from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container node-cache ready: true, restart count 1
    Nov 30 03:20:38.329: INFO: eric-pm-alertmanager-76c454d9f7-knmfv from monitoring started at 2022-11-30 02:59:03 +0000 UTC (2 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: eric-pm-node-exporter-rlfnb from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: eric-pm-server-utils-7585bb6b5d-b7sll from monitoring started at 2022-11-30 02:59:52 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: eric-victoria-metrics-alert-server-54c5c5474c-6djgg from monitoring started at 2022-11-30 02:58:01 +0000 UTC (2 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: 	Container vmalert-config-reload ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: eric-victoria-metrics-cluster-vminsert-6c76f96c54-cg7hw from monitoring started at 2022-11-30 02:57:04 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: eric-victoria-metrics-cluster-vmselect-55c4b48dfb-m4tk8 from monitoring started at 2022-11-30 02:57:04 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: node-cert-exporter-hn5jk from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-6bvhh from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 03:20:38.329: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 30 03:20:38.329: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins before test
    Nov 30 03:20:38.357: INFO: default-http-backend-6f4f64db57-dnh7m from ingress-nginx started at 2022-11-30 02:55:20 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container default-http-backend ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: nginx-ingress-controller-844dff9bc5-684fv from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: calico-node-thlq4 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: calicoctl-c47c68f5-42nfq from kube-system started at 2022-11-30 03:07:12 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container calicoctl ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: csi-cinder-nodeplugin-5hx92 from kube-system started at 2022-11-30 02:52:16 +0000 UTC (3 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: eric-app-sys-info-handler-754d6dcc6b-5xw88 from kube-system started at 2022-11-30 03:02:36 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: eric-data-document-database-pg-0 from kube-system started at 2022-11-30 03:03:48 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: eric-tm-external-connectivity-frontend-speaker-vkmgp from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: kube-multus-ds-amd64-lhrwf from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: kube-proxy-g68qp from kube-system started at 2022-11-30 02:51:41 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container kube-proxy ready: true, restart count 1
    Nov 30 03:20:38.357: INFO: kucero-29zhp from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: network-resources-injector-6dfc58d4f-rm8m2 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container webhook-server ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: node-local-dns-4fvm6 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container node-cache ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: eric-pm-kube-state-metrics-6696677569-fnnhj from monitoring started at 2022-11-30 02:58:51 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: eric-pm-node-exporter-dhtts from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: eric-pm-pushgateway-6db47dc9d7-fbx4n from monitoring started at 2022-11-30 02:59:19 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container pushgateway ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: eric-victoria-metrics-agent-c9f978858-cmz9r from monitoring started at 2022-11-30 02:57:48 +0000 UTC (2 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: 	Container vmagent-config-reload ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: node-cert-exporter-jbl7k from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: sonobuoy-e2e-job-681d55e1c5a74dc7 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 03:20:38.357: INFO: 	Container e2e ready: true, restart count 0
    Nov 30 03:20:38.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 03:20:38.358: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-nbjs4 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 03:20:38.358: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 03:20:38.358: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 30 03:20:38.358: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins before test
    Nov 30 03:20:38.371: INFO: nginx-ingress-controller-844dff9bc5-7std7 from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: calico-node-c6ckn from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: csi-cinder-nodeplugin-xvtjm from kube-system started at 2022-11-30 02:52:53 +0000 UTC (3 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: eric-lm-combined-server-license-server-client-d9b78bd6b-5r5rz from kube-system started at 2022-11-30 03:04:36 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: eric-tm-external-connectivity-frontend-speaker-9k8gj from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: kube-multus-ds-amd64-rk7dk from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: kube-proxy-2zzdw from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: kucero-nds94 from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: node-local-dns-4bsnx from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container node-cache ready: true, restart count 1
    Nov 30 03:20:38.371: INFO: eric-pm-node-exporter-xs7lt from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2022-11-30 02:57:08 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: node-cert-exporter-l77s4 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: pod-exec-websocket-c5e37abd-e69c-4593-ac93-e6df278f3896 from pods-5920 started at 2022-11-30 03:20:09 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container main ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-88nb5 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 03:20:38.371: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 30 03:20:38.371: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins before test
    Nov 30 03:20:38.383: INFO: calico-node-klj9m from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: ccd-license-consumer-5949d66498-mjgr8 from kube-system started at 2022-11-30 03:05:25 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container ccd-license-consumer ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: csi-cinder-nodeplugin-sw4wz from kube-system started at 2022-11-30 02:52:31 +0000 UTC (3 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: eric-lcm-container-registry-registry-7f7856985-4rfb5 from kube-system started at 2022-11-30 02:56:13 +0000 UTC (3 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: 	Container registry ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: 	Container sidecar ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: eric-tm-external-connectivity-frontend-speaker-vmfpl from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: kube-multus-ds-amd64-cf4v4 from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: kube-proxy-6s7bb from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: kucero-5bwbh from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: metrics-server-7567d6784b-dw2c8 from kube-system started at 2022-11-30 03:01:29 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container metrics-server ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: network-resources-injector-6dfc58d4f-nnqj7 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container webhook-server ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: node-local-dns-4hp9c from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container node-cache ready: true, restart count 1
    Nov 30 03:20:38.383: INFO: eric-pm-node-exporter-lmw6v from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: isp-logger-5b4d57f796-5ncwr from monitoring started at 2022-11-30 03:06:55 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container isp-logger ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: node-cert-exporter-v97f6 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: sonobuoy from sonobuoy started at 2022-11-30 03:19:54 +0000 UTC (1 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-z79zw from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 03:20:38.383: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 03:20:38.383: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 11/30/22 03:20:38.383
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.172c3ea495704460], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 7 node(s) didn't match Pod's node affinity/selector. preemption: 0/7 nodes are available: 7 Preemption is not helpful for scheduling.] 11/30/22 03:20:38.454
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 03:20:39.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3390" for this suite. 11/30/22 03:20:39.456
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:20:39.462
Nov 30 03:20:39.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename subpath 11/30/22 03:20:39.463
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:39.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:39.482
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/30/22 03:20:39.484
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-mjcp 11/30/22 03:20:39.496
STEP: Creating a pod to test atomic-volume-subpath 11/30/22 03:20:39.496
Nov 30 03:20:39.506: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mjcp" in namespace "subpath-6290" to be "Succeeded or Failed"
Nov 30 03:20:39.512: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.144568ms
Nov 30 03:20:41.516: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.010291341s
Nov 30 03:20:43.520: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 4.014011907s
Nov 30 03:20:45.520: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 6.014639204s
Nov 30 03:20:47.516: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 8.010689148s
Nov 30 03:20:49.516: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 10.010010978s
Nov 30 03:20:51.516: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 12.009870489s
Nov 30 03:20:53.515: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 14.009224914s
Nov 30 03:20:55.516: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 16.010445429s
Nov 30 03:20:57.516: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 18.01019205s
Nov 30 03:20:59.517: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 20.011362242s
Nov 30 03:21:01.515: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=false. Elapsed: 22.009272964s
Nov 30 03:21:03.517: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011039589s
STEP: Saw pod success 11/30/22 03:21:03.517
Nov 30 03:21:03.517: INFO: Pod "pod-subpath-test-configmap-mjcp" satisfied condition "Succeeded or Failed"
Nov 30 03:21:03.522: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-subpath-test-configmap-mjcp container test-container-subpath-configmap-mjcp: <nil>
STEP: delete the pod 11/30/22 03:21:03.53
Nov 30 03:21:03.544: INFO: Waiting for pod pod-subpath-test-configmap-mjcp to disappear
Nov 30 03:21:03.546: INFO: Pod pod-subpath-test-configmap-mjcp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mjcp 11/30/22 03:21:03.546
Nov 30 03:21:03.546: INFO: Deleting pod "pod-subpath-test-configmap-mjcp" in namespace "subpath-6290"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Nov 30 03:21:03.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6290" for this suite. 11/30/22 03:21:03.551
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":7,"skipped":198,"failed":0}
------------------------------
• [SLOW TEST] [24.094 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:20:39.462
    Nov 30 03:20:39.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename subpath 11/30/22 03:20:39.463
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:20:39.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:20:39.482
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/30/22 03:20:39.484
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-mjcp 11/30/22 03:20:39.496
    STEP: Creating a pod to test atomic-volume-subpath 11/30/22 03:20:39.496
    Nov 30 03:20:39.506: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mjcp" in namespace "subpath-6290" to be "Succeeded or Failed"
    Nov 30 03:20:39.512: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.144568ms
    Nov 30 03:20:41.516: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.010291341s
    Nov 30 03:20:43.520: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 4.014011907s
    Nov 30 03:20:45.520: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 6.014639204s
    Nov 30 03:20:47.516: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 8.010689148s
    Nov 30 03:20:49.516: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 10.010010978s
    Nov 30 03:20:51.516: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 12.009870489s
    Nov 30 03:20:53.515: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 14.009224914s
    Nov 30 03:20:55.516: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 16.010445429s
    Nov 30 03:20:57.516: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 18.01019205s
    Nov 30 03:20:59.517: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=true. Elapsed: 20.011362242s
    Nov 30 03:21:01.515: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Running", Reason="", readiness=false. Elapsed: 22.009272964s
    Nov 30 03:21:03.517: INFO: Pod "pod-subpath-test-configmap-mjcp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011039589s
    STEP: Saw pod success 11/30/22 03:21:03.517
    Nov 30 03:21:03.517: INFO: Pod "pod-subpath-test-configmap-mjcp" satisfied condition "Succeeded or Failed"
    Nov 30 03:21:03.522: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-subpath-test-configmap-mjcp container test-container-subpath-configmap-mjcp: <nil>
    STEP: delete the pod 11/30/22 03:21:03.53
    Nov 30 03:21:03.544: INFO: Waiting for pod pod-subpath-test-configmap-mjcp to disappear
    Nov 30 03:21:03.546: INFO: Pod pod-subpath-test-configmap-mjcp no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-mjcp 11/30/22 03:21:03.546
    Nov 30 03:21:03.546: INFO: Deleting pod "pod-subpath-test-configmap-mjcp" in namespace "subpath-6290"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Nov 30 03:21:03.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6290" for this suite. 11/30/22 03:21:03.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:21:03.556
Nov 30 03:21:03.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pods 11/30/22 03:21:03.557
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:03.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:03.576
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Nov 30 03:21:03.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: creating the pod 11/30/22 03:21:03.579
STEP: submitting the pod to kubernetes 11/30/22 03:21:03.579
Nov 30 03:21:03.618: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-0e862e99-fd37-481e-9526-b072684eeed8" in namespace "pods-8644" to be "running and ready"
Nov 30 03:21:03.621: INFO: Pod "pod-logs-websocket-0e862e99-fd37-481e-9526-b072684eeed8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.050011ms
Nov 30 03:21:03.621: INFO: The phase of Pod pod-logs-websocket-0e862e99-fd37-481e-9526-b072684eeed8 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:21:05.638: INFO: Pod "pod-logs-websocket-0e862e99-fd37-481e-9526-b072684eeed8": Phase="Running", Reason="", readiness=true. Elapsed: 2.020300646s
Nov 30 03:21:05.638: INFO: The phase of Pod pod-logs-websocket-0e862e99-fd37-481e-9526-b072684eeed8 is Running (Ready = true)
Nov 30 03:21:05.638: INFO: Pod "pod-logs-websocket-0e862e99-fd37-481e-9526-b072684eeed8" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Nov 30 03:21:05.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8644" for this suite. 11/30/22 03:21:05.698
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":8,"skipped":203,"failed":0}
------------------------------
• [2.158 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:21:03.556
    Nov 30 03:21:03.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pods 11/30/22 03:21:03.557
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:03.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:03.576
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Nov 30 03:21:03.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: creating the pod 11/30/22 03:21:03.579
    STEP: submitting the pod to kubernetes 11/30/22 03:21:03.579
    Nov 30 03:21:03.618: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-0e862e99-fd37-481e-9526-b072684eeed8" in namespace "pods-8644" to be "running and ready"
    Nov 30 03:21:03.621: INFO: Pod "pod-logs-websocket-0e862e99-fd37-481e-9526-b072684eeed8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.050011ms
    Nov 30 03:21:03.621: INFO: The phase of Pod pod-logs-websocket-0e862e99-fd37-481e-9526-b072684eeed8 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:21:05.638: INFO: Pod "pod-logs-websocket-0e862e99-fd37-481e-9526-b072684eeed8": Phase="Running", Reason="", readiness=true. Elapsed: 2.020300646s
    Nov 30 03:21:05.638: INFO: The phase of Pod pod-logs-websocket-0e862e99-fd37-481e-9526-b072684eeed8 is Running (Ready = true)
    Nov 30 03:21:05.638: INFO: Pod "pod-logs-websocket-0e862e99-fd37-481e-9526-b072684eeed8" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Nov 30 03:21:05.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8644" for this suite. 11/30/22 03:21:05.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:21:05.715
Nov 30 03:21:05.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-runtime 11/30/22 03:21:05.715
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:05.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:05.74
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 11/30/22 03:21:05.742
STEP: wait for the container to reach Succeeded 11/30/22 03:21:05.755
STEP: get the container status 11/30/22 03:21:08.782
STEP: the container should be terminated 11/30/22 03:21:08.785
STEP: the termination message should be set 11/30/22 03:21:08.785
Nov 30 03:21:08.785: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 11/30/22 03:21:08.785
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Nov 30 03:21:08.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7270" for this suite. 11/30/22 03:21:08.803
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":9,"skipped":224,"failed":0}
------------------------------
• [3.096 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:21:05.715
    Nov 30 03:21:05.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-runtime 11/30/22 03:21:05.715
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:05.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:05.74
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 11/30/22 03:21:05.742
    STEP: wait for the container to reach Succeeded 11/30/22 03:21:05.755
    STEP: get the container status 11/30/22 03:21:08.782
    STEP: the container should be terminated 11/30/22 03:21:08.785
    STEP: the termination message should be set 11/30/22 03:21:08.785
    Nov 30 03:21:08.785: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 11/30/22 03:21:08.785
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Nov 30 03:21:08.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7270" for this suite. 11/30/22 03:21:08.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:21:08.811
Nov 30 03:21:08.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 03:21:08.812
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:08.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:08.833
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 11/30/22 03:21:08.835
Nov 30 03:21:08.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 11/30/22 03:21:19.042
Nov 30 03:21:19.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:21:21.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:21:31.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4395" for this suite. 11/30/22 03:21:31.544
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":10,"skipped":233,"failed":0}
------------------------------
• [SLOW TEST] [22.741 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:21:08.811
    Nov 30 03:21:08.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 03:21:08.812
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:08.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:08.833
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 11/30/22 03:21:08.835
    Nov 30 03:21:08.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 11/30/22 03:21:19.042
    Nov 30 03:21:19.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:21:21.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:21:31.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4395" for this suite. 11/30/22 03:21:31.544
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:21:31.553
Nov 30 03:21:31.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:21:31.554
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:31.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:31.572
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 11/30/22 03:21:31.574
Nov 30 03:21:31.609: INFO: Waiting up to 5m0s for pod "labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34" in namespace "projected-3787" to be "running and ready"
Nov 30 03:21:31.612: INFO: Pod "labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.55323ms
Nov 30 03:21:31.612: INFO: The phase of Pod labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:21:33.615: INFO: Pod "labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005674629s
Nov 30 03:21:33.615: INFO: The phase of Pod labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:21:35.619: INFO: Pod "labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34": Phase="Running", Reason="", readiness=true. Elapsed: 4.010356014s
Nov 30 03:21:35.619: INFO: The phase of Pod labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34 is Running (Ready = true)
Nov 30 03:21:35.619: INFO: Pod "labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34" satisfied condition "running and ready"
Nov 30 03:21:36.180: INFO: Successfully updated pod "labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Nov 30 03:21:40.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3787" for this suite. 11/30/22 03:21:40.204
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":11,"skipped":235,"failed":0}
------------------------------
• [SLOW TEST] [8.657 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:21:31.553
    Nov 30 03:21:31.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:21:31.554
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:31.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:31.572
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 11/30/22 03:21:31.574
    Nov 30 03:21:31.609: INFO: Waiting up to 5m0s for pod "labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34" in namespace "projected-3787" to be "running and ready"
    Nov 30 03:21:31.612: INFO: Pod "labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.55323ms
    Nov 30 03:21:31.612: INFO: The phase of Pod labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:21:33.615: INFO: Pod "labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005674629s
    Nov 30 03:21:33.615: INFO: The phase of Pod labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:21:35.619: INFO: Pod "labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34": Phase="Running", Reason="", readiness=true. Elapsed: 4.010356014s
    Nov 30 03:21:35.619: INFO: The phase of Pod labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34 is Running (Ready = true)
    Nov 30 03:21:35.619: INFO: Pod "labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34" satisfied condition "running and ready"
    Nov 30 03:21:36.180: INFO: Successfully updated pod "labelsupdatef1db8d90-b1e4-4ef3-9bf6-05b8ac112a34"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Nov 30 03:21:40.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3787" for this suite. 11/30/22 03:21:40.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:21:40.211
Nov 30 03:21:40.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename resourcequota 11/30/22 03:21:40.211
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:40.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:40.231
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 11/30/22 03:21:40.233
STEP: Getting a ResourceQuota 11/30/22 03:21:40.238
STEP: Updating a ResourceQuota 11/30/22 03:21:40.241
STEP: Verifying a ResourceQuota was modified 11/30/22 03:21:40.245
STEP: Deleting a ResourceQuota 11/30/22 03:21:40.247
STEP: Verifying the deleted ResourceQuota 11/30/22 03:21:40.251
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Nov 30 03:21:40.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7677" for this suite. 11/30/22 03:21:40.256
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":12,"skipped":242,"failed":0}
------------------------------
• [0.051 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:21:40.211
    Nov 30 03:21:40.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename resourcequota 11/30/22 03:21:40.211
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:40.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:40.231
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 11/30/22 03:21:40.233
    STEP: Getting a ResourceQuota 11/30/22 03:21:40.238
    STEP: Updating a ResourceQuota 11/30/22 03:21:40.241
    STEP: Verifying a ResourceQuota was modified 11/30/22 03:21:40.245
    STEP: Deleting a ResourceQuota 11/30/22 03:21:40.247
    STEP: Verifying the deleted ResourceQuota 11/30/22 03:21:40.251
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Nov 30 03:21:40.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7677" for this suite. 11/30/22 03:21:40.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:21:40.263
Nov 30 03:21:40.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename resourcequota 11/30/22 03:21:40.264
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:40.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:40.284
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 11/30/22 03:21:40.286
STEP: Ensuring ResourceQuota status is calculated 11/30/22 03:21:40.292
STEP: Creating a ResourceQuota with not best effort scope 11/30/22 03:21:42.298
STEP: Ensuring ResourceQuota status is calculated 11/30/22 03:21:42.305
STEP: Creating a best-effort pod 11/30/22 03:21:44.309
STEP: Ensuring resource quota with best effort scope captures the pod usage 11/30/22 03:21:44.348
STEP: Ensuring resource quota with not best effort ignored the pod usage 11/30/22 03:21:46.352
STEP: Deleting the pod 11/30/22 03:21:48.362
STEP: Ensuring resource quota status released the pod usage 11/30/22 03:21:48.375
STEP: Creating a not best-effort pod 11/30/22 03:21:50.379
STEP: Ensuring resource quota with not best effort scope captures the pod usage 11/30/22 03:21:50.427
STEP: Ensuring resource quota with best effort scope ignored the pod usage 11/30/22 03:21:52.431
STEP: Deleting the pod 11/30/22 03:21:54.435
STEP: Ensuring resource quota status released the pod usage 11/30/22 03:21:54.454
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Nov 30 03:21:56.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1508" for this suite. 11/30/22 03:21:56.462
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":13,"skipped":263,"failed":0}
------------------------------
• [SLOW TEST] [16.207 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:21:40.263
    Nov 30 03:21:40.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename resourcequota 11/30/22 03:21:40.264
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:40.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:40.284
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 11/30/22 03:21:40.286
    STEP: Ensuring ResourceQuota status is calculated 11/30/22 03:21:40.292
    STEP: Creating a ResourceQuota with not best effort scope 11/30/22 03:21:42.298
    STEP: Ensuring ResourceQuota status is calculated 11/30/22 03:21:42.305
    STEP: Creating a best-effort pod 11/30/22 03:21:44.309
    STEP: Ensuring resource quota with best effort scope captures the pod usage 11/30/22 03:21:44.348
    STEP: Ensuring resource quota with not best effort ignored the pod usage 11/30/22 03:21:46.352
    STEP: Deleting the pod 11/30/22 03:21:48.362
    STEP: Ensuring resource quota status released the pod usage 11/30/22 03:21:48.375
    STEP: Creating a not best-effort pod 11/30/22 03:21:50.379
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 11/30/22 03:21:50.427
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 11/30/22 03:21:52.431
    STEP: Deleting the pod 11/30/22 03:21:54.435
    STEP: Ensuring resource quota status released the pod usage 11/30/22 03:21:54.454
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Nov 30 03:21:56.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1508" for this suite. 11/30/22 03:21:56.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:21:56.471
Nov 30 03:21:56.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename replication-controller 11/30/22 03:21:56.472
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:56.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:56.494
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Nov 30 03:21:56.496: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 11/30/22 03:21:57.509
STEP: Checking rc "condition-test" has the desired failure condition set 11/30/22 03:21:57.518
STEP: Scaling down rc "condition-test" to satisfy pod quota 11/30/22 03:21:58.524
Nov 30 03:21:58.533: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 11/30/22 03:21:58.533
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Nov 30 03:21:59.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8796" for this suite. 11/30/22 03:21:59.543
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":14,"skipped":273,"failed":0}
------------------------------
• [3.076 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:21:56.471
    Nov 30 03:21:56.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename replication-controller 11/30/22 03:21:56.472
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:56.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:56.494
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Nov 30 03:21:56.496: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 11/30/22 03:21:57.509
    STEP: Checking rc "condition-test" has the desired failure condition set 11/30/22 03:21:57.518
    STEP: Scaling down rc "condition-test" to satisfy pod quota 11/30/22 03:21:58.524
    Nov 30 03:21:58.533: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 11/30/22 03:21:58.533
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Nov 30 03:21:59.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8796" for this suite. 11/30/22 03:21:59.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:21:59.548
Nov 30 03:21:59.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename proxy 11/30/22 03:21:59.549
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:59.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:59.569
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Nov 30 03:21:59.571: INFO: Creating pod...
Nov 30 03:21:59.607: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5586" to be "running"
Nov 30 03:21:59.610: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.445335ms
Nov 30 03:22:01.614: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.006511643s
Nov 30 03:22:01.614: INFO: Pod "agnhost" satisfied condition "running"
Nov 30 03:22:01.614: INFO: Creating service...
Nov 30 03:22:01.637: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=DELETE
Nov 30 03:22:01.643: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 30 03:22:01.643: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=OPTIONS
Nov 30 03:22:01.646: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 30 03:22:01.646: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=PATCH
Nov 30 03:22:01.654: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 30 03:22:01.654: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=POST
Nov 30 03:22:01.660: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 30 03:22:01.660: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=PUT
Nov 30 03:22:01.664: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 30 03:22:01.664: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=DELETE
Nov 30 03:22:01.671: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 30 03:22:01.671: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=OPTIONS
Nov 30 03:22:01.676: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 30 03:22:01.676: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=PATCH
Nov 30 03:22:01.685: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 30 03:22:01.685: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=POST
Nov 30 03:22:01.700: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 30 03:22:01.700: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=PUT
Nov 30 03:22:01.705: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 30 03:22:01.705: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=GET
Nov 30 03:22:01.708: INFO: http.Client request:GET StatusCode:301
Nov 30 03:22:01.708: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=GET
Nov 30 03:22:01.713: INFO: http.Client request:GET StatusCode:301
Nov 30 03:22:01.713: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=HEAD
Nov 30 03:22:01.716: INFO: http.Client request:HEAD StatusCode:301
Nov 30 03:22:01.716: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=HEAD
Nov 30 03:22:01.723: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Nov 30 03:22:01.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5586" for this suite. 11/30/22 03:22:01.727
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":15,"skipped":278,"failed":0}
------------------------------
• [2.186 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:21:59.548
    Nov 30 03:21:59.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename proxy 11/30/22 03:21:59.549
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:21:59.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:21:59.569
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Nov 30 03:21:59.571: INFO: Creating pod...
    Nov 30 03:21:59.607: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5586" to be "running"
    Nov 30 03:21:59.610: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.445335ms
    Nov 30 03:22:01.614: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.006511643s
    Nov 30 03:22:01.614: INFO: Pod "agnhost" satisfied condition "running"
    Nov 30 03:22:01.614: INFO: Creating service...
    Nov 30 03:22:01.637: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=DELETE
    Nov 30 03:22:01.643: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 30 03:22:01.643: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=OPTIONS
    Nov 30 03:22:01.646: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 30 03:22:01.646: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=PATCH
    Nov 30 03:22:01.654: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 30 03:22:01.654: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=POST
    Nov 30 03:22:01.660: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 30 03:22:01.660: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=PUT
    Nov 30 03:22:01.664: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Nov 30 03:22:01.664: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=DELETE
    Nov 30 03:22:01.671: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 30 03:22:01.671: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Nov 30 03:22:01.676: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 30 03:22:01.676: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=PATCH
    Nov 30 03:22:01.685: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 30 03:22:01.685: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=POST
    Nov 30 03:22:01.700: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 30 03:22:01.700: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=PUT
    Nov 30 03:22:01.705: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Nov 30 03:22:01.705: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=GET
    Nov 30 03:22:01.708: INFO: http.Client request:GET StatusCode:301
    Nov 30 03:22:01.708: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=GET
    Nov 30 03:22:01.713: INFO: http.Client request:GET StatusCode:301
    Nov 30 03:22:01.713: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/pods/agnhost/proxy?method=HEAD
    Nov 30 03:22:01.716: INFO: http.Client request:HEAD StatusCode:301
    Nov 30 03:22:01.716: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5586/services/e2e-proxy-test-service/proxy?method=HEAD
    Nov 30 03:22:01.723: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Nov 30 03:22:01.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5586" for this suite. 11/30/22 03:22:01.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:22:01.735
Nov 30 03:22:01.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 03:22:01.736
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:22:01.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:22:01.768
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 11/30/22 03:22:01.771
Nov 30 03:22:01.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 create -f -'
Nov 30 03:22:02.254: INFO: stderr: ""
Nov 30 03:22:02.254: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/30/22 03:22:02.254
Nov 30 03:22:02.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 30 03:22:02.337: INFO: stderr: ""
Nov 30 03:22:02.337: INFO: stdout: "update-demo-nautilus-dsxx4 update-demo-nautilus-gbl6d "
Nov 30 03:22:02.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-dsxx4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 30 03:22:02.412: INFO: stderr: ""
Nov 30 03:22:02.412: INFO: stdout: ""
Nov 30 03:22:02.412: INFO: update-demo-nautilus-dsxx4 is created but not running
Nov 30 03:22:07.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 30 03:22:07.478: INFO: stderr: ""
Nov 30 03:22:07.478: INFO: stdout: "update-demo-nautilus-dsxx4 update-demo-nautilus-gbl6d "
Nov 30 03:22:07.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-dsxx4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 30 03:22:07.540: INFO: stderr: ""
Nov 30 03:22:07.540: INFO: stdout: "true"
Nov 30 03:22:07.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-dsxx4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 30 03:22:07.599: INFO: stderr: ""
Nov 30 03:22:07.599: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
Nov 30 03:22:07.599: INFO: validating pod update-demo-nautilus-dsxx4
Nov 30 03:22:07.607: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 30 03:22:07.607: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 30 03:22:07.607: INFO: update-demo-nautilus-dsxx4 is verified up and running
Nov 30 03:22:07.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-gbl6d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 30 03:22:07.695: INFO: stderr: ""
Nov 30 03:22:07.695: INFO: stdout: "true"
Nov 30 03:22:07.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-gbl6d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 30 03:22:07.791: INFO: stderr: ""
Nov 30 03:22:07.791: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
Nov 30 03:22:07.791: INFO: validating pod update-demo-nautilus-gbl6d
Nov 30 03:22:07.795: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 30 03:22:07.795: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 30 03:22:07.795: INFO: update-demo-nautilus-gbl6d is verified up and running
STEP: scaling down the replication controller 11/30/22 03:22:07.795
Nov 30 03:22:07.795: INFO: scanned /root for discovery docs: <nil>
Nov 30 03:22:07.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Nov 30 03:22:08.876: INFO: stderr: ""
Nov 30 03:22:08.876: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/30/22 03:22:08.876
Nov 30 03:22:08.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 30 03:22:08.934: INFO: stderr: ""
Nov 30 03:22:08.934: INFO: stdout: "update-demo-nautilus-dsxx4 update-demo-nautilus-gbl6d "
STEP: Replicas for name=update-demo: expected=1 actual=2 11/30/22 03:22:08.934
Nov 30 03:22:13.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 30 03:22:13.992: INFO: stderr: ""
Nov 30 03:22:13.992: INFO: stdout: "update-demo-nautilus-gbl6d "
Nov 30 03:22:13.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-gbl6d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 30 03:22:14.053: INFO: stderr: ""
Nov 30 03:22:14.053: INFO: stdout: "true"
Nov 30 03:22:14.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-gbl6d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 30 03:22:14.113: INFO: stderr: ""
Nov 30 03:22:14.113: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
Nov 30 03:22:14.113: INFO: validating pod update-demo-nautilus-gbl6d
Nov 30 03:22:14.116: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 30 03:22:14.116: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 30 03:22:14.116: INFO: update-demo-nautilus-gbl6d is verified up and running
STEP: scaling up the replication controller 11/30/22 03:22:14.116
Nov 30 03:22:14.118: INFO: scanned /root for discovery docs: <nil>
Nov 30 03:22:14.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Nov 30 03:22:15.210: INFO: stderr: ""
Nov 30 03:22:15.210: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/30/22 03:22:15.21
Nov 30 03:22:15.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 30 03:22:15.275: INFO: stderr: ""
Nov 30 03:22:15.275: INFO: stdout: "update-demo-nautilus-9f2l7 update-demo-nautilus-gbl6d "
Nov 30 03:22:15.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-9f2l7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 30 03:22:15.337: INFO: stderr: ""
Nov 30 03:22:15.337: INFO: stdout: "true"
Nov 30 03:22:15.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-9f2l7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 30 03:22:15.396: INFO: stderr: ""
Nov 30 03:22:15.396: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
Nov 30 03:22:15.396: INFO: validating pod update-demo-nautilus-9f2l7
Nov 30 03:22:15.400: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 30 03:22:15.400: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 30 03:22:15.400: INFO: update-demo-nautilus-9f2l7 is verified up and running
Nov 30 03:22:15.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-gbl6d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 30 03:22:15.457: INFO: stderr: ""
Nov 30 03:22:15.457: INFO: stdout: "true"
Nov 30 03:22:15.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-gbl6d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 30 03:22:15.523: INFO: stderr: ""
Nov 30 03:22:15.523: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
Nov 30 03:22:15.523: INFO: validating pod update-demo-nautilus-gbl6d
Nov 30 03:22:15.530: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 30 03:22:15.530: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 30 03:22:15.530: INFO: update-demo-nautilus-gbl6d is verified up and running
STEP: using delete to clean up resources 11/30/22 03:22:15.53
Nov 30 03:22:15.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 delete --grace-period=0 --force -f -'
Nov 30 03:22:15.632: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 30 03:22:15.632: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 30 03:22:15.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get rc,svc -l name=update-demo --no-headers'
Nov 30 03:22:15.695: INFO: stderr: "No resources found in kubectl-4015 namespace.\n"
Nov 30 03:22:15.695: INFO: stdout: ""
Nov 30 03:22:15.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 30 03:22:15.762: INFO: stderr: ""
Nov 30 03:22:15.762: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 03:22:15.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4015" for this suite. 11/30/22 03:22:15.767
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":16,"skipped":285,"failed":0}
------------------------------
• [SLOW TEST] [14.037 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:22:01.735
    Nov 30 03:22:01.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 03:22:01.736
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:22:01.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:22:01.768
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 11/30/22 03:22:01.771
    Nov 30 03:22:01.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 create -f -'
    Nov 30 03:22:02.254: INFO: stderr: ""
    Nov 30 03:22:02.254: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/30/22 03:22:02.254
    Nov 30 03:22:02.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 30 03:22:02.337: INFO: stderr: ""
    Nov 30 03:22:02.337: INFO: stdout: "update-demo-nautilus-dsxx4 update-demo-nautilus-gbl6d "
    Nov 30 03:22:02.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-dsxx4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 30 03:22:02.412: INFO: stderr: ""
    Nov 30 03:22:02.412: INFO: stdout: ""
    Nov 30 03:22:02.412: INFO: update-demo-nautilus-dsxx4 is created but not running
    Nov 30 03:22:07.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 30 03:22:07.478: INFO: stderr: ""
    Nov 30 03:22:07.478: INFO: stdout: "update-demo-nautilus-dsxx4 update-demo-nautilus-gbl6d "
    Nov 30 03:22:07.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-dsxx4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 30 03:22:07.540: INFO: stderr: ""
    Nov 30 03:22:07.540: INFO: stdout: "true"
    Nov 30 03:22:07.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-dsxx4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 30 03:22:07.599: INFO: stderr: ""
    Nov 30 03:22:07.599: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
    Nov 30 03:22:07.599: INFO: validating pod update-demo-nautilus-dsxx4
    Nov 30 03:22:07.607: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 30 03:22:07.607: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 30 03:22:07.607: INFO: update-demo-nautilus-dsxx4 is verified up and running
    Nov 30 03:22:07.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-gbl6d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 30 03:22:07.695: INFO: stderr: ""
    Nov 30 03:22:07.695: INFO: stdout: "true"
    Nov 30 03:22:07.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-gbl6d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 30 03:22:07.791: INFO: stderr: ""
    Nov 30 03:22:07.791: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
    Nov 30 03:22:07.791: INFO: validating pod update-demo-nautilus-gbl6d
    Nov 30 03:22:07.795: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 30 03:22:07.795: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 30 03:22:07.795: INFO: update-demo-nautilus-gbl6d is verified up and running
    STEP: scaling down the replication controller 11/30/22 03:22:07.795
    Nov 30 03:22:07.795: INFO: scanned /root for discovery docs: <nil>
    Nov 30 03:22:07.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Nov 30 03:22:08.876: INFO: stderr: ""
    Nov 30 03:22:08.876: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/30/22 03:22:08.876
    Nov 30 03:22:08.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 30 03:22:08.934: INFO: stderr: ""
    Nov 30 03:22:08.934: INFO: stdout: "update-demo-nautilus-dsxx4 update-demo-nautilus-gbl6d "
    STEP: Replicas for name=update-demo: expected=1 actual=2 11/30/22 03:22:08.934
    Nov 30 03:22:13.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 30 03:22:13.992: INFO: stderr: ""
    Nov 30 03:22:13.992: INFO: stdout: "update-demo-nautilus-gbl6d "
    Nov 30 03:22:13.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-gbl6d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 30 03:22:14.053: INFO: stderr: ""
    Nov 30 03:22:14.053: INFO: stdout: "true"
    Nov 30 03:22:14.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-gbl6d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 30 03:22:14.113: INFO: stderr: ""
    Nov 30 03:22:14.113: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
    Nov 30 03:22:14.113: INFO: validating pod update-demo-nautilus-gbl6d
    Nov 30 03:22:14.116: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 30 03:22:14.116: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 30 03:22:14.116: INFO: update-demo-nautilus-gbl6d is verified up and running
    STEP: scaling up the replication controller 11/30/22 03:22:14.116
    Nov 30 03:22:14.118: INFO: scanned /root for discovery docs: <nil>
    Nov 30 03:22:14.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Nov 30 03:22:15.210: INFO: stderr: ""
    Nov 30 03:22:15.210: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/30/22 03:22:15.21
    Nov 30 03:22:15.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 30 03:22:15.275: INFO: stderr: ""
    Nov 30 03:22:15.275: INFO: stdout: "update-demo-nautilus-9f2l7 update-demo-nautilus-gbl6d "
    Nov 30 03:22:15.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-9f2l7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 30 03:22:15.337: INFO: stderr: ""
    Nov 30 03:22:15.337: INFO: stdout: "true"
    Nov 30 03:22:15.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-9f2l7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 30 03:22:15.396: INFO: stderr: ""
    Nov 30 03:22:15.396: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
    Nov 30 03:22:15.396: INFO: validating pod update-demo-nautilus-9f2l7
    Nov 30 03:22:15.400: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 30 03:22:15.400: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 30 03:22:15.400: INFO: update-demo-nautilus-9f2l7 is verified up and running
    Nov 30 03:22:15.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-gbl6d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 30 03:22:15.457: INFO: stderr: ""
    Nov 30 03:22:15.457: INFO: stdout: "true"
    Nov 30 03:22:15.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods update-demo-nautilus-gbl6d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 30 03:22:15.523: INFO: stderr: ""
    Nov 30 03:22:15.523: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
    Nov 30 03:22:15.523: INFO: validating pod update-demo-nautilus-gbl6d
    Nov 30 03:22:15.530: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 30 03:22:15.530: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 30 03:22:15.530: INFO: update-demo-nautilus-gbl6d is verified up and running
    STEP: using delete to clean up resources 11/30/22 03:22:15.53
    Nov 30 03:22:15.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 delete --grace-period=0 --force -f -'
    Nov 30 03:22:15.632: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 30 03:22:15.632: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Nov 30 03:22:15.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get rc,svc -l name=update-demo --no-headers'
    Nov 30 03:22:15.695: INFO: stderr: "No resources found in kubectl-4015 namespace.\n"
    Nov 30 03:22:15.695: INFO: stdout: ""
    Nov 30 03:22:15.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4015 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Nov 30 03:22:15.762: INFO: stderr: ""
    Nov 30 03:22:15.762: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 03:22:15.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4015" for this suite. 11/30/22 03:22:15.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:22:15.773
Nov 30 03:22:15.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename crd-watch 11/30/22 03:22:15.774
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:22:15.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:22:15.796
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Nov 30 03:22:15.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Creating first CR  11/30/22 03:22:18.368
Nov 30 03:22:18.381: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-30T03:22:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-30T03:22:18Z]] name:name1 resourceVersion:16569 uid:2063aba0-6e25-49af-a28d-dd0664aada76] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 11/30/22 03:22:28.382
Nov 30 03:22:28.387: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-30T03:22:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-30T03:22:28Z]] name:name2 resourceVersion:16645 uid:5022e427-7528-42a7-8377-503115d330f9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 11/30/22 03:22:38.387
Nov 30 03:22:38.394: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-30T03:22:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-30T03:22:38Z]] name:name1 resourceVersion:16689 uid:2063aba0-6e25-49af-a28d-dd0664aada76] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 11/30/22 03:22:48.395
Nov 30 03:22:48.400: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-30T03:22:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-30T03:22:48Z]] name:name2 resourceVersion:16731 uid:5022e427-7528-42a7-8377-503115d330f9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 11/30/22 03:22:58.401
Nov 30 03:22:58.408: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-30T03:22:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-30T03:22:38Z]] name:name1 resourceVersion:16776 uid:2063aba0-6e25-49af-a28d-dd0664aada76] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 11/30/22 03:23:08.409
Nov 30 03:23:08.417: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-30T03:22:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-30T03:22:48Z]] name:name2 resourceVersion:16819 uid:5022e427-7528-42a7-8377-503115d330f9] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:23:18.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7444" for this suite. 11/30/22 03:23:18.934
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":17,"skipped":322,"failed":0}
------------------------------
• [SLOW TEST] [63.167 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:22:15.773
    Nov 30 03:22:15.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename crd-watch 11/30/22 03:22:15.774
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:22:15.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:22:15.796
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Nov 30 03:22:15.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Creating first CR  11/30/22 03:22:18.368
    Nov 30 03:22:18.381: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-30T03:22:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-30T03:22:18Z]] name:name1 resourceVersion:16569 uid:2063aba0-6e25-49af-a28d-dd0664aada76] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 11/30/22 03:22:28.382
    Nov 30 03:22:28.387: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-30T03:22:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-30T03:22:28Z]] name:name2 resourceVersion:16645 uid:5022e427-7528-42a7-8377-503115d330f9] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 11/30/22 03:22:38.387
    Nov 30 03:22:38.394: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-30T03:22:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-30T03:22:38Z]] name:name1 resourceVersion:16689 uid:2063aba0-6e25-49af-a28d-dd0664aada76] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 11/30/22 03:22:48.395
    Nov 30 03:22:48.400: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-30T03:22:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-30T03:22:48Z]] name:name2 resourceVersion:16731 uid:5022e427-7528-42a7-8377-503115d330f9] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 11/30/22 03:22:58.401
    Nov 30 03:22:58.408: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-30T03:22:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-30T03:22:38Z]] name:name1 resourceVersion:16776 uid:2063aba0-6e25-49af-a28d-dd0664aada76] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 11/30/22 03:23:08.409
    Nov 30 03:23:08.417: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-11-30T03:22:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-11-30T03:22:48Z]] name:name2 resourceVersion:16819 uid:5022e427-7528-42a7-8377-503115d330f9] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:23:18.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-7444" for this suite. 11/30/22 03:23:18.934
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:23:18.94
Nov 30 03:23:18.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:23:18.941
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:18.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:18.96
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-94a0259e-3af0-405e-bd06-52dd1d1bad68 11/30/22 03:23:18.963
STEP: Creating a pod to test consume configMaps 11/30/22 03:23:18.968
Nov 30 03:23:18.996: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9" in namespace "projected-5129" to be "Succeeded or Failed"
Nov 30 03:23:19.000: INFO: Pod "pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.900536ms
Nov 30 03:23:21.003: INFO: Pod "pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007339435s
Nov 30 03:23:23.007: INFO: Pod "pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011068195s
STEP: Saw pod success 11/30/22 03:23:23.007
Nov 30 03:23:23.007: INFO: Pod "pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9" satisfied condition "Succeeded or Failed"
Nov 30 03:23:23.011: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9 container agnhost-container: <nil>
STEP: delete the pod 11/30/22 03:23:23.022
Nov 30 03:23:23.033: INFO: Waiting for pod pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9 to disappear
Nov 30 03:23:23.036: INFO: Pod pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Nov 30 03:23:23.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5129" for this suite. 11/30/22 03:23:23.04
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":18,"skipped":325,"failed":0}
------------------------------
• [4.105 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:23:18.94
    Nov 30 03:23:18.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:23:18.941
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:18.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:18.96
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-94a0259e-3af0-405e-bd06-52dd1d1bad68 11/30/22 03:23:18.963
    STEP: Creating a pod to test consume configMaps 11/30/22 03:23:18.968
    Nov 30 03:23:18.996: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9" in namespace "projected-5129" to be "Succeeded or Failed"
    Nov 30 03:23:19.000: INFO: Pod "pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.900536ms
    Nov 30 03:23:21.003: INFO: Pod "pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007339435s
    Nov 30 03:23:23.007: INFO: Pod "pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011068195s
    STEP: Saw pod success 11/30/22 03:23:23.007
    Nov 30 03:23:23.007: INFO: Pod "pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9" satisfied condition "Succeeded or Failed"
    Nov 30 03:23:23.011: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9 container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 03:23:23.022
    Nov 30 03:23:23.033: INFO: Waiting for pod pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9 to disappear
    Nov 30 03:23:23.036: INFO: Pod pod-projected-configmaps-f209a96f-9e09-46b3-9601-3aa1a9f260e9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Nov 30 03:23:23.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5129" for this suite. 11/30/22 03:23:23.04
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:23:23.045
Nov 30 03:23:23.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 03:23:23.046
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:23.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:23.067
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4835 11/30/22 03:23:23.069
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/30/22 03:23:23.086
STEP: creating service externalsvc in namespace services-4835 11/30/22 03:23:23.086
STEP: creating replication controller externalsvc in namespace services-4835 11/30/22 03:23:23.105
I1130 03:23:23.112454      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4835, replica count: 2
I1130 03:23:26.163276      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 11/30/22 03:23:26.165
Nov 30 03:23:26.195: INFO: Creating new exec pod
Nov 30 03:23:26.227: INFO: Waiting up to 5m0s for pod "execpodqgst5" in namespace "services-4835" to be "running"
Nov 30 03:23:26.234: INFO: Pod "execpodqgst5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.883005ms
Nov 30 03:23:28.237: INFO: Pod "execpodqgst5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009787143s
Nov 30 03:23:28.237: INFO: Pod "execpodqgst5" satisfied condition "running"
Nov 30 03:23:28.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4835 exec execpodqgst5 -- /bin/sh -x -c nslookup nodeport-service.services-4835.svc.cluster.local'
Nov 30 03:23:28.395: INFO: stderr: "+ nslookup nodeport-service.services-4835.svc.cluster.local\n"
Nov 30 03:23:28.395: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-4835.svc.cluster.local\tcanonical name = externalsvc.services-4835.svc.cluster.local.\nName:\texternalsvc.services-4835.svc.cluster.local\nAddress: 10.97.250.0\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4835, will wait for the garbage collector to delete the pods 11/30/22 03:23:28.395
Nov 30 03:23:28.458: INFO: Deleting ReplicationController externalsvc took: 9.931361ms
Nov 30 03:23:28.558: INFO: Terminating ReplicationController externalsvc pods took: 100.258146ms
Nov 30 03:23:30.479: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 03:23:30.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4835" for this suite. 11/30/22 03:23:30.522
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":19,"skipped":328,"failed":0}
------------------------------
• [SLOW TEST] [7.534 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:23:23.045
    Nov 30 03:23:23.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 03:23:23.046
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:23.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:23.067
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-4835 11/30/22 03:23:23.069
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/30/22 03:23:23.086
    STEP: creating service externalsvc in namespace services-4835 11/30/22 03:23:23.086
    STEP: creating replication controller externalsvc in namespace services-4835 11/30/22 03:23:23.105
    I1130 03:23:23.112454      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4835, replica count: 2
    I1130 03:23:26.163276      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 11/30/22 03:23:26.165
    Nov 30 03:23:26.195: INFO: Creating new exec pod
    Nov 30 03:23:26.227: INFO: Waiting up to 5m0s for pod "execpodqgst5" in namespace "services-4835" to be "running"
    Nov 30 03:23:26.234: INFO: Pod "execpodqgst5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.883005ms
    Nov 30 03:23:28.237: INFO: Pod "execpodqgst5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009787143s
    Nov 30 03:23:28.237: INFO: Pod "execpodqgst5" satisfied condition "running"
    Nov 30 03:23:28.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4835 exec execpodqgst5 -- /bin/sh -x -c nslookup nodeport-service.services-4835.svc.cluster.local'
    Nov 30 03:23:28.395: INFO: stderr: "+ nslookup nodeport-service.services-4835.svc.cluster.local\n"
    Nov 30 03:23:28.395: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-4835.svc.cluster.local\tcanonical name = externalsvc.services-4835.svc.cluster.local.\nName:\texternalsvc.services-4835.svc.cluster.local\nAddress: 10.97.250.0\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-4835, will wait for the garbage collector to delete the pods 11/30/22 03:23:28.395
    Nov 30 03:23:28.458: INFO: Deleting ReplicationController externalsvc took: 9.931361ms
    Nov 30 03:23:28.558: INFO: Terminating ReplicationController externalsvc pods took: 100.258146ms
    Nov 30 03:23:30.479: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 03:23:30.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4835" for this suite. 11/30/22 03:23:30.522
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:23:30.58
Nov 30 03:23:30.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 03:23:30.581
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:30.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:30.609
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 11/30/22 03:23:30.611
Nov 30 03:23:30.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-5649 create -f -'
Nov 30 03:23:31.255: INFO: stderr: ""
Nov 30 03:23:31.255: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 11/30/22 03:23:31.255
Nov 30 03:23:31.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-5649 diff -f -'
Nov 30 03:23:31.962: INFO: rc: 1
Nov 30 03:23:31.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-5649 delete -f -'
Nov 30 03:23:32.023: INFO: stderr: ""
Nov 30 03:23:32.023: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 03:23:32.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5649" for this suite. 11/30/22 03:23:32.027
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":20,"skipped":340,"failed":0}
------------------------------
• [1.455 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:23:30.58
    Nov 30 03:23:30.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 03:23:30.581
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:30.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:30.609
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 11/30/22 03:23:30.611
    Nov 30 03:23:30.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-5649 create -f -'
    Nov 30 03:23:31.255: INFO: stderr: ""
    Nov 30 03:23:31.255: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 11/30/22 03:23:31.255
    Nov 30 03:23:31.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-5649 diff -f -'
    Nov 30 03:23:31.962: INFO: rc: 1
    Nov 30 03:23:31.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-5649 delete -f -'
    Nov 30 03:23:32.023: INFO: stderr: ""
    Nov 30 03:23:32.023: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 03:23:32.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5649" for this suite. 11/30/22 03:23:32.027
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:23:32.036
Nov 30 03:23:32.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename runtimeclass 11/30/22 03:23:32.036
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:32.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:32.053
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Nov 30 03:23:32.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5223" for this suite. 11/30/22 03:23:32.069
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":21,"skipped":343,"failed":0}
------------------------------
• [0.038 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:23:32.036
    Nov 30 03:23:32.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename runtimeclass 11/30/22 03:23:32.036
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:32.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:32.053
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Nov 30 03:23:32.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5223" for this suite. 11/30/22 03:23:32.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:23:32.074
Nov 30 03:23:32.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 03:23:32.075
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:32.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:32.092
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-e517492e-5d7f-4335-a165-3aea1457c324 11/30/22 03:23:32.094
STEP: Creating a pod to test consume secrets 11/30/22 03:23:32.099
Nov 30 03:23:32.127: INFO: Waiting up to 5m0s for pod "pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab" in namespace "secrets-6753" to be "Succeeded or Failed"
Nov 30 03:23:32.131: INFO: Pod "pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.206776ms
Nov 30 03:23:34.134: INFO: Pod "pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab": Phase="Running", Reason="", readiness=false. Elapsed: 2.006681186s
Nov 30 03:23:36.136: INFO: Pod "pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008410688s
STEP: Saw pod success 11/30/22 03:23:36.136
Nov 30 03:23:36.136: INFO: Pod "pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab" satisfied condition "Succeeded or Failed"
Nov 30 03:23:36.138: INFO: Trying to get logs from node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins pod pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab container secret-volume-test: <nil>
STEP: delete the pod 11/30/22 03:23:36.147
Nov 30 03:23:36.161: INFO: Waiting for pod pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab to disappear
Nov 30 03:23:36.163: INFO: Pod pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Nov 30 03:23:36.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6753" for this suite. 11/30/22 03:23:36.167
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":22,"skipped":350,"failed":0}
------------------------------
• [4.098 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:23:32.074
    Nov 30 03:23:32.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 03:23:32.075
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:32.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:32.092
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-e517492e-5d7f-4335-a165-3aea1457c324 11/30/22 03:23:32.094
    STEP: Creating a pod to test consume secrets 11/30/22 03:23:32.099
    Nov 30 03:23:32.127: INFO: Waiting up to 5m0s for pod "pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab" in namespace "secrets-6753" to be "Succeeded or Failed"
    Nov 30 03:23:32.131: INFO: Pod "pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.206776ms
    Nov 30 03:23:34.134: INFO: Pod "pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab": Phase="Running", Reason="", readiness=false. Elapsed: 2.006681186s
    Nov 30 03:23:36.136: INFO: Pod "pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008410688s
    STEP: Saw pod success 11/30/22 03:23:36.136
    Nov 30 03:23:36.136: INFO: Pod "pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab" satisfied condition "Succeeded or Failed"
    Nov 30 03:23:36.138: INFO: Trying to get logs from node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins pod pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab container secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 03:23:36.147
    Nov 30 03:23:36.161: INFO: Waiting for pod pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab to disappear
    Nov 30 03:23:36.163: INFO: Pod pod-secrets-cc32d97a-2300-4989-8c49-89f06dd85eab no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 03:23:36.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6753" for this suite. 11/30/22 03:23:36.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:23:36.173
Nov 30 03:23:36.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 03:23:36.174
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:36.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:36.193
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 03:23:36.287
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 03:23:36.595
STEP: Deploying the webhook pod 11/30/22 03:23:36.607
STEP: Wait for the deployment to be ready 11/30/22 03:23:36.62
Nov 30 03:23:36.627: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 03:23:38.637
STEP: Verifying the service has paired with the endpoint 11/30/22 03:23:38.649
Nov 30 03:23:39.649: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 11/30/22 03:23:39.652
STEP: create a pod that should be denied by the webhook 11/30/22 03:23:39.668
STEP: create a pod that causes the webhook to hang 11/30/22 03:23:39.698
STEP: create a configmap that should be denied by the webhook 11/30/22 03:23:49.708
STEP: create a configmap that should be admitted by the webhook 11/30/22 03:23:49.788
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 11/30/22 03:23:49.802
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 11/30/22 03:23:49.813
STEP: create a namespace that bypass the webhook 11/30/22 03:23:49.817
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 11/30/22 03:23:49.824
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:23:49.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8762" for this suite. 11/30/22 03:23:49.849
STEP: Destroying namespace "webhook-8762-markers" for this suite. 11/30/22 03:23:49.855
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":23,"skipped":399,"failed":0}
------------------------------
• [SLOW TEST] [13.740 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:23:36.173
    Nov 30 03:23:36.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 03:23:36.174
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:36.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:36.193
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 03:23:36.287
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 03:23:36.595
    STEP: Deploying the webhook pod 11/30/22 03:23:36.607
    STEP: Wait for the deployment to be ready 11/30/22 03:23:36.62
    Nov 30 03:23:36.627: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 03:23:38.637
    STEP: Verifying the service has paired with the endpoint 11/30/22 03:23:38.649
    Nov 30 03:23:39.649: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 11/30/22 03:23:39.652
    STEP: create a pod that should be denied by the webhook 11/30/22 03:23:39.668
    STEP: create a pod that causes the webhook to hang 11/30/22 03:23:39.698
    STEP: create a configmap that should be denied by the webhook 11/30/22 03:23:49.708
    STEP: create a configmap that should be admitted by the webhook 11/30/22 03:23:49.788
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 11/30/22 03:23:49.802
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 11/30/22 03:23:49.813
    STEP: create a namespace that bypass the webhook 11/30/22 03:23:49.817
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 11/30/22 03:23:49.824
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:23:49.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8762" for this suite. 11/30/22 03:23:49.849
    STEP: Destroying namespace "webhook-8762-markers" for this suite. 11/30/22 03:23:49.855
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:23:49.914
Nov 30 03:23:49.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 03:23:49.915
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:49.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:49.947
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-8106/configmap-test-7679459f-3222-445d-9fbf-4272fd0f17a0 11/30/22 03:23:49.98
STEP: Creating a pod to test consume configMaps 11/30/22 03:23:49.986
Nov 30 03:23:50.019: INFO: Waiting up to 5m0s for pod "pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0" in namespace "configmap-8106" to be "Succeeded or Failed"
Nov 30 03:23:50.023: INFO: Pod "pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.362054ms
Nov 30 03:23:52.026: INFO: Pod "pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00782459s
Nov 30 03:23:54.027: INFO: Pod "pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007962075s
STEP: Saw pod success 11/30/22 03:23:54.027
Nov 30 03:23:54.027: INFO: Pod "pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0" satisfied condition "Succeeded or Failed"
Nov 30 03:23:54.029: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0 container env-test: <nil>
STEP: delete the pod 11/30/22 03:23:54.034
Nov 30 03:23:54.042: INFO: Waiting for pod pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0 to disappear
Nov 30 03:23:54.045: INFO: Pod pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 03:23:54.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8106" for this suite. 11/30/22 03:23:54.048
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":24,"skipped":418,"failed":0}
------------------------------
• [4.139 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:23:49.914
    Nov 30 03:23:49.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 03:23:49.915
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:49.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:49.947
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-8106/configmap-test-7679459f-3222-445d-9fbf-4272fd0f17a0 11/30/22 03:23:49.98
    STEP: Creating a pod to test consume configMaps 11/30/22 03:23:49.986
    Nov 30 03:23:50.019: INFO: Waiting up to 5m0s for pod "pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0" in namespace "configmap-8106" to be "Succeeded or Failed"
    Nov 30 03:23:50.023: INFO: Pod "pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.362054ms
    Nov 30 03:23:52.026: INFO: Pod "pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00782459s
    Nov 30 03:23:54.027: INFO: Pod "pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007962075s
    STEP: Saw pod success 11/30/22 03:23:54.027
    Nov 30 03:23:54.027: INFO: Pod "pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0" satisfied condition "Succeeded or Failed"
    Nov 30 03:23:54.029: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0 container env-test: <nil>
    STEP: delete the pod 11/30/22 03:23:54.034
    Nov 30 03:23:54.042: INFO: Waiting for pod pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0 to disappear
    Nov 30 03:23:54.045: INFO: Pod pod-configmaps-8c4a0eb2-96bd-4d15-b8b7-4d167dcbd4f0 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 03:23:54.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8106" for this suite. 11/30/22 03:23:54.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:23:54.054
Nov 30 03:23:54.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubelet-test 11/30/22 03:23:54.055
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:54.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:54.073
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Nov 30 03:23:54.087: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs5b1515a7-37a2-4491-a87d-a7837f2256ac" in namespace "kubelet-test-6831" to be "running and ready"
Nov 30 03:23:54.089: INFO: Pod "busybox-readonly-fs5b1515a7-37a2-4491-a87d-a7837f2256ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.153036ms
Nov 30 03:23:54.089: INFO: The phase of Pod busybox-readonly-fs5b1515a7-37a2-4491-a87d-a7837f2256ac is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:23:56.093: INFO: Pod "busybox-readonly-fs5b1515a7-37a2-4491-a87d-a7837f2256ac": Phase="Running", Reason="", readiness=true. Elapsed: 2.006127683s
Nov 30 03:23:56.093: INFO: The phase of Pod busybox-readonly-fs5b1515a7-37a2-4491-a87d-a7837f2256ac is Running (Ready = true)
Nov 30 03:23:56.093: INFO: Pod "busybox-readonly-fs5b1515a7-37a2-4491-a87d-a7837f2256ac" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Nov 30 03:23:56.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6831" for this suite. 11/30/22 03:23:56.105
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":25,"skipped":442,"failed":0}
------------------------------
• [2.056 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:23:54.054
    Nov 30 03:23:54.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubelet-test 11/30/22 03:23:54.055
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:54.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:54.073
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Nov 30 03:23:54.087: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs5b1515a7-37a2-4491-a87d-a7837f2256ac" in namespace "kubelet-test-6831" to be "running and ready"
    Nov 30 03:23:54.089: INFO: Pod "busybox-readonly-fs5b1515a7-37a2-4491-a87d-a7837f2256ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.153036ms
    Nov 30 03:23:54.089: INFO: The phase of Pod busybox-readonly-fs5b1515a7-37a2-4491-a87d-a7837f2256ac is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:23:56.093: INFO: Pod "busybox-readonly-fs5b1515a7-37a2-4491-a87d-a7837f2256ac": Phase="Running", Reason="", readiness=true. Elapsed: 2.006127683s
    Nov 30 03:23:56.093: INFO: The phase of Pod busybox-readonly-fs5b1515a7-37a2-4491-a87d-a7837f2256ac is Running (Ready = true)
    Nov 30 03:23:56.093: INFO: Pod "busybox-readonly-fs5b1515a7-37a2-4491-a87d-a7837f2256ac" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Nov 30 03:23:56.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6831" for this suite. 11/30/22 03:23:56.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:23:56.11
Nov 30 03:23:56.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 03:23:56.111
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:56.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:56.132
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7945 11/30/22 03:23:56.134
STEP: changing the ExternalName service to type=NodePort 11/30/22 03:23:56.139
STEP: creating replication controller externalname-service in namespace services-7945 11/30/22 03:23:56.171
I1130 03:23:56.176883      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7945, replica count: 2
I1130 03:23:59.229345      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 30 03:23:59.229: INFO: Creating new exec pod
Nov 30 03:23:59.255: INFO: Waiting up to 5m0s for pod "execpodng6b7" in namespace "services-7945" to be "running"
Nov 30 03:23:59.257: INFO: Pod "execpodng6b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078962ms
Nov 30 03:24:01.262: INFO: Pod "execpodng6b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006279407s
Nov 30 03:24:01.262: INFO: Pod "execpodng6b7" satisfied condition "running"
Nov 30 03:24:02.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Nov 30 03:24:02.425: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 30 03:24:02.425: INFO: stdout: ""
Nov 30 03:24:03.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Nov 30 03:24:03.552: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 30 03:24:03.552: INFO: stdout: "externalname-service-vvrcn"
Nov 30 03:24:03.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.138.203 80'
Nov 30 03:24:03.671: INFO: stderr: "+ + nc -v -techo -w 2 10.111.138.203 hostName 80\n\nConnection to 10.111.138.203 80 port [tcp/http] succeeded!\n"
Nov 30 03:24:03.671: INFO: stdout: ""
Nov 30 03:24:04.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.138.203 80'
Nov 30 03:24:04.824: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.138.203 80\nConnection to 10.111.138.203 80 port [tcp/http] succeeded!\n"
Nov 30 03:24:04.824: INFO: stdout: "externalname-service-vvrcn"
Nov 30 03:24:04.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.26 30647'
Nov 30 03:24:04.960: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.26 30647\nConnection to 10.0.10.26 30647 port [tcp/*] succeeded!\n"
Nov 30 03:24:04.960: INFO: stdout: ""
Nov 30 03:24:05.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.26 30647'
Nov 30 03:24:06.080: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.26 30647\nConnection to 10.0.10.26 30647 port [tcp/*] succeeded!\n"
Nov 30 03:24:06.080: INFO: stdout: "externalname-service-vvrcn"
Nov 30 03:24:06.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.8 30647'
Nov 30 03:24:06.220: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.8 30647\nConnection to 10.0.10.8 30647 port [tcp/*] succeeded!\n"
Nov 30 03:24:06.220: INFO: stdout: ""
Nov 30 03:24:07.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.8 30647'
Nov 30 03:24:07.347: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.8 30647\nConnection to 10.0.10.8 30647 port [tcp/*] succeeded!\n"
Nov 30 03:24:07.347: INFO: stdout: ""
Nov 30 03:24:08.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.8 30647'
Nov 30 03:24:08.347: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.8 30647\nConnection to 10.0.10.8 30647 port [tcp/*] succeeded!\n"
Nov 30 03:24:08.347: INFO: stdout: "externalname-service-cdz2c"
Nov 30 03:24:08.347: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 03:24:08.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7945" for this suite. 11/30/22 03:24:08.391
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":26,"skipped":450,"failed":0}
------------------------------
• [SLOW TEST] [12.302 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:23:56.11
    Nov 30 03:23:56.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 03:23:56.111
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:23:56.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:23:56.132
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7945 11/30/22 03:23:56.134
    STEP: changing the ExternalName service to type=NodePort 11/30/22 03:23:56.139
    STEP: creating replication controller externalname-service in namespace services-7945 11/30/22 03:23:56.171
    I1130 03:23:56.176883      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7945, replica count: 2
    I1130 03:23:59.229345      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 30 03:23:59.229: INFO: Creating new exec pod
    Nov 30 03:23:59.255: INFO: Waiting up to 5m0s for pod "execpodng6b7" in namespace "services-7945" to be "running"
    Nov 30 03:23:59.257: INFO: Pod "execpodng6b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078962ms
    Nov 30 03:24:01.262: INFO: Pod "execpodng6b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006279407s
    Nov 30 03:24:01.262: INFO: Pod "execpodng6b7" satisfied condition "running"
    Nov 30 03:24:02.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Nov 30 03:24:02.425: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Nov 30 03:24:02.425: INFO: stdout: ""
    Nov 30 03:24:03.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Nov 30 03:24:03.552: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Nov 30 03:24:03.552: INFO: stdout: "externalname-service-vvrcn"
    Nov 30 03:24:03.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.138.203 80'
    Nov 30 03:24:03.671: INFO: stderr: "+ + nc -v -techo -w 2 10.111.138.203 hostName 80\n\nConnection to 10.111.138.203 80 port [tcp/http] succeeded!\n"
    Nov 30 03:24:03.671: INFO: stdout: ""
    Nov 30 03:24:04.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.138.203 80'
    Nov 30 03:24:04.824: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.138.203 80\nConnection to 10.111.138.203 80 port [tcp/http] succeeded!\n"
    Nov 30 03:24:04.824: INFO: stdout: "externalname-service-vvrcn"
    Nov 30 03:24:04.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.26 30647'
    Nov 30 03:24:04.960: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.26 30647\nConnection to 10.0.10.26 30647 port [tcp/*] succeeded!\n"
    Nov 30 03:24:04.960: INFO: stdout: ""
    Nov 30 03:24:05.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.26 30647'
    Nov 30 03:24:06.080: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.26 30647\nConnection to 10.0.10.26 30647 port [tcp/*] succeeded!\n"
    Nov 30 03:24:06.080: INFO: stdout: "externalname-service-vvrcn"
    Nov 30 03:24:06.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.8 30647'
    Nov 30 03:24:06.220: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.8 30647\nConnection to 10.0.10.8 30647 port [tcp/*] succeeded!\n"
    Nov 30 03:24:06.220: INFO: stdout: ""
    Nov 30 03:24:07.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.8 30647'
    Nov 30 03:24:07.347: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.8 30647\nConnection to 10.0.10.8 30647 port [tcp/*] succeeded!\n"
    Nov 30 03:24:07.347: INFO: stdout: ""
    Nov 30 03:24:08.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7945 exec execpodng6b7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.8 30647'
    Nov 30 03:24:08.347: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.8 30647\nConnection to 10.0.10.8 30647 port [tcp/*] succeeded!\n"
    Nov 30 03:24:08.347: INFO: stdout: "externalname-service-cdz2c"
    Nov 30 03:24:08.347: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 03:24:08.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7945" for this suite. 11/30/22 03:24:08.391
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:24:08.413
Nov 30 03:24:08.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename certificates 11/30/22 03:24:08.414
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:24:08.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:24:08.45
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 11/30/22 03:24:08.92
STEP: getting /apis/certificates.k8s.io 11/30/22 03:24:08.922
STEP: getting /apis/certificates.k8s.io/v1 11/30/22 03:24:08.923
STEP: creating 11/30/22 03:24:08.924
STEP: getting 11/30/22 03:24:08.943
STEP: listing 11/30/22 03:24:08.946
STEP: watching 11/30/22 03:24:08.949
Nov 30 03:24:08.949: INFO: starting watch
STEP: patching 11/30/22 03:24:08.949
STEP: updating 11/30/22 03:24:08.954
Nov 30 03:24:08.963: INFO: waiting for watch events with expected annotations
Nov 30 03:24:08.963: INFO: saw patched and updated annotations
STEP: getting /approval 11/30/22 03:24:08.963
STEP: patching /approval 11/30/22 03:24:08.966
STEP: updating /approval 11/30/22 03:24:08.974
STEP: getting /status 11/30/22 03:24:08.978
STEP: patching /status 11/30/22 03:24:08.981
STEP: updating /status 11/30/22 03:24:08.987
STEP: deleting 11/30/22 03:24:08.994
STEP: deleting a collection 11/30/22 03:24:09.004
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:24:09.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1362" for this suite. 11/30/22 03:24:09.026
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":27,"skipped":450,"failed":0}
------------------------------
• [0.625 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:24:08.413
    Nov 30 03:24:08.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename certificates 11/30/22 03:24:08.414
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:24:08.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:24:08.45
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 11/30/22 03:24:08.92
    STEP: getting /apis/certificates.k8s.io 11/30/22 03:24:08.922
    STEP: getting /apis/certificates.k8s.io/v1 11/30/22 03:24:08.923
    STEP: creating 11/30/22 03:24:08.924
    STEP: getting 11/30/22 03:24:08.943
    STEP: listing 11/30/22 03:24:08.946
    STEP: watching 11/30/22 03:24:08.949
    Nov 30 03:24:08.949: INFO: starting watch
    STEP: patching 11/30/22 03:24:08.949
    STEP: updating 11/30/22 03:24:08.954
    Nov 30 03:24:08.963: INFO: waiting for watch events with expected annotations
    Nov 30 03:24:08.963: INFO: saw patched and updated annotations
    STEP: getting /approval 11/30/22 03:24:08.963
    STEP: patching /approval 11/30/22 03:24:08.966
    STEP: updating /approval 11/30/22 03:24:08.974
    STEP: getting /status 11/30/22 03:24:08.978
    STEP: patching /status 11/30/22 03:24:08.981
    STEP: updating /status 11/30/22 03:24:08.987
    STEP: deleting 11/30/22 03:24:08.994
    STEP: deleting a collection 11/30/22 03:24:09.004
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:24:09.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-1362" for this suite. 11/30/22 03:24:09.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:24:09.039
Nov 30 03:24:09.039: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename var-expansion 11/30/22 03:24:09.039
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:24:09.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:24:09.063
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 11/30/22 03:24:09.065
Nov 30 03:24:09.098: INFO: Waiting up to 2m0s for pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52" in namespace "var-expansion-9452" to be "running"
Nov 30 03:24:09.103: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 5.387464ms
Nov 30 03:24:11.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009150887s
Nov 30 03:24:13.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009285936s
Nov 30 03:24:15.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00880289s
Nov 30 03:24:17.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009061022s
Nov 30 03:24:19.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009411296s
Nov 30 03:24:21.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008576753s
Nov 30 03:24:23.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010151036s
Nov 30 03:24:25.121: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023142695s
Nov 30 03:24:27.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009133589s
Nov 30 03:24:29.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009121764s
Nov 30 03:24:31.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009343115s
Nov 30 03:24:33.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009350128s
Nov 30 03:24:35.109: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011105628s
Nov 30 03:24:37.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008703121s
Nov 30 03:24:39.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009100513s
Nov 30 03:24:41.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00923697s
Nov 30 03:24:43.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009416544s
Nov 30 03:24:45.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009428278s
Nov 30 03:24:47.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009233946s
Nov 30 03:24:49.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008636371s
Nov 30 03:24:51.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 42.01008361s
Nov 30 03:24:53.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008986676s
Nov 30 03:24:55.109: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010705846s
Nov 30 03:24:57.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008695843s
Nov 30 03:24:59.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009604352s
Nov 30 03:25:01.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009702467s
Nov 30 03:25:03.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009642058s
Nov 30 03:25:05.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010080535s
Nov 30 03:25:07.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009643254s
Nov 30 03:25:09.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008465862s
Nov 30 03:25:11.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009162761s
Nov 30 03:25:13.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009425435s
Nov 30 03:25:15.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009026364s
Nov 30 03:25:17.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009555649s
Nov 30 03:25:19.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009630289s
Nov 30 03:25:21.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008598739s
Nov 30 03:25:23.106: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008324578s
Nov 30 03:25:25.109: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010563602s
Nov 30 03:25:27.106: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008356049s
Nov 30 03:25:29.106: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008049952s
Nov 30 03:25:31.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008535299s
Nov 30 03:25:33.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008462127s
Nov 30 03:25:35.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008636809s
Nov 30 03:25:37.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008426404s
Nov 30 03:25:39.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009433282s
Nov 30 03:25:41.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008756402s
Nov 30 03:25:43.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008594283s
Nov 30 03:25:45.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008655603s
Nov 30 03:25:47.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009086325s
Nov 30 03:25:49.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009306494s
Nov 30 03:25:51.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009489723s
Nov 30 03:25:53.106: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008207172s
Nov 30 03:25:55.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009302201s
Nov 30 03:25:57.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008474734s
Nov 30 03:25:59.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008475938s
Nov 30 03:26:01.106: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008110815s
Nov 30 03:26:03.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009853202s
Nov 30 03:26:05.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009796382s
Nov 30 03:26:07.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010173271s
Nov 30 03:26:09.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010150163s
Nov 30 03:26:09.111: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012647407s
STEP: updating the pod 11/30/22 03:26:09.111
Nov 30 03:26:09.629: INFO: Successfully updated pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52"
STEP: waiting for pod running 11/30/22 03:26:09.629
Nov 30 03:26:09.629: INFO: Waiting up to 2m0s for pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52" in namespace "var-expansion-9452" to be "running"
Nov 30 03:26:09.638: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 8.570013ms
Nov 30 03:26:11.642: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Running", Reason="", readiness=true. Elapsed: 2.013282189s
Nov 30 03:26:11.643: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52" satisfied condition "running"
STEP: deleting the pod gracefully 11/30/22 03:26:11.643
Nov 30 03:26:11.643: INFO: Deleting pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52" in namespace "var-expansion-9452"
Nov 30 03:26:11.650: INFO: Wait up to 5m0s for pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Nov 30 03:26:43.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9452" for this suite. 11/30/22 03:26:43.665
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":28,"skipped":478,"failed":0}
------------------------------
• [SLOW TEST] [154.633 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:24:09.039
    Nov 30 03:24:09.039: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename var-expansion 11/30/22 03:24:09.039
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:24:09.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:24:09.063
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 11/30/22 03:24:09.065
    Nov 30 03:24:09.098: INFO: Waiting up to 2m0s for pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52" in namespace "var-expansion-9452" to be "running"
    Nov 30 03:24:09.103: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 5.387464ms
    Nov 30 03:24:11.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009150887s
    Nov 30 03:24:13.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009285936s
    Nov 30 03:24:15.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00880289s
    Nov 30 03:24:17.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009061022s
    Nov 30 03:24:19.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009411296s
    Nov 30 03:24:21.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008576753s
    Nov 30 03:24:23.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010151036s
    Nov 30 03:24:25.121: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023142695s
    Nov 30 03:24:27.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009133589s
    Nov 30 03:24:29.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009121764s
    Nov 30 03:24:31.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009343115s
    Nov 30 03:24:33.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009350128s
    Nov 30 03:24:35.109: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011105628s
    Nov 30 03:24:37.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008703121s
    Nov 30 03:24:39.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009100513s
    Nov 30 03:24:41.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00923697s
    Nov 30 03:24:43.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009416544s
    Nov 30 03:24:45.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009428278s
    Nov 30 03:24:47.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009233946s
    Nov 30 03:24:49.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008636371s
    Nov 30 03:24:51.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 42.01008361s
    Nov 30 03:24:53.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008986676s
    Nov 30 03:24:55.109: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010705846s
    Nov 30 03:24:57.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008695843s
    Nov 30 03:24:59.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009604352s
    Nov 30 03:25:01.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009702467s
    Nov 30 03:25:03.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009642058s
    Nov 30 03:25:05.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010080535s
    Nov 30 03:25:07.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009643254s
    Nov 30 03:25:09.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008465862s
    Nov 30 03:25:11.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009162761s
    Nov 30 03:25:13.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009425435s
    Nov 30 03:25:15.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009026364s
    Nov 30 03:25:17.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009555649s
    Nov 30 03:25:19.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009630289s
    Nov 30 03:25:21.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008598739s
    Nov 30 03:25:23.106: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008324578s
    Nov 30 03:25:25.109: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010563602s
    Nov 30 03:25:27.106: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008356049s
    Nov 30 03:25:29.106: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008049952s
    Nov 30 03:25:31.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008535299s
    Nov 30 03:25:33.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008462127s
    Nov 30 03:25:35.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008636809s
    Nov 30 03:25:37.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008426404s
    Nov 30 03:25:39.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009433282s
    Nov 30 03:25:41.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008756402s
    Nov 30 03:25:43.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008594283s
    Nov 30 03:25:45.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008655603s
    Nov 30 03:25:47.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009086325s
    Nov 30 03:25:49.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009306494s
    Nov 30 03:25:51.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009489723s
    Nov 30 03:25:53.106: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008207172s
    Nov 30 03:25:55.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009302201s
    Nov 30 03:25:57.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008474734s
    Nov 30 03:25:59.107: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008475938s
    Nov 30 03:26:01.106: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008110815s
    Nov 30 03:26:03.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009853202s
    Nov 30 03:26:05.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009796382s
    Nov 30 03:26:07.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010173271s
    Nov 30 03:26:09.108: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010150163s
    Nov 30 03:26:09.111: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012647407s
    STEP: updating the pod 11/30/22 03:26:09.111
    Nov 30 03:26:09.629: INFO: Successfully updated pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52"
    STEP: waiting for pod running 11/30/22 03:26:09.629
    Nov 30 03:26:09.629: INFO: Waiting up to 2m0s for pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52" in namespace "var-expansion-9452" to be "running"
    Nov 30 03:26:09.638: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Pending", Reason="", readiness=false. Elapsed: 8.570013ms
    Nov 30 03:26:11.642: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52": Phase="Running", Reason="", readiness=true. Elapsed: 2.013282189s
    Nov 30 03:26:11.643: INFO: Pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52" satisfied condition "running"
    STEP: deleting the pod gracefully 11/30/22 03:26:11.643
    Nov 30 03:26:11.643: INFO: Deleting pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52" in namespace "var-expansion-9452"
    Nov 30 03:26:11.650: INFO: Wait up to 5m0s for pod "var-expansion-00400088-d8a2-4390-8d85-16f8af3f1c52" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Nov 30 03:26:43.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9452" for this suite. 11/30/22 03:26:43.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:26:43.673
Nov 30 03:26:43.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename gc 11/30/22 03:26:43.673
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:26:43.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:26:43.702
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 11/30/22 03:26:43.707
STEP: delete the rc 11/30/22 03:26:48.719
STEP: wait for all pods to be garbage collected 11/30/22 03:26:48.725
STEP: Gathering metrics 11/30/22 03:26:53.732
Nov 30 03:26:53.759: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
Nov 30 03:26:53.765: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 5.112581ms
Nov 30 03:26:53.765: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
Nov 30 03:26:53.765: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
Nov 30 03:26:53.808: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Nov 30 03:26:53.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5874" for this suite. 11/30/22 03:26:53.813
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":29,"skipped":499,"failed":0}
------------------------------
• [SLOW TEST] [10.146 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:26:43.673
    Nov 30 03:26:43.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename gc 11/30/22 03:26:43.673
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:26:43.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:26:43.702
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 11/30/22 03:26:43.707
    STEP: delete the rc 11/30/22 03:26:48.719
    STEP: wait for all pods to be garbage collected 11/30/22 03:26:48.725
    STEP: Gathering metrics 11/30/22 03:26:53.732
    Nov 30 03:26:53.759: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
    Nov 30 03:26:53.765: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 5.112581ms
    Nov 30 03:26:53.765: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
    Nov 30 03:26:53.765: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
    Nov 30 03:26:53.808: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Nov 30 03:26:53.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5874" for this suite. 11/30/22 03:26:53.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:26:53.819
Nov 30 03:26:53.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename podtemplate 11/30/22 03:26:53.819
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:26:53.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:26:53.839
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 11/30/22 03:26:53.841
Nov 30 03:26:53.848: INFO: created test-podtemplate-1
Nov 30 03:26:53.853: INFO: created test-podtemplate-2
Nov 30 03:26:53.857: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 11/30/22 03:26:53.857
STEP: delete collection of pod templates 11/30/22 03:26:53.859
Nov 30 03:26:53.859: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 11/30/22 03:26:53.872
Nov 30 03:26:53.872: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Nov 30 03:26:53.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1671" for this suite. 11/30/22 03:26:53.879
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":30,"skipped":504,"failed":0}
------------------------------
• [0.066 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:26:53.819
    Nov 30 03:26:53.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename podtemplate 11/30/22 03:26:53.819
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:26:53.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:26:53.839
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 11/30/22 03:26:53.841
    Nov 30 03:26:53.848: INFO: created test-podtemplate-1
    Nov 30 03:26:53.853: INFO: created test-podtemplate-2
    Nov 30 03:26:53.857: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 11/30/22 03:26:53.857
    STEP: delete collection of pod templates 11/30/22 03:26:53.859
    Nov 30 03:26:53.859: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 11/30/22 03:26:53.872
    Nov 30 03:26:53.872: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Nov 30 03:26:53.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1671" for this suite. 11/30/22 03:26:53.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:26:53.886
Nov 30 03:26:53.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 03:26:53.886
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:26:53.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:26:53.906
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-22f3057d-ecaa-4980-8d8f-f5a532bfd6e2 11/30/22 03:26:53.909
STEP: Creating a pod to test consume secrets 11/30/22 03:26:53.916
Nov 30 03:26:53.944: INFO: Waiting up to 5m0s for pod "pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68" in namespace "secrets-2904" to be "Succeeded or Failed"
Nov 30 03:26:53.946: INFO: Pod "pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.448596ms
Nov 30 03:26:55.949: INFO: Pod "pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005827684s
Nov 30 03:26:57.950: INFO: Pod "pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006256631s
STEP: Saw pod success 11/30/22 03:26:57.95
Nov 30 03:26:57.950: INFO: Pod "pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68" satisfied condition "Succeeded or Failed"
Nov 30 03:26:57.952: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68 container secret-volume-test: <nil>
STEP: delete the pod 11/30/22 03:26:57.958
Nov 30 03:26:57.968: INFO: Waiting for pod pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68 to disappear
Nov 30 03:26:57.971: INFO: Pod pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Nov 30 03:26:57.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2904" for this suite. 11/30/22 03:26:57.975
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":31,"skipped":531,"failed":0}
------------------------------
• [4.094 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:26:53.886
    Nov 30 03:26:53.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 03:26:53.886
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:26:53.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:26:53.906
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-22f3057d-ecaa-4980-8d8f-f5a532bfd6e2 11/30/22 03:26:53.909
    STEP: Creating a pod to test consume secrets 11/30/22 03:26:53.916
    Nov 30 03:26:53.944: INFO: Waiting up to 5m0s for pod "pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68" in namespace "secrets-2904" to be "Succeeded or Failed"
    Nov 30 03:26:53.946: INFO: Pod "pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.448596ms
    Nov 30 03:26:55.949: INFO: Pod "pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005827684s
    Nov 30 03:26:57.950: INFO: Pod "pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006256631s
    STEP: Saw pod success 11/30/22 03:26:57.95
    Nov 30 03:26:57.950: INFO: Pod "pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68" satisfied condition "Succeeded or Failed"
    Nov 30 03:26:57.952: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68 container secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 03:26:57.958
    Nov 30 03:26:57.968: INFO: Waiting for pod pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68 to disappear
    Nov 30 03:26:57.971: INFO: Pod pod-secrets-b3b9a1c6-1625-4ef3-b117-bfb105af8b68 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 03:26:57.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2904" for this suite. 11/30/22 03:26:57.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:26:57.981
Nov 30 03:26:57.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename security-context-test 11/30/22 03:26:57.982
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:26:57.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:26:57.998
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Nov 30 03:26:58.008: INFO: Waiting up to 5m0s for pod "busybox-user-65534-535afae1-683b-4c24-9967-0cceb83119fa" in namespace "security-context-test-4783" to be "Succeeded or Failed"
Nov 30 03:26:58.010: INFO: Pod "busybox-user-65534-535afae1-683b-4c24-9967-0cceb83119fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32669ms
Nov 30 03:27:00.014: INFO: Pod "busybox-user-65534-535afae1-683b-4c24-9967-0cceb83119fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006404739s
Nov 30 03:27:02.015: INFO: Pod "busybox-user-65534-535afae1-683b-4c24-9967-0cceb83119fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006716439s
Nov 30 03:27:02.015: INFO: Pod "busybox-user-65534-535afae1-683b-4c24-9967-0cceb83119fa" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Nov 30 03:27:02.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4783" for this suite. 11/30/22 03:27:02.019
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":32,"skipped":589,"failed":0}
------------------------------
• [4.044 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:26:57.981
    Nov 30 03:26:57.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename security-context-test 11/30/22 03:26:57.982
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:26:57.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:26:57.998
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Nov 30 03:26:58.008: INFO: Waiting up to 5m0s for pod "busybox-user-65534-535afae1-683b-4c24-9967-0cceb83119fa" in namespace "security-context-test-4783" to be "Succeeded or Failed"
    Nov 30 03:26:58.010: INFO: Pod "busybox-user-65534-535afae1-683b-4c24-9967-0cceb83119fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32669ms
    Nov 30 03:27:00.014: INFO: Pod "busybox-user-65534-535afae1-683b-4c24-9967-0cceb83119fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006404739s
    Nov 30 03:27:02.015: INFO: Pod "busybox-user-65534-535afae1-683b-4c24-9967-0cceb83119fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006716439s
    Nov 30 03:27:02.015: INFO: Pod "busybox-user-65534-535afae1-683b-4c24-9967-0cceb83119fa" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Nov 30 03:27:02.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4783" for this suite. 11/30/22 03:27:02.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:27:02.025
Nov 30 03:27:02.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename cronjob 11/30/22 03:27:02.026
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:27:02.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:27:02.05
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 11/30/22 03:27:02.052
STEP: Ensuring a job is scheduled 11/30/22 03:27:02.062
STEP: Ensuring exactly one is scheduled 11/30/22 03:28:02.065
STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/30/22 03:28:02.067
STEP: Ensuring the job is replaced with a new one 11/30/22 03:28:02.069
STEP: Removing cronjob 11/30/22 03:29:02.073
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Nov 30 03:29:02.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6935" for this suite. 11/30/22 03:29:02.082
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":33,"skipped":596,"failed":0}
------------------------------
• [SLOW TEST] [120.062 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:27:02.025
    Nov 30 03:27:02.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename cronjob 11/30/22 03:27:02.026
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:27:02.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:27:02.05
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 11/30/22 03:27:02.052
    STEP: Ensuring a job is scheduled 11/30/22 03:27:02.062
    STEP: Ensuring exactly one is scheduled 11/30/22 03:28:02.065
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/30/22 03:28:02.067
    STEP: Ensuring the job is replaced with a new one 11/30/22 03:28:02.069
    STEP: Removing cronjob 11/30/22 03:29:02.073
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Nov 30 03:29:02.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6935" for this suite. 11/30/22 03:29:02.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:29:02.088
Nov 30 03:29:02.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 03:29:02.088
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:29:02.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:29:02.118
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-4311 11/30/22 03:29:02.12
W1130 03:29:02.152248      20 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Nov 30 03:29:02.152: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4311" to be "running and ready"
Nov 30 03:29:02.154: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111881ms
Nov 30 03:29:02.154: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:29:04.158: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.006058945s
Nov 30 03:29:04.158: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Nov 30 03:29:04.158: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Nov 30 03:29:04.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 30 03:29:04.307: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Nov 30 03:29:04.307: INFO: stdout: "ipvs"
Nov 30 03:29:04.307: INFO: proxyMode: ipvs
Nov 30 03:29:04.320: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 30 03:29:04.323: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-4311 11/30/22 03:29:04.323
STEP: creating replication controller affinity-nodeport-timeout in namespace services-4311 11/30/22 03:29:04.341
I1130 03:29:04.347555      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4311, replica count: 3
I1130 03:29:07.399700      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 30 03:29:07.408: INFO: Creating new exec pod
Nov 30 03:29:07.433: INFO: Waiting up to 5m0s for pod "execpod-affinityr6pwg" in namespace "services-4311" to be "running"
Nov 30 03:29:07.442: INFO: Pod "execpod-affinityr6pwg": Phase="Pending", Reason="", readiness=false. Elapsed: 8.603388ms
Nov 30 03:29:09.445: INFO: Pod "execpod-affinityr6pwg": Phase="Running", Reason="", readiness=true. Elapsed: 2.011492979s
Nov 30 03:29:09.445: INFO: Pod "execpod-affinityr6pwg" satisfied condition "running"
Nov 30 03:29:10.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Nov 30 03:29:10.585: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Nov 30 03:29:10.585: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:29:10.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.100.187 80'
Nov 30 03:29:10.713: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.100.187 80\nConnection to 10.104.100.187 80 port [tcp/http] succeeded!\n"
Nov 30 03:29:10.713: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:29:10.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.2 31598'
Nov 30 03:29:10.848: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.2 31598\nConnection to 10.0.10.2 31598 port [tcp/*] succeeded!\n"
Nov 30 03:29:10.848: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:29:10.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.26 31598'
Nov 30 03:29:10.972: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.26 31598\nConnection to 10.0.10.26 31598 port [tcp/*] succeeded!\n"
Nov 30 03:29:10.972: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:29:10.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.8:31598/ ; done'
Nov 30 03:29:11.161: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n"
Nov 30 03:29:11.161: INFO: stdout: "\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8"
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
Nov 30 03:29:11.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.10.8:31598/'
Nov 30 03:29:11.292: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n"
Nov 30 03:29:11.292: INFO: stdout: "affinity-nodeport-timeout-pwsp8"
Nov 30 03:31:21.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.10.8:31598/'
Nov 30 03:31:21.436: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n"
Nov 30 03:31:21.436: INFO: stdout: "affinity-nodeport-timeout-68hbs"
Nov 30 03:31:21.436: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4311, will wait for the garbage collector to delete the pods 11/30/22 03:31:21.461
Nov 30 03:31:21.523: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.431234ms
Nov 30 03:31:21.623: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.456002ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 03:31:23.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4311" for this suite. 11/30/22 03:31:23.693
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":34,"skipped":602,"failed":0}
------------------------------
• [SLOW TEST] [141.622 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:29:02.088
    Nov 30 03:29:02.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 03:29:02.088
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:29:02.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:29:02.118
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-4311 11/30/22 03:29:02.12
    W1130 03:29:02.152248      20 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Nov 30 03:29:02.152: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4311" to be "running and ready"
    Nov 30 03:29:02.154: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111881ms
    Nov 30 03:29:02.154: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:29:04.158: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.006058945s
    Nov 30 03:29:04.158: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Nov 30 03:29:04.158: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Nov 30 03:29:04.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Nov 30 03:29:04.307: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Nov 30 03:29:04.307: INFO: stdout: "ipvs"
    Nov 30 03:29:04.307: INFO: proxyMode: ipvs
    Nov 30 03:29:04.320: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Nov 30 03:29:04.323: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-4311 11/30/22 03:29:04.323
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-4311 11/30/22 03:29:04.341
    I1130 03:29:04.347555      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4311, replica count: 3
    I1130 03:29:07.399700      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 30 03:29:07.408: INFO: Creating new exec pod
    Nov 30 03:29:07.433: INFO: Waiting up to 5m0s for pod "execpod-affinityr6pwg" in namespace "services-4311" to be "running"
    Nov 30 03:29:07.442: INFO: Pod "execpod-affinityr6pwg": Phase="Pending", Reason="", readiness=false. Elapsed: 8.603388ms
    Nov 30 03:29:09.445: INFO: Pod "execpod-affinityr6pwg": Phase="Running", Reason="", readiness=true. Elapsed: 2.011492979s
    Nov 30 03:29:09.445: INFO: Pod "execpod-affinityr6pwg" satisfied condition "running"
    Nov 30 03:29:10.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Nov 30 03:29:10.585: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Nov 30 03:29:10.585: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:29:10.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.100.187 80'
    Nov 30 03:29:10.713: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.100.187 80\nConnection to 10.104.100.187 80 port [tcp/http] succeeded!\n"
    Nov 30 03:29:10.713: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:29:10.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.2 31598'
    Nov 30 03:29:10.848: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.2 31598\nConnection to 10.0.10.2 31598 port [tcp/*] succeeded!\n"
    Nov 30 03:29:10.848: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:29:10.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.26 31598'
    Nov 30 03:29:10.972: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.26 31598\nConnection to 10.0.10.26 31598 port [tcp/*] succeeded!\n"
    Nov 30 03:29:10.972: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:29:10.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.8:31598/ ; done'
    Nov 30 03:29:11.161: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n"
    Nov 30 03:29:11.161: INFO: stdout: "\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8\naffinity-nodeport-timeout-pwsp8"
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Received response from host: affinity-nodeport-timeout-pwsp8
    Nov 30 03:29:11.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.10.8:31598/'
    Nov 30 03:29:11.292: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n"
    Nov 30 03:29:11.292: INFO: stdout: "affinity-nodeport-timeout-pwsp8"
    Nov 30 03:31:21.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4311 exec execpod-affinityr6pwg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.10.8:31598/'
    Nov 30 03:31:21.436: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.10.8:31598/\n"
    Nov 30 03:31:21.436: INFO: stdout: "affinity-nodeport-timeout-68hbs"
    Nov 30 03:31:21.436: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4311, will wait for the garbage collector to delete the pods 11/30/22 03:31:21.461
    Nov 30 03:31:21.523: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.431234ms
    Nov 30 03:31:21.623: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.456002ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 03:31:23.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4311" for this suite. 11/30/22 03:31:23.693
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:31:23.71
Nov 30 03:31:23.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-probe 11/30/22 03:31:23.711
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:31:23.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:31:23.733
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-becd49be-acc6-429a-a325-02112d0524ad in namespace container-probe-3543 11/30/22 03:31:23.738
Nov 30 03:31:23.773: INFO: Waiting up to 5m0s for pod "liveness-becd49be-acc6-429a-a325-02112d0524ad" in namespace "container-probe-3543" to be "not pending"
Nov 30 03:31:23.776: INFO: Pod "liveness-becd49be-acc6-429a-a325-02112d0524ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536943ms
Nov 30 03:31:25.779: INFO: Pod "liveness-becd49be-acc6-429a-a325-02112d0524ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.00568854s
Nov 30 03:31:25.779: INFO: Pod "liveness-becd49be-acc6-429a-a325-02112d0524ad" satisfied condition "not pending"
Nov 30 03:31:25.779: INFO: Started pod liveness-becd49be-acc6-429a-a325-02112d0524ad in namespace container-probe-3543
STEP: checking the pod's current state and verifying that restartCount is present 11/30/22 03:31:25.779
Nov 30 03:31:25.782: INFO: Initial restart count of pod liveness-becd49be-acc6-429a-a325-02112d0524ad is 0
STEP: deleting the pod 11/30/22 03:35:26.292
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Nov 30 03:35:26.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3543" for this suite. 11/30/22 03:35:26.309
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":35,"skipped":607,"failed":0}
------------------------------
• [SLOW TEST] [242.607 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:31:23.71
    Nov 30 03:31:23.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-probe 11/30/22 03:31:23.711
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:31:23.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:31:23.733
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-becd49be-acc6-429a-a325-02112d0524ad in namespace container-probe-3543 11/30/22 03:31:23.738
    Nov 30 03:31:23.773: INFO: Waiting up to 5m0s for pod "liveness-becd49be-acc6-429a-a325-02112d0524ad" in namespace "container-probe-3543" to be "not pending"
    Nov 30 03:31:23.776: INFO: Pod "liveness-becd49be-acc6-429a-a325-02112d0524ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536943ms
    Nov 30 03:31:25.779: INFO: Pod "liveness-becd49be-acc6-429a-a325-02112d0524ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.00568854s
    Nov 30 03:31:25.779: INFO: Pod "liveness-becd49be-acc6-429a-a325-02112d0524ad" satisfied condition "not pending"
    Nov 30 03:31:25.779: INFO: Started pod liveness-becd49be-acc6-429a-a325-02112d0524ad in namespace container-probe-3543
    STEP: checking the pod's current state and verifying that restartCount is present 11/30/22 03:31:25.779
    Nov 30 03:31:25.782: INFO: Initial restart count of pod liveness-becd49be-acc6-429a-a325-02112d0524ad is 0
    STEP: deleting the pod 11/30/22 03:35:26.292
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Nov 30 03:35:26.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3543" for this suite. 11/30/22 03:35:26.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:35:26.318
Nov 30 03:35:26.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:35:26.319
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:35:26.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:35:26.35
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-88e39572-93c4-45d4-9655-4d7c91a7dee9 11/30/22 03:35:26.352
STEP: Creating secret with name secret-projected-all-test-volume-f5576b94-b5ba-4a56-b1c0-f75a960e70de 11/30/22 03:35:26.365
STEP: Creating a pod to test Check all projections for projected volume plugin 11/30/22 03:35:26.375
Nov 30 03:35:26.404: INFO: Waiting up to 5m0s for pod "projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a" in namespace "projected-8033" to be "Succeeded or Failed"
Nov 30 03:35:26.407: INFO: Pod "projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.732996ms
Nov 30 03:35:28.411: INFO: Pod "projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a": Phase="Running", Reason="", readiness=false. Elapsed: 2.006412243s
Nov 30 03:35:30.412: INFO: Pod "projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007522997s
STEP: Saw pod success 11/30/22 03:35:30.412
Nov 30 03:35:30.412: INFO: Pod "projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a" satisfied condition "Succeeded or Failed"
Nov 30 03:35:30.415: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a container projected-all-volume-test: <nil>
STEP: delete the pod 11/30/22 03:35:30.426
Nov 30 03:35:30.445: INFO: Waiting for pod projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a to disappear
Nov 30 03:35:30.447: INFO: Pod projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Nov 30 03:35:30.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8033" for this suite. 11/30/22 03:35:30.451
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":36,"skipped":617,"failed":0}
------------------------------
• [4.138 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:35:26.318
    Nov 30 03:35:26.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:35:26.319
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:35:26.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:35:26.35
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-88e39572-93c4-45d4-9655-4d7c91a7dee9 11/30/22 03:35:26.352
    STEP: Creating secret with name secret-projected-all-test-volume-f5576b94-b5ba-4a56-b1c0-f75a960e70de 11/30/22 03:35:26.365
    STEP: Creating a pod to test Check all projections for projected volume plugin 11/30/22 03:35:26.375
    Nov 30 03:35:26.404: INFO: Waiting up to 5m0s for pod "projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a" in namespace "projected-8033" to be "Succeeded or Failed"
    Nov 30 03:35:26.407: INFO: Pod "projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.732996ms
    Nov 30 03:35:28.411: INFO: Pod "projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a": Phase="Running", Reason="", readiness=false. Elapsed: 2.006412243s
    Nov 30 03:35:30.412: INFO: Pod "projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007522997s
    STEP: Saw pod success 11/30/22 03:35:30.412
    Nov 30 03:35:30.412: INFO: Pod "projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a" satisfied condition "Succeeded or Failed"
    Nov 30 03:35:30.415: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a container projected-all-volume-test: <nil>
    STEP: delete the pod 11/30/22 03:35:30.426
    Nov 30 03:35:30.445: INFO: Waiting for pod projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a to disappear
    Nov 30 03:35:30.447: INFO: Pod projected-volume-a97341ce-ca43-4fde-a859-e3069414e33a no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Nov 30 03:35:30.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8033" for this suite. 11/30/22 03:35:30.451
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:35:30.456
Nov 30 03:35:30.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename deployment 11/30/22 03:35:30.457
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:35:30.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:35:30.482
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 11/30/22 03:35:30.494
Nov 30 03:35:30.494: INFO: Creating simple deployment test-deployment-tmlrc
Nov 30 03:35:30.504: INFO: new replicaset for deployment "test-deployment-tmlrc" is yet to be created
STEP: Getting /status 11/30/22 03:35:32.515
Nov 30 03:35:32.519: INFO: Deployment test-deployment-tmlrc has Conditions: [{Available True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tmlrc-85f747b754" has successfully progressed.}]
STEP: updating Deployment Status 11/30/22 03:35:32.519
Nov 30 03:35:32.527: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 3, 35, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 3, 35, 32, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 3, 35, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 3, 35, 30, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-tmlrc-85f747b754\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 11/30/22 03:35:32.527
Nov 30 03:35:32.529: INFO: Observed &Deployment event: ADDED
Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tmlrc-85f747b754"}
Nov 30 03:35:32.529: INFO: Observed &Deployment event: MODIFIED
Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tmlrc-85f747b754"}
Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 30 03:35:32.529: INFO: Observed &Deployment event: MODIFIED
Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tmlrc-85f747b754" is progressing.}
Nov 30 03:35:32.529: INFO: Observed &Deployment event: MODIFIED
Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tmlrc-85f747b754" has successfully progressed.}
Nov 30 03:35:32.529: INFO: Observed &Deployment event: MODIFIED
Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tmlrc-85f747b754" has successfully progressed.}
Nov 30 03:35:32.529: INFO: Found Deployment test-deployment-tmlrc in namespace deployment-628 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 30 03:35:32.529: INFO: Deployment test-deployment-tmlrc has an updated status
STEP: patching the Statefulset Status 11/30/22 03:35:32.529
Nov 30 03:35:32.529: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 30 03:35:32.536: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 11/30/22 03:35:32.536
Nov 30 03:35:32.537: INFO: Observed &Deployment event: ADDED
Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tmlrc-85f747b754"}
Nov 30 03:35:32.537: INFO: Observed &Deployment event: MODIFIED
Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tmlrc-85f747b754"}
Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 30 03:35:32.537: INFO: Observed &Deployment event: MODIFIED
Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tmlrc-85f747b754" is progressing.}
Nov 30 03:35:32.537: INFO: Observed &Deployment event: MODIFIED
Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tmlrc-85f747b754" has successfully progressed.}
Nov 30 03:35:32.537: INFO: Observed &Deployment event: MODIFIED
Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tmlrc-85f747b754" has successfully progressed.}
Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 30 03:35:32.538: INFO: Observed &Deployment event: MODIFIED
Nov 30 03:35:32.538: INFO: Found deployment test-deployment-tmlrc in namespace deployment-628 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Nov 30 03:35:32.538: INFO: Deployment test-deployment-tmlrc has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 30 03:35:32.542: INFO: Deployment "test-deployment-tmlrc":
&Deployment{ObjectMeta:{test-deployment-tmlrc  deployment-628  6071551d-deb4-47fb-b42e-50447dc7333f 21244 1 2022-11-30 03:35:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-11-30 03:35:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-11-30 03:35:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-11-30 03:35:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00239b628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-tmlrc-85f747b754",LastUpdateTime:2022-11-30 03:35:32 +0000 UTC,LastTransitionTime:2022-11-30 03:35:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 30 03:35:32.545: INFO: New ReplicaSet "test-deployment-tmlrc-85f747b754" of Deployment "test-deployment-tmlrc":
&ReplicaSet{ObjectMeta:{test-deployment-tmlrc-85f747b754  deployment-628  fcb75d4d-92f2-44f9-9067-1dc092753607 21240 1 2022-11-30 03:35:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:85f747b754] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-tmlrc 6071551d-deb4-47fb-b42e-50447dc7333f 0xc00239ba60 0xc00239ba61}] [] [{kube-controller-manager Update apps/v1 2022-11-30 03:35:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6071551d-deb4-47fb-b42e-50447dc7333f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 03:35:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 85f747b754,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:85f747b754] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00239bb08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 30 03:35:32.547: INFO: Pod "test-deployment-tmlrc-85f747b754-8zgbx" is available:
&Pod{ObjectMeta:{test-deployment-tmlrc-85f747b754-8zgbx test-deployment-tmlrc-85f747b754- deployment-628  2b10c681-7890-428d-989f-fee82e68611d 21239 0 2022-11-30 03:35:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:85f747b754] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.248"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.248"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-tmlrc-85f747b754 fcb75d4d-92f2-44f9-9067-1dc092753607 0xc000165730 0xc000165731}] [] [{kube-controller-manager Update v1 2022-11-30 03:35:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcb75d4d-92f2-44f9-9067-1dc092753607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 03:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 03:35:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lxqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lxqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 03:35:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 03:35:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 03:35:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 03:35:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.248,StartTime:2022-11-30 03:35:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 03:35:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://40daec50b589663431148b0a4e42332605ef722f671c1e330fd3bc9242c7d6d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Nov 30 03:35:32.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-628" for this suite. 11/30/22 03:35:32.55
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":37,"skipped":619,"failed":0}
------------------------------
• [2.100 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:35:30.456
    Nov 30 03:35:30.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename deployment 11/30/22 03:35:30.457
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:35:30.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:35:30.482
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 11/30/22 03:35:30.494
    Nov 30 03:35:30.494: INFO: Creating simple deployment test-deployment-tmlrc
    Nov 30 03:35:30.504: INFO: new replicaset for deployment "test-deployment-tmlrc" is yet to be created
    STEP: Getting /status 11/30/22 03:35:32.515
    Nov 30 03:35:32.519: INFO: Deployment test-deployment-tmlrc has Conditions: [{Available True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tmlrc-85f747b754" has successfully progressed.}]
    STEP: updating Deployment Status 11/30/22 03:35:32.519
    Nov 30 03:35:32.527: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 3, 35, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 3, 35, 32, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 3, 35, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 3, 35, 30, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-tmlrc-85f747b754\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 11/30/22 03:35:32.527
    Nov 30 03:35:32.529: INFO: Observed &Deployment event: ADDED
    Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tmlrc-85f747b754"}
    Nov 30 03:35:32.529: INFO: Observed &Deployment event: MODIFIED
    Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tmlrc-85f747b754"}
    Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 30 03:35:32.529: INFO: Observed &Deployment event: MODIFIED
    Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tmlrc-85f747b754" is progressing.}
    Nov 30 03:35:32.529: INFO: Observed &Deployment event: MODIFIED
    Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tmlrc-85f747b754" has successfully progressed.}
    Nov 30 03:35:32.529: INFO: Observed &Deployment event: MODIFIED
    Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 30 03:35:32.529: INFO: Observed Deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tmlrc-85f747b754" has successfully progressed.}
    Nov 30 03:35:32.529: INFO: Found Deployment test-deployment-tmlrc in namespace deployment-628 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 30 03:35:32.529: INFO: Deployment test-deployment-tmlrc has an updated status
    STEP: patching the Statefulset Status 11/30/22 03:35:32.529
    Nov 30 03:35:32.529: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Nov 30 03:35:32.536: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 11/30/22 03:35:32.536
    Nov 30 03:35:32.537: INFO: Observed &Deployment event: ADDED
    Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tmlrc-85f747b754"}
    Nov 30 03:35:32.537: INFO: Observed &Deployment event: MODIFIED
    Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tmlrc-85f747b754"}
    Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 30 03:35:32.537: INFO: Observed &Deployment event: MODIFIED
    Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:30 +0000 UTC 2022-11-30 03:35:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tmlrc-85f747b754" is progressing.}
    Nov 30 03:35:32.537: INFO: Observed &Deployment event: MODIFIED
    Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tmlrc-85f747b754" has successfully progressed.}
    Nov 30 03:35:32.537: INFO: Observed &Deployment event: MODIFIED
    Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-11-30 03:35:32 +0000 UTC 2022-11-30 03:35:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tmlrc-85f747b754" has successfully progressed.}
    Nov 30 03:35:32.537: INFO: Observed deployment test-deployment-tmlrc in namespace deployment-628 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 30 03:35:32.538: INFO: Observed &Deployment event: MODIFIED
    Nov 30 03:35:32.538: INFO: Found deployment test-deployment-tmlrc in namespace deployment-628 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Nov 30 03:35:32.538: INFO: Deployment test-deployment-tmlrc has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 30 03:35:32.542: INFO: Deployment "test-deployment-tmlrc":
    &Deployment{ObjectMeta:{test-deployment-tmlrc  deployment-628  6071551d-deb4-47fb-b42e-50447dc7333f 21244 1 2022-11-30 03:35:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-11-30 03:35:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-11-30 03:35:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-11-30 03:35:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00239b628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-tmlrc-85f747b754",LastUpdateTime:2022-11-30 03:35:32 +0000 UTC,LastTransitionTime:2022-11-30 03:35:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 30 03:35:32.545: INFO: New ReplicaSet "test-deployment-tmlrc-85f747b754" of Deployment "test-deployment-tmlrc":
    &ReplicaSet{ObjectMeta:{test-deployment-tmlrc-85f747b754  deployment-628  fcb75d4d-92f2-44f9-9067-1dc092753607 21240 1 2022-11-30 03:35:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:85f747b754] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-tmlrc 6071551d-deb4-47fb-b42e-50447dc7333f 0xc00239ba60 0xc00239ba61}] [] [{kube-controller-manager Update apps/v1 2022-11-30 03:35:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6071551d-deb4-47fb-b42e-50447dc7333f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 03:35:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 85f747b754,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:85f747b754] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00239bb08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 30 03:35:32.547: INFO: Pod "test-deployment-tmlrc-85f747b754-8zgbx" is available:
    &Pod{ObjectMeta:{test-deployment-tmlrc-85f747b754-8zgbx test-deployment-tmlrc-85f747b754- deployment-628  2b10c681-7890-428d-989f-fee82e68611d 21239 0 2022-11-30 03:35:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:85f747b754] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.248"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.248"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-tmlrc-85f747b754 fcb75d4d-92f2-44f9-9067-1dc092753607 0xc000165730 0xc000165731}] [] [{kube-controller-manager Update v1 2022-11-30 03:35:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcb75d4d-92f2-44f9-9067-1dc092753607\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 03:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 03:35:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lxqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lxqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 03:35:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 03:35:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 03:35:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 03:35:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.248,StartTime:2022-11-30 03:35:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 03:35:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://40daec50b589663431148b0a4e42332605ef722f671c1e330fd3bc9242c7d6d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Nov 30 03:35:32.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-628" for this suite. 11/30/22 03:35:32.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:35:32.557
Nov 30 03:35:32.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-lifecycle-hook 11/30/22 03:35:32.557
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:35:32.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:35:32.582
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 11/30/22 03:35:32.594
Nov 30 03:35:32.623: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8270" to be "running and ready"
Nov 30 03:35:32.629: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.794122ms
Nov 30 03:35:32.629: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:35:34.633: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009915269s
Nov 30 03:35:34.633: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 30 03:35:34.633: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 11/30/22 03:35:34.635
Nov 30 03:35:34.642: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8270" to be "running and ready"
Nov 30 03:35:34.644: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09854ms
Nov 30 03:35:34.644: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:35:36.648: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006188674s
Nov 30 03:35:36.648: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Nov 30 03:35:36.648: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 11/30/22 03:35:36.651
STEP: delete the pod with lifecycle hook 11/30/22 03:35:36.661
Nov 30 03:35:36.670: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 30 03:35:36.679: INFO: Pod pod-with-poststart-http-hook still exists
Nov 30 03:35:38.679: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 30 03:35:38.690: INFO: Pod pod-with-poststart-http-hook still exists
Nov 30 03:35:40.680: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 30 03:35:40.683: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Nov 30 03:35:40.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8270" for this suite. 11/30/22 03:35:40.687
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":38,"skipped":626,"failed":0}
------------------------------
• [SLOW TEST] [8.135 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:35:32.557
    Nov 30 03:35:32.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/30/22 03:35:32.557
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:35:32.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:35:32.582
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 11/30/22 03:35:32.594
    Nov 30 03:35:32.623: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8270" to be "running and ready"
    Nov 30 03:35:32.629: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.794122ms
    Nov 30 03:35:32.629: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:35:34.633: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009915269s
    Nov 30 03:35:34.633: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 30 03:35:34.633: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 11/30/22 03:35:34.635
    Nov 30 03:35:34.642: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8270" to be "running and ready"
    Nov 30 03:35:34.644: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09854ms
    Nov 30 03:35:34.644: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:35:36.648: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006188674s
    Nov 30 03:35:36.648: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Nov 30 03:35:36.648: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 11/30/22 03:35:36.651
    STEP: delete the pod with lifecycle hook 11/30/22 03:35:36.661
    Nov 30 03:35:36.670: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Nov 30 03:35:36.679: INFO: Pod pod-with-poststart-http-hook still exists
    Nov 30 03:35:38.679: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Nov 30 03:35:38.690: INFO: Pod pod-with-poststart-http-hook still exists
    Nov 30 03:35:40.680: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Nov 30 03:35:40.683: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Nov 30 03:35:40.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8270" for this suite. 11/30/22 03:35:40.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:35:40.693
Nov 30 03:35:40.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename statefulset 11/30/22 03:35:40.694
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:35:40.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:35:40.713
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7080 11/30/22 03:35:40.715
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 11/30/22 03:35:40.727
STEP: Creating pod with conflicting port in namespace statefulset-7080 11/30/22 03:35:40.732
W1130 03:35:40.760087      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
STEP: Waiting until pod test-pod will start running in namespace statefulset-7080 11/30/22 03:35:40.76
Nov 30 03:35:40.760: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7080" to be "running"
Nov 30 03:35:40.762: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.492446ms
Nov 30 03:35:42.766: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006464592s
Nov 30 03:35:42.766: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-7080 11/30/22 03:35:42.766
W1130 03:35:42.775704      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7080 11/30/22 03:35:42.775
Nov 30 03:35:42.824: INFO: Observed stateful pod in namespace: statefulset-7080, name: ss-0, uid: ff492c24-dc00-4877-8885-d6684baffe50, status phase: Pending. Waiting for statefulset controller to delete.
Nov 30 03:35:42.857: INFO: Observed stateful pod in namespace: statefulset-7080, name: ss-0, uid: ff492c24-dc00-4877-8885-d6684baffe50, status phase: Failed. Waiting for statefulset controller to delete.
Nov 30 03:35:42.870: INFO: Observed stateful pod in namespace: statefulset-7080, name: ss-0, uid: ff492c24-dc00-4877-8885-d6684baffe50, status phase: Failed. Waiting for statefulset controller to delete.
Nov 30 03:35:42.876: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7080
STEP: Removing pod with conflicting port in namespace statefulset-7080 11/30/22 03:35:42.876
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7080 and will be in running state 11/30/22 03:35:42.895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Nov 30 03:35:44.906: INFO: Deleting all statefulset in ns statefulset-7080
Nov 30 03:35:44.909: INFO: Scaling statefulset ss to 0
W1130 03:35:44.916976      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
Nov 30 03:35:54.925: INFO: Waiting for statefulset status.replicas updated to 0
Nov 30 03:35:54.927: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Nov 30 03:35:54.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7080" for this suite. 11/30/22 03:35:54.942
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":39,"skipped":655,"failed":0}
------------------------------
• [SLOW TEST] [14.257 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:35:40.693
    Nov 30 03:35:40.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename statefulset 11/30/22 03:35:40.694
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:35:40.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:35:40.713
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7080 11/30/22 03:35:40.715
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 11/30/22 03:35:40.727
    STEP: Creating pod with conflicting port in namespace statefulset-7080 11/30/22 03:35:40.732
    W1130 03:35:40.760087      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
    STEP: Waiting until pod test-pod will start running in namespace statefulset-7080 11/30/22 03:35:40.76
    Nov 30 03:35:40.760: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7080" to be "running"
    Nov 30 03:35:40.762: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.492446ms
    Nov 30 03:35:42.766: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006464592s
    Nov 30 03:35:42.766: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-7080 11/30/22 03:35:42.766
    W1130 03:35:42.775704      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7080 11/30/22 03:35:42.775
    Nov 30 03:35:42.824: INFO: Observed stateful pod in namespace: statefulset-7080, name: ss-0, uid: ff492c24-dc00-4877-8885-d6684baffe50, status phase: Pending. Waiting for statefulset controller to delete.
    Nov 30 03:35:42.857: INFO: Observed stateful pod in namespace: statefulset-7080, name: ss-0, uid: ff492c24-dc00-4877-8885-d6684baffe50, status phase: Failed. Waiting for statefulset controller to delete.
    Nov 30 03:35:42.870: INFO: Observed stateful pod in namespace: statefulset-7080, name: ss-0, uid: ff492c24-dc00-4877-8885-d6684baffe50, status phase: Failed. Waiting for statefulset controller to delete.
    Nov 30 03:35:42.876: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7080
    STEP: Removing pod with conflicting port in namespace statefulset-7080 11/30/22 03:35:42.876
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7080 and will be in running state 11/30/22 03:35:42.895
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Nov 30 03:35:44.906: INFO: Deleting all statefulset in ns statefulset-7080
    Nov 30 03:35:44.909: INFO: Scaling statefulset ss to 0
    W1130 03:35:44.916976      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
    Nov 30 03:35:54.925: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 30 03:35:54.927: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Nov 30 03:35:54.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7080" for this suite. 11/30/22 03:35:54.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:35:54.95
Nov 30 03:35:54.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 03:35:54.951
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:35:54.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:35:54.976
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 03:35:55.016
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 03:35:55.29
STEP: Deploying the webhook pod 11/30/22 03:35:55.301
STEP: Wait for the deployment to be ready 11/30/22 03:35:55.311
Nov 30 03:35:55.316: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 11/30/22 03:35:57.325
STEP: Verifying the service has paired with the endpoint 11/30/22 03:35:57.34
Nov 30 03:35:58.340: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 11/30/22 03:35:58.343
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 11/30/22 03:35:58.344
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 11/30/22 03:35:58.344
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 11/30/22 03:35:58.344
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 11/30/22 03:35:58.345
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 11/30/22 03:35:58.345
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 11/30/22 03:35:58.346
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:35:58.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5051" for this suite. 11/30/22 03:35:58.35
STEP: Destroying namespace "webhook-5051-markers" for this suite. 11/30/22 03:35:58.355
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":40,"skipped":660,"failed":0}
------------------------------
• [3.466 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:35:54.95
    Nov 30 03:35:54.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 03:35:54.951
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:35:54.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:35:54.976
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 03:35:55.016
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 03:35:55.29
    STEP: Deploying the webhook pod 11/30/22 03:35:55.301
    STEP: Wait for the deployment to be ready 11/30/22 03:35:55.311
    Nov 30 03:35:55.316: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 11/30/22 03:35:57.325
    STEP: Verifying the service has paired with the endpoint 11/30/22 03:35:57.34
    Nov 30 03:35:58.340: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 11/30/22 03:35:58.343
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 11/30/22 03:35:58.344
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 11/30/22 03:35:58.344
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 11/30/22 03:35:58.344
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 11/30/22 03:35:58.345
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 11/30/22 03:35:58.345
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 11/30/22 03:35:58.346
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:35:58.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5051" for this suite. 11/30/22 03:35:58.35
    STEP: Destroying namespace "webhook-5051-markers" for this suite. 11/30/22 03:35:58.355
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:35:58.418
Nov 30 03:35:58.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 03:35:58.418
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:35:58.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:35:58.508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 03:35:58.536
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 03:35:58.888
STEP: Deploying the webhook pod 11/30/22 03:35:58.893
STEP: Wait for the deployment to be ready 11/30/22 03:35:58.906
Nov 30 03:35:58.913: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 03:36:00.921
STEP: Verifying the service has paired with the endpoint 11/30/22 03:36:00.947
Nov 30 03:36:01.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 11/30/22 03:36:01.95
STEP: create a namespace for the webhook 11/30/22 03:36:01.962
STEP: create a configmap should be unconditionally rejected by the webhook 11/30/22 03:36:01.969
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:36:01.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4331" for this suite. 11/30/22 03:36:02.002
STEP: Destroying namespace "webhook-4331-markers" for this suite. 11/30/22 03:36:02.007
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":41,"skipped":691,"failed":0}
------------------------------
• [3.640 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:35:58.418
    Nov 30 03:35:58.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 03:35:58.418
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:35:58.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:35:58.508
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 03:35:58.536
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 03:35:58.888
    STEP: Deploying the webhook pod 11/30/22 03:35:58.893
    STEP: Wait for the deployment to be ready 11/30/22 03:35:58.906
    Nov 30 03:35:58.913: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 03:36:00.921
    STEP: Verifying the service has paired with the endpoint 11/30/22 03:36:00.947
    Nov 30 03:36:01.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 11/30/22 03:36:01.95
    STEP: create a namespace for the webhook 11/30/22 03:36:01.962
    STEP: create a configmap should be unconditionally rejected by the webhook 11/30/22 03:36:01.969
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:36:01.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4331" for this suite. 11/30/22 03:36:02.002
    STEP: Destroying namespace "webhook-4331-markers" for this suite. 11/30/22 03:36:02.007
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:36:02.058
Nov 30 03:36:02.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 03:36:02.058
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:02.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:02.093
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 11/30/22 03:36:02.095
Nov 30 03:36:02.129: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50" in namespace "downward-api-6922" to be "Succeeded or Failed"
Nov 30 03:36:02.134: INFO: Pod "downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50": Phase="Pending", Reason="", readiness=false. Elapsed: 5.521226ms
Nov 30 03:36:04.145: INFO: Pod "downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016553721s
Nov 30 03:36:06.139: INFO: Pod "downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009935571s
STEP: Saw pod success 11/30/22 03:36:06.139
Nov 30 03:36:06.139: INFO: Pod "downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50" satisfied condition "Succeeded or Failed"
Nov 30 03:36:06.142: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50 container client-container: <nil>
STEP: delete the pod 11/30/22 03:36:06.147
Nov 30 03:36:06.160: INFO: Waiting for pod downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50 to disappear
Nov 30 03:36:06.163: INFO: Pod downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Nov 30 03:36:06.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6922" for this suite. 11/30/22 03:36:06.166
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":42,"skipped":709,"failed":0}
------------------------------
• [4.115 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:36:02.058
    Nov 30 03:36:02.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 03:36:02.058
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:02.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:02.093
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 11/30/22 03:36:02.095
    Nov 30 03:36:02.129: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50" in namespace "downward-api-6922" to be "Succeeded or Failed"
    Nov 30 03:36:02.134: INFO: Pod "downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50": Phase="Pending", Reason="", readiness=false. Elapsed: 5.521226ms
    Nov 30 03:36:04.145: INFO: Pod "downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016553721s
    Nov 30 03:36:06.139: INFO: Pod "downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009935571s
    STEP: Saw pod success 11/30/22 03:36:06.139
    Nov 30 03:36:06.139: INFO: Pod "downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50" satisfied condition "Succeeded or Failed"
    Nov 30 03:36:06.142: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50 container client-container: <nil>
    STEP: delete the pod 11/30/22 03:36:06.147
    Nov 30 03:36:06.160: INFO: Waiting for pod downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50 to disappear
    Nov 30 03:36:06.163: INFO: Pod downwardapi-volume-1f5fa4a1-3aed-4de0-80b1-50ec6a3c8d50 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Nov 30 03:36:06.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6922" for this suite. 11/30/22 03:36:06.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:36:06.173
Nov 30 03:36:06.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:36:06.174
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:06.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:06.191
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 11/30/22 03:36:06.193
Nov 30 03:36:06.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51" in namespace "projected-5603" to be "Succeeded or Failed"
Nov 30 03:36:06.215: INFO: Pod "downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51": Phase="Pending", Reason="", readiness=false. Elapsed: 13.960215ms
Nov 30 03:36:08.219: INFO: Pod "downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018109399s
Nov 30 03:36:10.220: INFO: Pod "downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019020437s
STEP: Saw pod success 11/30/22 03:36:10.22
Nov 30 03:36:10.220: INFO: Pod "downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51" satisfied condition "Succeeded or Failed"
Nov 30 03:36:10.223: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51 container client-container: <nil>
STEP: delete the pod 11/30/22 03:36:10.227
Nov 30 03:36:10.237: INFO: Waiting for pod downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51 to disappear
Nov 30 03:36:10.239: INFO: Pod downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Nov 30 03:36:10.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5603" for this suite. 11/30/22 03:36:10.243
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":43,"skipped":720,"failed":0}
------------------------------
• [4.074 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:36:06.173
    Nov 30 03:36:06.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:36:06.174
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:06.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:06.191
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 11/30/22 03:36:06.193
    Nov 30 03:36:06.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51" in namespace "projected-5603" to be "Succeeded or Failed"
    Nov 30 03:36:06.215: INFO: Pod "downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51": Phase="Pending", Reason="", readiness=false. Elapsed: 13.960215ms
    Nov 30 03:36:08.219: INFO: Pod "downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018109399s
    Nov 30 03:36:10.220: INFO: Pod "downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019020437s
    STEP: Saw pod success 11/30/22 03:36:10.22
    Nov 30 03:36:10.220: INFO: Pod "downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51" satisfied condition "Succeeded or Failed"
    Nov 30 03:36:10.223: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51 container client-container: <nil>
    STEP: delete the pod 11/30/22 03:36:10.227
    Nov 30 03:36:10.237: INFO: Waiting for pod downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51 to disappear
    Nov 30 03:36:10.239: INFO: Pod downwardapi-volume-6c0b6b47-ddb5-4f56-893b-ae386835cf51 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Nov 30 03:36:10.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5603" for this suite. 11/30/22 03:36:10.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:36:10.249
Nov 30 03:36:10.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename dns 11/30/22 03:36:10.25
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:10.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:10.268
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 11/30/22 03:36:10.271
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1145 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1145;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1145 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1145;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1145.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1145.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1145.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1145.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1145.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1145.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1145.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1145.svc;check="$$(dig +notcp +noall +answer +search 28.97.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.97.28_udp@PTR;check="$$(dig +tcp +noall +answer +search 28.97.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.97.28_tcp@PTR;sleep 1; done
 11/30/22 03:36:10.291
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1145 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1145;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1145 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1145;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1145.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1145.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1145.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1145.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1145.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1145.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1145.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1145.svc;check="$$(dig +notcp +noall +answer +search 28.97.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.97.28_udp@PTR;check="$$(dig +tcp +noall +answer +search 28.97.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.97.28_tcp@PTR;sleep 1; done
 11/30/22 03:36:10.291
STEP: creating a pod to probe DNS 11/30/22 03:36:10.291
STEP: submitting the pod to kubernetes 11/30/22 03:36:10.291
Nov 30 03:36:10.306: INFO: Waiting up to 15m0s for pod "dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f" in namespace "dns-1145" to be "running"
Nov 30 03:36:10.311: INFO: Pod "dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.090815ms
Nov 30 03:36:12.315: INFO: Pod "dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009041479s
Nov 30 03:36:14.316: INFO: Pod "dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009998439s
Nov 30 03:36:16.316: INFO: Pod "dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f": Phase="Running", Reason="", readiness=true. Elapsed: 6.009535027s
Nov 30 03:36:16.316: INFO: Pod "dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f" satisfied condition "running"
STEP: retrieving the pod 11/30/22 03:36:16.316
STEP: looking for the results for each expected name from probers 11/30/22 03:36:16.319
Nov 30 03:36:16.332: INFO: Unable to read wheezy_udp@dns-test-service.dns-1145 from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.335: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1145 from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.339: INFO: Unable to read wheezy_udp@dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.343: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.346: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.352: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.366: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.368: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.370: INFO: Unable to read jessie_udp@dns-test-service.dns-1145 from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.373: INFO: Unable to read jessie_tcp@dns-test-service.dns-1145 from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.375: INFO: Unable to read jessie_udp@dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.378: INFO: Unable to read jessie_tcp@dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.380: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.382: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
Nov 30 03:36:16.403: INFO: Lookups using dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f failed for: [wheezy_udp@dns-test-service.dns-1145 wheezy_tcp@dns-test-service.dns-1145 wheezy_udp@dns-test-service.dns-1145.svc wheezy_tcp@dns-test-service.dns-1145.svc wheezy_udp@_http._tcp.dns-test-service.dns-1145.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1145.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1145 jessie_tcp@dns-test-service.dns-1145 jessie_udp@dns-test-service.dns-1145.svc jessie_tcp@dns-test-service.dns-1145.svc jessie_udp@_http._tcp.dns-test-service.dns-1145.svc jessie_tcp@_http._tcp.dns-test-service.dns-1145.svc]

Nov 30 03:36:21.481: INFO: DNS probes using dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f succeeded

STEP: deleting the pod 11/30/22 03:36:21.481
STEP: deleting the test service 11/30/22 03:36:21.49
STEP: deleting the test headless service 11/30/22 03:36:21.526
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Nov 30 03:36:21.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1145" for this suite. 11/30/22 03:36:21.545
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":44,"skipped":791,"failed":0}
------------------------------
• [SLOW TEST] [11.305 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:36:10.249
    Nov 30 03:36:10.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename dns 11/30/22 03:36:10.25
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:10.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:10.268
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 11/30/22 03:36:10.271
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1145 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1145;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1145 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1145;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1145.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1145.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1145.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1145.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1145.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1145.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1145.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1145.svc;check="$$(dig +notcp +noall +answer +search 28.97.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.97.28_udp@PTR;check="$$(dig +tcp +noall +answer +search 28.97.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.97.28_tcp@PTR;sleep 1; done
     11/30/22 03:36:10.291
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1145 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1145;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1145 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1145;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1145.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1145.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1145.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1145.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1145.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1145.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1145.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1145.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1145.svc;check="$$(dig +notcp +noall +answer +search 28.97.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.97.28_udp@PTR;check="$$(dig +tcp +noall +answer +search 28.97.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.97.28_tcp@PTR;sleep 1; done
     11/30/22 03:36:10.291
    STEP: creating a pod to probe DNS 11/30/22 03:36:10.291
    STEP: submitting the pod to kubernetes 11/30/22 03:36:10.291
    Nov 30 03:36:10.306: INFO: Waiting up to 15m0s for pod "dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f" in namespace "dns-1145" to be "running"
    Nov 30 03:36:10.311: INFO: Pod "dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.090815ms
    Nov 30 03:36:12.315: INFO: Pod "dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009041479s
    Nov 30 03:36:14.316: INFO: Pod "dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009998439s
    Nov 30 03:36:16.316: INFO: Pod "dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f": Phase="Running", Reason="", readiness=true. Elapsed: 6.009535027s
    Nov 30 03:36:16.316: INFO: Pod "dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f" satisfied condition "running"
    STEP: retrieving the pod 11/30/22 03:36:16.316
    STEP: looking for the results for each expected name from probers 11/30/22 03:36:16.319
    Nov 30 03:36:16.332: INFO: Unable to read wheezy_udp@dns-test-service.dns-1145 from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.335: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1145 from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.339: INFO: Unable to read wheezy_udp@dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.343: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.346: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.352: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.366: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.368: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.370: INFO: Unable to read jessie_udp@dns-test-service.dns-1145 from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.373: INFO: Unable to read jessie_tcp@dns-test-service.dns-1145 from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.375: INFO: Unable to read jessie_udp@dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.378: INFO: Unable to read jessie_tcp@dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.380: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.382: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1145.svc from pod dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f: the server could not find the requested resource (get pods dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f)
    Nov 30 03:36:16.403: INFO: Lookups using dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f failed for: [wheezy_udp@dns-test-service.dns-1145 wheezy_tcp@dns-test-service.dns-1145 wheezy_udp@dns-test-service.dns-1145.svc wheezy_tcp@dns-test-service.dns-1145.svc wheezy_udp@_http._tcp.dns-test-service.dns-1145.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1145.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1145 jessie_tcp@dns-test-service.dns-1145 jessie_udp@dns-test-service.dns-1145.svc jessie_tcp@dns-test-service.dns-1145.svc jessie_udp@_http._tcp.dns-test-service.dns-1145.svc jessie_tcp@_http._tcp.dns-test-service.dns-1145.svc]

    Nov 30 03:36:21.481: INFO: DNS probes using dns-1145/dns-test-bc83428c-aa5c-4124-b0d1-66c94b1af25f succeeded

    STEP: deleting the pod 11/30/22 03:36:21.481
    STEP: deleting the test service 11/30/22 03:36:21.49
    STEP: deleting the test headless service 11/30/22 03:36:21.526
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Nov 30 03:36:21.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1145" for this suite. 11/30/22 03:36:21.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:36:21.555
Nov 30 03:36:21.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename dns 11/30/22 03:36:21.556
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:21.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:21.585
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 11/30/22 03:36:21.588
Nov 30 03:36:21.620: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-659  c60113f6-333d-4517-954b-a56db3b0b54c 22004 0 2022-11-30 03:36:21 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-11-30 03:36:21 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6xbcx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6xbcx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 03:36:21.620: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-659" to be "running and ready"
Nov 30 03:36:21.629: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.797063ms
Nov 30 03:36:21.629: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:36:23.635: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.014661668s
Nov 30 03:36:23.635: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Nov 30 03:36:23.635: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 11/30/22 03:36:23.635
Nov 30 03:36:23.635: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-659 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:36:23.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:36:23.636: INFO: ExecWithOptions: Clientset creation
Nov 30 03:36:23.636: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-659/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 11/30/22 03:36:23.721
Nov 30 03:36:23.721: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-659 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:36:23.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:36:23.721: INFO: ExecWithOptions: Clientset creation
Nov 30 03:36:23.721: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-659/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 30 03:36:23.794: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Nov 30 03:36:23.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-659" for this suite. 11/30/22 03:36:23.809
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":45,"skipped":801,"failed":0}
------------------------------
• [2.260 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:36:21.555
    Nov 30 03:36:21.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename dns 11/30/22 03:36:21.556
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:21.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:21.585
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 11/30/22 03:36:21.588
    Nov 30 03:36:21.620: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-659  c60113f6-333d-4517-954b-a56db3b0b54c 22004 0 2022-11-30 03:36:21 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-11-30 03:36:21 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6xbcx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6xbcx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 03:36:21.620: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-659" to be "running and ready"
    Nov 30 03:36:21.629: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.797063ms
    Nov 30 03:36:21.629: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:36:23.635: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.014661668s
    Nov 30 03:36:23.635: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Nov 30 03:36:23.635: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 11/30/22 03:36:23.635
    Nov 30 03:36:23.635: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-659 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:36:23.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:36:23.636: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:36:23.636: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-659/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 11/30/22 03:36:23.721
    Nov 30 03:36:23.721: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-659 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:36:23.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:36:23.721: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:36:23.721: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-659/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 30 03:36:23.794: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Nov 30 03:36:23.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-659" for this suite. 11/30/22 03:36:23.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:36:23.815
Nov 30 03:36:23.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename namespaces 11/30/22 03:36:23.816
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:23.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:23.837
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 11/30/22 03:36:23.839
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:23.856
STEP: Creating a pod in the namespace 11/30/22 03:36:23.859
STEP: Waiting for the pod to have running status 11/30/22 03:36:23.87
Nov 30 03:36:23.870: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6026" to be "running"
Nov 30 03:36:23.872: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.264102ms
Nov 30 03:36:25.877: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006857443s
Nov 30 03:36:25.877: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 11/30/22 03:36:25.877
STEP: Waiting for the namespace to be removed. 11/30/22 03:36:25.882
STEP: Recreating the namespace 11/30/22 03:36:36.886
STEP: Verifying there are no pods in the namespace 11/30/22 03:36:36.902
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Nov 30 03:36:36.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9043" for this suite. 11/30/22 03:36:36.91
STEP: Destroying namespace "nsdeletetest-6026" for this suite. 11/30/22 03:36:36.916
Nov 30 03:36:36.922: INFO: Namespace nsdeletetest-6026 was already deleted
STEP: Destroying namespace "nsdeletetest-8164" for this suite. 11/30/22 03:36:36.922
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":46,"skipped":809,"failed":0}
------------------------------
• [SLOW TEST] [13.112 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:36:23.815
    Nov 30 03:36:23.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename namespaces 11/30/22 03:36:23.816
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:23.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:23.837
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 11/30/22 03:36:23.839
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:23.856
    STEP: Creating a pod in the namespace 11/30/22 03:36:23.859
    STEP: Waiting for the pod to have running status 11/30/22 03:36:23.87
    Nov 30 03:36:23.870: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6026" to be "running"
    Nov 30 03:36:23.872: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.264102ms
    Nov 30 03:36:25.877: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006857443s
    Nov 30 03:36:25.877: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 11/30/22 03:36:25.877
    STEP: Waiting for the namespace to be removed. 11/30/22 03:36:25.882
    STEP: Recreating the namespace 11/30/22 03:36:36.886
    STEP: Verifying there are no pods in the namespace 11/30/22 03:36:36.902
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 03:36:36.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9043" for this suite. 11/30/22 03:36:36.91
    STEP: Destroying namespace "nsdeletetest-6026" for this suite. 11/30/22 03:36:36.916
    Nov 30 03:36:36.922: INFO: Namespace nsdeletetest-6026 was already deleted
    STEP: Destroying namespace "nsdeletetest-8164" for this suite. 11/30/22 03:36:36.922
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:36:36.928
Nov 30 03:36:36.929: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:36:36.93
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:36.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:36.954
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 11/30/22 03:36:36.956
Nov 30 03:36:36.985: INFO: Waiting up to 5m0s for pod "downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a" in namespace "projected-6705" to be "Succeeded or Failed"
Nov 30 03:36:36.988: INFO: Pod "downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.94821ms
Nov 30 03:36:38.993: INFO: Pod "downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007413582s
Nov 30 03:36:40.993: INFO: Pod "downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007694522s
STEP: Saw pod success 11/30/22 03:36:40.993
Nov 30 03:36:40.993: INFO: Pod "downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a" satisfied condition "Succeeded or Failed"
Nov 30 03:36:40.996: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a container client-container: <nil>
STEP: delete the pod 11/30/22 03:36:41
Nov 30 03:36:41.011: INFO: Waiting for pod downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a to disappear
Nov 30 03:36:41.014: INFO: Pod downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Nov 30 03:36:41.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6705" for this suite. 11/30/22 03:36:41.018
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":47,"skipped":811,"failed":0}
------------------------------
• [4.095 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:36:36.928
    Nov 30 03:36:36.929: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:36:36.93
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:36.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:36.954
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 11/30/22 03:36:36.956
    Nov 30 03:36:36.985: INFO: Waiting up to 5m0s for pod "downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a" in namespace "projected-6705" to be "Succeeded or Failed"
    Nov 30 03:36:36.988: INFO: Pod "downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.94821ms
    Nov 30 03:36:38.993: INFO: Pod "downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007413582s
    Nov 30 03:36:40.993: INFO: Pod "downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007694522s
    STEP: Saw pod success 11/30/22 03:36:40.993
    Nov 30 03:36:40.993: INFO: Pod "downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a" satisfied condition "Succeeded or Failed"
    Nov 30 03:36:40.996: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a container client-container: <nil>
    STEP: delete the pod 11/30/22 03:36:41
    Nov 30 03:36:41.011: INFO: Waiting for pod downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a to disappear
    Nov 30 03:36:41.014: INFO: Pod downwardapi-volume-695b0f9c-3384-4243-8a7c-aaf637f7447a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Nov 30 03:36:41.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6705" for this suite. 11/30/22 03:36:41.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:36:41.024
Nov 30 03:36:41.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 03:36:41.026
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:41.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:41.049
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-3543 11/30/22 03:36:41.052
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3543 to expose endpoints map[] 11/30/22 03:36:41.066
Nov 30 03:36:41.069: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Nov 30 03:36:42.094: INFO: successfully validated that service multi-endpoint-test in namespace services-3543 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3543 11/30/22 03:36:42.094
Nov 30 03:36:42.130: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3543" to be "running and ready"
Nov 30 03:36:42.134: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.461113ms
Nov 30 03:36:42.135: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:36:44.138: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008137403s
Nov 30 03:36:44.138: INFO: The phase of Pod pod1 is Running (Ready = true)
Nov 30 03:36:44.138: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3543 to expose endpoints map[pod1:[100]] 11/30/22 03:36:44.141
Nov 30 03:36:44.149: INFO: successfully validated that service multi-endpoint-test in namespace services-3543 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3543 11/30/22 03:36:44.149
Nov 30 03:36:44.156: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3543" to be "running and ready"
Nov 30 03:36:44.161: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.474818ms
Nov 30 03:36:44.161: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:36:46.164: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007852457s
Nov 30 03:36:46.164: INFO: The phase of Pod pod2 is Running (Ready = true)
Nov 30 03:36:46.164: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3543 to expose endpoints map[pod1:[100] pod2:[101]] 11/30/22 03:36:46.169
Nov 30 03:36:46.178: INFO: successfully validated that service multi-endpoint-test in namespace services-3543 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 11/30/22 03:36:46.178
Nov 30 03:36:46.179: INFO: Creating new exec pod
Nov 30 03:36:46.185: INFO: Waiting up to 5m0s for pod "execpod78m5m" in namespace "services-3543" to be "running"
Nov 30 03:36:46.188: INFO: Pod "execpod78m5m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949219ms
Nov 30 03:36:48.192: INFO: Pod "execpod78m5m": Phase="Running", Reason="", readiness=true. Elapsed: 2.006708796s
Nov 30 03:36:48.192: INFO: Pod "execpod78m5m" satisfied condition "running"
Nov 30 03:36:49.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3543 exec execpod78m5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Nov 30 03:36:49.350: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Nov 30 03:36:49.350: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:36:49.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3543 exec execpod78m5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.8.140 80'
Nov 30 03:36:49.480: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.8.140 80\nConnection to 10.109.8.140 80 port [tcp/http] succeeded!\n"
Nov 30 03:36:49.480: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:36:49.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3543 exec execpod78m5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Nov 30 03:36:49.607: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Nov 30 03:36:49.607: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:36:49.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3543 exec execpod78m5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.8.140 81'
Nov 30 03:36:49.719: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.8.140 81\nConnection to 10.109.8.140 81 port [tcp/*] succeeded!\n"
Nov 30 03:36:49.719: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3543 11/30/22 03:36:49.719
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3543 to expose endpoints map[pod2:[101]] 11/30/22 03:36:49.737
Nov 30 03:36:50.759: INFO: successfully validated that service multi-endpoint-test in namespace services-3543 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3543 11/30/22 03:36:50.759
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3543 to expose endpoints map[] 11/30/22 03:36:50.788
Nov 30 03:36:50.799: INFO: successfully validated that service multi-endpoint-test in namespace services-3543 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 03:36:50.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3543" for this suite. 11/30/22 03:36:50.833
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":48,"skipped":831,"failed":0}
------------------------------
• [SLOW TEST] [9.816 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:36:41.024
    Nov 30 03:36:41.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 03:36:41.026
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:41.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:41.049
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-3543 11/30/22 03:36:41.052
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3543 to expose endpoints map[] 11/30/22 03:36:41.066
    Nov 30 03:36:41.069: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Nov 30 03:36:42.094: INFO: successfully validated that service multi-endpoint-test in namespace services-3543 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3543 11/30/22 03:36:42.094
    Nov 30 03:36:42.130: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3543" to be "running and ready"
    Nov 30 03:36:42.134: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.461113ms
    Nov 30 03:36:42.135: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:36:44.138: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008137403s
    Nov 30 03:36:44.138: INFO: The phase of Pod pod1 is Running (Ready = true)
    Nov 30 03:36:44.138: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3543 to expose endpoints map[pod1:[100]] 11/30/22 03:36:44.141
    Nov 30 03:36:44.149: INFO: successfully validated that service multi-endpoint-test in namespace services-3543 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-3543 11/30/22 03:36:44.149
    Nov 30 03:36:44.156: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3543" to be "running and ready"
    Nov 30 03:36:44.161: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.474818ms
    Nov 30 03:36:44.161: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:36:46.164: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007852457s
    Nov 30 03:36:46.164: INFO: The phase of Pod pod2 is Running (Ready = true)
    Nov 30 03:36:46.164: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3543 to expose endpoints map[pod1:[100] pod2:[101]] 11/30/22 03:36:46.169
    Nov 30 03:36:46.178: INFO: successfully validated that service multi-endpoint-test in namespace services-3543 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 11/30/22 03:36:46.178
    Nov 30 03:36:46.179: INFO: Creating new exec pod
    Nov 30 03:36:46.185: INFO: Waiting up to 5m0s for pod "execpod78m5m" in namespace "services-3543" to be "running"
    Nov 30 03:36:46.188: INFO: Pod "execpod78m5m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949219ms
    Nov 30 03:36:48.192: INFO: Pod "execpod78m5m": Phase="Running", Reason="", readiness=true. Elapsed: 2.006708796s
    Nov 30 03:36:48.192: INFO: Pod "execpod78m5m" satisfied condition "running"
    Nov 30 03:36:49.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3543 exec execpod78m5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Nov 30 03:36:49.350: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Nov 30 03:36:49.350: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:36:49.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3543 exec execpod78m5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.8.140 80'
    Nov 30 03:36:49.480: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.8.140 80\nConnection to 10.109.8.140 80 port [tcp/http] succeeded!\n"
    Nov 30 03:36:49.480: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:36:49.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3543 exec execpod78m5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Nov 30 03:36:49.607: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Nov 30 03:36:49.607: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:36:49.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3543 exec execpod78m5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.8.140 81'
    Nov 30 03:36:49.719: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.8.140 81\nConnection to 10.109.8.140 81 port [tcp/*] succeeded!\n"
    Nov 30 03:36:49.719: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-3543 11/30/22 03:36:49.719
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3543 to expose endpoints map[pod2:[101]] 11/30/22 03:36:49.737
    Nov 30 03:36:50.759: INFO: successfully validated that service multi-endpoint-test in namespace services-3543 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-3543 11/30/22 03:36:50.759
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3543 to expose endpoints map[] 11/30/22 03:36:50.788
    Nov 30 03:36:50.799: INFO: successfully validated that service multi-endpoint-test in namespace services-3543 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 03:36:50.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3543" for this suite. 11/30/22 03:36:50.833
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:36:50.841
Nov 30 03:36:50.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 03:36:50.842
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:50.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:50.864
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 11/30/22 03:36:50.873
Nov 30 03:36:50.886: INFO: Waiting up to 5m0s for pod "pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad" in namespace "emptydir-6504" to be "Succeeded or Failed"
Nov 30 03:36:50.893: INFO: Pod "pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.367916ms
Nov 30 03:36:52.896: INFO: Pod "pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009835207s
Nov 30 03:36:54.896: INFO: Pod "pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009284415s
STEP: Saw pod success 11/30/22 03:36:54.896
Nov 30 03:36:54.896: INFO: Pod "pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad" satisfied condition "Succeeded or Failed"
Nov 30 03:36:54.898: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad container test-container: <nil>
STEP: delete the pod 11/30/22 03:36:54.903
Nov 30 03:36:54.913: INFO: Waiting for pod pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad to disappear
Nov 30 03:36:54.915: INFO: Pod pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 03:36:54.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6504" for this suite. 11/30/22 03:36:54.918
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":49,"skipped":853,"failed":0}
------------------------------
• [4.082 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:36:50.841
    Nov 30 03:36:50.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 03:36:50.842
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:50.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:50.864
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 11/30/22 03:36:50.873
    Nov 30 03:36:50.886: INFO: Waiting up to 5m0s for pod "pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad" in namespace "emptydir-6504" to be "Succeeded or Failed"
    Nov 30 03:36:50.893: INFO: Pod "pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.367916ms
    Nov 30 03:36:52.896: INFO: Pod "pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009835207s
    Nov 30 03:36:54.896: INFO: Pod "pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009284415s
    STEP: Saw pod success 11/30/22 03:36:54.896
    Nov 30 03:36:54.896: INFO: Pod "pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad" satisfied condition "Succeeded or Failed"
    Nov 30 03:36:54.898: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad container test-container: <nil>
    STEP: delete the pod 11/30/22 03:36:54.903
    Nov 30 03:36:54.913: INFO: Waiting for pod pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad to disappear
    Nov 30 03:36:54.915: INFO: Pod pod-fb805d2f-2b10-4c25-bcdd-1ce5d6b34bad no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 03:36:54.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6504" for this suite. 11/30/22 03:36:54.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:36:54.924
Nov 30 03:36:54.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename custom-resource-definition 11/30/22 03:36:54.924
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:54.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:54.939
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Nov 30 03:36:54.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:36:55.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1858" for this suite. 11/30/22 03:36:55.522
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":50,"skipped":868,"failed":0}
------------------------------
• [0.609 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:36:54.924
    Nov 30 03:36:54.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename custom-resource-definition 11/30/22 03:36:54.924
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:54.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:54.939
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Nov 30 03:36:54.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:36:55.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1858" for this suite. 11/30/22 03:36:55.522
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:36:55.533
Nov 30 03:36:55.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename security-context-test 11/30/22 03:36:55.534
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:55.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:55.598
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Nov 30 03:36:55.620: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-350b0fe1-4dd7-4761-b650-f5e8a9be539e" in namespace "security-context-test-9752" to be "Succeeded or Failed"
Nov 30 03:36:55.643: INFO: Pod "busybox-readonly-false-350b0fe1-4dd7-4761-b650-f5e8a9be539e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.177748ms
Nov 30 03:36:57.648: INFO: Pod "busybox-readonly-false-350b0fe1-4dd7-4761-b650-f5e8a9be539e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028359352s
Nov 30 03:36:59.647: INFO: Pod "busybox-readonly-false-350b0fe1-4dd7-4761-b650-f5e8a9be539e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027343641s
Nov 30 03:36:59.647: INFO: Pod "busybox-readonly-false-350b0fe1-4dd7-4761-b650-f5e8a9be539e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Nov 30 03:36:59.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9752" for this suite. 11/30/22 03:36:59.651
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":51,"skipped":872,"failed":0}
------------------------------
• [4.122 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:36:55.533
    Nov 30 03:36:55.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename security-context-test 11/30/22 03:36:55.534
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:55.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:55.598
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Nov 30 03:36:55.620: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-350b0fe1-4dd7-4761-b650-f5e8a9be539e" in namespace "security-context-test-9752" to be "Succeeded or Failed"
    Nov 30 03:36:55.643: INFO: Pod "busybox-readonly-false-350b0fe1-4dd7-4761-b650-f5e8a9be539e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.177748ms
    Nov 30 03:36:57.648: INFO: Pod "busybox-readonly-false-350b0fe1-4dd7-4761-b650-f5e8a9be539e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028359352s
    Nov 30 03:36:59.647: INFO: Pod "busybox-readonly-false-350b0fe1-4dd7-4761-b650-f5e8a9be539e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027343641s
    Nov 30 03:36:59.647: INFO: Pod "busybox-readonly-false-350b0fe1-4dd7-4761-b650-f5e8a9be539e" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Nov 30 03:36:59.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9752" for this suite. 11/30/22 03:36:59.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:36:59.655
Nov 30 03:36:59.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:36:59.656
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:59.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:59.678
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-cdc07226-3f89-4ece-849d-05a51dbd4e12 11/30/22 03:36:59.681
STEP: Creating a pod to test consume secrets 11/30/22 03:36:59.687
Nov 30 03:36:59.697: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67" in namespace "projected-6119" to be "Succeeded or Failed"
Nov 30 03:36:59.703: INFO: Pod "pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67": Phase="Pending", Reason="", readiness=false. Elapsed: 5.779114ms
Nov 30 03:37:01.707: INFO: Pod "pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009603828s
Nov 30 03:37:03.707: INFO: Pod "pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010235176s
STEP: Saw pod success 11/30/22 03:37:03.707
Nov 30 03:37:03.707: INFO: Pod "pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67" satisfied condition "Succeeded or Failed"
Nov 30 03:37:03.710: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67 container secret-volume-test: <nil>
STEP: delete the pod 11/30/22 03:37:03.714
Nov 30 03:37:03.726: INFO: Waiting for pod pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67 to disappear
Nov 30 03:37:03.728: INFO: Pod pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Nov 30 03:37:03.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6119" for this suite. 11/30/22 03:37:03.732
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":52,"skipped":883,"failed":0}
------------------------------
• [4.082 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:36:59.655
    Nov 30 03:36:59.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:36:59.656
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:36:59.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:36:59.678
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-cdc07226-3f89-4ece-849d-05a51dbd4e12 11/30/22 03:36:59.681
    STEP: Creating a pod to test consume secrets 11/30/22 03:36:59.687
    Nov 30 03:36:59.697: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67" in namespace "projected-6119" to be "Succeeded or Failed"
    Nov 30 03:36:59.703: INFO: Pod "pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67": Phase="Pending", Reason="", readiness=false. Elapsed: 5.779114ms
    Nov 30 03:37:01.707: INFO: Pod "pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009603828s
    Nov 30 03:37:03.707: INFO: Pod "pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010235176s
    STEP: Saw pod success 11/30/22 03:37:03.707
    Nov 30 03:37:03.707: INFO: Pod "pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67" satisfied condition "Succeeded or Failed"
    Nov 30 03:37:03.710: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67 container secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 03:37:03.714
    Nov 30 03:37:03.726: INFO: Waiting for pod pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67 to disappear
    Nov 30 03:37:03.728: INFO: Pod pod-projected-secrets-1f20b7e5-4fb6-40c5-8e07-3f36e1d3ca67 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Nov 30 03:37:03.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6119" for this suite. 11/30/22 03:37:03.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:37:03.738
Nov 30 03:37:03.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:37:03.739
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:03.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:03.755
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 11/30/22 03:37:03.757
Nov 30 03:37:03.780: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e" in namespace "projected-6833" to be "Succeeded or Failed"
Nov 30 03:37:03.796: INFO: Pod "downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.292473ms
Nov 30 03:37:05.800: INFO: Pod "downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019190922s
Nov 30 03:37:07.801: INFO: Pod "downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020741858s
STEP: Saw pod success 11/30/22 03:37:07.801
Nov 30 03:37:07.801: INFO: Pod "downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e" satisfied condition "Succeeded or Failed"
Nov 30 03:37:07.804: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e container client-container: <nil>
STEP: delete the pod 11/30/22 03:37:07.81
Nov 30 03:37:07.822: INFO: Waiting for pod downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e to disappear
Nov 30 03:37:07.830: INFO: Pod downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Nov 30 03:37:07.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6833" for this suite. 11/30/22 03:37:07.834
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":53,"skipped":927,"failed":0}
------------------------------
• [4.105 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:37:03.738
    Nov 30 03:37:03.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:37:03.739
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:03.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:03.755
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 11/30/22 03:37:03.757
    Nov 30 03:37:03.780: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e" in namespace "projected-6833" to be "Succeeded or Failed"
    Nov 30 03:37:03.796: INFO: Pod "downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.292473ms
    Nov 30 03:37:05.800: INFO: Pod "downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019190922s
    Nov 30 03:37:07.801: INFO: Pod "downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020741858s
    STEP: Saw pod success 11/30/22 03:37:07.801
    Nov 30 03:37:07.801: INFO: Pod "downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e" satisfied condition "Succeeded or Failed"
    Nov 30 03:37:07.804: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e container client-container: <nil>
    STEP: delete the pod 11/30/22 03:37:07.81
    Nov 30 03:37:07.822: INFO: Waiting for pod downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e to disappear
    Nov 30 03:37:07.830: INFO: Pod downwardapi-volume-11abda63-a19b-4182-9dda-c2dcab7fda3e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Nov 30 03:37:07.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6833" for this suite. 11/30/22 03:37:07.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:37:07.844
Nov 30 03:37:07.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename namespaces 11/30/22 03:37:07.844
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:07.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:07.863
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 11/30/22 03:37:07.865
Nov 30 03:37:07.869: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 11/30/22 03:37:07.869
Nov 30 03:37:07.873: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 11/30/22 03:37:07.873
Nov 30 03:37:07.881: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Nov 30 03:37:07.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8729" for this suite. 11/30/22 03:37:07.885
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":54,"skipped":941,"failed":0}
------------------------------
• [0.046 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:37:07.844
    Nov 30 03:37:07.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename namespaces 11/30/22 03:37:07.844
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:07.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:07.863
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 11/30/22 03:37:07.865
    Nov 30 03:37:07.869: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 11/30/22 03:37:07.869
    Nov 30 03:37:07.873: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 11/30/22 03:37:07.873
    Nov 30 03:37:07.881: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 03:37:07.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8729" for this suite. 11/30/22 03:37:07.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:37:07.89
Nov 30 03:37:07.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 03:37:07.891
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:07.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:07.907
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 11/30/22 03:37:07.91
Nov 30 03:37:07.919: INFO: Waiting up to 5m0s for pod "pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec" in namespace "emptydir-9347" to be "Succeeded or Failed"
Nov 30 03:37:07.922: INFO: Pod "pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.49892ms
Nov 30 03:37:09.926: INFO: Pod "pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006955342s
Nov 30 03:37:11.926: INFO: Pod "pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007386856s
STEP: Saw pod success 11/30/22 03:37:11.926
Nov 30 03:37:11.926: INFO: Pod "pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec" satisfied condition "Succeeded or Failed"
Nov 30 03:37:11.932: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec container test-container: <nil>
STEP: delete the pod 11/30/22 03:37:11.937
Nov 30 03:37:11.953: INFO: Waiting for pod pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec to disappear
Nov 30 03:37:11.957: INFO: Pod pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 03:37:11.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9347" for this suite. 11/30/22 03:37:11.96
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":55,"skipped":959,"failed":0}
------------------------------
• [4.079 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:37:07.89
    Nov 30 03:37:07.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 03:37:07.891
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:07.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:07.907
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 11/30/22 03:37:07.91
    Nov 30 03:37:07.919: INFO: Waiting up to 5m0s for pod "pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec" in namespace "emptydir-9347" to be "Succeeded or Failed"
    Nov 30 03:37:07.922: INFO: Pod "pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.49892ms
    Nov 30 03:37:09.926: INFO: Pod "pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006955342s
    Nov 30 03:37:11.926: INFO: Pod "pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007386856s
    STEP: Saw pod success 11/30/22 03:37:11.926
    Nov 30 03:37:11.926: INFO: Pod "pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec" satisfied condition "Succeeded or Failed"
    Nov 30 03:37:11.932: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec container test-container: <nil>
    STEP: delete the pod 11/30/22 03:37:11.937
    Nov 30 03:37:11.953: INFO: Waiting for pod pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec to disappear
    Nov 30 03:37:11.957: INFO: Pod pod-3b7f8fd9-5db5-4576-ab78-b3eb0acf34ec no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 03:37:11.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9347" for this suite. 11/30/22 03:37:11.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:37:11.97
Nov 30 03:37:11.970: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:37:11.971
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:11.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:11.992
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-068265b9-12a5-4f6d-8fc7-7c97995c854a 11/30/22 03:37:11.995
STEP: Creating a pod to test consume configMaps 11/30/22 03:37:12.001
Nov 30 03:37:12.017: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc" in namespace "projected-5414" to be "Succeeded or Failed"
Nov 30 03:37:12.022: INFO: Pod "pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.841284ms
Nov 30 03:37:14.027: INFO: Pod "pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0106679s
Nov 30 03:37:16.025: INFO: Pod "pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008372415s
STEP: Saw pod success 11/30/22 03:37:16.025
Nov 30 03:37:16.025: INFO: Pod "pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc" satisfied condition "Succeeded or Failed"
Nov 30 03:37:16.028: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc container agnhost-container: <nil>
STEP: delete the pod 11/30/22 03:37:16.033
Nov 30 03:37:16.042: INFO: Waiting for pod pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc to disappear
Nov 30 03:37:16.046: INFO: Pod pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Nov 30 03:37:16.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5414" for this suite. 11/30/22 03:37:16.05
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":56,"skipped":997,"failed":0}
------------------------------
• [4.085 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:37:11.97
    Nov 30 03:37:11.970: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:37:11.971
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:11.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:11.992
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-068265b9-12a5-4f6d-8fc7-7c97995c854a 11/30/22 03:37:11.995
    STEP: Creating a pod to test consume configMaps 11/30/22 03:37:12.001
    Nov 30 03:37:12.017: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc" in namespace "projected-5414" to be "Succeeded or Failed"
    Nov 30 03:37:12.022: INFO: Pod "pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.841284ms
    Nov 30 03:37:14.027: INFO: Pod "pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0106679s
    Nov 30 03:37:16.025: INFO: Pod "pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008372415s
    STEP: Saw pod success 11/30/22 03:37:16.025
    Nov 30 03:37:16.025: INFO: Pod "pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc" satisfied condition "Succeeded or Failed"
    Nov 30 03:37:16.028: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 03:37:16.033
    Nov 30 03:37:16.042: INFO: Waiting for pod pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc to disappear
    Nov 30 03:37:16.046: INFO: Pod pod-projected-configmaps-30f0ec18-611a-4a79-828e-c0664acfacfc no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Nov 30 03:37:16.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5414" for this suite. 11/30/22 03:37:16.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:37:16.057
Nov 30 03:37:16.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 03:37:16.058
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:16.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:16.078
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-2adcfb5c-5edc-4ffa-b10d-8796244e68db 11/30/22 03:37:16.081
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 03:37:16.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7399" for this suite. 11/30/22 03:37:16.087
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":57,"skipped":1075,"failed":0}
------------------------------
• [0.041 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:37:16.057
    Nov 30 03:37:16.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 03:37:16.058
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:16.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:16.078
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-2adcfb5c-5edc-4ffa-b10d-8796244e68db 11/30/22 03:37:16.081
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 03:37:16.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7399" for this suite. 11/30/22 03:37:16.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:37:16.098
Nov 30 03:37:16.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename security-context-test 11/30/22 03:37:16.099
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:16.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:16.128
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Nov 30 03:37:16.149: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-48540a3a-9ef8-4e1a-ba63-f5fda14a16d0" in namespace "security-context-test-937" to be "Succeeded or Failed"
Nov 30 03:37:16.154: INFO: Pod "alpine-nnp-false-48540a3a-9ef8-4e1a-ba63-f5fda14a16d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.99658ms
Nov 30 03:37:18.157: INFO: Pod "alpine-nnp-false-48540a3a-9ef8-4e1a-ba63-f5fda14a16d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008139151s
Nov 30 03:37:20.161: INFO: Pod "alpine-nnp-false-48540a3a-9ef8-4e1a-ba63-f5fda14a16d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012131841s
Nov 30 03:37:22.157: INFO: Pod "alpine-nnp-false-48540a3a-9ef8-4e1a-ba63-f5fda14a16d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008332701s
Nov 30 03:37:22.157: INFO: Pod "alpine-nnp-false-48540a3a-9ef8-4e1a-ba63-f5fda14a16d0" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Nov 30 03:37:22.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-937" for this suite. 11/30/22 03:37:22.165
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":58,"skipped":1080,"failed":0}
------------------------------
• [SLOW TEST] [6.075 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:37:16.098
    Nov 30 03:37:16.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename security-context-test 11/30/22 03:37:16.099
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:16.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:16.128
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Nov 30 03:37:16.149: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-48540a3a-9ef8-4e1a-ba63-f5fda14a16d0" in namespace "security-context-test-937" to be "Succeeded or Failed"
    Nov 30 03:37:16.154: INFO: Pod "alpine-nnp-false-48540a3a-9ef8-4e1a-ba63-f5fda14a16d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.99658ms
    Nov 30 03:37:18.157: INFO: Pod "alpine-nnp-false-48540a3a-9ef8-4e1a-ba63-f5fda14a16d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008139151s
    Nov 30 03:37:20.161: INFO: Pod "alpine-nnp-false-48540a3a-9ef8-4e1a-ba63-f5fda14a16d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012131841s
    Nov 30 03:37:22.157: INFO: Pod "alpine-nnp-false-48540a3a-9ef8-4e1a-ba63-f5fda14a16d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008332701s
    Nov 30 03:37:22.157: INFO: Pod "alpine-nnp-false-48540a3a-9ef8-4e1a-ba63-f5fda14a16d0" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Nov 30 03:37:22.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-937" for this suite. 11/30/22 03:37:22.165
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:37:22.174
Nov 30 03:37:22.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 03:37:22.175
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:22.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:22.199
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9264 11/30/22 03:37:22.201
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/30/22 03:37:22.216
STEP: creating service externalsvc in namespace services-9264 11/30/22 03:37:22.216
STEP: creating replication controller externalsvc in namespace services-9264 11/30/22 03:37:22.23
I1130 03:37:22.237428      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9264, replica count: 2
I1130 03:37:25.288818      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 11/30/22 03:37:25.292
Nov 30 03:37:25.306: INFO: Creating new exec pod
Nov 30 03:37:25.338: INFO: Waiting up to 5m0s for pod "execpodlvnh5" in namespace "services-9264" to be "running"
Nov 30 03:37:25.342: INFO: Pod "execpodlvnh5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520152ms
Nov 30 03:37:27.346: INFO: Pod "execpodlvnh5": Phase="Running", Reason="", readiness=true. Elapsed: 2.00740192s
Nov 30 03:37:27.346: INFO: Pod "execpodlvnh5" satisfied condition "running"
Nov 30 03:37:27.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9264 exec execpodlvnh5 -- /bin/sh -x -c nslookup clusterip-service.services-9264.svc.cluster.local'
Nov 30 03:37:27.513: INFO: stderr: "+ nslookup clusterip-service.services-9264.svc.cluster.local\n"
Nov 30 03:37:27.513: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-9264.svc.cluster.local\tcanonical name = externalsvc.services-9264.svc.cluster.local.\nName:\texternalsvc.services-9264.svc.cluster.local\nAddress: 10.100.57.83\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9264, will wait for the garbage collector to delete the pods 11/30/22 03:37:27.513
Nov 30 03:37:27.574: INFO: Deleting ReplicationController externalsvc took: 7.135326ms
Nov 30 03:37:27.675: INFO: Terminating ReplicationController externalsvc pods took: 100.820327ms
Nov 30 03:37:29.900: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 03:37:29.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9264" for this suite. 11/30/22 03:37:29.931
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":59,"skipped":1083,"failed":0}
------------------------------
• [SLOW TEST] [7.763 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:37:22.174
    Nov 30 03:37:22.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 03:37:22.175
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:22.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:22.199
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9264 11/30/22 03:37:22.201
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/30/22 03:37:22.216
    STEP: creating service externalsvc in namespace services-9264 11/30/22 03:37:22.216
    STEP: creating replication controller externalsvc in namespace services-9264 11/30/22 03:37:22.23
    I1130 03:37:22.237428      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9264, replica count: 2
    I1130 03:37:25.288818      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 11/30/22 03:37:25.292
    Nov 30 03:37:25.306: INFO: Creating new exec pod
    Nov 30 03:37:25.338: INFO: Waiting up to 5m0s for pod "execpodlvnh5" in namespace "services-9264" to be "running"
    Nov 30 03:37:25.342: INFO: Pod "execpodlvnh5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520152ms
    Nov 30 03:37:27.346: INFO: Pod "execpodlvnh5": Phase="Running", Reason="", readiness=true. Elapsed: 2.00740192s
    Nov 30 03:37:27.346: INFO: Pod "execpodlvnh5" satisfied condition "running"
    Nov 30 03:37:27.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9264 exec execpodlvnh5 -- /bin/sh -x -c nslookup clusterip-service.services-9264.svc.cluster.local'
    Nov 30 03:37:27.513: INFO: stderr: "+ nslookup clusterip-service.services-9264.svc.cluster.local\n"
    Nov 30 03:37:27.513: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-9264.svc.cluster.local\tcanonical name = externalsvc.services-9264.svc.cluster.local.\nName:\texternalsvc.services-9264.svc.cluster.local\nAddress: 10.100.57.83\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9264, will wait for the garbage collector to delete the pods 11/30/22 03:37:27.513
    Nov 30 03:37:27.574: INFO: Deleting ReplicationController externalsvc took: 7.135326ms
    Nov 30 03:37:27.675: INFO: Terminating ReplicationController externalsvc pods took: 100.820327ms
    Nov 30 03:37:29.900: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 03:37:29.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9264" for this suite. 11/30/22 03:37:29.931
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:37:29.938
Nov 30 03:37:29.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename endpointslice 11/30/22 03:37:29.938
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:29.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:29.969
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Nov 30 03:37:29.985: INFO: Endpoints addresses: [10.0.10.32 10.0.10.4 10.0.10.6] , ports: [6443]
Nov 30 03:37:29.985: INFO: EndpointSlices addresses: [10.0.10.32 10.0.10.4 10.0.10.6] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Nov 30 03:37:29.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3677" for this suite. 11/30/22 03:37:29.988
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":60,"skipped":1090,"failed":0}
------------------------------
• [0.060 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:37:29.938
    Nov 30 03:37:29.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename endpointslice 11/30/22 03:37:29.938
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:29.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:29.969
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Nov 30 03:37:29.985: INFO: Endpoints addresses: [10.0.10.32 10.0.10.4 10.0.10.6] , ports: [6443]
    Nov 30 03:37:29.985: INFO: EndpointSlices addresses: [10.0.10.32 10.0.10.4 10.0.10.6] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Nov 30 03:37:29.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3677" for this suite. 11/30/22 03:37:29.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:37:29.998
Nov 30 03:37:29.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename taint-single-pod 11/30/22 03:37:29.999
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:30.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:30.022
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Nov 30 03:37:30.025: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 30 03:38:30.094: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Nov 30 03:38:30.097: INFO: Starting informer...
STEP: Starting pod... 11/30/22 03:38:30.097
Nov 30 03:38:30.334: INFO: Pod is running on worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins. Tainting Node
STEP: Trying to apply a taint on the Node 11/30/22 03:38:30.334
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/30/22 03:38:30.346
STEP: Waiting short time to make sure Pod is queued for deletion 11/30/22 03:38:30.35
Nov 30 03:38:30.350: INFO: Pod wasn't evicted. Proceeding
Nov 30 03:38:30.350: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/30/22 03:38:30.363
STEP: Waiting some time to make sure that toleration time passed. 11/30/22 03:38:30.427
Nov 30 03:39:45.430: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Nov 30 03:39:45.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6464" for this suite. 11/30/22 03:39:45.435
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":61,"skipped":1111,"failed":0}
------------------------------
• [SLOW TEST] [135.443 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:37:29.998
    Nov 30 03:37:29.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename taint-single-pod 11/30/22 03:37:29.999
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:37:30.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:37:30.022
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Nov 30 03:37:30.025: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 30 03:38:30.094: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Nov 30 03:38:30.097: INFO: Starting informer...
    STEP: Starting pod... 11/30/22 03:38:30.097
    Nov 30 03:38:30.334: INFO: Pod is running on worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins. Tainting Node
    STEP: Trying to apply a taint on the Node 11/30/22 03:38:30.334
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/30/22 03:38:30.346
    STEP: Waiting short time to make sure Pod is queued for deletion 11/30/22 03:38:30.35
    Nov 30 03:38:30.350: INFO: Pod wasn't evicted. Proceeding
    Nov 30 03:38:30.350: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/30/22 03:38:30.363
    STEP: Waiting some time to make sure that toleration time passed. 11/30/22 03:38:30.427
    Nov 30 03:39:45.430: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 03:39:45.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-6464" for this suite. 11/30/22 03:39:45.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:39:45.442
Nov 30 03:39:45.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-lifecycle-hook 11/30/22 03:39:45.443
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:39:45.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:39:45.462
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 11/30/22 03:39:45.47
Nov 30 03:39:45.504: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4511" to be "running and ready"
Nov 30 03:39:45.513: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.78999ms
Nov 30 03:39:45.513: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:39:47.516: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011759907s
Nov 30 03:39:47.516: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 30 03:39:47.516: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 11/30/22 03:39:47.518
Nov 30 03:39:47.525: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4511" to be "running and ready"
Nov 30 03:39:47.529: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.672875ms
Nov 30 03:39:47.529: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:39:49.532: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006530821s
Nov 30 03:39:49.532: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Nov 30 03:39:49.532: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 11/30/22 03:39:49.534
Nov 30 03:39:49.539: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 30 03:39:49.541: INFO: Pod pod-with-prestop-http-hook still exists
Nov 30 03:39:51.542: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 30 03:39:51.545: INFO: Pod pod-with-prestop-http-hook still exists
Nov 30 03:39:53.542: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 30 03:39:53.545: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 11/30/22 03:39:53.545
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Nov 30 03:39:53.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4511" for this suite. 11/30/22 03:39:53.562
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":62,"skipped":1118,"failed":0}
------------------------------
• [SLOW TEST] [8.124 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:39:45.442
    Nov 30 03:39:45.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/30/22 03:39:45.443
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:39:45.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:39:45.462
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 11/30/22 03:39:45.47
    Nov 30 03:39:45.504: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4511" to be "running and ready"
    Nov 30 03:39:45.513: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.78999ms
    Nov 30 03:39:45.513: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:39:47.516: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011759907s
    Nov 30 03:39:47.516: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 30 03:39:47.516: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 11/30/22 03:39:47.518
    Nov 30 03:39:47.525: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4511" to be "running and ready"
    Nov 30 03:39:47.529: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.672875ms
    Nov 30 03:39:47.529: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:39:49.532: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006530821s
    Nov 30 03:39:49.532: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Nov 30 03:39:49.532: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 11/30/22 03:39:49.534
    Nov 30 03:39:49.539: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Nov 30 03:39:49.541: INFO: Pod pod-with-prestop-http-hook still exists
    Nov 30 03:39:51.542: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Nov 30 03:39:51.545: INFO: Pod pod-with-prestop-http-hook still exists
    Nov 30 03:39:53.542: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Nov 30 03:39:53.545: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 11/30/22 03:39:53.545
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Nov 30 03:39:53.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4511" for this suite. 11/30/22 03:39:53.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:39:53.567
Nov 30 03:39:53.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 03:39:53.567
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:39:53.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:39:53.599
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 11/30/22 03:39:53.601
Nov 30 03:39:53.630: INFO: Waiting up to 5m0s for pod "annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420" in namespace "downward-api-2812" to be "running and ready"
Nov 30 03:39:53.632: INFO: Pod "annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420": Phase="Pending", Reason="", readiness=false. Elapsed: 2.594681ms
Nov 30 03:39:53.632: INFO: The phase of Pod annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:39:55.637: INFO: Pod "annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420": Phase="Running", Reason="", readiness=true. Elapsed: 2.007639952s
Nov 30 03:39:55.637: INFO: The phase of Pod annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420 is Running (Ready = true)
Nov 30 03:39:55.637: INFO: Pod "annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420" satisfied condition "running and ready"
Nov 30 03:39:56.165: INFO: Successfully updated pod "annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Nov 30 03:40:00.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2812" for this suite. 11/30/22 03:40:00.188
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":63,"skipped":1144,"failed":0}
------------------------------
• [SLOW TEST] [6.626 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:39:53.567
    Nov 30 03:39:53.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 03:39:53.567
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:39:53.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:39:53.599
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 11/30/22 03:39:53.601
    Nov 30 03:39:53.630: INFO: Waiting up to 5m0s for pod "annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420" in namespace "downward-api-2812" to be "running and ready"
    Nov 30 03:39:53.632: INFO: Pod "annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420": Phase="Pending", Reason="", readiness=false. Elapsed: 2.594681ms
    Nov 30 03:39:53.632: INFO: The phase of Pod annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:39:55.637: INFO: Pod "annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420": Phase="Running", Reason="", readiness=true. Elapsed: 2.007639952s
    Nov 30 03:39:55.637: INFO: The phase of Pod annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420 is Running (Ready = true)
    Nov 30 03:39:55.637: INFO: Pod "annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420" satisfied condition "running and ready"
    Nov 30 03:39:56.165: INFO: Successfully updated pod "annotationupdatebc10a2f2-6128-4913-9e3c-8aea0b578420"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Nov 30 03:40:00.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2812" for this suite. 11/30/22 03:40:00.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:40:00.194
Nov 30 03:40:00.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename svcaccounts 11/30/22 03:40:00.195
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:00.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:00.214
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  11/30/22 03:40:00.216
Nov 30 03:40:00.245: INFO: Waiting up to 5m0s for pod "test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d" in namespace "svcaccounts-6296" to be "Succeeded or Failed"
Nov 30 03:40:00.248: INFO: Pod "test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.847493ms
Nov 30 03:40:02.253: INFO: Pod "test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d": Phase="Running", Reason="", readiness=false. Elapsed: 2.007290208s
Nov 30 03:40:04.252: INFO: Pod "test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006787773s
STEP: Saw pod success 11/30/22 03:40:04.252
Nov 30 03:40:04.252: INFO: Pod "test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d" satisfied condition "Succeeded or Failed"
Nov 30 03:40:04.255: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d container agnhost-container: <nil>
STEP: delete the pod 11/30/22 03:40:04.26
Nov 30 03:40:04.274: INFO: Waiting for pod test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d to disappear
Nov 30 03:40:04.279: INFO: Pod test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Nov 30 03:40:04.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6296" for this suite. 11/30/22 03:40:04.283
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":64,"skipped":1172,"failed":0}
------------------------------
• [4.093 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:40:00.194
    Nov 30 03:40:00.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename svcaccounts 11/30/22 03:40:00.195
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:00.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:00.214
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  11/30/22 03:40:00.216
    Nov 30 03:40:00.245: INFO: Waiting up to 5m0s for pod "test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d" in namespace "svcaccounts-6296" to be "Succeeded or Failed"
    Nov 30 03:40:00.248: INFO: Pod "test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.847493ms
    Nov 30 03:40:02.253: INFO: Pod "test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d": Phase="Running", Reason="", readiness=false. Elapsed: 2.007290208s
    Nov 30 03:40:04.252: INFO: Pod "test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006787773s
    STEP: Saw pod success 11/30/22 03:40:04.252
    Nov 30 03:40:04.252: INFO: Pod "test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d" satisfied condition "Succeeded or Failed"
    Nov 30 03:40:04.255: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 03:40:04.26
    Nov 30 03:40:04.274: INFO: Waiting for pod test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d to disappear
    Nov 30 03:40:04.279: INFO: Pod test-pod-abb1c9f4-7d89-4456-9dc8-78c97d47937d no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Nov 30 03:40:04.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6296" for this suite. 11/30/22 03:40:04.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:40:04.288
Nov 30 03:40:04.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 03:40:04.288
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:04.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:04.304
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-6310 11/30/22 03:40:04.306
STEP: creating service affinity-clusterip-transition in namespace services-6310 11/30/22 03:40:04.306
STEP: creating replication controller affinity-clusterip-transition in namespace services-6310 11/30/22 03:40:04.331
I1130 03:40:04.342838      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6310, replica count: 3
I1130 03:40:07.393894      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1130 03:40:10.394591      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 30 03:40:10.399: INFO: Creating new exec pod
Nov 30 03:40:10.426: INFO: Waiting up to 5m0s for pod "execpod-affinitytrqrs" in namespace "services-6310" to be "running"
Nov 30 03:40:10.429: INFO: Pod "execpod-affinitytrqrs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.995285ms
Nov 30 03:40:12.433: INFO: Pod "execpod-affinitytrqrs": Phase="Running", Reason="", readiness=true. Elapsed: 2.006506872s
Nov 30 03:40:12.433: INFO: Pod "execpod-affinitytrqrs" satisfied condition "running"
Nov 30 03:40:13.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-6310 exec execpod-affinitytrqrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Nov 30 03:40:13.552: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Nov 30 03:40:13.552: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:40:13.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-6310 exec execpod-affinitytrqrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.80.19 80'
Nov 30 03:40:13.679: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.80.19 80\nConnection to 10.105.80.19 80 port [tcp/http] succeeded!\n"
Nov 30 03:40:13.679: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:40:13.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-6310 exec execpod-affinitytrqrs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.80.19:80/ ; done'
Nov 30 03:40:13.882: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n"
Nov 30 03:40:13.882: INFO: stdout: "\naffinity-clusterip-transition-dsbl5\naffinity-clusterip-transition-c65rz\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-dsbl5\naffinity-clusterip-transition-c65rz\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-dsbl5\naffinity-clusterip-transition-c65rz\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-dsbl5\naffinity-clusterip-transition-c65rz\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-dsbl5\naffinity-clusterip-transition-c65rz\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-dsbl5"
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-dsbl5
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-c65rz
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-dsbl5
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-c65rz
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-dsbl5
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-c65rz
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-dsbl5
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-c65rz
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-dsbl5
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-c65rz
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-dsbl5
Nov 30 03:40:13.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-6310 exec execpod-affinitytrqrs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.80.19:80/ ; done'
Nov 30 03:40:14.082: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n"
Nov 30 03:40:14.082: INFO: stdout: "\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj"
Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
Nov 30 03:40:14.083: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6310, will wait for the garbage collector to delete the pods 11/30/22 03:40:14.096
Nov 30 03:40:14.157: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.400717ms
Nov 30 03:40:14.258: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.992087ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 03:40:16.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6310" for this suite. 11/30/22 03:40:16.608
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":65,"skipped":1179,"failed":0}
------------------------------
• [SLOW TEST] [12.333 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:40:04.288
    Nov 30 03:40:04.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 03:40:04.288
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:04.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:04.304
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-6310 11/30/22 03:40:04.306
    STEP: creating service affinity-clusterip-transition in namespace services-6310 11/30/22 03:40:04.306
    STEP: creating replication controller affinity-clusterip-transition in namespace services-6310 11/30/22 03:40:04.331
    I1130 03:40:04.342838      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6310, replica count: 3
    I1130 03:40:07.393894      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1130 03:40:10.394591      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 30 03:40:10.399: INFO: Creating new exec pod
    Nov 30 03:40:10.426: INFO: Waiting up to 5m0s for pod "execpod-affinitytrqrs" in namespace "services-6310" to be "running"
    Nov 30 03:40:10.429: INFO: Pod "execpod-affinitytrqrs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.995285ms
    Nov 30 03:40:12.433: INFO: Pod "execpod-affinitytrqrs": Phase="Running", Reason="", readiness=true. Elapsed: 2.006506872s
    Nov 30 03:40:12.433: INFO: Pod "execpod-affinitytrqrs" satisfied condition "running"
    Nov 30 03:40:13.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-6310 exec execpod-affinitytrqrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Nov 30 03:40:13.552: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Nov 30 03:40:13.552: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:40:13.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-6310 exec execpod-affinitytrqrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.80.19 80'
    Nov 30 03:40:13.679: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.80.19 80\nConnection to 10.105.80.19 80 port [tcp/http] succeeded!\n"
    Nov 30 03:40:13.679: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:40:13.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-6310 exec execpod-affinitytrqrs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.80.19:80/ ; done'
    Nov 30 03:40:13.882: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n"
    Nov 30 03:40:13.882: INFO: stdout: "\naffinity-clusterip-transition-dsbl5\naffinity-clusterip-transition-c65rz\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-dsbl5\naffinity-clusterip-transition-c65rz\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-dsbl5\naffinity-clusterip-transition-c65rz\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-dsbl5\naffinity-clusterip-transition-c65rz\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-dsbl5\naffinity-clusterip-transition-c65rz\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-dsbl5"
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-dsbl5
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-c65rz
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-dsbl5
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-c65rz
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-dsbl5
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-c65rz
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-dsbl5
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-c65rz
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-dsbl5
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-c65rz
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:13.882: INFO: Received response from host: affinity-clusterip-transition-dsbl5
    Nov 30 03:40:13.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-6310 exec execpod-affinitytrqrs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.80.19:80/ ; done'
    Nov 30 03:40:14.082: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.80.19:80/\n"
    Nov 30 03:40:14.082: INFO: stdout: "\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj\naffinity-clusterip-transition-spcrj"
    Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.082: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.083: INFO: Received response from host: affinity-clusterip-transition-spcrj
    Nov 30 03:40:14.083: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6310, will wait for the garbage collector to delete the pods 11/30/22 03:40:14.096
    Nov 30 03:40:14.157: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.400717ms
    Nov 30 03:40:14.258: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.992087ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 03:40:16.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6310" for this suite. 11/30/22 03:40:16.608
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:40:16.621
Nov 30 03:40:16.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pods 11/30/22 03:40:16.621
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:16.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:16.661
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 11/30/22 03:40:16.667
STEP: submitting the pod to kubernetes 11/30/22 03:40:16.667
Nov 30 03:40:16.699: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365" in namespace "pods-2797" to be "running and ready"
Nov 30 03:40:16.704: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365": Phase="Pending", Reason="", readiness=false. Elapsed: 5.25469ms
Nov 30 03:40:16.704: INFO: The phase of Pod pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:40:18.708: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365": Phase="Running", Reason="", readiness=true. Elapsed: 2.009148843s
Nov 30 03:40:18.708: INFO: The phase of Pod pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365 is Running (Ready = true)
Nov 30 03:40:18.708: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 11/30/22 03:40:18.711
STEP: updating the pod 11/30/22 03:40:18.713
Nov 30 03:40:19.226: INFO: Successfully updated pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365"
Nov 30 03:40:19.226: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365" in namespace "pods-2797" to be "terminated with reason DeadlineExceeded"
Nov 30 03:40:19.230: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365": Phase="Running", Reason="", readiness=true. Elapsed: 3.452396ms
Nov 30 03:40:21.235: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365": Phase="Running", Reason="", readiness=true. Elapsed: 2.009097844s
Nov 30 03:40:23.234: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007230629s
Nov 30 03:40:23.234: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Nov 30 03:40:23.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2797" for this suite. 11/30/22 03:40:23.237
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":66,"skipped":1179,"failed":0}
------------------------------
• [SLOW TEST] [6.622 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:40:16.621
    Nov 30 03:40:16.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pods 11/30/22 03:40:16.621
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:16.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:16.661
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 11/30/22 03:40:16.667
    STEP: submitting the pod to kubernetes 11/30/22 03:40:16.667
    Nov 30 03:40:16.699: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365" in namespace "pods-2797" to be "running and ready"
    Nov 30 03:40:16.704: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365": Phase="Pending", Reason="", readiness=false. Elapsed: 5.25469ms
    Nov 30 03:40:16.704: INFO: The phase of Pod pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:40:18.708: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365": Phase="Running", Reason="", readiness=true. Elapsed: 2.009148843s
    Nov 30 03:40:18.708: INFO: The phase of Pod pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365 is Running (Ready = true)
    Nov 30 03:40:18.708: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 11/30/22 03:40:18.711
    STEP: updating the pod 11/30/22 03:40:18.713
    Nov 30 03:40:19.226: INFO: Successfully updated pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365"
    Nov 30 03:40:19.226: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365" in namespace "pods-2797" to be "terminated with reason DeadlineExceeded"
    Nov 30 03:40:19.230: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365": Phase="Running", Reason="", readiness=true. Elapsed: 3.452396ms
    Nov 30 03:40:21.235: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365": Phase="Running", Reason="", readiness=true. Elapsed: 2.009097844s
    Nov 30 03:40:23.234: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007230629s
    Nov 30 03:40:23.234: INFO: Pod "pod-update-activedeadlineseconds-44116da7-f3a1-42b0-8ed5-ef04a2e52365" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Nov 30 03:40:23.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2797" for this suite. 11/30/22 03:40:23.237
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:40:23.242
Nov 30 03:40:23.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename server-version 11/30/22 03:40:23.243
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:23.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:23.261
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 11/30/22 03:40:23.263
STEP: Confirm major version 11/30/22 03:40:23.265
Nov 30 03:40:23.265: INFO: Major version: 1
STEP: Confirm minor version 11/30/22 03:40:23.265
Nov 30 03:40:23.265: INFO: cleanMinorVersion: 25
Nov 30 03:40:23.265: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Nov 30 03:40:23.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7501" for this suite. 11/30/22 03:40:23.27
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":67,"skipped":1181,"failed":0}
------------------------------
• [0.035 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:40:23.242
    Nov 30 03:40:23.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename server-version 11/30/22 03:40:23.243
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:23.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:23.261
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 11/30/22 03:40:23.263
    STEP: Confirm major version 11/30/22 03:40:23.265
    Nov 30 03:40:23.265: INFO: Major version: 1
    STEP: Confirm minor version 11/30/22 03:40:23.265
    Nov 30 03:40:23.265: INFO: cleanMinorVersion: 25
    Nov 30 03:40:23.265: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Nov 30 03:40:23.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-7501" for this suite. 11/30/22 03:40:23.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:40:23.279
Nov 30 03:40:23.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename replication-controller 11/30/22 03:40:23.28
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:23.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:23.3
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f 11/30/22 03:40:23.303
Nov 30 03:40:23.310: INFO: Pod name my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f: Found 0 pods out of 1
Nov 30 03:40:28.314: INFO: Pod name my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f: Found 1 pods out of 1
Nov 30 03:40:28.314: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f" are running
Nov 30 03:40:28.314: INFO: Waiting up to 5m0s for pod "my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f-47pvj" in namespace "replication-controller-389" to be "running"
Nov 30 03:40:28.316: INFO: Pod "my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f-47pvj": Phase="Running", Reason="", readiness=true. Elapsed: 2.451298ms
Nov 30 03:40:28.316: INFO: Pod "my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f-47pvj" satisfied condition "running"
Nov 30 03:40:28.316: INFO: Pod "my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f-47pvj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 03:40:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 03:40:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 03:40:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 03:40:23 +0000 UTC Reason: Message:}])
Nov 30 03:40:28.316: INFO: Trying to dial the pod
Nov 30 03:40:33.326: INFO: Controller my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f: Got expected result from replica 1 [my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f-47pvj]: "my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f-47pvj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Nov 30 03:40:33.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-389" for this suite. 11/30/22 03:40:33.331
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":68,"skipped":1204,"failed":0}
------------------------------
• [SLOW TEST] [10.057 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:40:23.279
    Nov 30 03:40:23.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename replication-controller 11/30/22 03:40:23.28
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:23.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:23.3
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f 11/30/22 03:40:23.303
    Nov 30 03:40:23.310: INFO: Pod name my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f: Found 0 pods out of 1
    Nov 30 03:40:28.314: INFO: Pod name my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f: Found 1 pods out of 1
    Nov 30 03:40:28.314: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f" are running
    Nov 30 03:40:28.314: INFO: Waiting up to 5m0s for pod "my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f-47pvj" in namespace "replication-controller-389" to be "running"
    Nov 30 03:40:28.316: INFO: Pod "my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f-47pvj": Phase="Running", Reason="", readiness=true. Elapsed: 2.451298ms
    Nov 30 03:40:28.316: INFO: Pod "my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f-47pvj" satisfied condition "running"
    Nov 30 03:40:28.316: INFO: Pod "my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f-47pvj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 03:40:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 03:40:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 03:40:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 03:40:23 +0000 UTC Reason: Message:}])
    Nov 30 03:40:28.316: INFO: Trying to dial the pod
    Nov 30 03:40:33.326: INFO: Controller my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f: Got expected result from replica 1 [my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f-47pvj]: "my-hostname-basic-bafe5216-2ee9-4401-bd3b-6aa2d9eaf43f-47pvj", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Nov 30 03:40:33.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-389" for this suite. 11/30/22 03:40:33.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:40:33.336
Nov 30 03:40:33.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 03:40:33.337
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:33.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:33.355
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 11/30/22 03:40:33.358
Nov 30 03:40:33.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 create -f -'
Nov 30 03:40:34.012: INFO: stderr: ""
Nov 30 03:40:34.012: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/30/22 03:40:34.012
Nov 30 03:40:34.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 30 03:40:34.088: INFO: stderr: ""
Nov 30 03:40:34.088: INFO: stdout: "update-demo-nautilus-qnq8x update-demo-nautilus-sskr4 "
Nov 30 03:40:34.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods update-demo-nautilus-qnq8x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 30 03:40:34.149: INFO: stderr: ""
Nov 30 03:40:34.149: INFO: stdout: ""
Nov 30 03:40:34.149: INFO: update-demo-nautilus-qnq8x is created but not running
Nov 30 03:40:39.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 30 03:40:39.218: INFO: stderr: ""
Nov 30 03:40:39.218: INFO: stdout: "update-demo-nautilus-qnq8x update-demo-nautilus-sskr4 "
Nov 30 03:40:39.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods update-demo-nautilus-qnq8x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 30 03:40:39.287: INFO: stderr: ""
Nov 30 03:40:39.287: INFO: stdout: "true"
Nov 30 03:40:39.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods update-demo-nautilus-qnq8x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 30 03:40:39.347: INFO: stderr: ""
Nov 30 03:40:39.347: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
Nov 30 03:40:39.347: INFO: validating pod update-demo-nautilus-qnq8x
Nov 30 03:40:39.351: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 30 03:40:39.351: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 30 03:40:39.351: INFO: update-demo-nautilus-qnq8x is verified up and running
Nov 30 03:40:39.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods update-demo-nautilus-sskr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 30 03:40:39.406: INFO: stderr: ""
Nov 30 03:40:39.406: INFO: stdout: "true"
Nov 30 03:40:39.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods update-demo-nautilus-sskr4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 30 03:40:39.462: INFO: stderr: ""
Nov 30 03:40:39.462: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
Nov 30 03:40:39.462: INFO: validating pod update-demo-nautilus-sskr4
Nov 30 03:40:39.465: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 30 03:40:39.466: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 30 03:40:39.466: INFO: update-demo-nautilus-sskr4 is verified up and running
STEP: using delete to clean up resources 11/30/22 03:40:39.466
Nov 30 03:40:39.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 delete --grace-period=0 --force -f -'
Nov 30 03:40:39.525: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 30 03:40:39.525: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 30 03:40:39.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get rc,svc -l name=update-demo --no-headers'
Nov 30 03:40:39.585: INFO: stderr: "No resources found in kubectl-4064 namespace.\n"
Nov 30 03:40:39.585: INFO: stdout: ""
Nov 30 03:40:39.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 30 03:40:39.647: INFO: stderr: ""
Nov 30 03:40:39.647: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 03:40:39.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4064" for this suite. 11/30/22 03:40:39.652
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":69,"skipped":1211,"failed":0}
------------------------------
• [SLOW TEST] [6.321 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:40:33.336
    Nov 30 03:40:33.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 03:40:33.337
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:33.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:33.355
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 11/30/22 03:40:33.358
    Nov 30 03:40:33.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 create -f -'
    Nov 30 03:40:34.012: INFO: stderr: ""
    Nov 30 03:40:34.012: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/30/22 03:40:34.012
    Nov 30 03:40:34.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 30 03:40:34.088: INFO: stderr: ""
    Nov 30 03:40:34.088: INFO: stdout: "update-demo-nautilus-qnq8x update-demo-nautilus-sskr4 "
    Nov 30 03:40:34.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods update-demo-nautilus-qnq8x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 30 03:40:34.149: INFO: stderr: ""
    Nov 30 03:40:34.149: INFO: stdout: ""
    Nov 30 03:40:34.149: INFO: update-demo-nautilus-qnq8x is created but not running
    Nov 30 03:40:39.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 30 03:40:39.218: INFO: stderr: ""
    Nov 30 03:40:39.218: INFO: stdout: "update-demo-nautilus-qnq8x update-demo-nautilus-sskr4 "
    Nov 30 03:40:39.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods update-demo-nautilus-qnq8x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 30 03:40:39.287: INFO: stderr: ""
    Nov 30 03:40:39.287: INFO: stdout: "true"
    Nov 30 03:40:39.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods update-demo-nautilus-qnq8x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 30 03:40:39.347: INFO: stderr: ""
    Nov 30 03:40:39.347: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
    Nov 30 03:40:39.347: INFO: validating pod update-demo-nautilus-qnq8x
    Nov 30 03:40:39.351: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 30 03:40:39.351: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 30 03:40:39.351: INFO: update-demo-nautilus-qnq8x is verified up and running
    Nov 30 03:40:39.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods update-demo-nautilus-sskr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 30 03:40:39.406: INFO: stderr: ""
    Nov 30 03:40:39.406: INFO: stdout: "true"
    Nov 30 03:40:39.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods update-demo-nautilus-sskr4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 30 03:40:39.462: INFO: stderr: ""
    Nov 30 03:40:39.462: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.5"
    Nov 30 03:40:39.462: INFO: validating pod update-demo-nautilus-sskr4
    Nov 30 03:40:39.465: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 30 03:40:39.466: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 30 03:40:39.466: INFO: update-demo-nautilus-sskr4 is verified up and running
    STEP: using delete to clean up resources 11/30/22 03:40:39.466
    Nov 30 03:40:39.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 delete --grace-period=0 --force -f -'
    Nov 30 03:40:39.525: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 30 03:40:39.525: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Nov 30 03:40:39.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get rc,svc -l name=update-demo --no-headers'
    Nov 30 03:40:39.585: INFO: stderr: "No resources found in kubectl-4064 namespace.\n"
    Nov 30 03:40:39.585: INFO: stdout: ""
    Nov 30 03:40:39.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4064 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Nov 30 03:40:39.647: INFO: stderr: ""
    Nov 30 03:40:39.647: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 03:40:39.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4064" for this suite. 11/30/22 03:40:39.652
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:40:39.657
Nov 30 03:40:39.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 03:40:39.658
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:39.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:39.682
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Nov 30 03:40:39.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/30/22 03:40:42.187
Nov 30 03:40:42.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-7187 --namespace=crd-publish-openapi-7187 create -f -'
Nov 30 03:40:42.768: INFO: stderr: ""
Nov 30 03:40:42.768: INFO: stdout: "e2e-test-crd-publish-openapi-9974-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 30 03:40:42.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-7187 --namespace=crd-publish-openapi-7187 delete e2e-test-crd-publish-openapi-9974-crds test-cr'
Nov 30 03:40:42.867: INFO: stderr: ""
Nov 30 03:40:42.867: INFO: stdout: "e2e-test-crd-publish-openapi-9974-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 30 03:40:42.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-7187 --namespace=crd-publish-openapi-7187 apply -f -'
Nov 30 03:40:43.375: INFO: stderr: ""
Nov 30 03:40:43.375: INFO: stdout: "e2e-test-crd-publish-openapi-9974-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 30 03:40:43.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-7187 --namespace=crd-publish-openapi-7187 delete e2e-test-crd-publish-openapi-9974-crds test-cr'
Nov 30 03:40:43.442: INFO: stderr: ""
Nov 30 03:40:43.442: INFO: stdout: "e2e-test-crd-publish-openapi-9974-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 11/30/22 03:40:43.442
Nov 30 03:40:43.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-7187 explain e2e-test-crd-publish-openapi-9974-crds'
Nov 30 03:40:43.935: INFO: stderr: ""
Nov 30 03:40:43.935: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9974-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:40:47.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7187" for this suite. 11/30/22 03:40:47.308
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":70,"skipped":1212,"failed":0}
------------------------------
• [SLOW TEST] [7.656 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:40:39.657
    Nov 30 03:40:39.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 03:40:39.658
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:39.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:39.682
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Nov 30 03:40:39.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/30/22 03:40:42.187
    Nov 30 03:40:42.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-7187 --namespace=crd-publish-openapi-7187 create -f -'
    Nov 30 03:40:42.768: INFO: stderr: ""
    Nov 30 03:40:42.768: INFO: stdout: "e2e-test-crd-publish-openapi-9974-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Nov 30 03:40:42.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-7187 --namespace=crd-publish-openapi-7187 delete e2e-test-crd-publish-openapi-9974-crds test-cr'
    Nov 30 03:40:42.867: INFO: stderr: ""
    Nov 30 03:40:42.867: INFO: stdout: "e2e-test-crd-publish-openapi-9974-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Nov 30 03:40:42.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-7187 --namespace=crd-publish-openapi-7187 apply -f -'
    Nov 30 03:40:43.375: INFO: stderr: ""
    Nov 30 03:40:43.375: INFO: stdout: "e2e-test-crd-publish-openapi-9974-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Nov 30 03:40:43.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-7187 --namespace=crd-publish-openapi-7187 delete e2e-test-crd-publish-openapi-9974-crds test-cr'
    Nov 30 03:40:43.442: INFO: stderr: ""
    Nov 30 03:40:43.442: INFO: stdout: "e2e-test-crd-publish-openapi-9974-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 11/30/22 03:40:43.442
    Nov 30 03:40:43.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-7187 explain e2e-test-crd-publish-openapi-9974-crds'
    Nov 30 03:40:43.935: INFO: stderr: ""
    Nov 30 03:40:43.935: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9974-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:40:47.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7187" for this suite. 11/30/22 03:40:47.308
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:40:47.313
Nov 30 03:40:47.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename runtimeclass 11/30/22 03:40:47.314
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:47.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:47.327
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Nov 30 03:40:47.365: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4061 to be scheduled
Nov 30 03:40:47.370: INFO: 1 pods are not scheduled: [runtimeclass-4061/test-runtimeclass-runtimeclass-4061-preconfigured-handler-v9rs2(d59c17d1-55ee-422a-bf4b-36d6edadc4e6)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Nov 30 03:40:49.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4061" for this suite. 11/30/22 03:40:49.383
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":71,"skipped":1216,"failed":0}
------------------------------
• [2.074 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:40:47.313
    Nov 30 03:40:47.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename runtimeclass 11/30/22 03:40:47.314
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:47.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:47.327
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Nov 30 03:40:47.365: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4061 to be scheduled
    Nov 30 03:40:47.370: INFO: 1 pods are not scheduled: [runtimeclass-4061/test-runtimeclass-runtimeclass-4061-preconfigured-handler-v9rs2(d59c17d1-55ee-422a-bf4b-36d6edadc4e6)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Nov 30 03:40:49.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4061" for this suite. 11/30/22 03:40:49.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:40:49.388
Nov 30 03:40:49.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename conformance-tests 11/30/22 03:40:49.389
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:49.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:49.402
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 11/30/22 03:40:49.405
Nov 30 03:40:49.405: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Nov 30 03:40:49.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-4278" for this suite. 11/30/22 03:40:49.424
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":72,"skipped":1222,"failed":0}
------------------------------
• [0.040 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:40:49.388
    Nov 30 03:40:49.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename conformance-tests 11/30/22 03:40:49.389
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:49.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:49.402
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 11/30/22 03:40:49.405
    Nov 30 03:40:49.405: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Nov 30 03:40:49.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-4278" for this suite. 11/30/22 03:40:49.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:40:49.429
Nov 30 03:40:49.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 03:40:49.43
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:49.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:49.444
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-4219 11/30/22 03:40:49.446
W1130 03:40:49.456630      20 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Nov 30 03:40:49.456: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4219" to be "running and ready"
Nov 30 03:40:49.468: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 11.596451ms
Nov 30 03:40:49.468: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:40:51.472: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.015725505s
Nov 30 03:40:51.472: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Nov 30 03:40:51.472: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Nov 30 03:40:51.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4219 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 30 03:40:51.593: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Nov 30 03:40:51.593: INFO: stdout: "ipvs"
Nov 30 03:40:51.593: INFO: proxyMode: ipvs
Nov 30 03:40:51.605: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 30 03:40:51.608: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-4219 11/30/22 03:40:51.608
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4219 11/30/22 03:40:51.623
I1130 03:40:51.633304      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4219, replica count: 3
I1130 03:40:54.683809      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 30 03:40:54.690: INFO: Creating new exec pod
Nov 30 03:40:54.697: INFO: Waiting up to 5m0s for pod "execpod-affinityqmk4h" in namespace "services-4219" to be "running"
Nov 30 03:40:54.701: INFO: Pod "execpod-affinityqmk4h": Phase="Pending", Reason="", readiness=false. Elapsed: 3.775179ms
Nov 30 03:40:56.705: INFO: Pod "execpod-affinityqmk4h": Phase="Running", Reason="", readiness=true. Elapsed: 2.008404545s
Nov 30 03:40:56.705: INFO: Pod "execpod-affinityqmk4h" satisfied condition "running"
Nov 30 03:40:57.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4219 exec execpod-affinityqmk4h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Nov 30 03:40:57.828: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Nov 30 03:40:57.828: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:40:57.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4219 exec execpod-affinityqmk4h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.154.254 80'
Nov 30 03:40:57.949: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.154.254 80\nConnection to 10.110.154.254 80 port [tcp/http] succeeded!\n"
Nov 30 03:40:57.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:40:57.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4219 exec execpod-affinityqmk4h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.154.254:80/ ; done'
Nov 30 03:40:58.152: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n"
Nov 30 03:40:58.152: INFO: stdout: "\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp"
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
Nov 30 03:40:58.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4219 exec execpod-affinityqmk4h -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.110.154.254:80/'
Nov 30 03:40:58.277: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n"
Nov 30 03:40:58.277: INFO: stdout: "affinity-clusterip-timeout-vb8dp"
Nov 30 03:43:08.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4219 exec execpod-affinityqmk4h -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.110.154.254:80/'
Nov 30 03:43:08.400: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n"
Nov 30 03:43:08.400: INFO: stdout: "affinity-clusterip-timeout-8b55t"
Nov 30 03:43:08.400: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4219, will wait for the garbage collector to delete the pods 11/30/22 03:43:08.418
Nov 30 03:43:08.478: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.580729ms
Nov 30 03:43:08.578: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.18897ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 03:43:10.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4219" for this suite. 11/30/22 03:43:10.744
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":73,"skipped":1249,"failed":0}
------------------------------
• [SLOW TEST] [141.326 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:40:49.429
    Nov 30 03:40:49.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 03:40:49.43
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:40:49.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:40:49.444
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-4219 11/30/22 03:40:49.446
    W1130 03:40:49.456630      20 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Nov 30 03:40:49.456: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4219" to be "running and ready"
    Nov 30 03:40:49.468: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 11.596451ms
    Nov 30 03:40:49.468: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:40:51.472: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.015725505s
    Nov 30 03:40:51.472: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Nov 30 03:40:51.472: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Nov 30 03:40:51.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4219 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Nov 30 03:40:51.593: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Nov 30 03:40:51.593: INFO: stdout: "ipvs"
    Nov 30 03:40:51.593: INFO: proxyMode: ipvs
    Nov 30 03:40:51.605: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Nov 30 03:40:51.608: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-4219 11/30/22 03:40:51.608
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-4219 11/30/22 03:40:51.623
    I1130 03:40:51.633304      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4219, replica count: 3
    I1130 03:40:54.683809      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 30 03:40:54.690: INFO: Creating new exec pod
    Nov 30 03:40:54.697: INFO: Waiting up to 5m0s for pod "execpod-affinityqmk4h" in namespace "services-4219" to be "running"
    Nov 30 03:40:54.701: INFO: Pod "execpod-affinityqmk4h": Phase="Pending", Reason="", readiness=false. Elapsed: 3.775179ms
    Nov 30 03:40:56.705: INFO: Pod "execpod-affinityqmk4h": Phase="Running", Reason="", readiness=true. Elapsed: 2.008404545s
    Nov 30 03:40:56.705: INFO: Pod "execpod-affinityqmk4h" satisfied condition "running"
    Nov 30 03:40:57.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4219 exec execpod-affinityqmk4h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Nov 30 03:40:57.828: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Nov 30 03:40:57.828: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:40:57.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4219 exec execpod-affinityqmk4h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.154.254 80'
    Nov 30 03:40:57.949: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.154.254 80\nConnection to 10.110.154.254 80 port [tcp/http] succeeded!\n"
    Nov 30 03:40:57.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:40:57.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4219 exec execpod-affinityqmk4h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.154.254:80/ ; done'
    Nov 30 03:40:58.152: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n"
    Nov 30 03:40:58.152: INFO: stdout: "\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp\naffinity-clusterip-timeout-vb8dp"
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Received response from host: affinity-clusterip-timeout-vb8dp
    Nov 30 03:40:58.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4219 exec execpod-affinityqmk4h -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.110.154.254:80/'
    Nov 30 03:40:58.277: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n"
    Nov 30 03:40:58.277: INFO: stdout: "affinity-clusterip-timeout-vb8dp"
    Nov 30 03:43:08.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-4219 exec execpod-affinityqmk4h -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.110.154.254:80/'
    Nov 30 03:43:08.400: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.110.154.254:80/\n"
    Nov 30 03:43:08.400: INFO: stdout: "affinity-clusterip-timeout-8b55t"
    Nov 30 03:43:08.400: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4219, will wait for the garbage collector to delete the pods 11/30/22 03:43:08.418
    Nov 30 03:43:08.478: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.580729ms
    Nov 30 03:43:08.578: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.18897ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 03:43:10.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4219" for this suite. 11/30/22 03:43:10.744
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:43:10.756
Nov 30 03:43:10.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename subpath 11/30/22 03:43:10.756
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:43:10.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:43:10.787
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/30/22 03:43:10.791
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-p5mb 11/30/22 03:43:10.81
STEP: Creating a pod to test atomic-volume-subpath 11/30/22 03:43:10.81
Nov 30 03:43:10.847: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p5mb" in namespace "subpath-4951" to be "Succeeded or Failed"
Nov 30 03:43:10.851: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.299791ms
Nov 30 03:43:12.854: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 2.00735121s
Nov 30 03:43:14.857: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 4.009572268s
Nov 30 03:43:16.855: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 6.007517356s
Nov 30 03:43:18.856: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 8.009345183s
Nov 30 03:43:20.855: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 10.00757349s
Nov 30 03:43:22.855: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 12.007454472s
Nov 30 03:43:24.856: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 14.009386394s
Nov 30 03:43:26.857: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 16.009429334s
Nov 30 03:43:28.855: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 18.007828301s
Nov 30 03:43:30.856: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 20.008861462s
Nov 30 03:43:32.855: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=false. Elapsed: 22.00781386s
Nov 30 03:43:34.856: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008897367s
STEP: Saw pod success 11/30/22 03:43:34.856
Nov 30 03:43:34.856: INFO: Pod "pod-subpath-test-configmap-p5mb" satisfied condition "Succeeded or Failed"
Nov 30 03:43:34.859: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-subpath-test-configmap-p5mb container test-container-subpath-configmap-p5mb: <nil>
STEP: delete the pod 11/30/22 03:43:34.876
Nov 30 03:43:34.892: INFO: Waiting for pod pod-subpath-test-configmap-p5mb to disappear
Nov 30 03:43:34.894: INFO: Pod pod-subpath-test-configmap-p5mb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-p5mb 11/30/22 03:43:34.894
Nov 30 03:43:34.894: INFO: Deleting pod "pod-subpath-test-configmap-p5mb" in namespace "subpath-4951"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Nov 30 03:43:34.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4951" for this suite. 11/30/22 03:43:34.901
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":74,"skipped":1258,"failed":0}
------------------------------
• [SLOW TEST] [24.151 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:43:10.756
    Nov 30 03:43:10.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename subpath 11/30/22 03:43:10.756
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:43:10.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:43:10.787
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/30/22 03:43:10.791
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-p5mb 11/30/22 03:43:10.81
    STEP: Creating a pod to test atomic-volume-subpath 11/30/22 03:43:10.81
    Nov 30 03:43:10.847: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p5mb" in namespace "subpath-4951" to be "Succeeded or Failed"
    Nov 30 03:43:10.851: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.299791ms
    Nov 30 03:43:12.854: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 2.00735121s
    Nov 30 03:43:14.857: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 4.009572268s
    Nov 30 03:43:16.855: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 6.007517356s
    Nov 30 03:43:18.856: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 8.009345183s
    Nov 30 03:43:20.855: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 10.00757349s
    Nov 30 03:43:22.855: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 12.007454472s
    Nov 30 03:43:24.856: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 14.009386394s
    Nov 30 03:43:26.857: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 16.009429334s
    Nov 30 03:43:28.855: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 18.007828301s
    Nov 30 03:43:30.856: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=true. Elapsed: 20.008861462s
    Nov 30 03:43:32.855: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Running", Reason="", readiness=false. Elapsed: 22.00781386s
    Nov 30 03:43:34.856: INFO: Pod "pod-subpath-test-configmap-p5mb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008897367s
    STEP: Saw pod success 11/30/22 03:43:34.856
    Nov 30 03:43:34.856: INFO: Pod "pod-subpath-test-configmap-p5mb" satisfied condition "Succeeded or Failed"
    Nov 30 03:43:34.859: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-subpath-test-configmap-p5mb container test-container-subpath-configmap-p5mb: <nil>
    STEP: delete the pod 11/30/22 03:43:34.876
    Nov 30 03:43:34.892: INFO: Waiting for pod pod-subpath-test-configmap-p5mb to disappear
    Nov 30 03:43:34.894: INFO: Pod pod-subpath-test-configmap-p5mb no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-p5mb 11/30/22 03:43:34.894
    Nov 30 03:43:34.894: INFO: Deleting pod "pod-subpath-test-configmap-p5mb" in namespace "subpath-4951"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Nov 30 03:43:34.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4951" for this suite. 11/30/22 03:43:34.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:43:34.907
Nov 30 03:43:34.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir-wrapper 11/30/22 03:43:34.908
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:43:34.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:43:34.93
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 11/30/22 03:43:34.931
STEP: Creating RC which spawns configmap-volume pods 11/30/22 03:43:35.171
Nov 30 03:43:35.263: INFO: Pod name wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c: Found 1 pods out of 5
Nov 30 03:43:40.271: INFO: Pod name wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c: Found 5 pods out of 5
STEP: Ensuring each pod is running 11/30/22 03:43:40.271
Nov 30 03:43:40.271: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:43:40.275: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.476988ms
Nov 30 03:43:42.279: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007513221s
Nov 30 03:43:44.278: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007041211s
Nov 30 03:43:46.279: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007333301s
Nov 30 03:43:48.278: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006802568s
Nov 30 03:43:50.280: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s": Phase="Running", Reason="", readiness=true. Elapsed: 10.008514682s
Nov 30 03:43:50.280: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s" satisfied condition "running"
Nov 30 03:43:50.280: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-nm9s5" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:43:50.283: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-nm9s5": Phase="Running", Reason="", readiness=true. Elapsed: 3.299264ms
Nov 30 03:43:50.283: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-nm9s5" satisfied condition "running"
Nov 30 03:43:50.283: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-qsbjs" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:43:50.286: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-qsbjs": Phase="Running", Reason="", readiness=true. Elapsed: 3.025447ms
Nov 30 03:43:50.286: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-qsbjs" satisfied condition "running"
Nov 30 03:43:50.286: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-wmfwc" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:43:50.290: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-wmfwc": Phase="Running", Reason="", readiness=true. Elapsed: 3.45417ms
Nov 30 03:43:50.290: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-wmfwc" satisfied condition "running"
Nov 30 03:43:50.290: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-zfk4r" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:43:50.295: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-zfk4r": Phase="Running", Reason="", readiness=true. Elapsed: 5.371877ms
Nov 30 03:43:50.295: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-zfk4r" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c in namespace emptydir-wrapper-6701, will wait for the garbage collector to delete the pods 11/30/22 03:43:50.295
Nov 30 03:43:50.358: INFO: Deleting ReplicationController wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c took: 8.632626ms
Nov 30 03:43:50.458: INFO: Terminating ReplicationController wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c pods took: 100.716655ms
STEP: Creating RC which spawns configmap-volume pods 11/30/22 03:43:54.163
Nov 30 03:43:54.179: INFO: Pod name wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43: Found 0 pods out of 5
Nov 30 03:43:59.184: INFO: Pod name wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43: Found 5 pods out of 5
STEP: Ensuring each pod is running 11/30/22 03:43:59.184
Nov 30 03:43:59.184: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:43:59.188: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.347868ms
Nov 30 03:44:01.191: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006549024s
Nov 30 03:44:03.191: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007058208s
Nov 30 03:44:05.194: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009964507s
Nov 30 03:44:07.191: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00684959s
Nov 30 03:44:09.194: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c": Phase="Running", Reason="", readiness=true. Elapsed: 10.010097675s
Nov 30 03:44:09.194: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c" satisfied condition "running"
Nov 30 03:44:09.194: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-78x4f" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:44:09.197: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-78x4f": Phase="Running", Reason="", readiness=true. Elapsed: 2.64091ms
Nov 30 03:44:09.197: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-78x4f" satisfied condition "running"
Nov 30 03:44:09.197: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-bgck7" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:44:09.200: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-bgck7": Phase="Running", Reason="", readiness=true. Elapsed: 2.940262ms
Nov 30 03:44:09.200: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-bgck7" satisfied condition "running"
Nov 30 03:44:09.200: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-qq75n" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:44:09.202: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-qq75n": Phase="Running", Reason="", readiness=true. Elapsed: 2.383652ms
Nov 30 03:44:09.202: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-qq75n" satisfied condition "running"
Nov 30 03:44:09.202: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-v9626" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:44:09.205: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-v9626": Phase="Running", Reason="", readiness=true. Elapsed: 2.704125ms
Nov 30 03:44:09.205: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-v9626" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43 in namespace emptydir-wrapper-6701, will wait for the garbage collector to delete the pods 11/30/22 03:44:09.205
Nov 30 03:44:09.265: INFO: Deleting ReplicationController wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43 took: 6.916365ms
Nov 30 03:44:09.366: INFO: Terminating ReplicationController wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43 pods took: 101.125602ms
STEP: Creating RC which spawns configmap-volume pods 11/30/22 03:44:12.471
Nov 30 03:44:12.484: INFO: Pod name wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6: Found 0 pods out of 5
Nov 30 03:44:17.491: INFO: Pod name wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6: Found 5 pods out of 5
STEP: Ensuring each pod is running 11/30/22 03:44:17.491
Nov 30 03:44:17.491: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:44:17.494: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.922387ms
Nov 30 03:44:19.498: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006588722s
Nov 30 03:44:21.498: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00731954s
Nov 30 03:44:23.499: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007524128s
Nov 30 03:44:25.502: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011278921s
Nov 30 03:44:27.498: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk": Phase="Running", Reason="", readiness=true. Elapsed: 10.007137925s
Nov 30 03:44:27.498: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk" satisfied condition "running"
Nov 30 03:44:27.498: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-6j6v8" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:44:27.501: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-6j6v8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139455ms
Nov 30 03:44:29.506: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-6j6v8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008074526s
Nov 30 03:44:29.506: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-6j6v8" satisfied condition "running"
Nov 30 03:44:29.506: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-8rkl5" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:44:29.513: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-8rkl5": Phase="Running", Reason="", readiness=true. Elapsed: 6.046638ms
Nov 30 03:44:29.513: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-8rkl5" satisfied condition "running"
Nov 30 03:44:29.513: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-sj9zd" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:44:29.516: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-sj9zd": Phase="Running", Reason="", readiness=true. Elapsed: 3.83314ms
Nov 30 03:44:29.516: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-sj9zd" satisfied condition "running"
Nov 30 03:44:29.516: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-x62jc" in namespace "emptydir-wrapper-6701" to be "running"
Nov 30 03:44:29.520: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-x62jc": Phase="Running", Reason="", readiness=true. Elapsed: 3.143102ms
Nov 30 03:44:29.520: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-x62jc" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6 in namespace emptydir-wrapper-6701, will wait for the garbage collector to delete the pods 11/30/22 03:44:29.52
Nov 30 03:44:29.579: INFO: Deleting ReplicationController wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6 took: 6.148962ms
Nov 30 03:44:29.680: INFO: Terminating ReplicationController wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6 pods took: 100.478097ms
STEP: Cleaning up the configMaps 11/30/22 03:44:33.28
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Nov 30 03:44:33.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6701" for this suite. 11/30/22 03:44:33.679
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":75,"skipped":1289,"failed":0}
------------------------------
• [SLOW TEST] [58.777 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:43:34.907
    Nov 30 03:43:34.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir-wrapper 11/30/22 03:43:34.908
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:43:34.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:43:34.93
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 11/30/22 03:43:34.931
    STEP: Creating RC which spawns configmap-volume pods 11/30/22 03:43:35.171
    Nov 30 03:43:35.263: INFO: Pod name wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c: Found 1 pods out of 5
    Nov 30 03:43:40.271: INFO: Pod name wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c: Found 5 pods out of 5
    STEP: Ensuring each pod is running 11/30/22 03:43:40.271
    Nov 30 03:43:40.271: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:43:40.275: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.476988ms
    Nov 30 03:43:42.279: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007513221s
    Nov 30 03:43:44.278: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007041211s
    Nov 30 03:43:46.279: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007333301s
    Nov 30 03:43:48.278: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006802568s
    Nov 30 03:43:50.280: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s": Phase="Running", Reason="", readiness=true. Elapsed: 10.008514682s
    Nov 30 03:43:50.280: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-5w95s" satisfied condition "running"
    Nov 30 03:43:50.280: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-nm9s5" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:43:50.283: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-nm9s5": Phase="Running", Reason="", readiness=true. Elapsed: 3.299264ms
    Nov 30 03:43:50.283: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-nm9s5" satisfied condition "running"
    Nov 30 03:43:50.283: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-qsbjs" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:43:50.286: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-qsbjs": Phase="Running", Reason="", readiness=true. Elapsed: 3.025447ms
    Nov 30 03:43:50.286: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-qsbjs" satisfied condition "running"
    Nov 30 03:43:50.286: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-wmfwc" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:43:50.290: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-wmfwc": Phase="Running", Reason="", readiness=true. Elapsed: 3.45417ms
    Nov 30 03:43:50.290: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-wmfwc" satisfied condition "running"
    Nov 30 03:43:50.290: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-zfk4r" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:43:50.295: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-zfk4r": Phase="Running", Reason="", readiness=true. Elapsed: 5.371877ms
    Nov 30 03:43:50.295: INFO: Pod "wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c-zfk4r" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c in namespace emptydir-wrapper-6701, will wait for the garbage collector to delete the pods 11/30/22 03:43:50.295
    Nov 30 03:43:50.358: INFO: Deleting ReplicationController wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c took: 8.632626ms
    Nov 30 03:43:50.458: INFO: Terminating ReplicationController wrapped-volume-race-4c9c14e2-3984-4771-9414-97669c89562c pods took: 100.716655ms
    STEP: Creating RC which spawns configmap-volume pods 11/30/22 03:43:54.163
    Nov 30 03:43:54.179: INFO: Pod name wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43: Found 0 pods out of 5
    Nov 30 03:43:59.184: INFO: Pod name wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43: Found 5 pods out of 5
    STEP: Ensuring each pod is running 11/30/22 03:43:59.184
    Nov 30 03:43:59.184: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:43:59.188: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.347868ms
    Nov 30 03:44:01.191: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006549024s
    Nov 30 03:44:03.191: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007058208s
    Nov 30 03:44:05.194: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009964507s
    Nov 30 03:44:07.191: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00684959s
    Nov 30 03:44:09.194: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c": Phase="Running", Reason="", readiness=true. Elapsed: 10.010097675s
    Nov 30 03:44:09.194: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-5wn6c" satisfied condition "running"
    Nov 30 03:44:09.194: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-78x4f" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:44:09.197: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-78x4f": Phase="Running", Reason="", readiness=true. Elapsed: 2.64091ms
    Nov 30 03:44:09.197: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-78x4f" satisfied condition "running"
    Nov 30 03:44:09.197: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-bgck7" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:44:09.200: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-bgck7": Phase="Running", Reason="", readiness=true. Elapsed: 2.940262ms
    Nov 30 03:44:09.200: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-bgck7" satisfied condition "running"
    Nov 30 03:44:09.200: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-qq75n" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:44:09.202: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-qq75n": Phase="Running", Reason="", readiness=true. Elapsed: 2.383652ms
    Nov 30 03:44:09.202: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-qq75n" satisfied condition "running"
    Nov 30 03:44:09.202: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-v9626" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:44:09.205: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-v9626": Phase="Running", Reason="", readiness=true. Elapsed: 2.704125ms
    Nov 30 03:44:09.205: INFO: Pod "wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43-v9626" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43 in namespace emptydir-wrapper-6701, will wait for the garbage collector to delete the pods 11/30/22 03:44:09.205
    Nov 30 03:44:09.265: INFO: Deleting ReplicationController wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43 took: 6.916365ms
    Nov 30 03:44:09.366: INFO: Terminating ReplicationController wrapped-volume-race-5a270fc4-692a-4f44-b21c-58507773ca43 pods took: 101.125602ms
    STEP: Creating RC which spawns configmap-volume pods 11/30/22 03:44:12.471
    Nov 30 03:44:12.484: INFO: Pod name wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6: Found 0 pods out of 5
    Nov 30 03:44:17.491: INFO: Pod name wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6: Found 5 pods out of 5
    STEP: Ensuring each pod is running 11/30/22 03:44:17.491
    Nov 30 03:44:17.491: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:44:17.494: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.922387ms
    Nov 30 03:44:19.498: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006588722s
    Nov 30 03:44:21.498: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00731954s
    Nov 30 03:44:23.499: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007524128s
    Nov 30 03:44:25.502: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011278921s
    Nov 30 03:44:27.498: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk": Phase="Running", Reason="", readiness=true. Elapsed: 10.007137925s
    Nov 30 03:44:27.498: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-2ckpk" satisfied condition "running"
    Nov 30 03:44:27.498: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-6j6v8" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:44:27.501: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-6j6v8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139455ms
    Nov 30 03:44:29.506: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-6j6v8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008074526s
    Nov 30 03:44:29.506: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-6j6v8" satisfied condition "running"
    Nov 30 03:44:29.506: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-8rkl5" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:44:29.513: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-8rkl5": Phase="Running", Reason="", readiness=true. Elapsed: 6.046638ms
    Nov 30 03:44:29.513: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-8rkl5" satisfied condition "running"
    Nov 30 03:44:29.513: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-sj9zd" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:44:29.516: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-sj9zd": Phase="Running", Reason="", readiness=true. Elapsed: 3.83314ms
    Nov 30 03:44:29.516: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-sj9zd" satisfied condition "running"
    Nov 30 03:44:29.516: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-x62jc" in namespace "emptydir-wrapper-6701" to be "running"
    Nov 30 03:44:29.520: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-x62jc": Phase="Running", Reason="", readiness=true. Elapsed: 3.143102ms
    Nov 30 03:44:29.520: INFO: Pod "wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6-x62jc" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6 in namespace emptydir-wrapper-6701, will wait for the garbage collector to delete the pods 11/30/22 03:44:29.52
    Nov 30 03:44:29.579: INFO: Deleting ReplicationController wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6 took: 6.148962ms
    Nov 30 03:44:29.680: INFO: Terminating ReplicationController wrapped-volume-race-88374ed5-6661-409e-b42a-dd44d7b5ffd6 pods took: 100.478097ms
    STEP: Cleaning up the configMaps 11/30/22 03:44:33.28
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Nov 30 03:44:33.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-6701" for this suite. 11/30/22 03:44:33.679
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:44:33.685
Nov 30 03:44:33.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:44:33.685
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:44:33.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:44:33.714
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-78fdf78d-b93d-4493-b12b-46129be0e879 11/30/22 03:44:33.728
STEP: Creating the pod 11/30/22 03:44:33.733
Nov 30 03:44:33.771: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-db046323-985a-4002-99e7-bf867007cfba" in namespace "projected-1167" to be "running and ready"
Nov 30 03:44:33.774: INFO: Pod "pod-projected-configmaps-db046323-985a-4002-99e7-bf867007cfba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.846377ms
Nov 30 03:44:33.774: INFO: The phase of Pod pod-projected-configmaps-db046323-985a-4002-99e7-bf867007cfba is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:44:35.779: INFO: Pod "pod-projected-configmaps-db046323-985a-4002-99e7-bf867007cfba": Phase="Running", Reason="", readiness=true. Elapsed: 2.007840082s
Nov 30 03:44:35.779: INFO: The phase of Pod pod-projected-configmaps-db046323-985a-4002-99e7-bf867007cfba is Running (Ready = true)
Nov 30 03:44:35.779: INFO: Pod "pod-projected-configmaps-db046323-985a-4002-99e7-bf867007cfba" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-78fdf78d-b93d-4493-b12b-46129be0e879 11/30/22 03:44:35.786
STEP: waiting to observe update in volume 11/30/22 03:44:35.791
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Nov 30 03:44:37.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1167" for this suite. 11/30/22 03:44:37.807
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":76,"skipped":1289,"failed":0}
------------------------------
• [4.132 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:44:33.685
    Nov 30 03:44:33.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:44:33.685
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:44:33.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:44:33.714
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-78fdf78d-b93d-4493-b12b-46129be0e879 11/30/22 03:44:33.728
    STEP: Creating the pod 11/30/22 03:44:33.733
    Nov 30 03:44:33.771: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-db046323-985a-4002-99e7-bf867007cfba" in namespace "projected-1167" to be "running and ready"
    Nov 30 03:44:33.774: INFO: Pod "pod-projected-configmaps-db046323-985a-4002-99e7-bf867007cfba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.846377ms
    Nov 30 03:44:33.774: INFO: The phase of Pod pod-projected-configmaps-db046323-985a-4002-99e7-bf867007cfba is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:44:35.779: INFO: Pod "pod-projected-configmaps-db046323-985a-4002-99e7-bf867007cfba": Phase="Running", Reason="", readiness=true. Elapsed: 2.007840082s
    Nov 30 03:44:35.779: INFO: The phase of Pod pod-projected-configmaps-db046323-985a-4002-99e7-bf867007cfba is Running (Ready = true)
    Nov 30 03:44:35.779: INFO: Pod "pod-projected-configmaps-db046323-985a-4002-99e7-bf867007cfba" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-78fdf78d-b93d-4493-b12b-46129be0e879 11/30/22 03:44:35.786
    STEP: waiting to observe update in volume 11/30/22 03:44:35.791
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Nov 30 03:44:37.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1167" for this suite. 11/30/22 03:44:37.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:44:37.818
Nov 30 03:44:37.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:44:37.818
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:44:37.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:44:37.844
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 11/30/22 03:44:37.846
Nov 30 03:44:37.866: INFO: Waiting up to 5m0s for pod "annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50" in namespace "projected-6304" to be "running and ready"
Nov 30 03:44:37.869: INFO: Pod "annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50": Phase="Pending", Reason="", readiness=false. Elapsed: 3.11329ms
Nov 30 03:44:37.869: INFO: The phase of Pod annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:44:39.873: INFO: Pod "annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50": Phase="Running", Reason="", readiness=true. Elapsed: 2.006871179s
Nov 30 03:44:39.873: INFO: The phase of Pod annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50 is Running (Ready = true)
Nov 30 03:44:39.873: INFO: Pod "annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50" satisfied condition "running and ready"
Nov 30 03:44:40.392: INFO: Successfully updated pod "annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Nov 30 03:44:44.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6304" for this suite. 11/30/22 03:44:44.417
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":77,"skipped":1308,"failed":0}
------------------------------
• [SLOW TEST] [6.607 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:44:37.818
    Nov 30 03:44:37.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:44:37.818
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:44:37.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:44:37.844
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 11/30/22 03:44:37.846
    Nov 30 03:44:37.866: INFO: Waiting up to 5m0s for pod "annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50" in namespace "projected-6304" to be "running and ready"
    Nov 30 03:44:37.869: INFO: Pod "annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50": Phase="Pending", Reason="", readiness=false. Elapsed: 3.11329ms
    Nov 30 03:44:37.869: INFO: The phase of Pod annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:44:39.873: INFO: Pod "annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50": Phase="Running", Reason="", readiness=true. Elapsed: 2.006871179s
    Nov 30 03:44:39.873: INFO: The phase of Pod annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50 is Running (Ready = true)
    Nov 30 03:44:39.873: INFO: Pod "annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50" satisfied condition "running and ready"
    Nov 30 03:44:40.392: INFO: Successfully updated pod "annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Nov 30 03:44:44.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6304" for this suite. 11/30/22 03:44:44.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:44:44.425
Nov 30 03:44:44.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename sched-pred 11/30/22 03:44:44.426
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:44:44.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:44:44.445
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Nov 30 03:44:44.447: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 30 03:44:44.458: INFO: Waiting for terminating namespaces to be deleted...
Nov 30 03:44:44.462: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins before test
Nov 30 03:44:44.474: INFO: calico-node-9bcg6 from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 03:44:44.474: INFO: csi-cinder-nodeplugin-hnpf8 from kube-system started at 2022-11-30 02:52:37 +0000 UTC (3 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 03:44:44.474: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 03:44:44.474: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 03:44:44.474: INFO: eric-tm-external-connectivity-frontend-controller-689dd9fd5zc6t from kube-system started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container controller ready: true, restart count 0
Nov 30 03:44:44.474: INFO: eric-tm-external-connectivity-frontend-speaker-wdz6h from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container speaker ready: true, restart count 0
Nov 30 03:44:44.474: INFO: kube-multus-ds-amd64-nw2qp from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 03:44:44.474: INFO: kube-proxy-stdnr from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 30 03:44:44.474: INFO: kucero-qz2ld from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container kucero ready: true, restart count 0
Nov 30 03:44:44.474: INFO: node-local-dns-wvqkc from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container node-cache ready: true, restart count 1
Nov 30 03:44:44.474: INFO: eric-pm-alertmanager-76c454d9f7-vplrf from monitoring started at 2022-11-30 03:38:30 +0000 UTC (2 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
Nov 30 03:44:44.474: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
Nov 30 03:44:44.474: INFO: eric-pm-node-exporter-rlfnb from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 03:44:44.474: INFO: node-cert-exporter-hn5jk from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 03:44:44.474: INFO: annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50 from projected-6304 started at 2022-11-30 03:44:37 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container client-container ready: true, restart count 0
Nov 30 03:44:44.474: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-6bvhh from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 03:44:44.474: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 03:44:44.474: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 30 03:44:44.474: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins before test
Nov 30 03:44:44.486: INFO: default-http-backend-6f4f64db57-dnh7m from ingress-nginx started at 2022-11-30 02:55:20 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container default-http-backend ready: true, restart count 0
Nov 30 03:44:44.486: INFO: nginx-ingress-controller-844dff9bc5-684fv from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Nov 30 03:44:44.486: INFO: calico-node-thlq4 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 03:44:44.486: INFO: calicoctl-c47c68f5-42nfq from kube-system started at 2022-11-30 03:07:12 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container calicoctl ready: true, restart count 0
Nov 30 03:44:44.486: INFO: csi-cinder-nodeplugin-5hx92 from kube-system started at 2022-11-30 02:52:16 +0000 UTC (3 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 03:44:44.486: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 03:44:44.486: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 03:44:44.486: INFO: eric-app-sys-info-handler-754d6dcc6b-5xw88 from kube-system started at 2022-11-30 03:02:36 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
Nov 30 03:44:44.486: INFO: eric-data-document-database-pg-0 from kube-system started at 2022-11-30 03:03:48 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
Nov 30 03:44:44.486: INFO: eric-tm-external-connectivity-frontend-speaker-vkmgp from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container speaker ready: true, restart count 0
Nov 30 03:44:44.486: INFO: kube-multus-ds-amd64-lhrwf from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 03:44:44.486: INFO: kube-proxy-g68qp from kube-system started at 2022-11-30 02:51:41 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container kube-proxy ready: true, restart count 1
Nov 30 03:44:44.486: INFO: kucero-29zhp from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container kucero ready: true, restart count 0
Nov 30 03:44:44.486: INFO: network-resources-injector-6dfc58d4f-rm8m2 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container webhook-server ready: true, restart count 0
Nov 30 03:44:44.486: INFO: node-local-dns-4fvm6 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.486: INFO: 	Container node-cache ready: true, restart count 0
Nov 30 03:44:44.487: INFO: eric-pm-kube-state-metrics-6696677569-fnnhj from monitoring started at 2022-11-30 02:58:51 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.487: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
Nov 30 03:44:44.487: INFO: eric-pm-node-exporter-dhtts from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.487: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 03:44:44.487: INFO: eric-pm-pushgateway-6db47dc9d7-fbx4n from monitoring started at 2022-11-30 02:59:19 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.487: INFO: 	Container pushgateway ready: true, restart count 0
Nov 30 03:44:44.487: INFO: eric-victoria-metrics-agent-c9f978858-cmz9r from monitoring started at 2022-11-30 02:57:48 +0000 UTC (2 container statuses recorded)
Nov 30 03:44:44.487: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
Nov 30 03:44:44.487: INFO: 	Container vmagent-config-reload ready: true, restart count 0
Nov 30 03:44:44.487: INFO: node-cert-exporter-jbl7k from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.487: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 03:44:44.487: INFO: sonobuoy-e2e-job-681d55e1c5a74dc7 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 03:44:44.487: INFO: 	Container e2e ready: true, restart count 0
Nov 30 03:44:44.487: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 03:44:44.487: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-nbjs4 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 03:44:44.487: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 03:44:44.487: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 30 03:44:44.487: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins before test
Nov 30 03:44:44.498: INFO: nginx-ingress-controller-844dff9bc5-7std7 from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Nov 30 03:44:44.498: INFO: calico-node-c6ckn from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 03:44:44.498: INFO: csi-cinder-nodeplugin-xvtjm from kube-system started at 2022-11-30 02:52:53 +0000 UTC (3 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 03:44:44.498: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 03:44:44.498: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 03:44:44.498: INFO: eric-lm-combined-server-license-server-client-d9b78bd6b-5r5rz from kube-system started at 2022-11-30 03:04:36 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Nov 30 03:44:44.498: INFO: eric-tm-external-connectivity-frontend-speaker-9k8gj from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container speaker ready: true, restart count 0
Nov 30 03:44:44.498: INFO: kube-multus-ds-amd64-rk7dk from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 03:44:44.498: INFO: kube-proxy-2zzdw from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 30 03:44:44.498: INFO: kucero-nds94 from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container kucero ready: true, restart count 0
Nov 30 03:44:44.498: INFO: node-local-dns-4bsnx from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container node-cache ready: true, restart count 1
Nov 30 03:44:44.498: INFO: eric-pm-node-exporter-xs7lt from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 03:44:44.498: INFO: eric-pm-server-utils-7585bb6b5d-7btvg from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
Nov 30 03:44:44.498: INFO: eric-victoria-metrics-alert-server-54c5c5474c-mhjjn from monitoring started at 2022-11-30 03:38:30 +0000 UTC (2 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
Nov 30 03:44:44.498: INFO: 	Container vmalert-config-reload ready: true, restart count 0
Nov 30 03:44:44.498: INFO: eric-victoria-metrics-cluster-vmselect-55c4b48dfb-lnx7j from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
Nov 30 03:44:44.498: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2022-11-30 02:57:08 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
Nov 30 03:44:44.498: INFO: node-cert-exporter-l77s4 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 03:44:44.498: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-88nb5 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 03:44:44.498: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 03:44:44.498: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 30 03:44:44.498: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins before test
Nov 30 03:44:44.510: INFO: calico-node-klj9m from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 03:44:44.510: INFO: ccd-license-consumer-5949d66498-mjgr8 from kube-system started at 2022-11-30 03:05:25 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Nov 30 03:44:44.510: INFO: csi-cinder-nodeplugin-sw4wz from kube-system started at 2022-11-30 02:52:31 +0000 UTC (3 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 03:44:44.510: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 03:44:44.510: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 03:44:44.510: INFO: eric-lcm-container-registry-registry-7f7856985-4rfb5 from kube-system started at 2022-11-30 02:56:13 +0000 UTC (3 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
Nov 30 03:44:44.510: INFO: 	Container registry ready: true, restart count 0
Nov 30 03:44:44.510: INFO: 	Container sidecar ready: true, restart count 0
Nov 30 03:44:44.510: INFO: eric-lm-combined-server-license-consumer-handler-75bfcf7bc2tbcb from kube-system started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Nov 30 03:44:44.510: INFO: eric-tm-external-connectivity-frontend-speaker-vmfpl from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container speaker ready: true, restart count 0
Nov 30 03:44:44.510: INFO: kube-multus-ds-amd64-cf4v4 from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 03:44:44.510: INFO: kube-proxy-6s7bb from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 30 03:44:44.510: INFO: kucero-5bwbh from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container kucero ready: true, restart count 0
Nov 30 03:44:44.510: INFO: metrics-server-7567d6784b-dw2c8 from kube-system started at 2022-11-30 03:01:29 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container metrics-server ready: true, restart count 0
Nov 30 03:44:44.510: INFO: network-resources-injector-6dfc58d4f-nnqj7 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container webhook-server ready: true, restart count 0
Nov 30 03:44:44.510: INFO: node-local-dns-4hp9c from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container node-cache ready: true, restart count 1
Nov 30 03:44:44.510: INFO: eric-pm-node-exporter-lmw6v from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 03:44:44.510: INFO: eric-victoria-metrics-cluster-vminsert-6c76f96c54-jsw5z from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
Nov 30 03:44:44.510: INFO: isp-logger-5b4d57f796-5ncwr from monitoring started at 2022-11-30 03:06:55 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container isp-logger ready: true, restart count 0
Nov 30 03:44:44.510: INFO: node-cert-exporter-v97f6 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 03:44:44.510: INFO: sonobuoy from sonobuoy started at 2022-11-30 03:19:54 +0000 UTC (1 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 30 03:44:44.510: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-z79zw from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 03:44:44.510: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 03:44:44.510: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 11/30/22 03:44:44.51
Nov 30 03:44:44.545: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1333" to be "running"
Nov 30 03:44:44.547: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.692737ms
Nov 30 03:44:46.552: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007649125s
Nov 30 03:44:46.552: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 11/30/22 03:44:46.557
STEP: Trying to apply a random label on the found node. 11/30/22 03:44:46.582
STEP: verifying the node has the label kubernetes.io/e2e-41715ea7-5338-4b79-a66b-fbfcd5403c09 95 11/30/22 03:44:46.594
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 11/30/22 03:44:46.597
W1130 03:44:46.609321      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54322)
Nov 30 03:44:46.609: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1333" to be "not pending"
Nov 30 03:44:46.612: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.670643ms
Nov 30 03:44:48.616: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006985356s
Nov 30 03:44:48.616: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.10.8 on the node which pod4 resides and expect not scheduled 11/30/22 03:44:48.616
W1130 03:44:48.625900      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54322)
Nov 30 03:44:48.625: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1333" to be "not pending"
Nov 30 03:44:48.628: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.248832ms
Nov 30 03:44:50.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006194313s
Nov 30 03:44:52.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005844255s
Nov 30 03:44:54.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010559333s
Nov 30 03:44:56.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007094916s
Nov 30 03:44:58.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006771869s
Nov 30 03:45:00.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006903874s
Nov 30 03:45:02.642: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016582414s
Nov 30 03:45:04.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010067663s
Nov 30 03:45:06.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006096046s
Nov 30 03:45:08.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006737281s
Nov 30 03:45:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006712563s
Nov 30 03:45:12.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005637572s
Nov 30 03:45:14.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007873997s
Nov 30 03:45:16.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006330087s
Nov 30 03:45:18.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005579079s
Nov 30 03:45:20.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.0056786s
Nov 30 03:45:22.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005964557s
Nov 30 03:45:24.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007075192s
Nov 30 03:45:26.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007295895s
Nov 30 03:45:28.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006699421s
Nov 30 03:45:30.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006465306s
Nov 30 03:45:32.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006773508s
Nov 30 03:45:34.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007336975s
Nov 30 03:45:36.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007527856s
Nov 30 03:45:38.630: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00496199s
Nov 30 03:45:40.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005815849s
Nov 30 03:45:42.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005793914s
Nov 30 03:45:44.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005999284s
Nov 30 03:45:46.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006663009s
Nov 30 03:45:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00591095s
Nov 30 03:45:50.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006040615s
Nov 30 03:45:52.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006525899s
Nov 30 03:45:54.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008926144s
Nov 30 03:45:56.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007395024s
Nov 30 03:45:58.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005627288s
Nov 30 03:46:00.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005738481s
Nov 30 03:46:02.642: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.01635113s
Nov 30 03:46:04.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007324449s
Nov 30 03:46:06.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007480104s
Nov 30 03:46:08.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005123448s
Nov 30 03:46:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006299355s
Nov 30 03:46:12.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005470743s
Nov 30 03:46:14.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005753709s
Nov 30 03:46:16.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007000915s
Nov 30 03:46:18.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.005570062s
Nov 30 03:46:20.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006008946s
Nov 30 03:46:22.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005919721s
Nov 30 03:46:24.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009350171s
Nov 30 03:46:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006899433s
Nov 30 03:46:28.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005060948s
Nov 30 03:46:30.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007621582s
Nov 30 03:46:32.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006488375s
Nov 30 03:46:34.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006387905s
Nov 30 03:46:36.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007644865s
Nov 30 03:46:38.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005288295s
Nov 30 03:46:40.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006889744s
Nov 30 03:46:42.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005605502s
Nov 30 03:46:44.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009750215s
Nov 30 03:46:46.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006028261s
Nov 30 03:46:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005543221s
Nov 30 03:46:50.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.006726645s
Nov 30 03:46:52.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006788553s
Nov 30 03:46:54.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009921674s
Nov 30 03:46:56.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.006554162s
Nov 30 03:46:58.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006808395s
Nov 30 03:47:00.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.0073717s
Nov 30 03:47:02.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.005809389s
Nov 30 03:47:04.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.006770756s
Nov 30 03:47:06.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.006672584s
Nov 30 03:47:08.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.00540168s
Nov 30 03:47:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.006056304s
Nov 30 03:47:12.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.006144054s
Nov 30 03:47:14.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008008841s
Nov 30 03:47:16.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.00650463s
Nov 30 03:47:18.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.006724974s
Nov 30 03:47:20.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.00609524s
Nov 30 03:47:22.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.005651616s
Nov 30 03:47:24.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.006879224s
Nov 30 03:47:26.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.007384137s
Nov 30 03:47:28.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.006484908s
Nov 30 03:47:30.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.006808478s
Nov 30 03:47:32.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007242932s
Nov 30 03:47:34.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.008246712s
Nov 30 03:47:36.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005867542s
Nov 30 03:47:38.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006573689s
Nov 30 03:47:40.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007422712s
Nov 30 03:47:42.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.006011273s
Nov 30 03:47:44.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005778831s
Nov 30 03:47:46.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.005832721s
Nov 30 03:47:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.005549095s
Nov 30 03:47:50.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005913982s
Nov 30 03:47:52.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.005879586s
Nov 30 03:47:54.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.006473003s
Nov 30 03:47:56.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.005975983s
Nov 30 03:47:58.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005910127s
Nov 30 03:48:00.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007711371s
Nov 30 03:48:02.642: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.01666156s
Nov 30 03:48:04.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007317016s
Nov 30 03:48:06.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007508918s
Nov 30 03:48:08.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005954913s
Nov 30 03:48:10.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.00535002s
Nov 30 03:48:12.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005649259s
Nov 30 03:48:14.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008754628s
Nov 30 03:48:16.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.005635261s
Nov 30 03:48:18.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.006733067s
Nov 30 03:48:20.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005762432s
Nov 30 03:48:22.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.006051281s
Nov 30 03:48:24.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.006364894s
Nov 30 03:48:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006253471s
Nov 30 03:48:28.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.005228577s
Nov 30 03:48:30.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.007416298s
Nov 30 03:48:32.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.00745475s
Nov 30 03:48:34.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.009430408s
Nov 30 03:48:36.664: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.038370511s
Nov 30 03:48:38.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005698887s
Nov 30 03:48:40.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.005782031s
Nov 30 03:48:42.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.005529034s
Nov 30 03:48:44.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007789069s
Nov 30 03:48:46.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006678058s
Nov 30 03:48:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.00572319s
Nov 30 03:48:50.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007069832s
Nov 30 03:48:52.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.00585588s
Nov 30 03:48:54.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.010323269s
Nov 30 03:48:56.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.00697308s
Nov 30 03:48:58.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.006559221s
Nov 30 03:49:00.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.00607671s
Nov 30 03:49:02.643: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.017095298s
Nov 30 03:49:04.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006898508s
Nov 30 03:49:06.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005872291s
Nov 30 03:49:08.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.005654244s
Nov 30 03:49:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.006644583s
Nov 30 03:49:12.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.005682074s
Nov 30 03:49:14.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.006615629s
Nov 30 03:49:16.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005879828s
Nov 30 03:49:18.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.00543195s
Nov 30 03:49:20.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.007523652s
Nov 30 03:49:22.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.006129058s
Nov 30 03:49:24.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.010566981s
Nov 30 03:49:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.006532174s
Nov 30 03:49:28.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.006605863s
Nov 30 03:49:30.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.005426631s
Nov 30 03:49:32.644: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.018142328s
Nov 30 03:49:34.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.006784248s
Nov 30 03:49:36.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.007151581s
Nov 30 03:49:38.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.005965288s
Nov 30 03:49:40.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.006075027s
Nov 30 03:49:42.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.00611543s
Nov 30 03:49:44.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.008463689s
Nov 30 03:49:46.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.005657952s
Nov 30 03:49:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005481773s
Nov 30 03:49:48.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008350197s
STEP: removing the label kubernetes.io/e2e-41715ea7-5338-4b79-a66b-fbfcd5403c09 off the node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins 11/30/22 03:49:48.634
STEP: verifying the node doesn't have the label kubernetes.io/e2e-41715ea7-5338-4b79-a66b-fbfcd5403c09 11/30/22 03:49:48.648
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Nov 30 03:49:48.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1333" for this suite. 11/30/22 03:49:48.659
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":78,"skipped":1314,"failed":0}
------------------------------
• [SLOW TEST] [304.242 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:44:44.425
    Nov 30 03:44:44.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename sched-pred 11/30/22 03:44:44.426
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:44:44.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:44:44.445
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Nov 30 03:44:44.447: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 30 03:44:44.458: INFO: Waiting for terminating namespaces to be deleted...
    Nov 30 03:44:44.462: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins before test
    Nov 30 03:44:44.474: INFO: calico-node-9bcg6 from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: csi-cinder-nodeplugin-hnpf8 from kube-system started at 2022-11-30 02:52:37 +0000 UTC (3 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: eric-tm-external-connectivity-frontend-controller-689dd9fd5zc6t from kube-system started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container controller ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: eric-tm-external-connectivity-frontend-speaker-wdz6h from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: kube-multus-ds-amd64-nw2qp from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: kube-proxy-stdnr from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: kucero-qz2ld from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: node-local-dns-wvqkc from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container node-cache ready: true, restart count 1
    Nov 30 03:44:44.474: INFO: eric-pm-alertmanager-76c454d9f7-vplrf from monitoring started at 2022-11-30 03:38:30 +0000 UTC (2 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: eric-pm-node-exporter-rlfnb from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: node-cert-exporter-hn5jk from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: annotationupdate07ab6f77-6edf-4fa2-adc6-98c0ea5cce50 from projected-6304 started at 2022-11-30 03:44:37 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container client-container ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-6bvhh from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 03:44:44.474: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 30 03:44:44.474: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins before test
    Nov 30 03:44:44.486: INFO: default-http-backend-6f4f64db57-dnh7m from ingress-nginx started at 2022-11-30 02:55:20 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container default-http-backend ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: nginx-ingress-controller-844dff9bc5-684fv from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: calico-node-thlq4 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: calicoctl-c47c68f5-42nfq from kube-system started at 2022-11-30 03:07:12 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container calicoctl ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: csi-cinder-nodeplugin-5hx92 from kube-system started at 2022-11-30 02:52:16 +0000 UTC (3 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: eric-app-sys-info-handler-754d6dcc6b-5xw88 from kube-system started at 2022-11-30 03:02:36 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: eric-data-document-database-pg-0 from kube-system started at 2022-11-30 03:03:48 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: eric-tm-external-connectivity-frontend-speaker-vkmgp from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: kube-multus-ds-amd64-lhrwf from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: kube-proxy-g68qp from kube-system started at 2022-11-30 02:51:41 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container kube-proxy ready: true, restart count 1
    Nov 30 03:44:44.486: INFO: kucero-29zhp from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: network-resources-injector-6dfc58d4f-rm8m2 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container webhook-server ready: true, restart count 0
    Nov 30 03:44:44.486: INFO: node-local-dns-4fvm6 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.486: INFO: 	Container node-cache ready: true, restart count 0
    Nov 30 03:44:44.487: INFO: eric-pm-kube-state-metrics-6696677569-fnnhj from monitoring started at 2022-11-30 02:58:51 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.487: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
    Nov 30 03:44:44.487: INFO: eric-pm-node-exporter-dhtts from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.487: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 03:44:44.487: INFO: eric-pm-pushgateway-6db47dc9d7-fbx4n from monitoring started at 2022-11-30 02:59:19 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.487: INFO: 	Container pushgateway ready: true, restart count 0
    Nov 30 03:44:44.487: INFO: eric-victoria-metrics-agent-c9f978858-cmz9r from monitoring started at 2022-11-30 02:57:48 +0000 UTC (2 container statuses recorded)
    Nov 30 03:44:44.487: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
    Nov 30 03:44:44.487: INFO: 	Container vmagent-config-reload ready: true, restart count 0
    Nov 30 03:44:44.487: INFO: node-cert-exporter-jbl7k from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.487: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 03:44:44.487: INFO: sonobuoy-e2e-job-681d55e1c5a74dc7 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 03:44:44.487: INFO: 	Container e2e ready: true, restart count 0
    Nov 30 03:44:44.487: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 03:44:44.487: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-nbjs4 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 03:44:44.487: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 03:44:44.487: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 30 03:44:44.487: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins before test
    Nov 30 03:44:44.498: INFO: nginx-ingress-controller-844dff9bc5-7std7 from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: calico-node-c6ckn from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: csi-cinder-nodeplugin-xvtjm from kube-system started at 2022-11-30 02:52:53 +0000 UTC (3 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: eric-lm-combined-server-license-server-client-d9b78bd6b-5r5rz from kube-system started at 2022-11-30 03:04:36 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: eric-tm-external-connectivity-frontend-speaker-9k8gj from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: kube-multus-ds-amd64-rk7dk from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: kube-proxy-2zzdw from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: kucero-nds94 from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: node-local-dns-4bsnx from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container node-cache ready: true, restart count 1
    Nov 30 03:44:44.498: INFO: eric-pm-node-exporter-xs7lt from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: eric-pm-server-utils-7585bb6b5d-7btvg from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: eric-victoria-metrics-alert-server-54c5c5474c-mhjjn from monitoring started at 2022-11-30 03:38:30 +0000 UTC (2 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: 	Container vmalert-config-reload ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: eric-victoria-metrics-cluster-vmselect-55c4b48dfb-lnx7j from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2022-11-30 02:57:08 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: node-cert-exporter-l77s4 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-88nb5 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 03:44:44.498: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 30 03:44:44.498: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins before test
    Nov 30 03:44:44.510: INFO: calico-node-klj9m from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: ccd-license-consumer-5949d66498-mjgr8 from kube-system started at 2022-11-30 03:05:25 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container ccd-license-consumer ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: csi-cinder-nodeplugin-sw4wz from kube-system started at 2022-11-30 02:52:31 +0000 UTC (3 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: eric-lcm-container-registry-registry-7f7856985-4rfb5 from kube-system started at 2022-11-30 02:56:13 +0000 UTC (3 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: 	Container registry ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: 	Container sidecar ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: eric-lm-combined-server-license-consumer-handler-75bfcf7bc2tbcb from kube-system started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: eric-tm-external-connectivity-frontend-speaker-vmfpl from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: kube-multus-ds-amd64-cf4v4 from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: kube-proxy-6s7bb from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: kucero-5bwbh from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: metrics-server-7567d6784b-dw2c8 from kube-system started at 2022-11-30 03:01:29 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container metrics-server ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: network-resources-injector-6dfc58d4f-nnqj7 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container webhook-server ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: node-local-dns-4hp9c from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container node-cache ready: true, restart count 1
    Nov 30 03:44:44.510: INFO: eric-pm-node-exporter-lmw6v from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: eric-victoria-metrics-cluster-vminsert-6c76f96c54-jsw5z from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: isp-logger-5b4d57f796-5ncwr from monitoring started at 2022-11-30 03:06:55 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container isp-logger ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: node-cert-exporter-v97f6 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: sonobuoy from sonobuoy started at 2022-11-30 03:19:54 +0000 UTC (1 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-z79zw from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 03:44:44.510: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 03:44:44.510: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 11/30/22 03:44:44.51
    Nov 30 03:44:44.545: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1333" to be "running"
    Nov 30 03:44:44.547: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.692737ms
    Nov 30 03:44:46.552: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007649125s
    Nov 30 03:44:46.552: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 11/30/22 03:44:46.557
    STEP: Trying to apply a random label on the found node. 11/30/22 03:44:46.582
    STEP: verifying the node has the label kubernetes.io/e2e-41715ea7-5338-4b79-a66b-fbfcd5403c09 95 11/30/22 03:44:46.594
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 11/30/22 03:44:46.597
    W1130 03:44:46.609321      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54322)
    Nov 30 03:44:46.609: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1333" to be "not pending"
    Nov 30 03:44:46.612: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.670643ms
    Nov 30 03:44:48.616: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006985356s
    Nov 30 03:44:48.616: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.10.8 on the node which pod4 resides and expect not scheduled 11/30/22 03:44:48.616
    W1130 03:44:48.625900      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54322)
    Nov 30 03:44:48.625: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1333" to be "not pending"
    Nov 30 03:44:48.628: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.248832ms
    Nov 30 03:44:50.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006194313s
    Nov 30 03:44:52.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005844255s
    Nov 30 03:44:54.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010559333s
    Nov 30 03:44:56.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007094916s
    Nov 30 03:44:58.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006771869s
    Nov 30 03:45:00.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006903874s
    Nov 30 03:45:02.642: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016582414s
    Nov 30 03:45:04.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010067663s
    Nov 30 03:45:06.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006096046s
    Nov 30 03:45:08.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006737281s
    Nov 30 03:45:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006712563s
    Nov 30 03:45:12.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005637572s
    Nov 30 03:45:14.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007873997s
    Nov 30 03:45:16.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006330087s
    Nov 30 03:45:18.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005579079s
    Nov 30 03:45:20.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.0056786s
    Nov 30 03:45:22.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005964557s
    Nov 30 03:45:24.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007075192s
    Nov 30 03:45:26.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007295895s
    Nov 30 03:45:28.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006699421s
    Nov 30 03:45:30.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006465306s
    Nov 30 03:45:32.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006773508s
    Nov 30 03:45:34.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007336975s
    Nov 30 03:45:36.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007527856s
    Nov 30 03:45:38.630: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00496199s
    Nov 30 03:45:40.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005815849s
    Nov 30 03:45:42.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005793914s
    Nov 30 03:45:44.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005999284s
    Nov 30 03:45:46.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006663009s
    Nov 30 03:45:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00591095s
    Nov 30 03:45:50.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006040615s
    Nov 30 03:45:52.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006525899s
    Nov 30 03:45:54.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008926144s
    Nov 30 03:45:56.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007395024s
    Nov 30 03:45:58.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005627288s
    Nov 30 03:46:00.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005738481s
    Nov 30 03:46:02.642: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.01635113s
    Nov 30 03:46:04.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007324449s
    Nov 30 03:46:06.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007480104s
    Nov 30 03:46:08.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005123448s
    Nov 30 03:46:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006299355s
    Nov 30 03:46:12.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005470743s
    Nov 30 03:46:14.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005753709s
    Nov 30 03:46:16.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007000915s
    Nov 30 03:46:18.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.005570062s
    Nov 30 03:46:20.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006008946s
    Nov 30 03:46:22.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005919721s
    Nov 30 03:46:24.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009350171s
    Nov 30 03:46:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006899433s
    Nov 30 03:46:28.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005060948s
    Nov 30 03:46:30.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007621582s
    Nov 30 03:46:32.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006488375s
    Nov 30 03:46:34.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006387905s
    Nov 30 03:46:36.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007644865s
    Nov 30 03:46:38.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005288295s
    Nov 30 03:46:40.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006889744s
    Nov 30 03:46:42.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005605502s
    Nov 30 03:46:44.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009750215s
    Nov 30 03:46:46.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006028261s
    Nov 30 03:46:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005543221s
    Nov 30 03:46:50.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.006726645s
    Nov 30 03:46:52.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006788553s
    Nov 30 03:46:54.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009921674s
    Nov 30 03:46:56.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.006554162s
    Nov 30 03:46:58.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006808395s
    Nov 30 03:47:00.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.0073717s
    Nov 30 03:47:02.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.005809389s
    Nov 30 03:47:04.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.006770756s
    Nov 30 03:47:06.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.006672584s
    Nov 30 03:47:08.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.00540168s
    Nov 30 03:47:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.006056304s
    Nov 30 03:47:12.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.006144054s
    Nov 30 03:47:14.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008008841s
    Nov 30 03:47:16.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.00650463s
    Nov 30 03:47:18.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.006724974s
    Nov 30 03:47:20.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.00609524s
    Nov 30 03:47:22.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.005651616s
    Nov 30 03:47:24.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.006879224s
    Nov 30 03:47:26.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.007384137s
    Nov 30 03:47:28.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.006484908s
    Nov 30 03:47:30.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.006808478s
    Nov 30 03:47:32.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007242932s
    Nov 30 03:47:34.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.008246712s
    Nov 30 03:47:36.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005867542s
    Nov 30 03:47:38.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006573689s
    Nov 30 03:47:40.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007422712s
    Nov 30 03:47:42.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.006011273s
    Nov 30 03:47:44.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005778831s
    Nov 30 03:47:46.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.005832721s
    Nov 30 03:47:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.005549095s
    Nov 30 03:47:50.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005913982s
    Nov 30 03:47:52.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.005879586s
    Nov 30 03:47:54.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.006473003s
    Nov 30 03:47:56.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.005975983s
    Nov 30 03:47:58.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005910127s
    Nov 30 03:48:00.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007711371s
    Nov 30 03:48:02.642: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.01666156s
    Nov 30 03:48:04.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007317016s
    Nov 30 03:48:06.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007508918s
    Nov 30 03:48:08.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005954913s
    Nov 30 03:48:10.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.00535002s
    Nov 30 03:48:12.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005649259s
    Nov 30 03:48:14.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008754628s
    Nov 30 03:48:16.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.005635261s
    Nov 30 03:48:18.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.006733067s
    Nov 30 03:48:20.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005762432s
    Nov 30 03:48:22.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.006051281s
    Nov 30 03:48:24.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.006364894s
    Nov 30 03:48:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006253471s
    Nov 30 03:48:28.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.005228577s
    Nov 30 03:48:30.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.007416298s
    Nov 30 03:48:32.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.00745475s
    Nov 30 03:48:34.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.009430408s
    Nov 30 03:48:36.664: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.038370511s
    Nov 30 03:48:38.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005698887s
    Nov 30 03:48:40.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.005782031s
    Nov 30 03:48:42.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.005529034s
    Nov 30 03:48:44.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007789069s
    Nov 30 03:48:46.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006678058s
    Nov 30 03:48:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.00572319s
    Nov 30 03:48:50.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007069832s
    Nov 30 03:48:52.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.00585588s
    Nov 30 03:48:54.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.010323269s
    Nov 30 03:48:56.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.00697308s
    Nov 30 03:48:58.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.006559221s
    Nov 30 03:49:00.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.00607671s
    Nov 30 03:49:02.643: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.017095298s
    Nov 30 03:49:04.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006898508s
    Nov 30 03:49:06.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005872291s
    Nov 30 03:49:08.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.005654244s
    Nov 30 03:49:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.006644583s
    Nov 30 03:49:12.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.005682074s
    Nov 30 03:49:14.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.006615629s
    Nov 30 03:49:16.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005879828s
    Nov 30 03:49:18.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.00543195s
    Nov 30 03:49:20.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.007523652s
    Nov 30 03:49:22.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.006129058s
    Nov 30 03:49:24.636: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.010566981s
    Nov 30 03:49:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.006532174s
    Nov 30 03:49:28.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.006605863s
    Nov 30 03:49:30.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.005426631s
    Nov 30 03:49:32.644: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.018142328s
    Nov 30 03:49:34.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.006784248s
    Nov 30 03:49:36.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.007151581s
    Nov 30 03:49:38.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.005965288s
    Nov 30 03:49:40.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.006075027s
    Nov 30 03:49:42.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.00611543s
    Nov 30 03:49:44.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.008463689s
    Nov 30 03:49:46.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.005657952s
    Nov 30 03:49:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005481773s
    Nov 30 03:49:48.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008350197s
    STEP: removing the label kubernetes.io/e2e-41715ea7-5338-4b79-a66b-fbfcd5403c09 off the node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins 11/30/22 03:49:48.634
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-41715ea7-5338-4b79-a66b-fbfcd5403c09 11/30/22 03:49:48.648
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 03:49:48.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1333" for this suite. 11/30/22 03:49:48.659
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:49:48.668
Nov 30 03:49:48.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename endpointslicemirroring 11/30/22 03:49:48.669
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:49:48.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:49:48.69
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 11/30/22 03:49:48.729
Nov 30 03:49:48.744: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 11/30/22 03:49:50.749
Nov 30 03:49:50.756: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 11/30/22 03:49:52.763
Nov 30 03:49:52.778: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Nov 30 03:49:54.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-3411" for this suite. 11/30/22 03:49:54.786
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":79,"skipped":1342,"failed":0}
------------------------------
• [SLOW TEST] [6.127 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:49:48.668
    Nov 30 03:49:48.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename endpointslicemirroring 11/30/22 03:49:48.669
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:49:48.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:49:48.69
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 11/30/22 03:49:48.729
    Nov 30 03:49:48.744: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 11/30/22 03:49:50.749
    Nov 30 03:49:50.756: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 11/30/22 03:49:52.763
    Nov 30 03:49:52.778: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Nov 30 03:49:54.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-3411" for this suite. 11/30/22 03:49:54.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:49:54.795
Nov 30 03:49:54.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename resourcequota 11/30/22 03:49:54.796
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:49:54.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:49:54.82
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 11/30/22 03:49:54.823
STEP: Creating a ResourceQuota 11/30/22 03:49:59.826
STEP: Ensuring resource quota status is calculated 11/30/22 03:49:59.835
STEP: Creating a ReplicaSet 11/30/22 03:50:01.839
STEP: Ensuring resource quota status captures replicaset creation 11/30/22 03:50:01.856
STEP: Deleting a ReplicaSet 11/30/22 03:50:03.86
STEP: Ensuring resource quota status released usage 11/30/22 03:50:03.867
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Nov 30 03:50:05.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9490" for this suite. 11/30/22 03:50:05.876
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":80,"skipped":1364,"failed":0}
------------------------------
• [SLOW TEST] [11.090 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:49:54.795
    Nov 30 03:49:54.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename resourcequota 11/30/22 03:49:54.796
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:49:54.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:49:54.82
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 11/30/22 03:49:54.823
    STEP: Creating a ResourceQuota 11/30/22 03:49:59.826
    STEP: Ensuring resource quota status is calculated 11/30/22 03:49:59.835
    STEP: Creating a ReplicaSet 11/30/22 03:50:01.839
    STEP: Ensuring resource quota status captures replicaset creation 11/30/22 03:50:01.856
    STEP: Deleting a ReplicaSet 11/30/22 03:50:03.86
    STEP: Ensuring resource quota status released usage 11/30/22 03:50:03.867
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Nov 30 03:50:05.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9490" for this suite. 11/30/22 03:50:05.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:50:05.888
Nov 30 03:50:05.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename lease-test 11/30/22 03:50:05.889
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:05.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:05.906
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Nov 30 03:50:05.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-7078" for this suite. 11/30/22 03:50:05.982
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":81,"skipped":1444,"failed":0}
------------------------------
• [0.101 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:50:05.888
    Nov 30 03:50:05.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename lease-test 11/30/22 03:50:05.889
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:05.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:05.906
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Nov 30 03:50:05.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-7078" for this suite. 11/30/22 03:50:05.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:50:05.989
Nov 30 03:50:05.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename disruption 11/30/22 03:50:05.99
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:06.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:06.022
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 11/30/22 03:50:06.025
STEP: Waiting for the pdb to be processed 11/30/22 03:50:06.041
STEP: First trying to evict a pod which shouldn't be evictable 11/30/22 03:50:08.057
STEP: Waiting for all pods to be running 11/30/22 03:50:08.057
Nov 30 03:50:08.061: INFO: pods: 0 < 3
STEP: locating a running pod 11/30/22 03:50:10.065
STEP: Updating the pdb to allow a pod to be evicted 11/30/22 03:50:10.076
STEP: Waiting for the pdb to be processed 11/30/22 03:50:10.084
STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/30/22 03:50:12.093
STEP: Waiting for all pods to be running 11/30/22 03:50:12.093
STEP: Waiting for the pdb to observed all healthy pods 11/30/22 03:50:12.098
STEP: Patching the pdb to disallow a pod to be evicted 11/30/22 03:50:12.128
STEP: Waiting for the pdb to be processed 11/30/22 03:50:12.149
STEP: Waiting for all pods to be running 11/30/22 03:50:14.156
STEP: locating a running pod 11/30/22 03:50:14.159
STEP: Deleting the pdb to allow a pod to be evicted 11/30/22 03:50:14.166
STEP: Waiting for the pdb to be deleted 11/30/22 03:50:14.172
STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/30/22 03:50:14.174
STEP: Waiting for all pods to be running 11/30/22 03:50:14.174
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Nov 30 03:50:14.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4660" for this suite. 11/30/22 03:50:14.197
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":82,"skipped":1455,"failed":0}
------------------------------
• [SLOW TEST] [8.218 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:50:05.989
    Nov 30 03:50:05.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename disruption 11/30/22 03:50:05.99
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:06.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:06.022
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 11/30/22 03:50:06.025
    STEP: Waiting for the pdb to be processed 11/30/22 03:50:06.041
    STEP: First trying to evict a pod which shouldn't be evictable 11/30/22 03:50:08.057
    STEP: Waiting for all pods to be running 11/30/22 03:50:08.057
    Nov 30 03:50:08.061: INFO: pods: 0 < 3
    STEP: locating a running pod 11/30/22 03:50:10.065
    STEP: Updating the pdb to allow a pod to be evicted 11/30/22 03:50:10.076
    STEP: Waiting for the pdb to be processed 11/30/22 03:50:10.084
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/30/22 03:50:12.093
    STEP: Waiting for all pods to be running 11/30/22 03:50:12.093
    STEP: Waiting for the pdb to observed all healthy pods 11/30/22 03:50:12.098
    STEP: Patching the pdb to disallow a pod to be evicted 11/30/22 03:50:12.128
    STEP: Waiting for the pdb to be processed 11/30/22 03:50:12.149
    STEP: Waiting for all pods to be running 11/30/22 03:50:14.156
    STEP: locating a running pod 11/30/22 03:50:14.159
    STEP: Deleting the pdb to allow a pod to be evicted 11/30/22 03:50:14.166
    STEP: Waiting for the pdb to be deleted 11/30/22 03:50:14.172
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/30/22 03:50:14.174
    STEP: Waiting for all pods to be running 11/30/22 03:50:14.174
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Nov 30 03:50:14.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4660" for this suite. 11/30/22 03:50:14.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:50:14.208
Nov 30 03:50:14.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:50:14.208
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:14.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:14.244
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-1cb722dd-7a2b-4894-ac72-d4f81f51a11f 11/30/22 03:50:14.247
STEP: Creating a pod to test consume secrets 11/30/22 03:50:14.259
Nov 30 03:50:14.289: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2" in namespace "projected-1628" to be "Succeeded or Failed"
Nov 30 03:50:14.294: INFO: Pod "pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.749318ms
Nov 30 03:50:16.298: INFO: Pod "pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00881283s
Nov 30 03:50:18.298: INFO: Pod "pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008415618s
STEP: Saw pod success 11/30/22 03:50:18.298
Nov 30 03:50:18.298: INFO: Pod "pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2" satisfied condition "Succeeded or Failed"
Nov 30 03:50:18.300: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/30/22 03:50:18.307
Nov 30 03:50:18.323: INFO: Waiting for pod pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2 to disappear
Nov 30 03:50:18.326: INFO: Pod pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Nov 30 03:50:18.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1628" for this suite. 11/30/22 03:50:18.33
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":83,"skipped":1465,"failed":0}
------------------------------
• [4.128 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:50:14.208
    Nov 30 03:50:14.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:50:14.208
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:14.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:14.244
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-1cb722dd-7a2b-4894-ac72-d4f81f51a11f 11/30/22 03:50:14.247
    STEP: Creating a pod to test consume secrets 11/30/22 03:50:14.259
    Nov 30 03:50:14.289: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2" in namespace "projected-1628" to be "Succeeded or Failed"
    Nov 30 03:50:14.294: INFO: Pod "pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.749318ms
    Nov 30 03:50:16.298: INFO: Pod "pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00881283s
    Nov 30 03:50:18.298: INFO: Pod "pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008415618s
    STEP: Saw pod success 11/30/22 03:50:18.298
    Nov 30 03:50:18.298: INFO: Pod "pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2" satisfied condition "Succeeded or Failed"
    Nov 30 03:50:18.300: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 03:50:18.307
    Nov 30 03:50:18.323: INFO: Waiting for pod pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2 to disappear
    Nov 30 03:50:18.326: INFO: Pod pod-projected-secrets-58d1f04a-0074-49dc-aa01-6252c536dbd2 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Nov 30 03:50:18.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1628" for this suite. 11/30/22 03:50:18.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:50:18.337
Nov 30 03:50:18.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 03:50:18.338
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:18.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:18.363
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 11/30/22 03:50:18.366
Nov 30 03:50:18.366: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9698 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 11/30/22 03:50:18.413
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 03:50:18.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9698" for this suite. 11/30/22 03:50:18.425
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":84,"skipped":1482,"failed":0}
------------------------------
• [0.093 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:50:18.337
    Nov 30 03:50:18.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 03:50:18.338
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:18.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:18.363
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 11/30/22 03:50:18.366
    Nov 30 03:50:18.366: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9698 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 11/30/22 03:50:18.413
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 03:50:18.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9698" for this suite. 11/30/22 03:50:18.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:50:18.431
Nov 30 03:50:18.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubelet-test 11/30/22 03:50:18.432
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:18.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:18.451
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 11/30/22 03:50:18.471
Nov 30 03:50:18.471: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases7873f19a-e88e-4913-b313-0b195007b3e6" in namespace "kubelet-test-1964" to be "completed"
Nov 30 03:50:18.477: INFO: Pod "agnhost-host-aliases7873f19a-e88e-4913-b313-0b195007b3e6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.641248ms
Nov 30 03:50:20.480: INFO: Pod "agnhost-host-aliases7873f19a-e88e-4913-b313-0b195007b3e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009489748s
Nov 30 03:50:22.481: INFO: Pod "agnhost-host-aliases7873f19a-e88e-4913-b313-0b195007b3e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00995055s
Nov 30 03:50:22.481: INFO: Pod "agnhost-host-aliases7873f19a-e88e-4913-b313-0b195007b3e6" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Nov 30 03:50:22.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1964" for this suite. 11/30/22 03:50:22.491
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":85,"skipped":1506,"failed":0}
------------------------------
• [4.067 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:50:18.431
    Nov 30 03:50:18.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubelet-test 11/30/22 03:50:18.432
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:18.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:18.451
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 11/30/22 03:50:18.471
    Nov 30 03:50:18.471: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases7873f19a-e88e-4913-b313-0b195007b3e6" in namespace "kubelet-test-1964" to be "completed"
    Nov 30 03:50:18.477: INFO: Pod "agnhost-host-aliases7873f19a-e88e-4913-b313-0b195007b3e6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.641248ms
    Nov 30 03:50:20.480: INFO: Pod "agnhost-host-aliases7873f19a-e88e-4913-b313-0b195007b3e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009489748s
    Nov 30 03:50:22.481: INFO: Pod "agnhost-host-aliases7873f19a-e88e-4913-b313-0b195007b3e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00995055s
    Nov 30 03:50:22.481: INFO: Pod "agnhost-host-aliases7873f19a-e88e-4913-b313-0b195007b3e6" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Nov 30 03:50:22.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-1964" for this suite. 11/30/22 03:50:22.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:50:22.499
Nov 30 03:50:22.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename disruption 11/30/22 03:50:22.5
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:22.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:22.522
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 11/30/22 03:50:22.532
STEP: Updating PodDisruptionBudget status 11/30/22 03:50:22.543
STEP: Waiting for all pods to be running 11/30/22 03:50:22.553
Nov 30 03:50:22.555: INFO: running pods: 0 < 1
STEP: locating a running pod 11/30/22 03:50:24.559
STEP: Waiting for the pdb to be processed 11/30/22 03:50:24.57
STEP: Patching PodDisruptionBudget status 11/30/22 03:50:24.583
STEP: Waiting for the pdb to be processed 11/30/22 03:50:24.593
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Nov 30 03:50:24.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5215" for this suite. 11/30/22 03:50:24.601
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":86,"skipped":1527,"failed":0}
------------------------------
• [2.111 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:50:22.499
    Nov 30 03:50:22.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename disruption 11/30/22 03:50:22.5
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:22.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:22.522
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 11/30/22 03:50:22.532
    STEP: Updating PodDisruptionBudget status 11/30/22 03:50:22.543
    STEP: Waiting for all pods to be running 11/30/22 03:50:22.553
    Nov 30 03:50:22.555: INFO: running pods: 0 < 1
    STEP: locating a running pod 11/30/22 03:50:24.559
    STEP: Waiting for the pdb to be processed 11/30/22 03:50:24.57
    STEP: Patching PodDisruptionBudget status 11/30/22 03:50:24.583
    STEP: Waiting for the pdb to be processed 11/30/22 03:50:24.593
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Nov 30 03:50:24.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5215" for this suite. 11/30/22 03:50:24.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:50:24.61
Nov 30 03:50:24.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 03:50:24.611
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:24.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:24.637
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-66e18b23-a776-4063-8b18-a799ef9b656e 11/30/22 03:50:24.639
STEP: Creating a pod to test consume configMaps 11/30/22 03:50:24.644
Nov 30 03:50:24.659: INFO: Waiting up to 5m0s for pod "pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02" in namespace "configmap-6044" to be "Succeeded or Failed"
Nov 30 03:50:24.662: INFO: Pod "pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.542705ms
Nov 30 03:50:26.671: INFO: Pod "pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011962123s
Nov 30 03:50:28.666: INFO: Pod "pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007261581s
STEP: Saw pod success 11/30/22 03:50:28.666
Nov 30 03:50:28.666: INFO: Pod "pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02" satisfied condition "Succeeded or Failed"
Nov 30 03:50:28.669: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02 container agnhost-container: <nil>
STEP: delete the pod 11/30/22 03:50:28.682
Nov 30 03:50:28.695: INFO: Waiting for pod pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02 to disappear
Nov 30 03:50:28.698: INFO: Pod pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 03:50:28.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6044" for this suite. 11/30/22 03:50:28.702
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":87,"skipped":1542,"failed":0}
------------------------------
• [4.100 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:50:24.61
    Nov 30 03:50:24.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 03:50:24.611
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:24.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:24.637
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-66e18b23-a776-4063-8b18-a799ef9b656e 11/30/22 03:50:24.639
    STEP: Creating a pod to test consume configMaps 11/30/22 03:50:24.644
    Nov 30 03:50:24.659: INFO: Waiting up to 5m0s for pod "pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02" in namespace "configmap-6044" to be "Succeeded or Failed"
    Nov 30 03:50:24.662: INFO: Pod "pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.542705ms
    Nov 30 03:50:26.671: INFO: Pod "pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011962123s
    Nov 30 03:50:28.666: INFO: Pod "pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007261581s
    STEP: Saw pod success 11/30/22 03:50:28.666
    Nov 30 03:50:28.666: INFO: Pod "pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02" satisfied condition "Succeeded or Failed"
    Nov 30 03:50:28.669: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02 container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 03:50:28.682
    Nov 30 03:50:28.695: INFO: Waiting for pod pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02 to disappear
    Nov 30 03:50:28.698: INFO: Pod pod-configmaps-b4248cd6-a37c-4fbc-9b8a-7b8e03ce0b02 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 03:50:28.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6044" for this suite. 11/30/22 03:50:28.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:50:28.711
Nov 30 03:50:28.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:50:28.711
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:28.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:28.734
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 11/30/22 03:50:28.736
Nov 30 03:50:28.751: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081" in namespace "projected-9002" to be "Succeeded or Failed"
Nov 30 03:50:28.754: INFO: Pod "downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081": Phase="Pending", Reason="", readiness=false. Elapsed: 2.642541ms
Nov 30 03:50:30.757: INFO: Pod "downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005872079s
Nov 30 03:50:32.759: INFO: Pod "downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007190994s
STEP: Saw pod success 11/30/22 03:50:32.759
Nov 30 03:50:32.759: INFO: Pod "downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081" satisfied condition "Succeeded or Failed"
Nov 30 03:50:32.762: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081 container client-container: <nil>
STEP: delete the pod 11/30/22 03:50:32.769
Nov 30 03:50:32.785: INFO: Waiting for pod downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081 to disappear
Nov 30 03:50:32.789: INFO: Pod downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Nov 30 03:50:32.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9002" for this suite. 11/30/22 03:50:32.794
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":88,"skipped":1556,"failed":0}
------------------------------
• [4.099 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:50:28.711
    Nov 30 03:50:28.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:50:28.711
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:28.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:28.734
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 11/30/22 03:50:28.736
    Nov 30 03:50:28.751: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081" in namespace "projected-9002" to be "Succeeded or Failed"
    Nov 30 03:50:28.754: INFO: Pod "downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081": Phase="Pending", Reason="", readiness=false. Elapsed: 2.642541ms
    Nov 30 03:50:30.757: INFO: Pod "downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005872079s
    Nov 30 03:50:32.759: INFO: Pod "downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007190994s
    STEP: Saw pod success 11/30/22 03:50:32.759
    Nov 30 03:50:32.759: INFO: Pod "downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081" satisfied condition "Succeeded or Failed"
    Nov 30 03:50:32.762: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081 container client-container: <nil>
    STEP: delete the pod 11/30/22 03:50:32.769
    Nov 30 03:50:32.785: INFO: Waiting for pod downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081 to disappear
    Nov 30 03:50:32.789: INFO: Pod downwardapi-volume-7dcfc277-058c-4a1e-a4e7-86591c6e3081 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Nov 30 03:50:32.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9002" for this suite. 11/30/22 03:50:32.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:50:32.811
Nov 30 03:50:32.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename var-expansion 11/30/22 03:50:32.812
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:32.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:32.844
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 11/30/22 03:50:32.846
STEP: waiting for pod running 11/30/22 03:50:32.858
Nov 30 03:50:32.858: INFO: Waiting up to 2m0s for pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac" in namespace "var-expansion-5383" to be "running"
Nov 30 03:50:32.861: INFO: Pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.010453ms
Nov 30 03:50:34.865: INFO: Pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac": Phase="Running", Reason="", readiness=true. Elapsed: 2.007146171s
Nov 30 03:50:34.865: INFO: Pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac" satisfied condition "running"
STEP: creating a file in subpath 11/30/22 03:50:34.865
Nov 30 03:50:34.868: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5383 PodName:var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:50:34.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:50:34.869: INFO: ExecWithOptions: Clientset creation
Nov 30 03:50:34.869: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-5383/pods/var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 11/30/22 03:50:34.935
Nov 30 03:50:34.939: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5383 PodName:var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:50:34.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:50:34.940: INFO: ExecWithOptions: Clientset creation
Nov 30 03:50:34.940: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-5383/pods/var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 11/30/22 03:50:35.005
Nov 30 03:50:35.520: INFO: Successfully updated pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac"
STEP: waiting for annotated pod running 11/30/22 03:50:35.52
Nov 30 03:50:35.520: INFO: Waiting up to 2m0s for pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac" in namespace "var-expansion-5383" to be "running"
Nov 30 03:50:35.524: INFO: Pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac": Phase="Running", Reason="", readiness=true. Elapsed: 3.321496ms
Nov 30 03:50:35.524: INFO: Pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac" satisfied condition "running"
STEP: deleting the pod gracefully 11/30/22 03:50:35.524
Nov 30 03:50:35.524: INFO: Deleting pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac" in namespace "var-expansion-5383"
Nov 30 03:50:35.533: INFO: Wait up to 5m0s for pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Nov 30 03:51:09.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5383" for this suite. 11/30/22 03:51:09.548
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":89,"skipped":1627,"failed":0}
------------------------------
• [SLOW TEST] [36.744 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:50:32.811
    Nov 30 03:50:32.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename var-expansion 11/30/22 03:50:32.812
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:50:32.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:50:32.844
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 11/30/22 03:50:32.846
    STEP: waiting for pod running 11/30/22 03:50:32.858
    Nov 30 03:50:32.858: INFO: Waiting up to 2m0s for pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac" in namespace "var-expansion-5383" to be "running"
    Nov 30 03:50:32.861: INFO: Pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.010453ms
    Nov 30 03:50:34.865: INFO: Pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac": Phase="Running", Reason="", readiness=true. Elapsed: 2.007146171s
    Nov 30 03:50:34.865: INFO: Pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac" satisfied condition "running"
    STEP: creating a file in subpath 11/30/22 03:50:34.865
    Nov 30 03:50:34.868: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5383 PodName:var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:50:34.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:50:34.869: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:50:34.869: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-5383/pods/var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 11/30/22 03:50:34.935
    Nov 30 03:50:34.939: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5383 PodName:var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:50:34.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:50:34.940: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:50:34.940: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-5383/pods/var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 11/30/22 03:50:35.005
    Nov 30 03:50:35.520: INFO: Successfully updated pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac"
    STEP: waiting for annotated pod running 11/30/22 03:50:35.52
    Nov 30 03:50:35.520: INFO: Waiting up to 2m0s for pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac" in namespace "var-expansion-5383" to be "running"
    Nov 30 03:50:35.524: INFO: Pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac": Phase="Running", Reason="", readiness=true. Elapsed: 3.321496ms
    Nov 30 03:50:35.524: INFO: Pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac" satisfied condition "running"
    STEP: deleting the pod gracefully 11/30/22 03:50:35.524
    Nov 30 03:50:35.524: INFO: Deleting pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac" in namespace "var-expansion-5383"
    Nov 30 03:50:35.533: INFO: Wait up to 5m0s for pod "var-expansion-8e63f1ec-6fba-4a77-a3ad-3ccb80b763ac" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Nov 30 03:51:09.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5383" for this suite. 11/30/22 03:51:09.548
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:51:09.555
Nov 30 03:51:09.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename controllerrevisions 11/30/22 03:51:09.556
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:51:09.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:51:09.575
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-dtqkx-daemon-set" 11/30/22 03:51:09.599
STEP: Check that daemon pods launch on every node of the cluster. 11/30/22 03:51:09.604
Nov 30 03:51:09.607: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:09.607: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:09.607: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:09.611: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 0
Nov 30 03:51:09.611: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 03:51:10.616: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:10.616: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:10.616: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:10.619: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 1
Nov 30 03:51:10.619: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 03:51:11.616: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:11.616: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:11.616: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:11.620: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 2
Nov 30 03:51:11.620: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 03:51:12.615: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:12.615: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:12.615: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:12.619: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 2
Nov 30 03:51:12.619: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 03:51:13.616: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:13.616: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:13.616: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:13.618: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 3
Nov 30 03:51:13.618: INFO: Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 03:51:14.616: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:14.616: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:14.616: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:51:14.619: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 4
Nov 30 03:51:14.619: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-dtqkx-daemon-set
STEP: Confirm DaemonSet "e2e-dtqkx-daemon-set" successfully created with "daemonset-name=e2e-dtqkx-daemon-set" label 11/30/22 03:51:14.622
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-dtqkx-daemon-set" 11/30/22 03:51:14.631
Nov 30 03:51:14.634: INFO: Located ControllerRevision: "e2e-dtqkx-daemon-set-59c656cbb6"
STEP: Patching ControllerRevision "e2e-dtqkx-daemon-set-59c656cbb6" 11/30/22 03:51:14.636
Nov 30 03:51:14.646: INFO: e2e-dtqkx-daemon-set-59c656cbb6 has been patched
STEP: Create a new ControllerRevision 11/30/22 03:51:14.646
Nov 30 03:51:14.650: INFO: Created ControllerRevision: e2e-dtqkx-daemon-set-5d88fd6968
STEP: Confirm that there are two ControllerRevisions 11/30/22 03:51:14.65
Nov 30 03:51:14.650: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 30 03:51:14.653: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-dtqkx-daemon-set-59c656cbb6" 11/30/22 03:51:14.653
STEP: Confirm that there is only one ControllerRevision 11/30/22 03:51:14.658
Nov 30 03:51:14.658: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 30 03:51:14.661: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-dtqkx-daemon-set-5d88fd6968" 11/30/22 03:51:14.663
Nov 30 03:51:14.670: INFO: e2e-dtqkx-daemon-set-5d88fd6968 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 11/30/22 03:51:14.67
W1130 03:51:14.682029      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 11/30/22 03:51:14.682
Nov 30 03:51:14.682: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 30 03:51:15.687: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 30 03:51:15.690: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-dtqkx-daemon-set-5d88fd6968=updated" 11/30/22 03:51:15.69
STEP: Confirm that there is only one ControllerRevision 11/30/22 03:51:15.697
Nov 30 03:51:15.697: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 30 03:51:15.699: INFO: Found 1 ControllerRevisions
Nov 30 03:51:15.702: INFO: ControllerRevision "e2e-dtqkx-daemon-set-65f854fd97" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-dtqkx-daemon-set" 11/30/22 03:51:15.704
STEP: deleting DaemonSet.extensions e2e-dtqkx-daemon-set in namespace controllerrevisions-9000, will wait for the garbage collector to delete the pods 11/30/22 03:51:15.704
Nov 30 03:51:15.765: INFO: Deleting DaemonSet.extensions e2e-dtqkx-daemon-set took: 8.861876ms
Nov 30 03:51:15.866: INFO: Terminating DaemonSet.extensions e2e-dtqkx-daemon-set pods took: 100.51612ms
Nov 30 03:51:17.769: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 0
Nov 30 03:51:17.769: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-dtqkx-daemon-set
Nov 30 03:51:17.772: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29527"},"items":null}

Nov 30 03:51:17.774: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29527"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Nov 30 03:51:17.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-9000" for this suite. 11/30/22 03:51:17.795
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":90,"skipped":1628,"failed":0}
------------------------------
• [SLOW TEST] [8.247 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:51:09.555
    Nov 30 03:51:09.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename controllerrevisions 11/30/22 03:51:09.556
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:51:09.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:51:09.575
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-dtqkx-daemon-set" 11/30/22 03:51:09.599
    STEP: Check that daemon pods launch on every node of the cluster. 11/30/22 03:51:09.604
    Nov 30 03:51:09.607: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:09.607: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:09.607: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:09.611: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 0
    Nov 30 03:51:09.611: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 03:51:10.616: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:10.616: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:10.616: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:10.619: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 1
    Nov 30 03:51:10.619: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 03:51:11.616: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:11.616: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:11.616: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:11.620: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 2
    Nov 30 03:51:11.620: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 03:51:12.615: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:12.615: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:12.615: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:12.619: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 2
    Nov 30 03:51:12.619: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 03:51:13.616: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:13.616: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:13.616: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:13.618: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 3
    Nov 30 03:51:13.618: INFO: Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 03:51:14.616: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:14.616: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:14.616: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:51:14.619: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 4
    Nov 30 03:51:14.619: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-dtqkx-daemon-set
    STEP: Confirm DaemonSet "e2e-dtqkx-daemon-set" successfully created with "daemonset-name=e2e-dtqkx-daemon-set" label 11/30/22 03:51:14.622
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-dtqkx-daemon-set" 11/30/22 03:51:14.631
    Nov 30 03:51:14.634: INFO: Located ControllerRevision: "e2e-dtqkx-daemon-set-59c656cbb6"
    STEP: Patching ControllerRevision "e2e-dtqkx-daemon-set-59c656cbb6" 11/30/22 03:51:14.636
    Nov 30 03:51:14.646: INFO: e2e-dtqkx-daemon-set-59c656cbb6 has been patched
    STEP: Create a new ControllerRevision 11/30/22 03:51:14.646
    Nov 30 03:51:14.650: INFO: Created ControllerRevision: e2e-dtqkx-daemon-set-5d88fd6968
    STEP: Confirm that there are two ControllerRevisions 11/30/22 03:51:14.65
    Nov 30 03:51:14.650: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 30 03:51:14.653: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-dtqkx-daemon-set-59c656cbb6" 11/30/22 03:51:14.653
    STEP: Confirm that there is only one ControllerRevision 11/30/22 03:51:14.658
    Nov 30 03:51:14.658: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 30 03:51:14.661: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-dtqkx-daemon-set-5d88fd6968" 11/30/22 03:51:14.663
    Nov 30 03:51:14.670: INFO: e2e-dtqkx-daemon-set-5d88fd6968 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 11/30/22 03:51:14.67
    W1130 03:51:14.682029      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 11/30/22 03:51:14.682
    Nov 30 03:51:14.682: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 30 03:51:15.687: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 30 03:51:15.690: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-dtqkx-daemon-set-5d88fd6968=updated" 11/30/22 03:51:15.69
    STEP: Confirm that there is only one ControllerRevision 11/30/22 03:51:15.697
    Nov 30 03:51:15.697: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 30 03:51:15.699: INFO: Found 1 ControllerRevisions
    Nov 30 03:51:15.702: INFO: ControllerRevision "e2e-dtqkx-daemon-set-65f854fd97" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-dtqkx-daemon-set" 11/30/22 03:51:15.704
    STEP: deleting DaemonSet.extensions e2e-dtqkx-daemon-set in namespace controllerrevisions-9000, will wait for the garbage collector to delete the pods 11/30/22 03:51:15.704
    Nov 30 03:51:15.765: INFO: Deleting DaemonSet.extensions e2e-dtqkx-daemon-set took: 8.861876ms
    Nov 30 03:51:15.866: INFO: Terminating DaemonSet.extensions e2e-dtqkx-daemon-set pods took: 100.51612ms
    Nov 30 03:51:17.769: INFO: Number of nodes with available pods controlled by daemonset e2e-dtqkx-daemon-set: 0
    Nov 30 03:51:17.769: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-dtqkx-daemon-set
    Nov 30 03:51:17.772: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29527"},"items":null}

    Nov 30 03:51:17.774: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29527"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 03:51:17.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-9000" for this suite. 11/30/22 03:51:17.795
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:51:17.802
Nov 30 03:51:17.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-probe 11/30/22 03:51:17.803
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:51:17.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:51:17.823
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-d8febef2-d4c3-446c-96eb-1752ecd75741 in namespace container-probe-657 11/30/22 03:51:17.826
Nov 30 03:51:17.863: INFO: Waiting up to 5m0s for pod "liveness-d8febef2-d4c3-446c-96eb-1752ecd75741" in namespace "container-probe-657" to be "not pending"
Nov 30 03:51:17.869: INFO: Pod "liveness-d8febef2-d4c3-446c-96eb-1752ecd75741": Phase="Pending", Reason="", readiness=false. Elapsed: 6.779296ms
Nov 30 03:51:19.873: INFO: Pod "liveness-d8febef2-d4c3-446c-96eb-1752ecd75741": Phase="Running", Reason="", readiness=true. Elapsed: 2.010819735s
Nov 30 03:51:19.873: INFO: Pod "liveness-d8febef2-d4c3-446c-96eb-1752ecd75741" satisfied condition "not pending"
Nov 30 03:51:19.873: INFO: Started pod liveness-d8febef2-d4c3-446c-96eb-1752ecd75741 in namespace container-probe-657
STEP: checking the pod's current state and verifying that restartCount is present 11/30/22 03:51:19.873
Nov 30 03:51:19.877: INFO: Initial restart count of pod liveness-d8febef2-d4c3-446c-96eb-1752ecd75741 is 0
Nov 30 03:51:39.927: INFO: Restart count of pod container-probe-657/liveness-d8febef2-d4c3-446c-96eb-1752ecd75741 is now 1 (20.050725674s elapsed)
STEP: deleting the pod 11/30/22 03:51:39.927
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Nov 30 03:51:39.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-657" for this suite. 11/30/22 03:51:39.942
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":91,"skipped":1631,"failed":0}
------------------------------
• [SLOW TEST] [22.144 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:51:17.802
    Nov 30 03:51:17.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-probe 11/30/22 03:51:17.803
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:51:17.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:51:17.823
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-d8febef2-d4c3-446c-96eb-1752ecd75741 in namespace container-probe-657 11/30/22 03:51:17.826
    Nov 30 03:51:17.863: INFO: Waiting up to 5m0s for pod "liveness-d8febef2-d4c3-446c-96eb-1752ecd75741" in namespace "container-probe-657" to be "not pending"
    Nov 30 03:51:17.869: INFO: Pod "liveness-d8febef2-d4c3-446c-96eb-1752ecd75741": Phase="Pending", Reason="", readiness=false. Elapsed: 6.779296ms
    Nov 30 03:51:19.873: INFO: Pod "liveness-d8febef2-d4c3-446c-96eb-1752ecd75741": Phase="Running", Reason="", readiness=true. Elapsed: 2.010819735s
    Nov 30 03:51:19.873: INFO: Pod "liveness-d8febef2-d4c3-446c-96eb-1752ecd75741" satisfied condition "not pending"
    Nov 30 03:51:19.873: INFO: Started pod liveness-d8febef2-d4c3-446c-96eb-1752ecd75741 in namespace container-probe-657
    STEP: checking the pod's current state and verifying that restartCount is present 11/30/22 03:51:19.873
    Nov 30 03:51:19.877: INFO: Initial restart count of pod liveness-d8febef2-d4c3-446c-96eb-1752ecd75741 is 0
    Nov 30 03:51:39.927: INFO: Restart count of pod container-probe-657/liveness-d8febef2-d4c3-446c-96eb-1752ecd75741 is now 1 (20.050725674s elapsed)
    STEP: deleting the pod 11/30/22 03:51:39.927
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Nov 30 03:51:39.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-657" for this suite. 11/30/22 03:51:39.942
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:51:39.947
Nov 30 03:51:39.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 03:51:39.948
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:51:39.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:51:39.965
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 11/30/22 03:51:39.968
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 03:51:39.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3812" for this suite. 11/30/22 03:51:39.982
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":92,"skipped":1633,"failed":0}
------------------------------
• [0.040 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:51:39.947
    Nov 30 03:51:39.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 03:51:39.948
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:51:39.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:51:39.965
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 11/30/22 03:51:39.968
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 03:51:39.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3812" for this suite. 11/30/22 03:51:39.982
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:51:39.988
Nov 30 03:51:39.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename sched-preemption 11/30/22 03:51:39.988
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:51:40.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:51:40.008
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Nov 30 03:51:40.027: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 30 03:52:40.095: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 11/30/22 03:52:40.098
Nov 30 03:52:40.140: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 30 03:52:40.149: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 30 03:52:40.173: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 30 03:52:40.195: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Nov 30 03:52:40.226: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Nov 30 03:52:40.237: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Nov 30 03:52:40.263: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Nov 30 03:52:40.277: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 11/30/22 03:52:40.277
Nov 30 03:52:40.277: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9379" to be "running"
Nov 30 03:52:40.282: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.304438ms
Nov 30 03:52:42.286: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009537889s
Nov 30 03:52:44.287: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009679638s
Nov 30 03:52:44.287: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Nov 30 03:52:44.287: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
Nov 30 03:52:44.289: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.246363ms
Nov 30 03:52:44.289: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 30 03:52:44.289: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
Nov 30 03:52:44.291: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.231591ms
Nov 30 03:52:44.291: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 30 03:52:44.291: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
Nov 30 03:52:44.293: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.151027ms
Nov 30 03:52:44.293: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 30 03:52:44.293: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
Nov 30 03:52:44.295: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028796ms
Nov 30 03:52:46.300: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006722101s
Nov 30 03:52:48.299: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00568061s
Nov 30 03:52:50.303: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009796297s
Nov 30 03:52:52.299: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.006037948s
Nov 30 03:52:52.299: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 30 03:52:52.299: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
Nov 30 03:52:52.302: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.69271ms
Nov 30 03:52:52.302: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 30 03:52:52.302: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
Nov 30 03:52:52.309: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.624826ms
Nov 30 03:52:52.309: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 30 03:52:52.309: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
Nov 30 03:52:52.311: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.548561ms
Nov 30 03:52:52.311: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 11/30/22 03:52:52.311
Nov 30 03:52:52.338: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9379" to be "running"
Nov 30 03:52:52.350: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.394002ms
Nov 30 03:52:54.355: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016147241s
Nov 30 03:52:56.355: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016843602s
Nov 30 03:52:56.355: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Nov 30 03:52:56.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9379" for this suite. 11/30/22 03:52:56.382
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":93,"skipped":1637,"failed":0}
------------------------------
• [SLOW TEST] [76.459 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:51:39.988
    Nov 30 03:51:39.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename sched-preemption 11/30/22 03:51:39.988
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:51:40.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:51:40.008
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Nov 30 03:51:40.027: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 30 03:52:40.095: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 11/30/22 03:52:40.098
    Nov 30 03:52:40.140: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Nov 30 03:52:40.149: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Nov 30 03:52:40.173: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Nov 30 03:52:40.195: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Nov 30 03:52:40.226: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Nov 30 03:52:40.237: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Nov 30 03:52:40.263: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Nov 30 03:52:40.277: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 11/30/22 03:52:40.277
    Nov 30 03:52:40.277: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9379" to be "running"
    Nov 30 03:52:40.282: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.304438ms
    Nov 30 03:52:42.286: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009537889s
    Nov 30 03:52:44.287: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009679638s
    Nov 30 03:52:44.287: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Nov 30 03:52:44.287: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
    Nov 30 03:52:44.289: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.246363ms
    Nov 30 03:52:44.289: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 30 03:52:44.289: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
    Nov 30 03:52:44.291: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.231591ms
    Nov 30 03:52:44.291: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 30 03:52:44.291: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
    Nov 30 03:52:44.293: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.151027ms
    Nov 30 03:52:44.293: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 30 03:52:44.293: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
    Nov 30 03:52:44.295: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028796ms
    Nov 30 03:52:46.300: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006722101s
    Nov 30 03:52:48.299: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00568061s
    Nov 30 03:52:50.303: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009796297s
    Nov 30 03:52:52.299: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.006037948s
    Nov 30 03:52:52.299: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 30 03:52:52.299: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
    Nov 30 03:52:52.302: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.69271ms
    Nov 30 03:52:52.302: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 30 03:52:52.302: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
    Nov 30 03:52:52.309: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.624826ms
    Nov 30 03:52:52.309: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 30 03:52:52.309: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-9379" to be "running"
    Nov 30 03:52:52.311: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.548561ms
    Nov 30 03:52:52.311: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 11/30/22 03:52:52.311
    Nov 30 03:52:52.338: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9379" to be "running"
    Nov 30 03:52:52.350: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.394002ms
    Nov 30 03:52:54.355: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016147241s
    Nov 30 03:52:56.355: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016843602s
    Nov 30 03:52:56.355: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 03:52:56.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9379" for this suite. 11/30/22 03:52:56.382
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:52:56.448
Nov 30 03:52:56.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-lifecycle-hook 11/30/22 03:52:56.449
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:52:56.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:52:56.475
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 11/30/22 03:52:56.482
Nov 30 03:52:56.491: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6431" to be "running and ready"
Nov 30 03:52:56.493: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071975ms
Nov 30 03:52:56.493: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:52:58.497: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006172673s
Nov 30 03:52:58.497: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 30 03:52:58.497: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 11/30/22 03:52:58.499
Nov 30 03:52:58.508: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6431" to be "running and ready"
Nov 30 03:52:58.511: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.014528ms
Nov 30 03:52:58.511: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:53:00.516: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008079262s
Nov 30 03:53:00.516: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Nov 30 03:53:00.516: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 11/30/22 03:53:00.519
STEP: delete the pod with lifecycle hook 11/30/22 03:53:00.53
Nov 30 03:53:00.538: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 30 03:53:00.540: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 30 03:53:02.541: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 30 03:53:02.544: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 30 03:53:04.542: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 30 03:53:04.546: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Nov 30 03:53:04.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6431" for this suite. 11/30/22 03:53:04.551
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":94,"skipped":1709,"failed":0}
------------------------------
• [SLOW TEST] [8.109 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:52:56.448
    Nov 30 03:52:56.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/30/22 03:52:56.449
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:52:56.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:52:56.475
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 11/30/22 03:52:56.482
    Nov 30 03:52:56.491: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6431" to be "running and ready"
    Nov 30 03:52:56.493: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071975ms
    Nov 30 03:52:56.493: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:52:58.497: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006172673s
    Nov 30 03:52:58.497: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 30 03:52:58.497: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 11/30/22 03:52:58.499
    Nov 30 03:52:58.508: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6431" to be "running and ready"
    Nov 30 03:52:58.511: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.014528ms
    Nov 30 03:52:58.511: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:53:00.516: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008079262s
    Nov 30 03:53:00.516: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Nov 30 03:53:00.516: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 11/30/22 03:53:00.519
    STEP: delete the pod with lifecycle hook 11/30/22 03:53:00.53
    Nov 30 03:53:00.538: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Nov 30 03:53:00.540: INFO: Pod pod-with-poststart-exec-hook still exists
    Nov 30 03:53:02.541: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Nov 30 03:53:02.544: INFO: Pod pod-with-poststart-exec-hook still exists
    Nov 30 03:53:04.542: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Nov 30 03:53:04.546: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Nov 30 03:53:04.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6431" for this suite. 11/30/22 03:53:04.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:04.559
Nov 30 03:53:04.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 03:53:04.56
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:04.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:04.584
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-2177cd4b-ac8d-4d26-bcfc-c64249794ebf 11/30/22 03:53:04.593
STEP: Creating secret with name s-test-opt-upd-0f0b78c3-9bdd-4a23-b0d6-b5bed7eeed88 11/30/22 03:53:04.596
STEP: Creating the pod 11/30/22 03:53:04.6
Nov 30 03:53:04.635: INFO: Waiting up to 5m0s for pod "pod-secrets-a26b9807-2a7c-46d4-90e4-eb4505829f11" in namespace "secrets-5700" to be "running and ready"
Nov 30 03:53:04.644: INFO: Pod "pod-secrets-a26b9807-2a7c-46d4-90e4-eb4505829f11": Phase="Pending", Reason="", readiness=false. Elapsed: 9.89815ms
Nov 30 03:53:04.645: INFO: The phase of Pod pod-secrets-a26b9807-2a7c-46d4-90e4-eb4505829f11 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:53:06.649: INFO: Pod "pod-secrets-a26b9807-2a7c-46d4-90e4-eb4505829f11": Phase="Running", Reason="", readiness=true. Elapsed: 2.014138645s
Nov 30 03:53:06.649: INFO: The phase of Pod pod-secrets-a26b9807-2a7c-46d4-90e4-eb4505829f11 is Running (Ready = true)
Nov 30 03:53:06.649: INFO: Pod "pod-secrets-a26b9807-2a7c-46d4-90e4-eb4505829f11" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-2177cd4b-ac8d-4d26-bcfc-c64249794ebf 11/30/22 03:53:06.668
STEP: Updating secret s-test-opt-upd-0f0b78c3-9bdd-4a23-b0d6-b5bed7eeed88 11/30/22 03:53:06.679
STEP: Creating secret with name s-test-opt-create-33f1cdee-2dec-4b00-8310-06238c03af1d 11/30/22 03:53:06.686
STEP: waiting to observe update in volume 11/30/22 03:53:06.692
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Nov 30 03:53:08.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5700" for this suite. 11/30/22 03:53:08.72
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":95,"skipped":1763,"failed":0}
------------------------------
• [4.167 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:04.559
    Nov 30 03:53:04.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 03:53:04.56
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:04.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:04.584
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-2177cd4b-ac8d-4d26-bcfc-c64249794ebf 11/30/22 03:53:04.593
    STEP: Creating secret with name s-test-opt-upd-0f0b78c3-9bdd-4a23-b0d6-b5bed7eeed88 11/30/22 03:53:04.596
    STEP: Creating the pod 11/30/22 03:53:04.6
    Nov 30 03:53:04.635: INFO: Waiting up to 5m0s for pod "pod-secrets-a26b9807-2a7c-46d4-90e4-eb4505829f11" in namespace "secrets-5700" to be "running and ready"
    Nov 30 03:53:04.644: INFO: Pod "pod-secrets-a26b9807-2a7c-46d4-90e4-eb4505829f11": Phase="Pending", Reason="", readiness=false. Elapsed: 9.89815ms
    Nov 30 03:53:04.645: INFO: The phase of Pod pod-secrets-a26b9807-2a7c-46d4-90e4-eb4505829f11 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:53:06.649: INFO: Pod "pod-secrets-a26b9807-2a7c-46d4-90e4-eb4505829f11": Phase="Running", Reason="", readiness=true. Elapsed: 2.014138645s
    Nov 30 03:53:06.649: INFO: The phase of Pod pod-secrets-a26b9807-2a7c-46d4-90e4-eb4505829f11 is Running (Ready = true)
    Nov 30 03:53:06.649: INFO: Pod "pod-secrets-a26b9807-2a7c-46d4-90e4-eb4505829f11" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-2177cd4b-ac8d-4d26-bcfc-c64249794ebf 11/30/22 03:53:06.668
    STEP: Updating secret s-test-opt-upd-0f0b78c3-9bdd-4a23-b0d6-b5bed7eeed88 11/30/22 03:53:06.679
    STEP: Creating secret with name s-test-opt-create-33f1cdee-2dec-4b00-8310-06238c03af1d 11/30/22 03:53:06.686
    STEP: waiting to observe update in volume 11/30/22 03:53:06.692
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 03:53:08.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5700" for this suite. 11/30/22 03:53:08.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:08.727
Nov 30 03:53:08.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename prestop 11/30/22 03:53:08.727
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:08.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:08.748
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-3362 11/30/22 03:53:08.751
STEP: Waiting for pods to come up. 11/30/22 03:53:08.771
Nov 30 03:53:08.771: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-3362" to be "running"
Nov 30 03:53:08.777: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.37429ms
Nov 30 03:53:10.781: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.009653686s
Nov 30 03:53:10.781: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-3362 11/30/22 03:53:10.784
Nov 30 03:53:10.791: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-3362" to be "running"
Nov 30 03:53:10.794: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.268407ms
Nov 30 03:53:12.798: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006672491s
Nov 30 03:53:12.798: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 11/30/22 03:53:12.798
Nov 30 03:53:17.817: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 11/30/22 03:53:17.817
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Nov 30 03:53:17.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3362" for this suite. 11/30/22 03:53:17.836
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":96,"skipped":1790,"failed":0}
------------------------------
• [SLOW TEST] [9.115 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:08.727
    Nov 30 03:53:08.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename prestop 11/30/22 03:53:08.727
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:08.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:08.748
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-3362 11/30/22 03:53:08.751
    STEP: Waiting for pods to come up. 11/30/22 03:53:08.771
    Nov 30 03:53:08.771: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-3362" to be "running"
    Nov 30 03:53:08.777: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.37429ms
    Nov 30 03:53:10.781: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.009653686s
    Nov 30 03:53:10.781: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-3362 11/30/22 03:53:10.784
    Nov 30 03:53:10.791: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-3362" to be "running"
    Nov 30 03:53:10.794: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.268407ms
    Nov 30 03:53:12.798: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006672491s
    Nov 30 03:53:12.798: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 11/30/22 03:53:12.798
    Nov 30 03:53:17.817: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 11/30/22 03:53:17.817
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Nov 30 03:53:17.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-3362" for this suite. 11/30/22 03:53:17.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:17.842
Nov 30 03:53:17.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename replication-controller 11/30/22 03:53:17.843
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:17.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:17.863
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 11/30/22 03:53:17.865
Nov 30 03:53:17.901: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1649" to be "running and ready"
Nov 30 03:53:17.909: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 7.863138ms
Nov 30 03:53:17.909: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:53:19.912: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.011331208s
Nov 30 03:53:19.912: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Nov 30 03:53:19.912: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 11/30/22 03:53:19.915
STEP: Then the orphan pod is adopted 11/30/22 03:53:19.919
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Nov 30 03:53:20.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1649" for this suite. 11/30/22 03:53:20.93
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":97,"skipped":1799,"failed":0}
------------------------------
• [3.094 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:17.842
    Nov 30 03:53:17.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename replication-controller 11/30/22 03:53:17.843
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:17.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:17.863
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 11/30/22 03:53:17.865
    Nov 30 03:53:17.901: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1649" to be "running and ready"
    Nov 30 03:53:17.909: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 7.863138ms
    Nov 30 03:53:17.909: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:53:19.912: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.011331208s
    Nov 30 03:53:19.912: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Nov 30 03:53:19.912: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 11/30/22 03:53:19.915
    STEP: Then the orphan pod is adopted 11/30/22 03:53:19.919
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Nov 30 03:53:20.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1649" for this suite. 11/30/22 03:53:20.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:20.937
Nov 30 03:53:20.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename replication-controller 11/30/22 03:53:20.937
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:20.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:20.956
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 11/30/22 03:53:20.958
STEP: When the matched label of one of its pods change 11/30/22 03:53:20.968
Nov 30 03:53:20.970: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 30 03:53:25.978: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 11/30/22 03:53:25.992
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Nov 30 03:53:26.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9344" for this suite. 11/30/22 03:53:26.014
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":98,"skipped":1828,"failed":0}
------------------------------
• [SLOW TEST] [5.091 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:20.937
    Nov 30 03:53:20.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename replication-controller 11/30/22 03:53:20.937
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:20.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:20.956
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 11/30/22 03:53:20.958
    STEP: When the matched label of one of its pods change 11/30/22 03:53:20.968
    Nov 30 03:53:20.970: INFO: Pod name pod-release: Found 0 pods out of 1
    Nov 30 03:53:25.978: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 11/30/22 03:53:25.992
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Nov 30 03:53:26.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9344" for this suite. 11/30/22 03:53:26.014
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:26.028
Nov 30 03:53:26.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 03:53:26.03
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:26.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:26.066
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 11/30/22 03:53:26.07
Nov 30 03:53:26.106: INFO: Waiting up to 5m0s for pod "pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f" in namespace "emptydir-1658" to be "Succeeded or Failed"
Nov 30 03:53:26.115: INFO: Pod "pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.349284ms
Nov 30 03:53:28.119: INFO: Pod "pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012349272s
Nov 30 03:53:30.119: INFO: Pod "pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01216311s
STEP: Saw pod success 11/30/22 03:53:30.119
Nov 30 03:53:30.119: INFO: Pod "pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f" satisfied condition "Succeeded or Failed"
Nov 30 03:53:30.121: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f container test-container: <nil>
STEP: delete the pod 11/30/22 03:53:30.126
Nov 30 03:53:30.137: INFO: Waiting for pod pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f to disappear
Nov 30 03:53:30.140: INFO: Pod pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 03:53:30.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1658" for this suite. 11/30/22 03:53:30.145
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":99,"skipped":1832,"failed":0}
------------------------------
• [4.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:26.028
    Nov 30 03:53:26.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 03:53:26.03
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:26.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:26.066
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 11/30/22 03:53:26.07
    Nov 30 03:53:26.106: INFO: Waiting up to 5m0s for pod "pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f" in namespace "emptydir-1658" to be "Succeeded or Failed"
    Nov 30 03:53:26.115: INFO: Pod "pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.349284ms
    Nov 30 03:53:28.119: INFO: Pod "pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012349272s
    Nov 30 03:53:30.119: INFO: Pod "pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01216311s
    STEP: Saw pod success 11/30/22 03:53:30.119
    Nov 30 03:53:30.119: INFO: Pod "pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f" satisfied condition "Succeeded or Failed"
    Nov 30 03:53:30.121: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f container test-container: <nil>
    STEP: delete the pod 11/30/22 03:53:30.126
    Nov 30 03:53:30.137: INFO: Waiting for pod pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f to disappear
    Nov 30 03:53:30.140: INFO: Pod pod-288f7a7c-fa16-4b7a-88b6-105c192f4a7f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 03:53:30.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1658" for this suite. 11/30/22 03:53:30.145
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:30.15
Nov 30 03:53:30.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename disruption 11/30/22 03:53:30.15
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:30.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:30.167
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 11/30/22 03:53:30.174
STEP: Waiting for all pods to be running 11/30/22 03:53:32.231
Nov 30 03:53:32.245: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Nov 30 03:53:34.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3813" for this suite. 11/30/22 03:53:34.255
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":100,"skipped":1836,"failed":0}
------------------------------
• [4.112 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:30.15
    Nov 30 03:53:30.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename disruption 11/30/22 03:53:30.15
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:30.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:30.167
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 11/30/22 03:53:30.174
    STEP: Waiting for all pods to be running 11/30/22 03:53:32.231
    Nov 30 03:53:32.245: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Nov 30 03:53:34.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3813" for this suite. 11/30/22 03:53:34.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:34.263
Nov 30 03:53:34.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 03:53:34.263
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:34.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:34.284
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-8945 11/30/22 03:53:34.286
STEP: creating service affinity-nodeport in namespace services-8945 11/30/22 03:53:34.286
STEP: creating replication controller affinity-nodeport in namespace services-8945 11/30/22 03:53:34.31
I1130 03:53:34.320661      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8945, replica count: 3
I1130 03:53:37.372718      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 30 03:53:37.382: INFO: Creating new exec pod
Nov 30 03:53:37.410: INFO: Waiting up to 5m0s for pod "execpod-affinityjwfcr" in namespace "services-8945" to be "running"
Nov 30 03:53:37.412: INFO: Pod "execpod-affinityjwfcr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.711678ms
Nov 30 03:53:39.417: INFO: Pod "execpod-affinityjwfcr": Phase="Running", Reason="", readiness=true. Elapsed: 2.007502218s
Nov 30 03:53:39.417: INFO: Pod "execpod-affinityjwfcr" satisfied condition "running"
Nov 30 03:53:40.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-8945 exec execpod-affinityjwfcr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Nov 30 03:53:40.557: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Nov 30 03:53:40.557: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:53:40.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-8945 exec execpod-affinityjwfcr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.131.122 80'
Nov 30 03:53:40.722: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.131.122 80\nConnection to 10.111.131.122 80 port [tcp/http] succeeded!\n"
Nov 30 03:53:40.722: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:53:40.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-8945 exec execpod-affinityjwfcr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.17 30091'
Nov 30 03:53:40.836: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.17 30091\nConnection to 10.0.10.17 30091 port [tcp/*] succeeded!\n"
Nov 30 03:53:40.836: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:53:40.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-8945 exec execpod-affinityjwfcr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.2 30091'
Nov 30 03:53:40.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.2 30091\nConnection to 10.0.10.2 30091 port [tcp/*] succeeded!\n"
Nov 30 03:53:40.962: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 03:53:40.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-8945 exec execpod-affinityjwfcr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.8:30091/ ; done'
Nov 30 03:53:41.151: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n"
Nov 30 03:53:41.151: INFO: stdout: "\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92"
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
Nov 30 03:53:41.152: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8945, will wait for the garbage collector to delete the pods 11/30/22 03:53:41.164
Nov 30 03:53:41.223: INFO: Deleting ReplicationController affinity-nodeport took: 5.687073ms
Nov 30 03:53:41.323: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.262164ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 03:53:43.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8945" for this suite. 11/30/22 03:53:43.568
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":101,"skipped":1858,"failed":0}
------------------------------
• [SLOW TEST] [9.315 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:34.263
    Nov 30 03:53:34.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 03:53:34.263
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:34.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:34.284
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-8945 11/30/22 03:53:34.286
    STEP: creating service affinity-nodeport in namespace services-8945 11/30/22 03:53:34.286
    STEP: creating replication controller affinity-nodeport in namespace services-8945 11/30/22 03:53:34.31
    I1130 03:53:34.320661      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8945, replica count: 3
    I1130 03:53:37.372718      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 30 03:53:37.382: INFO: Creating new exec pod
    Nov 30 03:53:37.410: INFO: Waiting up to 5m0s for pod "execpod-affinityjwfcr" in namespace "services-8945" to be "running"
    Nov 30 03:53:37.412: INFO: Pod "execpod-affinityjwfcr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.711678ms
    Nov 30 03:53:39.417: INFO: Pod "execpod-affinityjwfcr": Phase="Running", Reason="", readiness=true. Elapsed: 2.007502218s
    Nov 30 03:53:39.417: INFO: Pod "execpod-affinityjwfcr" satisfied condition "running"
    Nov 30 03:53:40.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-8945 exec execpod-affinityjwfcr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Nov 30 03:53:40.557: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Nov 30 03:53:40.557: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:53:40.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-8945 exec execpod-affinityjwfcr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.131.122 80'
    Nov 30 03:53:40.722: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.131.122 80\nConnection to 10.111.131.122 80 port [tcp/http] succeeded!\n"
    Nov 30 03:53:40.722: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:53:40.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-8945 exec execpod-affinityjwfcr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.17 30091'
    Nov 30 03:53:40.836: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.17 30091\nConnection to 10.0.10.17 30091 port [tcp/*] succeeded!\n"
    Nov 30 03:53:40.836: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:53:40.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-8945 exec execpod-affinityjwfcr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.2 30091'
    Nov 30 03:53:40.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.2 30091\nConnection to 10.0.10.2 30091 port [tcp/*] succeeded!\n"
    Nov 30 03:53:40.962: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 03:53:40.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-8945 exec execpod-affinityjwfcr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.8:30091/ ; done'
    Nov 30 03:53:41.151: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:30091/\n"
    Nov 30 03:53:41.151: INFO: stdout: "\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92\naffinity-nodeport-8kr92"
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Received response from host: affinity-nodeport-8kr92
    Nov 30 03:53:41.152: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-8945, will wait for the garbage collector to delete the pods 11/30/22 03:53:41.164
    Nov 30 03:53:41.223: INFO: Deleting ReplicationController affinity-nodeport took: 5.687073ms
    Nov 30 03:53:41.323: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.262164ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 03:53:43.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8945" for this suite. 11/30/22 03:53:43.568
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:43.579
Nov 30 03:53:43.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 03:53:43.579
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:43.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:43.609
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 03:53:43.631
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 03:53:44.484
STEP: Deploying the webhook pod 11/30/22 03:53:44.496
STEP: Wait for the deployment to be ready 11/30/22 03:53:44.506
Nov 30 03:53:44.513: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 11/30/22 03:53:46.523
STEP: Verifying the service has paired with the endpoint 11/30/22 03:53:46.549
Nov 30 03:53:47.549: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 11/30/22 03:53:47.614
STEP: Creating a configMap that does not comply to the validation webhook rules 11/30/22 03:53:47.643
STEP: Deleting the collection of validation webhooks 11/30/22 03:53:47.677
STEP: Creating a configMap that does not comply to the validation webhook rules 11/30/22 03:53:47.75
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:53:47.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1323" for this suite. 11/30/22 03:53:47.766
STEP: Destroying namespace "webhook-1323-markers" for this suite. 11/30/22 03:53:47.772
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":102,"skipped":1892,"failed":0}
------------------------------
• [4.277 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:43.579
    Nov 30 03:53:43.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 03:53:43.579
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:43.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:43.609
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 03:53:43.631
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 03:53:44.484
    STEP: Deploying the webhook pod 11/30/22 03:53:44.496
    STEP: Wait for the deployment to be ready 11/30/22 03:53:44.506
    Nov 30 03:53:44.513: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 11/30/22 03:53:46.523
    STEP: Verifying the service has paired with the endpoint 11/30/22 03:53:46.549
    Nov 30 03:53:47.549: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 11/30/22 03:53:47.614
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/30/22 03:53:47.643
    STEP: Deleting the collection of validation webhooks 11/30/22 03:53:47.677
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/30/22 03:53:47.75
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:53:47.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1323" for this suite. 11/30/22 03:53:47.766
    STEP: Destroying namespace "webhook-1323-markers" for this suite. 11/30/22 03:53:47.772
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:47.856
Nov 30 03:53:47.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 03:53:47.857
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:47.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:47.891
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-6577ff9f-2326-4bbd-be76-bf2a910441b5 11/30/22 03:53:47.905
STEP: Creating the pod 11/30/22 03:53:47.916
Nov 30 03:53:47.951: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9417c03-7cb9-4dbd-9c0c-9fdc5a9618c6" in namespace "configmap-3941" to be "running"
Nov 30 03:53:47.963: INFO: Pod "pod-configmaps-e9417c03-7cb9-4dbd-9c0c-9fdc5a9618c6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.58087ms
Nov 30 03:53:49.967: INFO: Pod "pod-configmaps-e9417c03-7cb9-4dbd-9c0c-9fdc5a9618c6": Phase="Running", Reason="", readiness=false. Elapsed: 2.015555945s
Nov 30 03:53:49.967: INFO: Pod "pod-configmaps-e9417c03-7cb9-4dbd-9c0c-9fdc5a9618c6" satisfied condition "running"
STEP: Waiting for pod with text data 11/30/22 03:53:49.967
STEP: Waiting for pod with binary data 11/30/22 03:53:49.972
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 03:53:49.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3941" for this suite. 11/30/22 03:53:49.981
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":103,"skipped":1893,"failed":0}
------------------------------
• [2.131 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:47.856
    Nov 30 03:53:47.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 03:53:47.857
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:47.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:47.891
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-6577ff9f-2326-4bbd-be76-bf2a910441b5 11/30/22 03:53:47.905
    STEP: Creating the pod 11/30/22 03:53:47.916
    Nov 30 03:53:47.951: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9417c03-7cb9-4dbd-9c0c-9fdc5a9618c6" in namespace "configmap-3941" to be "running"
    Nov 30 03:53:47.963: INFO: Pod "pod-configmaps-e9417c03-7cb9-4dbd-9c0c-9fdc5a9618c6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.58087ms
    Nov 30 03:53:49.967: INFO: Pod "pod-configmaps-e9417c03-7cb9-4dbd-9c0c-9fdc5a9618c6": Phase="Running", Reason="", readiness=false. Elapsed: 2.015555945s
    Nov 30 03:53:49.967: INFO: Pod "pod-configmaps-e9417c03-7cb9-4dbd-9c0c-9fdc5a9618c6" satisfied condition "running"
    STEP: Waiting for pod with text data 11/30/22 03:53:49.967
    STEP: Waiting for pod with binary data 11/30/22 03:53:49.972
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 03:53:49.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3941" for this suite. 11/30/22 03:53:49.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:49.987
Nov 30 03:53:49.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename events 11/30/22 03:53:49.988
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:50.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:50.009
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 11/30/22 03:53:50.011
Nov 30 03:53:50.017: INFO: created test-event-1
Nov 30 03:53:50.021: INFO: created test-event-2
Nov 30 03:53:50.032: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 11/30/22 03:53:50.033
STEP: delete collection of events 11/30/22 03:53:50.035
Nov 30 03:53:50.035: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 11/30/22 03:53:50.05
Nov 30 03:53:50.050: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Nov 30 03:53:50.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2040" for this suite. 11/30/22 03:53:50.055
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":104,"skipped":1918,"failed":0}
------------------------------
• [0.100 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:49.987
    Nov 30 03:53:49.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename events 11/30/22 03:53:49.988
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:50.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:50.009
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 11/30/22 03:53:50.011
    Nov 30 03:53:50.017: INFO: created test-event-1
    Nov 30 03:53:50.021: INFO: created test-event-2
    Nov 30 03:53:50.032: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 11/30/22 03:53:50.033
    STEP: delete collection of events 11/30/22 03:53:50.035
    Nov 30 03:53:50.035: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 11/30/22 03:53:50.05
    Nov 30 03:53:50.050: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Nov 30 03:53:50.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-2040" for this suite. 11/30/22 03:53:50.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:50.088
Nov 30 03:53:50.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 03:53:50.089
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:50.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:50.115
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Nov 30 03:53:50.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 create -f -'
Nov 30 03:53:50.704: INFO: stderr: ""
Nov 30 03:53:50.704: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Nov 30 03:53:50.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 create -f -'
Nov 30 03:53:51.173: INFO: stderr: ""
Nov 30 03:53:51.173: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 11/30/22 03:53:51.173
Nov 30 03:53:52.189: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 30 03:53:52.189: INFO: Found 1 / 1
Nov 30 03:53:52.189: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 30 03:53:52.194: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 30 03:53:52.194: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 30 03:53:52.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 describe pod agnhost-primary-g6k7w'
Nov 30 03:53:52.266: INFO: stderr: ""
Nov 30 03:53:52.266: INFO: stdout: "Name:             agnhost-primary-g6k7w\nNamespace:        kubectl-4958\nPriority:         0\nService Account:  default\nNode:             worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins/10.0.10.8\nStart Time:       Wed, 30 Nov 2022 03:53:50 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"192.168.12.198\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"192.168.12.198\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\nStatus:           Running\nIP:               192.168.12.198\nIPs:\n  IP:           192.168.12.198\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://c3c6065f94630fff3d9d2616c6190f047abf3c9acb100cbc8beb0a1879e677e7\n    Image:          armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40\n    Image ID:       armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:1c9665737fa6e8ea5fa5031c5053c4bc36943d087d5d1d3926df50505b6236a8\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 30 Nov 2022 03:53:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wkhs4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-wkhs4:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       1s    default-scheduler  Successfully assigned kubectl-4958/agnhost-primary-g6k7w to worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins\n  Normal  AddedInterface  1s    multus             Add eth0 [192.168.12.198/32] from k8s-pod-network\n  Normal  Pulled          1s    kubelet            Container image \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Nov 30 03:53:52.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 describe rc agnhost-primary'
Nov 30 03:53:52.343: INFO: stderr: ""
Nov 30 03:53:52.343: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4958\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-g6k7w\n"
Nov 30 03:53:52.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 describe service agnhost-primary'
Nov 30 03:53:52.428: INFO: stderr: ""
Nov 30 03:53:52.428: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4958\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.108.242.170\nIPs:               10.108.242.170\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.12.198:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 30 03:53:52.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 describe node master-0-n92-ci-ibd-23-jenkins'
Nov 30 03:53:52.576: INFO: stderr: ""
Nov 30 03:53:52.576: INFO: stdout: "Name:               master-0-n92-ci-ibd-23-jenkins\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ccd.master.wd\n                    beta.kubernetes.io/os=linux\n                    ccd/version=2.24.0\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=nova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master-0-n92-ci-ibd-23-jenkins\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=ccd.master.wd\n                    node.uuid=f2f68d09-8446-4b98-a6fc-e44f2c3f911d\n                    node.uuid_source=cloud-init\n                    topology.cinder.csi.openstack.org/zone=nova\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=nova\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"f2f68d09-8446-4b98-a6fc-e44f2c3f911d\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 30 Nov 2022 02:40:04 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master-0-n92-ci-ibd-23-jenkins\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 30 Nov 2022 03:53:51 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 30 Nov 2022 02:44:59 +0000   Wed, 30 Nov 2022 02:44:59 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 30 Nov 2022 03:51:04 +0000   Wed, 30 Nov 2022 02:40:03 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 30 Nov 2022 03:51:04 +0000   Wed, 30 Nov 2022 02:40:03 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 30 Nov 2022 03:51:04 +0000   Wed, 30 Nov 2022 02:40:03 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 30 Nov 2022 03:51:04 +0000   Wed, 30 Nov 2022 02:44:56 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.10.4\n  Hostname:    master-0-n92-ci-ibd-23-jenkins\nCapacity:\n  cpu:                2\n  ephemeral-storage:  25125520Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3045256Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  23155679194\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2942856Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 f2f68d0984464b98a6fce44f2c3f911d\n  System UUID:                f2f68d09-8446-4b98-a6fc-e44f2c3f911d\n  Boot ID:                    4ec41f4b-8625-4c65-9512-3217953eb984\n  Kernel Version:             5.14.21-150400.24.33-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP4\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.6\n  Kubelet Version:            v1.25.3\n  Kube-Proxy Version:         v1.25.3\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nProviderID:                   openstack:///f2f68d09-8446-4b98-a6fc-e44f2c3f911d\nNon-terminated Pods:          (16 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-mnqzc                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         69m\n  kube-system                 coredns-64bc7b9f6b-6vx25                                   100m (5%)     0 (0%)      70Mi (2%)        170Mi (5%)     68m\n  kube-system                 csi-cinder-controllerplugin-775b68f5b4-vstkm               0 (0%)        0 (0%)      0 (0%)           0 (0%)         65m\n  kube-system                 csi-cinder-nodeplugin-l9q58                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         65m\n  kube-system                 kube-apiserver-master-0-n92-ci-ibd-23-jenkins              250m (12%)    0 (0%)      0 (0%)           0 (0%)         73m\n  kube-system                 kube-controller-manager-master-0-n92-ci-ibd-23-jenkins     200m (10%)    0 (0%)      0 (0%)           0 (0%)         73m\n  kube-system                 kube-multus-ds-amd64-62twg                                 100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      68m\n  kube-system                 kube-proxy-5nmxp                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         73m\n  kube-system                 kube-scheduler-master-0-n92-ci-ibd-23-jenkins              100m (5%)     0 (0%)      0 (0%)           0 (0%)         73m\n  kube-system                 kucero-np7tq                                               100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      67m\n  kube-system                 node-local-dns-8q9jp                                       25m (1%)      0 (0%)      5Mi (0%)         0 (0%)         73m\n  kube-system                 openstack-cloud-controller-manager-rx2xp                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         71m\n  kube-system                 subport-controller-kq5pn                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  monitoring                  eric-pm-node-exporter-9gz79                                100m (5%)     400m (20%)  100Mi (3%)       500Mi (17%)    55m\n  monitoring                  node-cert-exporter-kfb6b                                   100m (5%)     250m (12%)  128Mi (4%)       256Mi (8%)     54m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-44nhp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1525m (76%)  850m (42%)\n  memory             403Mi (14%)  1026Mi (35%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
Nov 30 03:53:52.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 describe namespace kubectl-4958'
Nov 30 03:53:52.652: INFO: stderr: ""
Nov 30 03:53:52.652: INFO: stdout: "Name:         kubectl-4958\nLabels:       e2e-framework=kubectl\n              e2e-run=39215a65-baa7-45d5-8b0e-6ad21501392e\n              kubernetes.io/metadata.name=kubectl-4958\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 03:53:52.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4958" for this suite. 11/30/22 03:53:52.659
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":105,"skipped":1932,"failed":0}
------------------------------
• [2.577 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:50.088
    Nov 30 03:53:50.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 03:53:50.089
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:50.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:50.115
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Nov 30 03:53:50.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 create -f -'
    Nov 30 03:53:50.704: INFO: stderr: ""
    Nov 30 03:53:50.704: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Nov 30 03:53:50.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 create -f -'
    Nov 30 03:53:51.173: INFO: stderr: ""
    Nov 30 03:53:51.173: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 11/30/22 03:53:51.173
    Nov 30 03:53:52.189: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 30 03:53:52.189: INFO: Found 1 / 1
    Nov 30 03:53:52.189: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Nov 30 03:53:52.194: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 30 03:53:52.194: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Nov 30 03:53:52.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 describe pod agnhost-primary-g6k7w'
    Nov 30 03:53:52.266: INFO: stderr: ""
    Nov 30 03:53:52.266: INFO: stdout: "Name:             agnhost-primary-g6k7w\nNamespace:        kubectl-4958\nPriority:         0\nService Account:  default\nNode:             worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins/10.0.10.8\nStart Time:       Wed, 30 Nov 2022 03:53:50 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"192.168.12.198\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"192.168.12.198\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\nStatus:           Running\nIP:               192.168.12.198\nIPs:\n  IP:           192.168.12.198\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://c3c6065f94630fff3d9d2616c6190f047abf3c9acb100cbc8beb0a1879e677e7\n    Image:          armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40\n    Image ID:       armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:1c9665737fa6e8ea5fa5031c5053c4bc36943d087d5d1d3926df50505b6236a8\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 30 Nov 2022 03:53:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wkhs4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-wkhs4:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       1s    default-scheduler  Successfully assigned kubectl-4958/agnhost-primary-g6k7w to worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins\n  Normal  AddedInterface  1s    multus             Add eth0 [192.168.12.198/32] from k8s-pod-network\n  Normal  Pulled          1s    kubelet            Container image \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
    Nov 30 03:53:52.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 describe rc agnhost-primary'
    Nov 30 03:53:52.343: INFO: stderr: ""
    Nov 30 03:53:52.343: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4958\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-g6k7w\n"
    Nov 30 03:53:52.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 describe service agnhost-primary'
    Nov 30 03:53:52.428: INFO: stderr: ""
    Nov 30 03:53:52.428: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4958\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.108.242.170\nIPs:               10.108.242.170\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.12.198:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Nov 30 03:53:52.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 describe node master-0-n92-ci-ibd-23-jenkins'
    Nov 30 03:53:52.576: INFO: stderr: ""
    Nov 30 03:53:52.576: INFO: stdout: "Name:               master-0-n92-ci-ibd-23-jenkins\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ccd.master.wd\n                    beta.kubernetes.io/os=linux\n                    ccd/version=2.24.0\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=nova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master-0-n92-ci-ibd-23-jenkins\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=ccd.master.wd\n                    node.uuid=f2f68d09-8446-4b98-a6fc-e44f2c3f911d\n                    node.uuid_source=cloud-init\n                    topology.cinder.csi.openstack.org/zone=nova\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=nova\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"f2f68d09-8446-4b98-a6fc-e44f2c3f911d\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 30 Nov 2022 02:40:04 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master-0-n92-ci-ibd-23-jenkins\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 30 Nov 2022 03:53:51 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 30 Nov 2022 02:44:59 +0000   Wed, 30 Nov 2022 02:44:59 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 30 Nov 2022 03:51:04 +0000   Wed, 30 Nov 2022 02:40:03 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 30 Nov 2022 03:51:04 +0000   Wed, 30 Nov 2022 02:40:03 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 30 Nov 2022 03:51:04 +0000   Wed, 30 Nov 2022 02:40:03 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 30 Nov 2022 03:51:04 +0000   Wed, 30 Nov 2022 02:44:56 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.10.4\n  Hostname:    master-0-n92-ci-ibd-23-jenkins\nCapacity:\n  cpu:                2\n  ephemeral-storage:  25125520Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3045256Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  23155679194\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2942856Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 f2f68d0984464b98a6fce44f2c3f911d\n  System UUID:                f2f68d09-8446-4b98-a6fc-e44f2c3f911d\n  Boot ID:                    4ec41f4b-8625-4c65-9512-3217953eb984\n  Kernel Version:             5.14.21-150400.24.33-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP4\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.6\n  Kubelet Version:            v1.25.3\n  Kube-Proxy Version:         v1.25.3\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nProviderID:                   openstack:///f2f68d09-8446-4b98-a6fc-e44f2c3f911d\nNon-terminated Pods:          (16 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-mnqzc                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         69m\n  kube-system                 coredns-64bc7b9f6b-6vx25                                   100m (5%)     0 (0%)      70Mi (2%)        170Mi (5%)     68m\n  kube-system                 csi-cinder-controllerplugin-775b68f5b4-vstkm               0 (0%)        0 (0%)      0 (0%)           0 (0%)         65m\n  kube-system                 csi-cinder-nodeplugin-l9q58                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         65m\n  kube-system                 kube-apiserver-master-0-n92-ci-ibd-23-jenkins              250m (12%)    0 (0%)      0 (0%)           0 (0%)         73m\n  kube-system                 kube-controller-manager-master-0-n92-ci-ibd-23-jenkins     200m (10%)    0 (0%)      0 (0%)           0 (0%)         73m\n  kube-system                 kube-multus-ds-amd64-62twg                                 100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      68m\n  kube-system                 kube-proxy-5nmxp                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         73m\n  kube-system                 kube-scheduler-master-0-n92-ci-ibd-23-jenkins              100m (5%)     0 (0%)      0 (0%)           0 (0%)         73m\n  kube-system                 kucero-np7tq                                               100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      67m\n  kube-system                 node-local-dns-8q9jp                                       25m (1%)      0 (0%)      5Mi (0%)         0 (0%)         73m\n  kube-system                 openstack-cloud-controller-manager-rx2xp                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         71m\n  kube-system                 subport-controller-kq5pn                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\n  monitoring                  eric-pm-node-exporter-9gz79                                100m (5%)     400m (20%)  100Mi (3%)       500Mi (17%)    55m\n  monitoring                  node-cert-exporter-kfb6b                                   100m (5%)     250m (12%)  128Mi (4%)       256Mi (8%)     54m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-44nhp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1525m (76%)  850m (42%)\n  memory             403Mi (14%)  1026Mi (35%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
    Nov 30 03:53:52.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4958 describe namespace kubectl-4958'
    Nov 30 03:53:52.652: INFO: stderr: ""
    Nov 30 03:53:52.652: INFO: stdout: "Name:         kubectl-4958\nLabels:       e2e-framework=kubectl\n              e2e-run=39215a65-baa7-45d5-8b0e-6ad21501392e\n              kubernetes.io/metadata.name=kubectl-4958\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 03:53:52.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4958" for this suite. 11/30/22 03:53:52.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:52.666
Nov 30 03:53:52.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 03:53:52.667
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:52.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:52.689
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-f0ce7ab0-d7f0-4529-9e0a-3a4feddff38b 11/30/22 03:53:52.691
STEP: Creating a pod to test consume configMaps 11/30/22 03:53:52.696
Nov 30 03:53:52.710: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6" in namespace "configmap-4595" to be "Succeeded or Failed"
Nov 30 03:53:52.716: INFO: Pod "pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.310344ms
Nov 30 03:53:54.719: INFO: Pod "pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009016848s
Nov 30 03:53:56.719: INFO: Pod "pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008966883s
STEP: Saw pod success 11/30/22 03:53:56.719
Nov 30 03:53:56.719: INFO: Pod "pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6" satisfied condition "Succeeded or Failed"
Nov 30 03:53:56.722: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6 container agnhost-container: <nil>
STEP: delete the pod 11/30/22 03:53:56.727
Nov 30 03:53:56.739: INFO: Waiting for pod pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6 to disappear
Nov 30 03:53:56.742: INFO: Pod pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 03:53:56.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4595" for this suite. 11/30/22 03:53:56.746
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":106,"skipped":1956,"failed":0}
------------------------------
• [4.086 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:52.666
    Nov 30 03:53:52.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 03:53:52.667
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:52.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:52.689
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-f0ce7ab0-d7f0-4529-9e0a-3a4feddff38b 11/30/22 03:53:52.691
    STEP: Creating a pod to test consume configMaps 11/30/22 03:53:52.696
    Nov 30 03:53:52.710: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6" in namespace "configmap-4595" to be "Succeeded or Failed"
    Nov 30 03:53:52.716: INFO: Pod "pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.310344ms
    Nov 30 03:53:54.719: INFO: Pod "pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009016848s
    Nov 30 03:53:56.719: INFO: Pod "pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008966883s
    STEP: Saw pod success 11/30/22 03:53:56.719
    Nov 30 03:53:56.719: INFO: Pod "pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6" satisfied condition "Succeeded or Failed"
    Nov 30 03:53:56.722: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6 container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 03:53:56.727
    Nov 30 03:53:56.739: INFO: Waiting for pod pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6 to disappear
    Nov 30 03:53:56.742: INFO: Pod pod-configmaps-b7787906-f7a2-4382-aa52-5acfe11e1ee6 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 03:53:56.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4595" for this suite. 11/30/22 03:53:56.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:56.752
Nov 30 03:53:56.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pods 11/30/22 03:53:56.753
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:56.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:56.77
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 11/30/22 03:53:56.772
STEP: submitting the pod to kubernetes 11/30/22 03:53:56.772
STEP: verifying QOS class is set on the pod 11/30/22 03:53:56.787
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Nov 30 03:53:56.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5850" for this suite. 11/30/22 03:53:56.794
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":107,"skipped":1969,"failed":0}
------------------------------
• [0.051 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:56.752
    Nov 30 03:53:56.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pods 11/30/22 03:53:56.753
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:56.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:56.77
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 11/30/22 03:53:56.772
    STEP: submitting the pod to kubernetes 11/30/22 03:53:56.772
    STEP: verifying QOS class is set on the pod 11/30/22 03:53:56.787
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Nov 30 03:53:56.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5850" for this suite. 11/30/22 03:53:56.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:53:56.803
Nov 30 03:53:56.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 03:53:56.804
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:56.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:56.82
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 11/30/22 03:53:56.822
Nov 30 03:53:56.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed" in namespace "downward-api-1521" to be "Succeeded or Failed"
Nov 30 03:53:56.837: INFO: Pod "downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.683246ms
Nov 30 03:53:58.840: INFO: Pod "downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008415796s
Nov 30 03:54:00.841: INFO: Pod "downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009220729s
STEP: Saw pod success 11/30/22 03:54:00.841
Nov 30 03:54:00.841: INFO: Pod "downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed" satisfied condition "Succeeded or Failed"
Nov 30 03:54:00.844: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed container client-container: <nil>
STEP: delete the pod 11/30/22 03:54:00.85
Nov 30 03:54:00.863: INFO: Waiting for pod downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed to disappear
Nov 30 03:54:00.866: INFO: Pod downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Nov 30 03:54:00.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1521" for this suite. 11/30/22 03:54:00.87
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":108,"skipped":1991,"failed":0}
------------------------------
• [4.076 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:53:56.803
    Nov 30 03:53:56.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 03:53:56.804
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:53:56.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:53:56.82
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 11/30/22 03:53:56.822
    Nov 30 03:53:56.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed" in namespace "downward-api-1521" to be "Succeeded or Failed"
    Nov 30 03:53:56.837: INFO: Pod "downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.683246ms
    Nov 30 03:53:58.840: INFO: Pod "downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008415796s
    Nov 30 03:54:00.841: INFO: Pod "downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009220729s
    STEP: Saw pod success 11/30/22 03:54:00.841
    Nov 30 03:54:00.841: INFO: Pod "downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed" satisfied condition "Succeeded or Failed"
    Nov 30 03:54:00.844: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed container client-container: <nil>
    STEP: delete the pod 11/30/22 03:54:00.85
    Nov 30 03:54:00.863: INFO: Waiting for pod downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed to disappear
    Nov 30 03:54:00.866: INFO: Pod downwardapi-volume-d1d9a1f2-7516-4ea2-b9ab-a15f8eb199ed no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Nov 30 03:54:00.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1521" for this suite. 11/30/22 03:54:00.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:54:00.88
Nov 30 03:54:00.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 03:54:00.88
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:00.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:00.906
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 11/30/22 03:54:00.91
Nov 30 03:54:01.008: INFO: Waiting up to 5m0s for pod "pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07" in namespace "emptydir-5598" to be "Succeeded or Failed"
Nov 30 03:54:01.011: INFO: Pod "pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.904256ms
Nov 30 03:54:03.018: INFO: Pod "pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009787271s
Nov 30 03:54:05.017: INFO: Pod "pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008270925s
STEP: Saw pod success 11/30/22 03:54:05.017
Nov 30 03:54:05.017: INFO: Pod "pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07" satisfied condition "Succeeded or Failed"
Nov 30 03:54:05.020: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07 container test-container: <nil>
STEP: delete the pod 11/30/22 03:54:05.029
Nov 30 03:54:05.044: INFO: Waiting for pod pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07 to disappear
Nov 30 03:54:05.047: INFO: Pod pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 03:54:05.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5598" for this suite. 11/30/22 03:54:05.05
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":109,"skipped":2012,"failed":0}
------------------------------
• [4.180 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:54:00.88
    Nov 30 03:54:00.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 03:54:00.88
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:00.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:00.906
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 11/30/22 03:54:00.91
    Nov 30 03:54:01.008: INFO: Waiting up to 5m0s for pod "pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07" in namespace "emptydir-5598" to be "Succeeded or Failed"
    Nov 30 03:54:01.011: INFO: Pod "pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.904256ms
    Nov 30 03:54:03.018: INFO: Pod "pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009787271s
    Nov 30 03:54:05.017: INFO: Pod "pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008270925s
    STEP: Saw pod success 11/30/22 03:54:05.017
    Nov 30 03:54:05.017: INFO: Pod "pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07" satisfied condition "Succeeded or Failed"
    Nov 30 03:54:05.020: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07 container test-container: <nil>
    STEP: delete the pod 11/30/22 03:54:05.029
    Nov 30 03:54:05.044: INFO: Waiting for pod pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07 to disappear
    Nov 30 03:54:05.047: INFO: Pod pod-ac3d072b-0649-4c03-a5a1-27efa1be3f07 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 03:54:05.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5598" for this suite. 11/30/22 03:54:05.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:54:05.061
Nov 30 03:54:05.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename custom-resource-definition 11/30/22 03:54:05.062
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:05.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:05.097
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Nov 30 03:54:05.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:54:10.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7957" for this suite. 11/30/22 03:54:10.444
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":110,"skipped":2041,"failed":0}
------------------------------
• [SLOW TEST] [5.389 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:54:05.061
    Nov 30 03:54:05.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename custom-resource-definition 11/30/22 03:54:05.062
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:05.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:05.097
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Nov 30 03:54:05.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:54:10.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7957" for this suite. 11/30/22 03:54:10.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:54:10.45
Nov 30 03:54:10.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename var-expansion 11/30/22 03:54:10.451
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:10.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:10.469
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Nov 30 03:54:10.505: INFO: Waiting up to 2m0s for pod "var-expansion-7a544192-cf20-4a02-a092-196ee9bd82ce" in namespace "var-expansion-5103" to be "container 0 failed with reason CreateContainerConfigError"
Nov 30 03:54:10.512: INFO: Pod "var-expansion-7a544192-cf20-4a02-a092-196ee9bd82ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.546859ms
Nov 30 03:54:12.522: INFO: Pod "var-expansion-7a544192-cf20-4a02-a092-196ee9bd82ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017524061s
Nov 30 03:54:12.522: INFO: Pod "var-expansion-7a544192-cf20-4a02-a092-196ee9bd82ce" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Nov 30 03:54:12.522: INFO: Deleting pod "var-expansion-7a544192-cf20-4a02-a092-196ee9bd82ce" in namespace "var-expansion-5103"
Nov 30 03:54:12.530: INFO: Wait up to 5m0s for pod "var-expansion-7a544192-cf20-4a02-a092-196ee9bd82ce" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Nov 30 03:54:16.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5103" for this suite. 11/30/22 03:54:16.539
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":111,"skipped":2064,"failed":0}
------------------------------
• [SLOW TEST] [6.115 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:54:10.45
    Nov 30 03:54:10.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename var-expansion 11/30/22 03:54:10.451
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:10.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:10.469
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Nov 30 03:54:10.505: INFO: Waiting up to 2m0s for pod "var-expansion-7a544192-cf20-4a02-a092-196ee9bd82ce" in namespace "var-expansion-5103" to be "container 0 failed with reason CreateContainerConfigError"
    Nov 30 03:54:10.512: INFO: Pod "var-expansion-7a544192-cf20-4a02-a092-196ee9bd82ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.546859ms
    Nov 30 03:54:12.522: INFO: Pod "var-expansion-7a544192-cf20-4a02-a092-196ee9bd82ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017524061s
    Nov 30 03:54:12.522: INFO: Pod "var-expansion-7a544192-cf20-4a02-a092-196ee9bd82ce" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Nov 30 03:54:12.522: INFO: Deleting pod "var-expansion-7a544192-cf20-4a02-a092-196ee9bd82ce" in namespace "var-expansion-5103"
    Nov 30 03:54:12.530: INFO: Wait up to 5m0s for pod "var-expansion-7a544192-cf20-4a02-a092-196ee9bd82ce" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Nov 30 03:54:16.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5103" for this suite. 11/30/22 03:54:16.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:54:16.566
Nov 30 03:54:16.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 03:54:16.566
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:16.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:16.593
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 11/30/22 03:54:16.595
Nov 30 03:54:16.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:54:19.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:54:30.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1148" for this suite. 11/30/22 03:54:30.405
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":112,"skipped":2071,"failed":0}
------------------------------
• [SLOW TEST] [13.848 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:54:16.566
    Nov 30 03:54:16.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 03:54:16.566
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:16.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:16.593
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 11/30/22 03:54:16.595
    Nov 30 03:54:16.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:54:19.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:54:30.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1148" for this suite. 11/30/22 03:54:30.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:54:30.414
Nov 30 03:54:30.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename disruption 11/30/22 03:54:30.415
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:30.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:30.443
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:54:30.445
Nov 30 03:54:30.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename disruption-2 11/30/22 03:54:30.446
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:30.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:30.469
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 11/30/22 03:54:30.489
STEP: Waiting for the pdb to be processed 11/30/22 03:54:32.5
STEP: Waiting for the pdb to be processed 11/30/22 03:54:34.517
STEP: listing a collection of PDBs across all namespaces 11/30/22 03:54:34.531
STEP: listing a collection of PDBs in namespace disruption-4532 11/30/22 03:54:34.534
STEP: deleting a collection of PDBs 11/30/22 03:54:34.538
STEP: Waiting for the PDB collection to be deleted 11/30/22 03:54:34.549
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Nov 30 03:54:34.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-6199" for this suite. 11/30/22 03:54:34.556
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Nov 30 03:54:34.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4532" for this suite. 11/30/22 03:54:34.569
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":113,"skipped":2096,"failed":0}
------------------------------
• [4.164 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:54:30.414
    Nov 30 03:54:30.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename disruption 11/30/22 03:54:30.415
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:30.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:30.443
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:54:30.445
    Nov 30 03:54:30.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename disruption-2 11/30/22 03:54:30.446
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:30.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:30.469
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 11/30/22 03:54:30.489
    STEP: Waiting for the pdb to be processed 11/30/22 03:54:32.5
    STEP: Waiting for the pdb to be processed 11/30/22 03:54:34.517
    STEP: listing a collection of PDBs across all namespaces 11/30/22 03:54:34.531
    STEP: listing a collection of PDBs in namespace disruption-4532 11/30/22 03:54:34.534
    STEP: deleting a collection of PDBs 11/30/22 03:54:34.538
    STEP: Waiting for the PDB collection to be deleted 11/30/22 03:54:34.549
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Nov 30 03:54:34.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-6199" for this suite. 11/30/22 03:54:34.556
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Nov 30 03:54:34.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4532" for this suite. 11/30/22 03:54:34.569
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:54:34.579
Nov 30 03:54:34.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename statefulset 11/30/22 03:54:34.579
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:34.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:34.601
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9367 11/30/22 03:54:34.604
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 11/30/22 03:54:34.616
STEP: Creating stateful set ss in namespace statefulset-9367 11/30/22 03:54:34.628
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9367 11/30/22 03:54:34.639
Nov 30 03:54:34.642: INFO: Found 0 stateful pods, waiting for 1
Nov 30 03:54:44.647: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 11/30/22 03:54:44.647
Nov 30 03:54:44.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 30 03:54:44.772: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 30 03:54:44.773: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 30 03:54:44.773: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 30 03:54:44.776: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 30 03:54:54.782: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 30 03:54:54.782: INFO: Waiting for statefulset status.replicas updated to 0
Nov 30 03:54:54.797: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999595s
Nov 30 03:54:55.801: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996909892s
Nov 30 03:54:56.804: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993280788s
Nov 30 03:54:57.809: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989058232s
Nov 30 03:54:58.813: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984306733s
Nov 30 03:54:59.816: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.980913952s
Nov 30 03:55:00.821: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976525038s
Nov 30 03:55:01.825: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972310833s
Nov 30 03:55:02.829: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.968545632s
Nov 30 03:55:03.833: INFO: Verifying statefulset ss doesn't scale past 1 for another 964.612066ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9367 11/30/22 03:55:04.834
Nov 30 03:55:04.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 30 03:55:04.968: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 30 03:55:04.968: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 30 03:55:04.968: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 30 03:55:04.971: INFO: Found 1 stateful pods, waiting for 3
Nov 30 03:55:14.975: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 30 03:55:14.975: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 30 03:55:14.975: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 11/30/22 03:55:14.975
STEP: Scale down will halt with unhealthy stateful pod 11/30/22 03:55:14.975
Nov 30 03:55:14.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 30 03:55:15.121: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 30 03:55:15.121: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 30 03:55:15.121: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 30 03:55:15.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 30 03:55:15.267: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 30 03:55:15.267: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 30 03:55:15.267: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 30 03:55:15.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 30 03:55:15.439: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 30 03:55:15.439: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 30 03:55:15.439: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 30 03:55:15.439: INFO: Waiting for statefulset status.replicas updated to 0
Nov 30 03:55:15.442: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 30 03:55:25.453: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 30 03:55:25.453: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 30 03:55:25.453: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 30 03:55:25.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999589s
Nov 30 03:55:26.491: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989767674s
Nov 30 03:55:27.495: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985208331s
Nov 30 03:55:28.500: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981082025s
Nov 30 03:55:29.503: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976880763s
Nov 30 03:55:30.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973107219s
Nov 30 03:55:31.510: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969864825s
Nov 30 03:55:32.516: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966142366s
Nov 30 03:55:33.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960780881s
Nov 30 03:55:34.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 956.090462ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9367 11/30/22 03:55:35.524
Nov 30 03:55:35.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 30 03:55:35.722: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 30 03:55:35.722: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 30 03:55:35.722: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 30 03:55:35.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 30 03:55:35.848: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 30 03:55:35.848: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 30 03:55:35.848: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 30 03:55:35.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 30 03:55:35.973: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 30 03:55:35.973: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 30 03:55:35.974: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 30 03:55:35.974: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 11/30/22 03:55:45.991
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Nov 30 03:55:45.991: INFO: Deleting all statefulset in ns statefulset-9367
Nov 30 03:55:45.994: INFO: Scaling statefulset ss to 0
Nov 30 03:55:46.004: INFO: Waiting for statefulset status.replicas updated to 0
Nov 30 03:55:46.007: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Nov 30 03:55:46.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9367" for this suite. 11/30/22 03:55:46.02
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":114,"skipped":2100,"failed":0}
------------------------------
• [SLOW TEST] [71.448 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:54:34.579
    Nov 30 03:54:34.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename statefulset 11/30/22 03:54:34.579
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:54:34.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:54:34.601
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9367 11/30/22 03:54:34.604
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 11/30/22 03:54:34.616
    STEP: Creating stateful set ss in namespace statefulset-9367 11/30/22 03:54:34.628
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9367 11/30/22 03:54:34.639
    Nov 30 03:54:34.642: INFO: Found 0 stateful pods, waiting for 1
    Nov 30 03:54:44.647: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 11/30/22 03:54:44.647
    Nov 30 03:54:44.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 30 03:54:44.772: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 30 03:54:44.773: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 30 03:54:44.773: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 30 03:54:44.776: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Nov 30 03:54:54.782: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 30 03:54:54.782: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 30 03:54:54.797: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999595s
    Nov 30 03:54:55.801: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996909892s
    Nov 30 03:54:56.804: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993280788s
    Nov 30 03:54:57.809: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989058232s
    Nov 30 03:54:58.813: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984306733s
    Nov 30 03:54:59.816: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.980913952s
    Nov 30 03:55:00.821: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976525038s
    Nov 30 03:55:01.825: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972310833s
    Nov 30 03:55:02.829: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.968545632s
    Nov 30 03:55:03.833: INFO: Verifying statefulset ss doesn't scale past 1 for another 964.612066ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9367 11/30/22 03:55:04.834
    Nov 30 03:55:04.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 30 03:55:04.968: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 30 03:55:04.968: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 30 03:55:04.968: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 30 03:55:04.971: INFO: Found 1 stateful pods, waiting for 3
    Nov 30 03:55:14.975: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 30 03:55:14.975: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 30 03:55:14.975: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 11/30/22 03:55:14.975
    STEP: Scale down will halt with unhealthy stateful pod 11/30/22 03:55:14.975
    Nov 30 03:55:14.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 30 03:55:15.121: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 30 03:55:15.121: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 30 03:55:15.121: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 30 03:55:15.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 30 03:55:15.267: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 30 03:55:15.267: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 30 03:55:15.267: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 30 03:55:15.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 30 03:55:15.439: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 30 03:55:15.439: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 30 03:55:15.439: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 30 03:55:15.439: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 30 03:55:15.442: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Nov 30 03:55:25.453: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 30 03:55:25.453: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Nov 30 03:55:25.453: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Nov 30 03:55:25.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999589s
    Nov 30 03:55:26.491: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989767674s
    Nov 30 03:55:27.495: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985208331s
    Nov 30 03:55:28.500: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981082025s
    Nov 30 03:55:29.503: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976880763s
    Nov 30 03:55:30.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973107219s
    Nov 30 03:55:31.510: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969864825s
    Nov 30 03:55:32.516: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966142366s
    Nov 30 03:55:33.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960780881s
    Nov 30 03:55:34.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 956.090462ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9367 11/30/22 03:55:35.524
    Nov 30 03:55:35.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 30 03:55:35.722: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 30 03:55:35.722: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 30 03:55:35.722: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 30 03:55:35.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 30 03:55:35.848: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 30 03:55:35.848: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 30 03:55:35.848: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 30 03:55:35.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-9367 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 30 03:55:35.973: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 30 03:55:35.973: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 30 03:55:35.974: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 30 03:55:35.974: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 11/30/22 03:55:45.991
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Nov 30 03:55:45.991: INFO: Deleting all statefulset in ns statefulset-9367
    Nov 30 03:55:45.994: INFO: Scaling statefulset ss to 0
    Nov 30 03:55:46.004: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 30 03:55:46.007: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Nov 30 03:55:46.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9367" for this suite. 11/30/22 03:55:46.02
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:55:46.027
Nov 30 03:55:46.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename init-container 11/30/22 03:55:46.028
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:55:46.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:55:46.05
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 11/30/22 03:55:46.053
Nov 30 03:55:46.053: INFO: PodSpec: initContainers in spec.initContainers
Nov 30 03:56:26.424: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ba80c428-8b8b-4ca9-b4ac-93c1748c420c", GenerateName:"", Namespace:"init-container-9808", SelfLink:"", UID:"8e467e3f-ee6f-4dd5-b783-9cfdb278fa25", ResourceVersion:"32737", Generation:0, CreationTimestamp:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"53529743"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.12.218\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.12.218\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012be6c0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012be738), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.November, 30, 3, 56, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012be768), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-h5xlt", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003e09680), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h5xlt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h5xlt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h5xlt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00412c028), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0007fb730), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00412c0c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00412c0f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00412c0f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00412c0fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003ac55f0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.10.8", PodIP:"192.168.12.218", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.12.218"}}, StartTime:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0007fb880)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0007fb8f0)}, Ready:false, RestartCount:3, Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-2", ImageID:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox@sha256:c26091c675b78a3ab71f78839315225279e6aa11660079136bf524d6285488ec", ContainerID:"containerd://2bad2d1ff837d292f2aef20dd7af53c361bd611bc6e689e9f9b8c0c8e58c57a3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e09700), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e096e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00412c18f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Nov 30 03:56:26.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9808" for this suite. 11/30/22 03:56:26.43
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":115,"skipped":2101,"failed":0}
------------------------------
• [SLOW TEST] [40.409 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:55:46.027
    Nov 30 03:55:46.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename init-container 11/30/22 03:55:46.028
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:55:46.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:55:46.05
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 11/30/22 03:55:46.053
    Nov 30 03:55:46.053: INFO: PodSpec: initContainers in spec.initContainers
    Nov 30 03:56:26.424: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ba80c428-8b8b-4ca9-b4ac-93c1748c420c", GenerateName:"", Namespace:"init-container-9808", SelfLink:"", UID:"8e467e3f-ee6f-4dd5-b783-9cfdb278fa25", ResourceVersion:"32737", Generation:0, CreationTimestamp:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"53529743"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.12.218\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.12.218\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012be6c0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012be738), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.November, 30, 3, 56, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012be768), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-h5xlt", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003e09680), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h5xlt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h5xlt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h5xlt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00412c028), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0007fb730), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00412c0c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00412c0f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00412c0f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00412c0fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003ac55f0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.10.8", PodIP:"192.168.12.218", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.12.218"}}, StartTime:time.Date(2022, time.November, 30, 3, 55, 46, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0007fb880)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0007fb8f0)}, Ready:false, RestartCount:3, Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-2", ImageID:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox@sha256:c26091c675b78a3ab71f78839315225279e6aa11660079136bf524d6285488ec", ContainerID:"containerd://2bad2d1ff837d292f2aef20dd7af53c361bd611bc6e689e9f9b8c0c8e58c57a3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e09700), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e096e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00412c18f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Nov 30 03:56:26.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9808" for this suite. 11/30/22 03:56:26.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:56:26.437
Nov 30 03:56:26.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename taint-multiple-pods 11/30/22 03:56:26.438
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:56:26.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:56:26.456
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Nov 30 03:56:26.459: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 30 03:57:26.529: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Nov 30 03:57:26.532: INFO: Starting informer...
STEP: Starting pods... 11/30/22 03:57:26.532
Nov 30 03:57:26.769: INFO: Pod1 is running on worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins. Tainting Node
Nov 30 03:57:26.978: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8603" to be "running"
Nov 30 03:57:26.980: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.518618ms
Nov 30 03:57:28.984: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006010578s
Nov 30 03:57:28.984: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Nov 30 03:57:28.984: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8603" to be "running"
Nov 30 03:57:28.987: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.841903ms
Nov 30 03:57:28.987: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Nov 30 03:57:28.987: INFO: Pod2 is running on worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins. Tainting Node
STEP: Trying to apply a taint on the Node 11/30/22 03:57:28.987
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/30/22 03:57:29.002
STEP: Waiting for Pod1 and Pod2 to be deleted 11/30/22 03:57:29.006
Nov 30 03:57:34.596: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 30 03:57:54.638: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/30/22 03:57:54.65
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Nov 30 03:57:54.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8603" for this suite. 11/30/22 03:57:54.658
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":116,"skipped":2126,"failed":0}
------------------------------
• [SLOW TEST] [88.226 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:56:26.437
    Nov 30 03:56:26.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename taint-multiple-pods 11/30/22 03:56:26.438
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:56:26.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:56:26.456
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Nov 30 03:56:26.459: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 30 03:57:26.529: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Nov 30 03:57:26.532: INFO: Starting informer...
    STEP: Starting pods... 11/30/22 03:57:26.532
    Nov 30 03:57:26.769: INFO: Pod1 is running on worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins. Tainting Node
    Nov 30 03:57:26.978: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8603" to be "running"
    Nov 30 03:57:26.980: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.518618ms
    Nov 30 03:57:28.984: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006010578s
    Nov 30 03:57:28.984: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Nov 30 03:57:28.984: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8603" to be "running"
    Nov 30 03:57:28.987: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.841903ms
    Nov 30 03:57:28.987: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Nov 30 03:57:28.987: INFO: Pod2 is running on worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins. Tainting Node
    STEP: Trying to apply a taint on the Node 11/30/22 03:57:28.987
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/30/22 03:57:29.002
    STEP: Waiting for Pod1 and Pod2 to be deleted 11/30/22 03:57:29.006
    Nov 30 03:57:34.596: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Nov 30 03:57:54.638: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/30/22 03:57:54.65
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 03:57:54.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-8603" for this suite. 11/30/22 03:57:54.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:57:54.665
Nov 30 03:57:54.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename sysctl 11/30/22 03:57:54.666
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:57:54.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:57:54.689
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 11/30/22 03:57:54.691
STEP: Watching for error events or started pod 11/30/22 03:57:54.722
STEP: Waiting for pod completion 11/30/22 03:57:56.726
Nov 30 03:57:56.726: INFO: Waiting up to 3m0s for pod "sysctl-9033f9b1-c6c1-46ee-9cf7-a2ed9fbf9196" in namespace "sysctl-3915" to be "completed"
Nov 30 03:57:56.736: INFO: Pod "sysctl-9033f9b1-c6c1-46ee-9cf7-a2ed9fbf9196": Phase="Pending", Reason="", readiness=false. Elapsed: 9.834032ms
Nov 30 03:57:58.739: INFO: Pod "sysctl-9033f9b1-c6c1-46ee-9cf7-a2ed9fbf9196": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012927007s
Nov 30 03:57:58.739: INFO: Pod "sysctl-9033f9b1-c6c1-46ee-9cf7-a2ed9fbf9196" satisfied condition "completed"
STEP: Checking that the pod succeeded 11/30/22 03:57:58.742
STEP: Getting logs from the pod 11/30/22 03:57:58.742
STEP: Checking that the sysctl is actually updated 11/30/22 03:57:58.749
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Nov 30 03:57:58.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3915" for this suite. 11/30/22 03:57:58.753
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":117,"skipped":2164,"failed":0}
------------------------------
• [4.094 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:57:54.665
    Nov 30 03:57:54.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename sysctl 11/30/22 03:57:54.666
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:57:54.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:57:54.689
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 11/30/22 03:57:54.691
    STEP: Watching for error events or started pod 11/30/22 03:57:54.722
    STEP: Waiting for pod completion 11/30/22 03:57:56.726
    Nov 30 03:57:56.726: INFO: Waiting up to 3m0s for pod "sysctl-9033f9b1-c6c1-46ee-9cf7-a2ed9fbf9196" in namespace "sysctl-3915" to be "completed"
    Nov 30 03:57:56.736: INFO: Pod "sysctl-9033f9b1-c6c1-46ee-9cf7-a2ed9fbf9196": Phase="Pending", Reason="", readiness=false. Elapsed: 9.834032ms
    Nov 30 03:57:58.739: INFO: Pod "sysctl-9033f9b1-c6c1-46ee-9cf7-a2ed9fbf9196": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012927007s
    Nov 30 03:57:58.739: INFO: Pod "sysctl-9033f9b1-c6c1-46ee-9cf7-a2ed9fbf9196" satisfied condition "completed"
    STEP: Checking that the pod succeeded 11/30/22 03:57:58.742
    STEP: Getting logs from the pod 11/30/22 03:57:58.742
    STEP: Checking that the sysctl is actually updated 11/30/22 03:57:58.749
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Nov 30 03:57:58.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-3915" for this suite. 11/30/22 03:57:58.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:57:58.76
Nov 30 03:57:58.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename sched-preemption 11/30/22 03:57:58.76
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:57:58.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:57:58.784
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Nov 30 03:57:58.807: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 30 03:58:58.873: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:58:58.876
Nov 30 03:58:58.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename sched-preemption-path 11/30/22 03:58:58.876
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:58:58.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:58:58.899
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Nov 30 03:58:58.913: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Nov 30 03:58:58.916: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Nov 30 03:58:58.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2129" for this suite. 11/30/22 03:58:58.934
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Nov 30 03:58:58.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5521" for this suite. 11/30/22 03:58:58.955
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":118,"skipped":2176,"failed":0}
------------------------------
• [SLOW TEST] [60.263 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:57:58.76
    Nov 30 03:57:58.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename sched-preemption 11/30/22 03:57:58.76
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:57:58.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:57:58.784
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Nov 30 03:57:58.807: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 30 03:58:58.873: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:58:58.876
    Nov 30 03:58:58.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename sched-preemption-path 11/30/22 03:58:58.876
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:58:58.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:58:58.899
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Nov 30 03:58:58.913: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Nov 30 03:58:58.916: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Nov 30 03:58:58.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-2129" for this suite. 11/30/22 03:58:58.934
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 03:58:58.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5521" for this suite. 11/30/22 03:58:58.955
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:58:59.024
Nov 30 03:58:59.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename replicaset 11/30/22 03:58:59.024
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:58:59.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:58:59.043
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Nov 30 03:58:59.063: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 30 03:59:04.066: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/30/22 03:59:04.066
STEP: Scaling up "test-rs" replicaset  11/30/22 03:59:04.066
Nov 30 03:59:04.076: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 11/30/22 03:59:04.076
W1130 03:59:04.086938      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Nov 30 03:59:04.088: INFO: observed ReplicaSet test-rs in namespace replicaset-3049 with ReadyReplicas 1, AvailableReplicas 1
Nov 30 03:59:04.115: INFO: observed ReplicaSet test-rs in namespace replicaset-3049 with ReadyReplicas 1, AvailableReplicas 1
Nov 30 03:59:04.149: INFO: observed ReplicaSet test-rs in namespace replicaset-3049 with ReadyReplicas 1, AvailableReplicas 1
Nov 30 03:59:04.167: INFO: observed ReplicaSet test-rs in namespace replicaset-3049 with ReadyReplicas 1, AvailableReplicas 1
Nov 30 03:59:05.203: INFO: observed ReplicaSet test-rs in namespace replicaset-3049 with ReadyReplicas 2, AvailableReplicas 2
Nov 30 03:59:05.782: INFO: observed Replicaset test-rs in namespace replicaset-3049 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Nov 30 03:59:05.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3049" for this suite. 11/30/22 03:59:05.787
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":119,"skipped":2184,"failed":0}
------------------------------
• [SLOW TEST] [6.770 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:58:59.024
    Nov 30 03:58:59.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename replicaset 11/30/22 03:58:59.024
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:58:59.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:58:59.043
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Nov 30 03:58:59.063: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 30 03:59:04.066: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/30/22 03:59:04.066
    STEP: Scaling up "test-rs" replicaset  11/30/22 03:59:04.066
    Nov 30 03:59:04.076: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 11/30/22 03:59:04.076
    W1130 03:59:04.086938      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Nov 30 03:59:04.088: INFO: observed ReplicaSet test-rs in namespace replicaset-3049 with ReadyReplicas 1, AvailableReplicas 1
    Nov 30 03:59:04.115: INFO: observed ReplicaSet test-rs in namespace replicaset-3049 with ReadyReplicas 1, AvailableReplicas 1
    Nov 30 03:59:04.149: INFO: observed ReplicaSet test-rs in namespace replicaset-3049 with ReadyReplicas 1, AvailableReplicas 1
    Nov 30 03:59:04.167: INFO: observed ReplicaSet test-rs in namespace replicaset-3049 with ReadyReplicas 1, AvailableReplicas 1
    Nov 30 03:59:05.203: INFO: observed ReplicaSet test-rs in namespace replicaset-3049 with ReadyReplicas 2, AvailableReplicas 2
    Nov 30 03:59:05.782: INFO: observed Replicaset test-rs in namespace replicaset-3049 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Nov 30 03:59:05.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3049" for this suite. 11/30/22 03:59:05.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:05.794
Nov 30 03:59:05.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 03:59:05.795
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:05.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:05.818
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 11/30/22 03:59:05.821
Nov 30 03:59:05.857: INFO: Waiting up to 5m0s for pod "pod-d79bd403-75f1-40dd-9034-9ed9fc194880" in namespace "emptydir-6942" to be "Succeeded or Failed"
Nov 30 03:59:05.860: INFO: Pod "pod-d79bd403-75f1-40dd-9034-9ed9fc194880": Phase="Pending", Reason="", readiness=false. Elapsed: 3.500932ms
Nov 30 03:59:07.865: INFO: Pod "pod-d79bd403-75f1-40dd-9034-9ed9fc194880": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008175641s
Nov 30 03:59:09.864: INFO: Pod "pod-d79bd403-75f1-40dd-9034-9ed9fc194880": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006793204s
STEP: Saw pod success 11/30/22 03:59:09.864
Nov 30 03:59:09.864: INFO: Pod "pod-d79bd403-75f1-40dd-9034-9ed9fc194880" satisfied condition "Succeeded or Failed"
Nov 30 03:59:09.867: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-d79bd403-75f1-40dd-9034-9ed9fc194880 container test-container: <nil>
STEP: delete the pod 11/30/22 03:59:09.874
Nov 30 03:59:09.891: INFO: Waiting for pod pod-d79bd403-75f1-40dd-9034-9ed9fc194880 to disappear
Nov 30 03:59:09.894: INFO: Pod pod-d79bd403-75f1-40dd-9034-9ed9fc194880 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 03:59:09.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6942" for this suite. 11/30/22 03:59:09.899
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":120,"skipped":2210,"failed":0}
------------------------------
• [4.111 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:05.794
    Nov 30 03:59:05.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 03:59:05.795
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:05.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:05.818
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 11/30/22 03:59:05.821
    Nov 30 03:59:05.857: INFO: Waiting up to 5m0s for pod "pod-d79bd403-75f1-40dd-9034-9ed9fc194880" in namespace "emptydir-6942" to be "Succeeded or Failed"
    Nov 30 03:59:05.860: INFO: Pod "pod-d79bd403-75f1-40dd-9034-9ed9fc194880": Phase="Pending", Reason="", readiness=false. Elapsed: 3.500932ms
    Nov 30 03:59:07.865: INFO: Pod "pod-d79bd403-75f1-40dd-9034-9ed9fc194880": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008175641s
    Nov 30 03:59:09.864: INFO: Pod "pod-d79bd403-75f1-40dd-9034-9ed9fc194880": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006793204s
    STEP: Saw pod success 11/30/22 03:59:09.864
    Nov 30 03:59:09.864: INFO: Pod "pod-d79bd403-75f1-40dd-9034-9ed9fc194880" satisfied condition "Succeeded or Failed"
    Nov 30 03:59:09.867: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-d79bd403-75f1-40dd-9034-9ed9fc194880 container test-container: <nil>
    STEP: delete the pod 11/30/22 03:59:09.874
    Nov 30 03:59:09.891: INFO: Waiting for pod pod-d79bd403-75f1-40dd-9034-9ed9fc194880 to disappear
    Nov 30 03:59:09.894: INFO: Pod pod-d79bd403-75f1-40dd-9034-9ed9fc194880 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 03:59:09.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6942" for this suite. 11/30/22 03:59:09.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:09.905
Nov 30 03:59:09.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 03:59:09.906
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:09.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:09.926
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 11/30/22 03:59:09.933
STEP: watching for the Service to be added 11/30/22 03:59:09.952
Nov 30 03:59:09.953: INFO: Found Service test-service-st7r8 in namespace services-688 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Nov 30 03:59:09.953: INFO: Service test-service-st7r8 created
STEP: Getting /status 11/30/22 03:59:09.953
Nov 30 03:59:09.958: INFO: Service test-service-st7r8 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 11/30/22 03:59:09.958
STEP: watching for the Service to be patched 11/30/22 03:59:09.964
Nov 30 03:59:09.965: INFO: observed Service test-service-st7r8 in namespace services-688 with annotations: map[] & LoadBalancer: {[]}
Nov 30 03:59:09.965: INFO: Found Service test-service-st7r8 in namespace services-688 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Nov 30 03:59:09.965: INFO: Service test-service-st7r8 has service status patched
STEP: updating the ServiceStatus 11/30/22 03:59:09.965
Nov 30 03:59:09.998: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 11/30/22 03:59:09.998
Nov 30 03:59:10.000: INFO: Observed Service test-service-st7r8 in namespace services-688 with annotations: map[] & Conditions: {[]}
Nov 30 03:59:10.000: INFO: Observed event: &Service{ObjectMeta:{test-service-st7r8  services-688  0e5dfb9a-ab41-49ad-9499-c2343cbf1667 33811 0 2022-11-30 03:59:09 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-11-30 03:59:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-11-30 03:59:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.103.12.70,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.103.12.70],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Nov 30 03:59:10.000: INFO: Found Service test-service-st7r8 in namespace services-688 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 30 03:59:10.000: INFO: Service test-service-st7r8 has service status updated
STEP: patching the service 11/30/22 03:59:10
STEP: watching for the Service to be patched 11/30/22 03:59:10.025
Nov 30 03:59:10.026: INFO: observed Service test-service-st7r8 in namespace services-688 with labels: map[test-service-static:true]
Nov 30 03:59:10.026: INFO: observed Service test-service-st7r8 in namespace services-688 with labels: map[test-service-static:true]
Nov 30 03:59:10.026: INFO: observed Service test-service-st7r8 in namespace services-688 with labels: map[test-service-static:true]
Nov 30 03:59:10.026: INFO: observed Service test-service-st7r8 in namespace services-688 with labels: map[test-service-static:true]
Nov 30 03:59:10.026: INFO: Found Service test-service-st7r8 in namespace services-688 with labels: map[test-service:patched test-service-static:true]
Nov 30 03:59:10.026: INFO: Service test-service-st7r8 patched
STEP: deleting the service 11/30/22 03:59:10.026
STEP: watching for the Service to be deleted 11/30/22 03:59:10.047
Nov 30 03:59:10.048: INFO: Observed event: ADDED
Nov 30 03:59:10.048: INFO: Observed event: MODIFIED
Nov 30 03:59:10.048: INFO: Observed event: MODIFIED
Nov 30 03:59:10.049: INFO: Observed event: MODIFIED
Nov 30 03:59:10.049: INFO: Observed event: MODIFIED
Nov 30 03:59:10.049: INFO: Found Service test-service-st7r8 in namespace services-688 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Nov 30 03:59:10.049: INFO: Service test-service-st7r8 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 03:59:10.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-688" for this suite. 11/30/22 03:59:10.058
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":121,"skipped":2226,"failed":0}
------------------------------
• [0.176 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:09.905
    Nov 30 03:59:09.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 03:59:09.906
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:09.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:09.926
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 11/30/22 03:59:09.933
    STEP: watching for the Service to be added 11/30/22 03:59:09.952
    Nov 30 03:59:09.953: INFO: Found Service test-service-st7r8 in namespace services-688 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Nov 30 03:59:09.953: INFO: Service test-service-st7r8 created
    STEP: Getting /status 11/30/22 03:59:09.953
    Nov 30 03:59:09.958: INFO: Service test-service-st7r8 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 11/30/22 03:59:09.958
    STEP: watching for the Service to be patched 11/30/22 03:59:09.964
    Nov 30 03:59:09.965: INFO: observed Service test-service-st7r8 in namespace services-688 with annotations: map[] & LoadBalancer: {[]}
    Nov 30 03:59:09.965: INFO: Found Service test-service-st7r8 in namespace services-688 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Nov 30 03:59:09.965: INFO: Service test-service-st7r8 has service status patched
    STEP: updating the ServiceStatus 11/30/22 03:59:09.965
    Nov 30 03:59:09.998: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 11/30/22 03:59:09.998
    Nov 30 03:59:10.000: INFO: Observed Service test-service-st7r8 in namespace services-688 with annotations: map[] & Conditions: {[]}
    Nov 30 03:59:10.000: INFO: Observed event: &Service{ObjectMeta:{test-service-st7r8  services-688  0e5dfb9a-ab41-49ad-9499-c2343cbf1667 33811 0 2022-11-30 03:59:09 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-11-30 03:59:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-11-30 03:59:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.103.12.70,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.103.12.70],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Nov 30 03:59:10.000: INFO: Found Service test-service-st7r8 in namespace services-688 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 30 03:59:10.000: INFO: Service test-service-st7r8 has service status updated
    STEP: patching the service 11/30/22 03:59:10
    STEP: watching for the Service to be patched 11/30/22 03:59:10.025
    Nov 30 03:59:10.026: INFO: observed Service test-service-st7r8 in namespace services-688 with labels: map[test-service-static:true]
    Nov 30 03:59:10.026: INFO: observed Service test-service-st7r8 in namespace services-688 with labels: map[test-service-static:true]
    Nov 30 03:59:10.026: INFO: observed Service test-service-st7r8 in namespace services-688 with labels: map[test-service-static:true]
    Nov 30 03:59:10.026: INFO: observed Service test-service-st7r8 in namespace services-688 with labels: map[test-service-static:true]
    Nov 30 03:59:10.026: INFO: Found Service test-service-st7r8 in namespace services-688 with labels: map[test-service:patched test-service-static:true]
    Nov 30 03:59:10.026: INFO: Service test-service-st7r8 patched
    STEP: deleting the service 11/30/22 03:59:10.026
    STEP: watching for the Service to be deleted 11/30/22 03:59:10.047
    Nov 30 03:59:10.048: INFO: Observed event: ADDED
    Nov 30 03:59:10.048: INFO: Observed event: MODIFIED
    Nov 30 03:59:10.048: INFO: Observed event: MODIFIED
    Nov 30 03:59:10.049: INFO: Observed event: MODIFIED
    Nov 30 03:59:10.049: INFO: Observed event: MODIFIED
    Nov 30 03:59:10.049: INFO: Found Service test-service-st7r8 in namespace services-688 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Nov 30 03:59:10.049: INFO: Service test-service-st7r8 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 03:59:10.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-688" for this suite. 11/30/22 03:59:10.058
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:10.081
Nov 30 03:59:10.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 03:59:10.082
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:10.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:10.114
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 11/30/22 03:59:10.116
Nov 30 03:59:10.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321" in namespace "downward-api-8789" to be "Succeeded or Failed"
Nov 30 03:59:10.136: INFO: Pod "downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321": Phase="Pending", Reason="", readiness=false. Elapsed: 4.264146ms
Nov 30 03:59:12.141: INFO: Pod "downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00863135s
Nov 30 03:59:14.140: INFO: Pod "downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008066253s
STEP: Saw pod success 11/30/22 03:59:14.14
Nov 30 03:59:14.140: INFO: Pod "downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321" satisfied condition "Succeeded or Failed"
Nov 30 03:59:14.143: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321 container client-container: <nil>
STEP: delete the pod 11/30/22 03:59:14.148
Nov 30 03:59:14.163: INFO: Waiting for pod downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321 to disappear
Nov 30 03:59:14.166: INFO: Pod downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Nov 30 03:59:14.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8789" for this suite. 11/30/22 03:59:14.17
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":122,"skipped":2229,"failed":0}
------------------------------
• [4.096 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:10.081
    Nov 30 03:59:10.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 03:59:10.082
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:10.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:10.114
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 11/30/22 03:59:10.116
    Nov 30 03:59:10.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321" in namespace "downward-api-8789" to be "Succeeded or Failed"
    Nov 30 03:59:10.136: INFO: Pod "downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321": Phase="Pending", Reason="", readiness=false. Elapsed: 4.264146ms
    Nov 30 03:59:12.141: INFO: Pod "downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00863135s
    Nov 30 03:59:14.140: INFO: Pod "downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008066253s
    STEP: Saw pod success 11/30/22 03:59:14.14
    Nov 30 03:59:14.140: INFO: Pod "downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321" satisfied condition "Succeeded or Failed"
    Nov 30 03:59:14.143: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321 container client-container: <nil>
    STEP: delete the pod 11/30/22 03:59:14.148
    Nov 30 03:59:14.163: INFO: Waiting for pod downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321 to disappear
    Nov 30 03:59:14.166: INFO: Pod downwardapi-volume-c2b88c59-d2c4-4d6e-a9be-56f6e9706321 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Nov 30 03:59:14.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8789" for this suite. 11/30/22 03:59:14.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:14.177
Nov 30 03:59:14.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 03:59:14.178
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:14.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:14.204
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-a5f5b7ea-2113-485e-bbee-ad6744d68f9a 11/30/22 03:59:14.207
STEP: Creating a pod to test consume configMaps 11/30/22 03:59:14.221
Nov 30 03:59:14.232: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855" in namespace "projected-5377" to be "Succeeded or Failed"
Nov 30 03:59:14.236: INFO: Pod "pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855": Phase="Pending", Reason="", readiness=false. Elapsed: 3.739645ms
Nov 30 03:59:16.240: INFO: Pod "pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007881742s
Nov 30 03:59:18.239: INFO: Pod "pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007407429s
STEP: Saw pod success 11/30/22 03:59:18.24
Nov 30 03:59:18.240: INFO: Pod "pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855" satisfied condition "Succeeded or Failed"
Nov 30 03:59:18.243: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855 container projected-configmap-volume-test: <nil>
STEP: delete the pod 11/30/22 03:59:18.248
Nov 30 03:59:18.258: INFO: Waiting for pod pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855 to disappear
Nov 30 03:59:18.261: INFO: Pod pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Nov 30 03:59:18.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5377" for this suite. 11/30/22 03:59:18.265
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":123,"skipped":2235,"failed":0}
------------------------------
• [4.093 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:14.177
    Nov 30 03:59:14.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 03:59:14.178
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:14.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:14.204
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-a5f5b7ea-2113-485e-bbee-ad6744d68f9a 11/30/22 03:59:14.207
    STEP: Creating a pod to test consume configMaps 11/30/22 03:59:14.221
    Nov 30 03:59:14.232: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855" in namespace "projected-5377" to be "Succeeded or Failed"
    Nov 30 03:59:14.236: INFO: Pod "pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855": Phase="Pending", Reason="", readiness=false. Elapsed: 3.739645ms
    Nov 30 03:59:16.240: INFO: Pod "pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007881742s
    Nov 30 03:59:18.239: INFO: Pod "pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007407429s
    STEP: Saw pod success 11/30/22 03:59:18.24
    Nov 30 03:59:18.240: INFO: Pod "pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855" satisfied condition "Succeeded or Failed"
    Nov 30 03:59:18.243: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 11/30/22 03:59:18.248
    Nov 30 03:59:18.258: INFO: Waiting for pod pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855 to disappear
    Nov 30 03:59:18.261: INFO: Pod pod-projected-configmaps-3655e7b3-609e-4f58-b5af-8134ba887855 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Nov 30 03:59:18.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5377" for this suite. 11/30/22 03:59:18.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:18.271
Nov 30 03:59:18.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 03:59:18.271
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:18.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:18.293
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-1664/secret-test-05294ca3-9ef0-49f1-bfb8-e2ca00ae8068 11/30/22 03:59:18.295
STEP: Creating a pod to test consume secrets 11/30/22 03:59:18.304
Nov 30 03:59:18.313: INFO: Waiting up to 5m0s for pod "pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d" in namespace "secrets-1664" to be "Succeeded or Failed"
Nov 30 03:59:18.320: INFO: Pod "pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.296587ms
Nov 30 03:59:20.325: INFO: Pod "pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011333998s
Nov 30 03:59:22.323: INFO: Pod "pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009690021s
STEP: Saw pod success 11/30/22 03:59:22.323
Nov 30 03:59:22.323: INFO: Pod "pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d" satisfied condition "Succeeded or Failed"
Nov 30 03:59:22.326: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d container env-test: <nil>
STEP: delete the pod 11/30/22 03:59:22.331
Nov 30 03:59:22.346: INFO: Waiting for pod pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d to disappear
Nov 30 03:59:22.349: INFO: Pod pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Nov 30 03:59:22.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1664" for this suite. 11/30/22 03:59:22.353
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":124,"skipped":2267,"failed":0}
------------------------------
• [4.088 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:18.271
    Nov 30 03:59:18.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 03:59:18.271
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:18.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:18.293
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-1664/secret-test-05294ca3-9ef0-49f1-bfb8-e2ca00ae8068 11/30/22 03:59:18.295
    STEP: Creating a pod to test consume secrets 11/30/22 03:59:18.304
    Nov 30 03:59:18.313: INFO: Waiting up to 5m0s for pod "pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d" in namespace "secrets-1664" to be "Succeeded or Failed"
    Nov 30 03:59:18.320: INFO: Pod "pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.296587ms
    Nov 30 03:59:20.325: INFO: Pod "pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011333998s
    Nov 30 03:59:22.323: INFO: Pod "pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009690021s
    STEP: Saw pod success 11/30/22 03:59:22.323
    Nov 30 03:59:22.323: INFO: Pod "pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d" satisfied condition "Succeeded or Failed"
    Nov 30 03:59:22.326: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d container env-test: <nil>
    STEP: delete the pod 11/30/22 03:59:22.331
    Nov 30 03:59:22.346: INFO: Waiting for pod pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d to disappear
    Nov 30 03:59:22.349: INFO: Pod pod-configmaps-158f318d-34f9-44b6-b6b3-d5edfec1a07d no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 03:59:22.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1664" for this suite. 11/30/22 03:59:22.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:22.359
Nov 30 03:59:22.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 03:59:22.36
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:22.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:22.385
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-bd27d311-5580-48ab-92f1-08363bf002ec 11/30/22 03:59:22.387
STEP: Creating a pod to test consume secrets 11/30/22 03:59:22.393
Nov 30 03:59:22.407: INFO: Waiting up to 5m0s for pod "pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41" in namespace "secrets-7248" to be "Succeeded or Failed"
Nov 30 03:59:22.410: INFO: Pod "pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.606511ms
Nov 30 03:59:24.414: INFO: Pod "pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006954815s
Nov 30 03:59:26.414: INFO: Pod "pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007244854s
STEP: Saw pod success 11/30/22 03:59:26.414
Nov 30 03:59:26.414: INFO: Pod "pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41" satisfied condition "Succeeded or Failed"
Nov 30 03:59:26.418: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41 container secret-volume-test: <nil>
STEP: delete the pod 11/30/22 03:59:26.43
Nov 30 03:59:26.443: INFO: Waiting for pod pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41 to disappear
Nov 30 03:59:26.445: INFO: Pod pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Nov 30 03:59:26.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7248" for this suite. 11/30/22 03:59:26.45
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":125,"skipped":2280,"failed":0}
------------------------------
• [4.115 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:22.359
    Nov 30 03:59:22.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 03:59:22.36
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:22.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:22.385
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-bd27d311-5580-48ab-92f1-08363bf002ec 11/30/22 03:59:22.387
    STEP: Creating a pod to test consume secrets 11/30/22 03:59:22.393
    Nov 30 03:59:22.407: INFO: Waiting up to 5m0s for pod "pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41" in namespace "secrets-7248" to be "Succeeded or Failed"
    Nov 30 03:59:22.410: INFO: Pod "pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.606511ms
    Nov 30 03:59:24.414: INFO: Pod "pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006954815s
    Nov 30 03:59:26.414: INFO: Pod "pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007244854s
    STEP: Saw pod success 11/30/22 03:59:26.414
    Nov 30 03:59:26.414: INFO: Pod "pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41" satisfied condition "Succeeded or Failed"
    Nov 30 03:59:26.418: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41 container secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 03:59:26.43
    Nov 30 03:59:26.443: INFO: Waiting for pod pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41 to disappear
    Nov 30 03:59:26.445: INFO: Pod pod-secrets-40c4240e-b678-40bb-aa78-7d3d98589e41 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 03:59:26.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7248" for this suite. 11/30/22 03:59:26.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:26.475
Nov 30 03:59:26.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 03:59:26.476
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:26.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:26.505
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 03:59:26.529
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 03:59:26.951
STEP: Deploying the webhook pod 11/30/22 03:59:26.961
STEP: Wait for the deployment to be ready 11/30/22 03:59:26.976
Nov 30 03:59:26.989: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 03:59:28.999
STEP: Verifying the service has paired with the endpoint 11/30/22 03:59:29.013
Nov 30 03:59:30.013: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Nov 30 03:59:30.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Registering the custom resource webhook via the AdmissionRegistration API 11/30/22 03:59:30.527
STEP: Creating a custom resource that should be denied by the webhook 11/30/22 03:59:30.542
STEP: Creating a custom resource whose deletion would be denied by the webhook 11/30/22 03:59:32.575
STEP: Updating the custom resource with disallowed data should be denied 11/30/22 03:59:32.584
STEP: Deleting the custom resource should be denied 11/30/22 03:59:32.594
STEP: Remove the offending key and value from the custom resource data 11/30/22 03:59:32.604
STEP: Deleting the updated custom resource should be successful 11/30/22 03:59:32.614
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:59:33.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8047" for this suite. 11/30/22 03:59:33.168
STEP: Destroying namespace "webhook-8047-markers" for this suite. 11/30/22 03:59:33.173
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":126,"skipped":2294,"failed":0}
------------------------------
• [SLOW TEST] [6.777 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:26.475
    Nov 30 03:59:26.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 03:59:26.476
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:26.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:26.505
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 03:59:26.529
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 03:59:26.951
    STEP: Deploying the webhook pod 11/30/22 03:59:26.961
    STEP: Wait for the deployment to be ready 11/30/22 03:59:26.976
    Nov 30 03:59:26.989: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 03:59:28.999
    STEP: Verifying the service has paired with the endpoint 11/30/22 03:59:29.013
    Nov 30 03:59:30.013: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Nov 30 03:59:30.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 11/30/22 03:59:30.527
    STEP: Creating a custom resource that should be denied by the webhook 11/30/22 03:59:30.542
    STEP: Creating a custom resource whose deletion would be denied by the webhook 11/30/22 03:59:32.575
    STEP: Updating the custom resource with disallowed data should be denied 11/30/22 03:59:32.584
    STEP: Deleting the custom resource should be denied 11/30/22 03:59:32.594
    STEP: Remove the offending key and value from the custom resource data 11/30/22 03:59:32.604
    STEP: Deleting the updated custom resource should be successful 11/30/22 03:59:32.614
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:59:33.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8047" for this suite. 11/30/22 03:59:33.168
    STEP: Destroying namespace "webhook-8047-markers" for this suite. 11/30/22 03:59:33.173
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:33.253
Nov 30 03:59:33.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 03:59:33.254
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:33.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:33.287
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 11/30/22 03:59:33.292
Nov 30 03:59:33.323: INFO: Waiting up to 5m0s for pod "pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a" in namespace "emptydir-3980" to be "Succeeded or Failed"
Nov 30 03:59:33.329: INFO: Pod "pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.289865ms
Nov 30 03:59:35.334: INFO: Pod "pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011232541s
Nov 30 03:59:37.333: INFO: Pod "pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010534649s
STEP: Saw pod success 11/30/22 03:59:37.333
Nov 30 03:59:37.333: INFO: Pod "pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a" satisfied condition "Succeeded or Failed"
Nov 30 03:59:37.336: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a container test-container: <nil>
STEP: delete the pod 11/30/22 03:59:37.342
Nov 30 03:59:37.352: INFO: Waiting for pod pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a to disappear
Nov 30 03:59:37.355: INFO: Pod pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 03:59:37.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3980" for this suite. 11/30/22 03:59:37.36
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":127,"skipped":2313,"failed":0}
------------------------------
• [4.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:33.253
    Nov 30 03:59:33.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 03:59:33.254
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:33.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:33.287
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 11/30/22 03:59:33.292
    Nov 30 03:59:33.323: INFO: Waiting up to 5m0s for pod "pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a" in namespace "emptydir-3980" to be "Succeeded or Failed"
    Nov 30 03:59:33.329: INFO: Pod "pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.289865ms
    Nov 30 03:59:35.334: INFO: Pod "pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011232541s
    Nov 30 03:59:37.333: INFO: Pod "pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010534649s
    STEP: Saw pod success 11/30/22 03:59:37.333
    Nov 30 03:59:37.333: INFO: Pod "pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a" satisfied condition "Succeeded or Failed"
    Nov 30 03:59:37.336: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a container test-container: <nil>
    STEP: delete the pod 11/30/22 03:59:37.342
    Nov 30 03:59:37.352: INFO: Waiting for pod pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a to disappear
    Nov 30 03:59:37.355: INFO: Pod pod-bf807f7c-0a7f-4a2a-9ff1-ad82aa8bce4a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 03:59:37.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3980" for this suite. 11/30/22 03:59:37.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:37.367
Nov 30 03:59:37.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename gc 11/30/22 03:59:37.368
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:37.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:37.39
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 11/30/22 03:59:37.392
STEP: Wait for the Deployment to create new ReplicaSet 11/30/22 03:59:37.397
STEP: delete the deployment 11/30/22 03:59:37.911
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 11/30/22 03:59:37.917
STEP: Gathering metrics 11/30/22 03:59:38.449
Nov 30 03:59:38.475: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
Nov 30 03:59:38.505: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 30.130941ms
Nov 30 03:59:38.505: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
Nov 30 03:59:38.505: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
Nov 30 03:59:38.558: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Nov 30 03:59:38.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1540" for this suite. 11/30/22 03:59:38.563
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":128,"skipped":2345,"failed":0}
------------------------------
• [1.201 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:37.367
    Nov 30 03:59:37.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename gc 11/30/22 03:59:37.368
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:37.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:37.39
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 11/30/22 03:59:37.392
    STEP: Wait for the Deployment to create new ReplicaSet 11/30/22 03:59:37.397
    STEP: delete the deployment 11/30/22 03:59:37.911
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 11/30/22 03:59:37.917
    STEP: Gathering metrics 11/30/22 03:59:38.449
    Nov 30 03:59:38.475: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
    Nov 30 03:59:38.505: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 30.130941ms
    Nov 30 03:59:38.505: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
    Nov 30 03:59:38.505: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
    Nov 30 03:59:38.558: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Nov 30 03:59:38.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1540" for this suite. 11/30/22 03:59:38.563
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:38.568
Nov 30 03:59:38.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename var-expansion 11/30/22 03:59:38.569
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:38.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:38.592
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Nov 30 03:59:38.624: INFO: Waiting up to 2m0s for pod "var-expansion-d6e3cd5a-6764-432e-803a-5715334c7794" in namespace "var-expansion-8975" to be "container 0 failed with reason CreateContainerConfigError"
Nov 30 03:59:38.628: INFO: Pod "var-expansion-d6e3cd5a-6764-432e-803a-5715334c7794": Phase="Pending", Reason="", readiness=false. Elapsed: 4.204001ms
Nov 30 03:59:40.632: INFO: Pod "var-expansion-d6e3cd5a-6764-432e-803a-5715334c7794": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008136346s
Nov 30 03:59:40.632: INFO: Pod "var-expansion-d6e3cd5a-6764-432e-803a-5715334c7794" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Nov 30 03:59:40.632: INFO: Deleting pod "var-expansion-d6e3cd5a-6764-432e-803a-5715334c7794" in namespace "var-expansion-8975"
Nov 30 03:59:40.640: INFO: Wait up to 5m0s for pod "var-expansion-d6e3cd5a-6764-432e-803a-5715334c7794" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Nov 30 03:59:44.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8975" for this suite. 11/30/22 03:59:44.653
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":129,"skipped":2346,"failed":0}
------------------------------
• [SLOW TEST] [6.092 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:38.568
    Nov 30 03:59:38.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename var-expansion 11/30/22 03:59:38.569
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:38.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:38.592
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Nov 30 03:59:38.624: INFO: Waiting up to 2m0s for pod "var-expansion-d6e3cd5a-6764-432e-803a-5715334c7794" in namespace "var-expansion-8975" to be "container 0 failed with reason CreateContainerConfigError"
    Nov 30 03:59:38.628: INFO: Pod "var-expansion-d6e3cd5a-6764-432e-803a-5715334c7794": Phase="Pending", Reason="", readiness=false. Elapsed: 4.204001ms
    Nov 30 03:59:40.632: INFO: Pod "var-expansion-d6e3cd5a-6764-432e-803a-5715334c7794": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008136346s
    Nov 30 03:59:40.632: INFO: Pod "var-expansion-d6e3cd5a-6764-432e-803a-5715334c7794" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Nov 30 03:59:40.632: INFO: Deleting pod "var-expansion-d6e3cd5a-6764-432e-803a-5715334c7794" in namespace "var-expansion-8975"
    Nov 30 03:59:40.640: INFO: Wait up to 5m0s for pod "var-expansion-d6e3cd5a-6764-432e-803a-5715334c7794" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Nov 30 03:59:44.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8975" for this suite. 11/30/22 03:59:44.653
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:44.661
Nov 30 03:59:44.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 11/30/22 03:59:44.661
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:44.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:44.688
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 11/30/22 03:59:44.691
STEP: Creating hostNetwork=false pod 11/30/22 03:59:44.691
W1130 03:59:44.722914      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPath volumes (volume "host-etc-hosts")
Nov 30 03:59:44.723: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4894" to be "running and ready"
Nov 30 03:59:44.725: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.521085ms
Nov 30 03:59:44.725: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:59:46.729: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006873778s
Nov 30 03:59:46.729: INFO: The phase of Pod test-pod is Running (Ready = true)
Nov 30 03:59:46.729: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 11/30/22 03:59:46.732
W1130 03:59:46.743374      20 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true), hostPath volumes (volume "host-etc-hosts")
Nov 30 03:59:46.743: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4894" to be "running and ready"
Nov 30 03:59:46.748: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.272011ms
Nov 30 03:59:46.748: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 30 03:59:48.752: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009382648s
Nov 30 03:59:48.752: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Nov 30 03:59:48.752: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 11/30/22 03:59:48.755
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 11/30/22 03:59:48.755
Nov 30 03:59:48.755: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:59:48.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:59:48.756: INFO: ExecWithOptions: Clientset creation
Nov 30 03:59:48.756: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 30 03:59:48.829: INFO: Exec stderr: ""
Nov 30 03:59:48.829: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:59:48.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:59:48.829: INFO: ExecWithOptions: Clientset creation
Nov 30 03:59:48.829: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 30 03:59:48.905: INFO: Exec stderr: ""
Nov 30 03:59:48.905: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:59:48.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:59:48.906: INFO: ExecWithOptions: Clientset creation
Nov 30 03:59:48.906: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 30 03:59:48.948: INFO: Exec stderr: ""
Nov 30 03:59:48.948: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:59:48.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:59:48.949: INFO: ExecWithOptions: Clientset creation
Nov 30 03:59:48.949: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 30 03:59:49.008: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 11/30/22 03:59:49.008
Nov 30 03:59:49.008: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:59:49.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:59:49.009: INFO: ExecWithOptions: Clientset creation
Nov 30 03:59:49.009: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Nov 30 03:59:49.065: INFO: Exec stderr: ""
Nov 30 03:59:49.065: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:59:49.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:59:49.065: INFO: ExecWithOptions: Clientset creation
Nov 30 03:59:49.065: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Nov 30 03:59:49.129: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 11/30/22 03:59:49.129
Nov 30 03:59:49.130: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:59:49.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:59:49.130: INFO: ExecWithOptions: Clientset creation
Nov 30 03:59:49.130: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 30 03:59:49.178: INFO: Exec stderr: ""
Nov 30 03:59:49.178: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:59:49.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:59:49.178: INFO: ExecWithOptions: Clientset creation
Nov 30 03:59:49.178: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 30 03:59:49.249: INFO: Exec stderr: ""
Nov 30 03:59:49.249: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:59:49.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:59:49.250: INFO: ExecWithOptions: Clientset creation
Nov 30 03:59:49.250: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 30 03:59:49.292: INFO: Exec stderr: ""
Nov 30 03:59:49.292: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 03:59:49.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 03:59:49.293: INFO: ExecWithOptions: Clientset creation
Nov 30 03:59:49.293: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 30 03:59:49.364: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Nov 30 03:59:49.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4894" for this suite. 11/30/22 03:59:49.368
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":130,"skipped":2350,"failed":0}
------------------------------
• [4.714 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:44.661
    Nov 30 03:59:44.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 11/30/22 03:59:44.661
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:44.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:44.688
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 11/30/22 03:59:44.691
    STEP: Creating hostNetwork=false pod 11/30/22 03:59:44.691
    W1130 03:59:44.722914      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPath volumes (volume "host-etc-hosts")
    Nov 30 03:59:44.723: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4894" to be "running and ready"
    Nov 30 03:59:44.725: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.521085ms
    Nov 30 03:59:44.725: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:59:46.729: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006873778s
    Nov 30 03:59:46.729: INFO: The phase of Pod test-pod is Running (Ready = true)
    Nov 30 03:59:46.729: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 11/30/22 03:59:46.732
    W1130 03:59:46.743374      20 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true), hostPath volumes (volume "host-etc-hosts")
    Nov 30 03:59:46.743: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4894" to be "running and ready"
    Nov 30 03:59:46.748: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.272011ms
    Nov 30 03:59:46.748: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 03:59:48.752: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009382648s
    Nov 30 03:59:48.752: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Nov 30 03:59:48.752: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 11/30/22 03:59:48.755
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 11/30/22 03:59:48.755
    Nov 30 03:59:48.755: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:59:48.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:59:48.756: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:59:48.756: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 30 03:59:48.829: INFO: Exec stderr: ""
    Nov 30 03:59:48.829: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:59:48.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:59:48.829: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:59:48.829: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 30 03:59:48.905: INFO: Exec stderr: ""
    Nov 30 03:59:48.905: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:59:48.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:59:48.906: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:59:48.906: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 30 03:59:48.948: INFO: Exec stderr: ""
    Nov 30 03:59:48.948: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:59:48.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:59:48.949: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:59:48.949: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 30 03:59:49.008: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 11/30/22 03:59:49.008
    Nov 30 03:59:49.008: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:59:49.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:59:49.009: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:59:49.009: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Nov 30 03:59:49.065: INFO: Exec stderr: ""
    Nov 30 03:59:49.065: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:59:49.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:59:49.065: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:59:49.065: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Nov 30 03:59:49.129: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 11/30/22 03:59:49.129
    Nov 30 03:59:49.130: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:59:49.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:59:49.130: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:59:49.130: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 30 03:59:49.178: INFO: Exec stderr: ""
    Nov 30 03:59:49.178: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:59:49.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:59:49.178: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:59:49.178: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 30 03:59:49.249: INFO: Exec stderr: ""
    Nov 30 03:59:49.249: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:59:49.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:59:49.250: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:59:49.250: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 30 03:59:49.292: INFO: Exec stderr: ""
    Nov 30 03:59:49.292: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4894 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 03:59:49.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 03:59:49.293: INFO: ExecWithOptions: Clientset creation
    Nov 30 03:59:49.293: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4894/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 30 03:59:49.364: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Nov 30 03:59:49.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-4894" for this suite. 11/30/22 03:59:49.368
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:49.375
Nov 30 03:59:49.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename custom-resource-definition 11/30/22 03:59:49.376
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:49.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:49.393
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Nov 30 03:59:49.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 03:59:50.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4490" for this suite. 11/30/22 03:59:50.425
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":131,"skipped":2354,"failed":0}
------------------------------
• [1.057 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:49.375
    Nov 30 03:59:49.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename custom-resource-definition 11/30/22 03:59:49.376
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:49.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:49.393
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Nov 30 03:59:49.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 03:59:50.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4490" for this suite. 11/30/22 03:59:50.425
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:50.432
Nov 30 03:59:50.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename daemonsets 11/30/22 03:59:50.433
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:50.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:50.453
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 11/30/22 03:59:50.487
STEP: Check that daemon pods launch on every node of the cluster. 11/30/22 03:59:50.491
Nov 30 03:59:50.495: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:50.495: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:50.495: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:50.499: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 03:59:50.499: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 03:59:51.505: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:51.505: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:51.505: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:51.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 30 03:59:51.508: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 03:59:52.505: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:52.505: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:52.505: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:52.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Nov 30 03:59:52.508: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 11/30/22 03:59:52.51
Nov 30 03:59:52.524: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:52.525: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:52.525: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:52.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 30 03:59:52.527: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 03:59:53.532: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:53.532: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:53.532: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:53.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 30 03:59:53.535: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 03:59:54.536: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:54.536: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:54.536: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:54.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 30 03:59:54.539: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 03:59:55.532: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:55.533: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:55.533: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:55.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 30 03:59:55.536: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 03:59:56.533: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:56.533: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:56.533: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 03:59:56.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Nov 30 03:59:56.536: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 11/30/22 03:59:56.539
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-151, will wait for the garbage collector to delete the pods 11/30/22 03:59:56.539
Nov 30 03:59:56.602: INFO: Deleting DaemonSet.extensions daemon-set took: 9.764351ms
Nov 30 03:59:56.703: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.94748ms
Nov 30 03:59:59.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 03:59:59.408: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 30 03:59:59.411: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34755"},"items":null}

Nov 30 03:59:59.413: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34755"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Nov 30 03:59:59.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-151" for this suite. 11/30/22 03:59:59.439
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":132,"skipped":2358,"failed":0}
------------------------------
• [SLOW TEST] [9.014 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:50.432
    Nov 30 03:59:50.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename daemonsets 11/30/22 03:59:50.433
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:50.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:50.453
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 11/30/22 03:59:50.487
    STEP: Check that daemon pods launch on every node of the cluster. 11/30/22 03:59:50.491
    Nov 30 03:59:50.495: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:50.495: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:50.495: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:50.499: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 03:59:50.499: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 03:59:51.505: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:51.505: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:51.505: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:51.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 30 03:59:51.508: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 03:59:52.505: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:52.505: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:52.505: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:52.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Nov 30 03:59:52.508: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 11/30/22 03:59:52.51
    Nov 30 03:59:52.524: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:52.525: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:52.525: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:52.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 30 03:59:52.527: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 03:59:53.532: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:53.532: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:53.532: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:53.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 30 03:59:53.535: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 03:59:54.536: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:54.536: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:54.536: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:54.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 30 03:59:54.539: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 03:59:55.532: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:55.533: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:55.533: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:55.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 30 03:59:55.536: INFO: Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 03:59:56.533: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:56.533: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:56.533: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 03:59:56.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Nov 30 03:59:56.536: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 11/30/22 03:59:56.539
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-151, will wait for the garbage collector to delete the pods 11/30/22 03:59:56.539
    Nov 30 03:59:56.602: INFO: Deleting DaemonSet.extensions daemon-set took: 9.764351ms
    Nov 30 03:59:56.703: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.94748ms
    Nov 30 03:59:59.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 03:59:59.408: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 30 03:59:59.411: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34755"},"items":null}

    Nov 30 03:59:59.413: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34755"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 03:59:59.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-151" for this suite. 11/30/22 03:59:59.439
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 03:59:59.446
Nov 30 03:59:59.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 03:59:59.447
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:59.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:59.47
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 03:59:59.487
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 03:59:59.997
STEP: Deploying the webhook pod 11/30/22 04:00:00.004
STEP: Wait for the deployment to be ready 11/30/22 04:00:00.015
Nov 30 04:00:00.021: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 04:00:02.033
STEP: Verifying the service has paired with the endpoint 11/30/22 04:00:02.049
Nov 30 04:00:03.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Nov 30 04:00:03.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-418-crds.webhook.example.com via the AdmissionRegistration API 11/30/22 04:00:03.568
STEP: Creating a custom resource that should be mutated by the webhook 11/30/22 04:00:03.581
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:00:06.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4223" for this suite. 11/30/22 04:00:06.205
STEP: Destroying namespace "webhook-4223-markers" for this suite. 11/30/22 04:00:06.215
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":133,"skipped":2359,"failed":0}
------------------------------
• [SLOW TEST] [6.879 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 03:59:59.446
    Nov 30 03:59:59.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 03:59:59.447
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 03:59:59.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 03:59:59.47
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 03:59:59.487
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 03:59:59.997
    STEP: Deploying the webhook pod 11/30/22 04:00:00.004
    STEP: Wait for the deployment to be ready 11/30/22 04:00:00.015
    Nov 30 04:00:00.021: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 04:00:02.033
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:00:02.049
    Nov 30 04:00:03.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Nov 30 04:00:03.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-418-crds.webhook.example.com via the AdmissionRegistration API 11/30/22 04:00:03.568
    STEP: Creating a custom resource that should be mutated by the webhook 11/30/22 04:00:03.581
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:00:06.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4223" for this suite. 11/30/22 04:00:06.205
    STEP: Destroying namespace "webhook-4223-markers" for this suite. 11/30/22 04:00:06.215
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:00:06.326
Nov 30 04:00:06.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pod-network-test 11/30/22 04:00:06.327
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:00:06.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:00:06.372
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2630 11/30/22 04:00:06.374
STEP: creating a selector 11/30/22 04:00:06.374
STEP: Creating the service pods in kubernetes 11/30/22 04:00:06.374
Nov 30 04:00:06.374: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 30 04:00:06.465: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2630" to be "running and ready"
Nov 30 04:00:06.481: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.029598ms
Nov 30 04:00:06.481: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:00:08.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.020549642s
Nov 30 04:00:08.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:00:10.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019761487s
Nov 30 04:00:10.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:00:12.484: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01915111s
Nov 30 04:00:12.484: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:00:14.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020066353s
Nov 30 04:00:14.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:00:16.484: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.019281308s
Nov 30 04:00:16.484: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:00:18.486: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.02103192s
Nov 30 04:00:18.486: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:00:20.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.019970639s
Nov 30 04:00:20.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:00:22.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01960788s
Nov 30 04:00:22.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:00:24.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.019659322s
Nov 30 04:00:24.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:00:26.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019886386s
Nov 30 04:00:26.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:00:28.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.020349102s
Nov 30 04:00:28.485: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 30 04:00:28.485: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 30 04:00:28.488: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2630" to be "running and ready"
Nov 30 04:00:28.491: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.937173ms
Nov 30 04:00:28.491: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 30 04:00:28.491: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 30 04:00:28.494: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2630" to be "running and ready"
Nov 30 04:00:28.497: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.522424ms
Nov 30 04:00:28.497: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 30 04:00:28.497: INFO: Pod "netserver-2" satisfied condition "running and ready"
Nov 30 04:00:28.502: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2630" to be "running and ready"
Nov 30 04:00:28.505: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.738983ms
Nov 30 04:00:28.505: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Nov 30 04:00:28.505: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 11/30/22 04:00:28.507
Nov 30 04:00:28.535: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2630" to be "running"
Nov 30 04:00:28.540: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.689522ms
Nov 30 04:00:30.544: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009027343s
Nov 30 04:00:30.544: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 30 04:00:30.546: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Nov 30 04:00:30.546: INFO: Breadth first check of 192.168.12.232 on host 10.0.10.8...
Nov 30 04:00:30.549: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.231:9080/dial?request=hostname&protocol=udp&host=192.168.12.232&port=8081&tries=1'] Namespace:pod-network-test-2630 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:00:30.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:00:30.549: INFO: ExecWithOptions: Clientset creation
Nov 30 04:00:30.550: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2630/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.231%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.12.232%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 30 04:00:30.625: INFO: Waiting for responses: map[]
Nov 30 04:00:30.625: INFO: reached 192.168.12.232 after 0/1 tries
Nov 30 04:00:30.625: INFO: Breadth first check of 192.168.228.187 on host 10.0.10.26...
Nov 30 04:00:30.628: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.231:9080/dial?request=hostname&protocol=udp&host=192.168.228.187&port=8081&tries=1'] Namespace:pod-network-test-2630 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:00:30.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:00:30.629: INFO: ExecWithOptions: Clientset creation
Nov 30 04:00:30.629: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2630/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.231%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.228.187%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 30 04:00:30.692: INFO: Waiting for responses: map[]
Nov 30 04:00:30.692: INFO: reached 192.168.228.187 after 0/1 tries
Nov 30 04:00:30.692: INFO: Breadth first check of 192.168.251.187 on host 10.0.10.17...
Nov 30 04:00:30.695: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.231:9080/dial?request=hostname&protocol=udp&host=192.168.251.187&port=8081&tries=1'] Namespace:pod-network-test-2630 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:00:30.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:00:30.696: INFO: ExecWithOptions: Clientset creation
Nov 30 04:00:30.696: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2630/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.231%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.251.187%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 30 04:00:30.770: INFO: Waiting for responses: map[]
Nov 30 04:00:30.770: INFO: reached 192.168.251.187 after 0/1 tries
Nov 30 04:00:30.770: INFO: Breadth first check of 192.168.254.53 on host 10.0.10.2...
Nov 30 04:00:30.773: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.231:9080/dial?request=hostname&protocol=udp&host=192.168.254.53&port=8081&tries=1'] Namespace:pod-network-test-2630 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:00:30.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:00:30.773: INFO: ExecWithOptions: Clientset creation
Nov 30 04:00:30.773: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2630/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.231%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.254.53%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 30 04:00:30.853: INFO: Waiting for responses: map[]
Nov 30 04:00:30.853: INFO: reached 192.168.254.53 after 0/1 tries
Nov 30 04:00:30.853: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Nov 30 04:00:30.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2630" for this suite. 11/30/22 04:00:30.858
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":134,"skipped":2365,"failed":0}
------------------------------
• [SLOW TEST] [24.539 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:00:06.326
    Nov 30 04:00:06.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pod-network-test 11/30/22 04:00:06.327
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:00:06.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:00:06.372
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2630 11/30/22 04:00:06.374
    STEP: creating a selector 11/30/22 04:00:06.374
    STEP: Creating the service pods in kubernetes 11/30/22 04:00:06.374
    Nov 30 04:00:06.374: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 30 04:00:06.465: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2630" to be "running and ready"
    Nov 30 04:00:06.481: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.029598ms
    Nov 30 04:00:06.481: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:00:08.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.020549642s
    Nov 30 04:00:08.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:00:10.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019761487s
    Nov 30 04:00:10.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:00:12.484: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01915111s
    Nov 30 04:00:12.484: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:00:14.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020066353s
    Nov 30 04:00:14.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:00:16.484: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.019281308s
    Nov 30 04:00:16.484: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:00:18.486: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.02103192s
    Nov 30 04:00:18.486: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:00:20.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.019970639s
    Nov 30 04:00:20.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:00:22.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01960788s
    Nov 30 04:00:22.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:00:24.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.019659322s
    Nov 30 04:00:24.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:00:26.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019886386s
    Nov 30 04:00:26.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:00:28.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.020349102s
    Nov 30 04:00:28.485: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 30 04:00:28.485: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 30 04:00:28.488: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2630" to be "running and ready"
    Nov 30 04:00:28.491: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.937173ms
    Nov 30 04:00:28.491: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 30 04:00:28.491: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 30 04:00:28.494: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2630" to be "running and ready"
    Nov 30 04:00:28.497: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.522424ms
    Nov 30 04:00:28.497: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 30 04:00:28.497: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Nov 30 04:00:28.502: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2630" to be "running and ready"
    Nov 30 04:00:28.505: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.738983ms
    Nov 30 04:00:28.505: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Nov 30 04:00:28.505: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 11/30/22 04:00:28.507
    Nov 30 04:00:28.535: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2630" to be "running"
    Nov 30 04:00:28.540: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.689522ms
    Nov 30 04:00:30.544: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009027343s
    Nov 30 04:00:30.544: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 30 04:00:30.546: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Nov 30 04:00:30.546: INFO: Breadth first check of 192.168.12.232 on host 10.0.10.8...
    Nov 30 04:00:30.549: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.231:9080/dial?request=hostname&protocol=udp&host=192.168.12.232&port=8081&tries=1'] Namespace:pod-network-test-2630 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:00:30.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:00:30.549: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:00:30.550: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2630/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.231%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.12.232%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 30 04:00:30.625: INFO: Waiting for responses: map[]
    Nov 30 04:00:30.625: INFO: reached 192.168.12.232 after 0/1 tries
    Nov 30 04:00:30.625: INFO: Breadth first check of 192.168.228.187 on host 10.0.10.26...
    Nov 30 04:00:30.628: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.231:9080/dial?request=hostname&protocol=udp&host=192.168.228.187&port=8081&tries=1'] Namespace:pod-network-test-2630 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:00:30.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:00:30.629: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:00:30.629: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2630/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.231%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.228.187%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 30 04:00:30.692: INFO: Waiting for responses: map[]
    Nov 30 04:00:30.692: INFO: reached 192.168.228.187 after 0/1 tries
    Nov 30 04:00:30.692: INFO: Breadth first check of 192.168.251.187 on host 10.0.10.17...
    Nov 30 04:00:30.695: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.231:9080/dial?request=hostname&protocol=udp&host=192.168.251.187&port=8081&tries=1'] Namespace:pod-network-test-2630 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:00:30.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:00:30.696: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:00:30.696: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2630/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.231%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.251.187%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 30 04:00:30.770: INFO: Waiting for responses: map[]
    Nov 30 04:00:30.770: INFO: reached 192.168.251.187 after 0/1 tries
    Nov 30 04:00:30.770: INFO: Breadth first check of 192.168.254.53 on host 10.0.10.2...
    Nov 30 04:00:30.773: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.231:9080/dial?request=hostname&protocol=udp&host=192.168.254.53&port=8081&tries=1'] Namespace:pod-network-test-2630 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:00:30.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:00:30.773: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:00:30.773: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2630/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.231%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.254.53%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 30 04:00:30.853: INFO: Waiting for responses: map[]
    Nov 30 04:00:30.853: INFO: reached 192.168.254.53 after 0/1 tries
    Nov 30 04:00:30.853: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Nov 30 04:00:30.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2630" for this suite. 11/30/22 04:00:30.858
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:00:30.864
Nov 30 04:00:30.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename daemonsets 11/30/22 04:00:30.865
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:00:30.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:00:30.901
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 11/30/22 04:00:30.928
STEP: Check that daemon pods launch on every node of the cluster. 11/30/22 04:00:30.936
Nov 30 04:00:30.944: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:30.944: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:30.944: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:30.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:00:30.948: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:00:31.953: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:31.953: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:31.953: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:31.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:00:31.957: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:00:32.955: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:32.955: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:32.955: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:32.958: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Nov 30 04:00:32.958: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 11/30/22 04:00:32.961
Nov 30 04:00:32.983: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:32.983: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:32.983: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:32.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 30 04:00:32.986: INFO: Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:00:33.991: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:33.991: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:33.991: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:33.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 30 04:00:33.995: INFO: Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:00:34.991: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:34.992: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:34.992: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:00:34.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Nov 30 04:00:34.995: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 11/30/22 04:00:34.995
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 11/30/22 04:00:35
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3339, will wait for the garbage collector to delete the pods 11/30/22 04:00:35
Nov 30 04:00:35.061: INFO: Deleting DaemonSet.extensions daemon-set took: 7.776478ms
Nov 30 04:00:35.161: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.174294ms
Nov 30 04:00:37.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:00:37.765: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 30 04:00:37.767: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35331"},"items":null}

Nov 30 04:00:37.770: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35331"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Nov 30 04:00:37.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3339" for this suite. 11/30/22 04:00:37.795
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":135,"skipped":2366,"failed":0}
------------------------------
• [SLOW TEST] [6.937 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:00:30.864
    Nov 30 04:00:30.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename daemonsets 11/30/22 04:00:30.865
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:00:30.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:00:30.901
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 11/30/22 04:00:30.928
    STEP: Check that daemon pods launch on every node of the cluster. 11/30/22 04:00:30.936
    Nov 30 04:00:30.944: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:30.944: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:30.944: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:30.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:00:30.948: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:00:31.953: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:31.953: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:31.953: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:31.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:00:31.957: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:00:32.955: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:32.955: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:32.955: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:32.958: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Nov 30 04:00:32.958: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 11/30/22 04:00:32.961
    Nov 30 04:00:32.983: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:32.983: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:32.983: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:32.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 30 04:00:32.986: INFO: Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:00:33.991: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:33.991: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:33.991: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:33.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 30 04:00:33.995: INFO: Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:00:34.991: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:34.992: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:34.992: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:00:34.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Nov 30 04:00:34.995: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 11/30/22 04:00:34.995
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 11/30/22 04:00:35
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3339, will wait for the garbage collector to delete the pods 11/30/22 04:00:35
    Nov 30 04:00:35.061: INFO: Deleting DaemonSet.extensions daemon-set took: 7.776478ms
    Nov 30 04:00:35.161: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.174294ms
    Nov 30 04:00:37.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:00:37.765: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 30 04:00:37.767: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35331"},"items":null}

    Nov 30 04:00:37.770: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35331"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 04:00:37.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3339" for this suite. 11/30/22 04:00:37.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:00:37.802
Nov 30 04:00:37.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename resourcequota 11/30/22 04:00:37.803
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:00:37.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:00:37.828
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 11/30/22 04:00:54.834
STEP: Creating a ResourceQuota 11/30/22 04:00:59.838
STEP: Ensuring resource quota status is calculated 11/30/22 04:00:59.843
STEP: Creating a ConfigMap 11/30/22 04:01:01.848
STEP: Ensuring resource quota status captures configMap creation 11/30/22 04:01:01.858
STEP: Deleting a ConfigMap 11/30/22 04:01:03.862
STEP: Ensuring resource quota status released usage 11/30/22 04:01:03.868
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Nov 30 04:01:05.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9419" for this suite. 11/30/22 04:01:05.877
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":136,"skipped":2371,"failed":0}
------------------------------
• [SLOW TEST] [28.080 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:00:37.802
    Nov 30 04:00:37.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename resourcequota 11/30/22 04:00:37.803
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:00:37.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:00:37.828
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 11/30/22 04:00:54.834
    STEP: Creating a ResourceQuota 11/30/22 04:00:59.838
    STEP: Ensuring resource quota status is calculated 11/30/22 04:00:59.843
    STEP: Creating a ConfigMap 11/30/22 04:01:01.848
    STEP: Ensuring resource quota status captures configMap creation 11/30/22 04:01:01.858
    STEP: Deleting a ConfigMap 11/30/22 04:01:03.862
    STEP: Ensuring resource quota status released usage 11/30/22 04:01:03.868
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Nov 30 04:01:05.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9419" for this suite. 11/30/22 04:01:05.877
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:05.882
Nov 30 04:01:05.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename job 11/30/22 04:01:05.883
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:05.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:05.9
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 11/30/22 04:01:05.902
STEP: Ensure pods equal to paralellism count is attached to the job 11/30/22 04:01:05.908
STEP: patching /status 11/30/22 04:01:07.912
STEP: updating /status 11/30/22 04:01:07.928
STEP: get /status 11/30/22 04:01:07.966
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Nov 30 04:01:07.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-403" for this suite. 11/30/22 04:01:07.973
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":137,"skipped":2373,"failed":0}
------------------------------
• [2.096 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:05.882
    Nov 30 04:01:05.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename job 11/30/22 04:01:05.883
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:05.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:05.9
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 11/30/22 04:01:05.902
    STEP: Ensure pods equal to paralellism count is attached to the job 11/30/22 04:01:05.908
    STEP: patching /status 11/30/22 04:01:07.912
    STEP: updating /status 11/30/22 04:01:07.928
    STEP: get /status 11/30/22 04:01:07.966
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Nov 30 04:01:07.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-403" for this suite. 11/30/22 04:01:07.973
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:07.979
Nov 30 04:01:07.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename discovery 11/30/22 04:01:07.979
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:07.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:08.001
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 11/30/22 04:01:08.005
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Nov 30 04:01:08.235: INFO: Checking APIGroup: apiregistration.k8s.io
Nov 30 04:01:08.236: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Nov 30 04:01:08.236: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Nov 30 04:01:08.236: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Nov 30 04:01:08.236: INFO: Checking APIGroup: apps
Nov 30 04:01:08.237: INFO: PreferredVersion.GroupVersion: apps/v1
Nov 30 04:01:08.237: INFO: Versions found [{apps/v1 v1}]
Nov 30 04:01:08.237: INFO: apps/v1 matches apps/v1
Nov 30 04:01:08.237: INFO: Checking APIGroup: events.k8s.io
Nov 30 04:01:08.238: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Nov 30 04:01:08.238: INFO: Versions found [{events.k8s.io/v1 v1}]
Nov 30 04:01:08.238: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Nov 30 04:01:08.238: INFO: Checking APIGroup: authentication.k8s.io
Nov 30 04:01:08.239: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Nov 30 04:01:08.239: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Nov 30 04:01:08.239: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Nov 30 04:01:08.239: INFO: Checking APIGroup: authorization.k8s.io
Nov 30 04:01:08.240: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Nov 30 04:01:08.240: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Nov 30 04:01:08.240: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Nov 30 04:01:08.240: INFO: Checking APIGroup: autoscaling
Nov 30 04:01:08.241: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Nov 30 04:01:08.241: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Nov 30 04:01:08.241: INFO: autoscaling/v2 matches autoscaling/v2
Nov 30 04:01:08.241: INFO: Checking APIGroup: batch
Nov 30 04:01:08.241: INFO: PreferredVersion.GroupVersion: batch/v1
Nov 30 04:01:08.241: INFO: Versions found [{batch/v1 v1}]
Nov 30 04:01:08.241: INFO: batch/v1 matches batch/v1
Nov 30 04:01:08.241: INFO: Checking APIGroup: certificates.k8s.io
Nov 30 04:01:08.242: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Nov 30 04:01:08.242: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Nov 30 04:01:08.242: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Nov 30 04:01:08.242: INFO: Checking APIGroup: networking.k8s.io
Nov 30 04:01:08.243: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Nov 30 04:01:08.243: INFO: Versions found [{networking.k8s.io/v1 v1}]
Nov 30 04:01:08.243: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Nov 30 04:01:08.243: INFO: Checking APIGroup: policy
Nov 30 04:01:08.244: INFO: PreferredVersion.GroupVersion: policy/v1
Nov 30 04:01:08.244: INFO: Versions found [{policy/v1 v1}]
Nov 30 04:01:08.244: INFO: policy/v1 matches policy/v1
Nov 30 04:01:08.244: INFO: Checking APIGroup: rbac.authorization.k8s.io
Nov 30 04:01:08.245: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Nov 30 04:01:08.245: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Nov 30 04:01:08.245: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Nov 30 04:01:08.245: INFO: Checking APIGroup: storage.k8s.io
Nov 30 04:01:08.245: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Nov 30 04:01:08.245: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Nov 30 04:01:08.245: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Nov 30 04:01:08.245: INFO: Checking APIGroup: admissionregistration.k8s.io
Nov 30 04:01:08.246: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Nov 30 04:01:08.246: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Nov 30 04:01:08.246: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Nov 30 04:01:08.246: INFO: Checking APIGroup: apiextensions.k8s.io
Nov 30 04:01:08.247: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Nov 30 04:01:08.247: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Nov 30 04:01:08.247: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Nov 30 04:01:08.247: INFO: Checking APIGroup: scheduling.k8s.io
Nov 30 04:01:08.248: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Nov 30 04:01:08.248: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Nov 30 04:01:08.248: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Nov 30 04:01:08.248: INFO: Checking APIGroup: coordination.k8s.io
Nov 30 04:01:08.249: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Nov 30 04:01:08.249: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Nov 30 04:01:08.249: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Nov 30 04:01:08.249: INFO: Checking APIGroup: node.k8s.io
Nov 30 04:01:08.250: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Nov 30 04:01:08.250: INFO: Versions found [{node.k8s.io/v1 v1}]
Nov 30 04:01:08.250: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Nov 30 04:01:08.250: INFO: Checking APIGroup: discovery.k8s.io
Nov 30 04:01:08.250: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Nov 30 04:01:08.250: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Nov 30 04:01:08.250: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Nov 30 04:01:08.250: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Nov 30 04:01:08.251: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Nov 30 04:01:08.251: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Nov 30 04:01:08.251: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Nov 30 04:01:08.251: INFO: Checking APIGroup: crd.projectcalico.org
Nov 30 04:01:08.252: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Nov 30 04:01:08.252: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Nov 30 04:01:08.252: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Nov 30 04:01:08.252: INFO: Checking APIGroup: k8s.cni.cncf.io
Nov 30 04:01:08.253: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Nov 30 04:01:08.253: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Nov 30 04:01:08.253: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Nov 30 04:01:08.253: INFO: Checking APIGroup: metrics.k8s.io
Nov 30 04:01:08.254: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Nov 30 04:01:08.254: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Nov 30 04:01:08.254: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Nov 30 04:01:08.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2307" for this suite. 11/30/22 04:01:08.257
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":138,"skipped":2377,"failed":0}
------------------------------
• [0.284 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:07.979
    Nov 30 04:01:07.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename discovery 11/30/22 04:01:07.979
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:07.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:08.001
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 11/30/22 04:01:08.005
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Nov 30 04:01:08.235: INFO: Checking APIGroup: apiregistration.k8s.io
    Nov 30 04:01:08.236: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Nov 30 04:01:08.236: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Nov 30 04:01:08.236: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Nov 30 04:01:08.236: INFO: Checking APIGroup: apps
    Nov 30 04:01:08.237: INFO: PreferredVersion.GroupVersion: apps/v1
    Nov 30 04:01:08.237: INFO: Versions found [{apps/v1 v1}]
    Nov 30 04:01:08.237: INFO: apps/v1 matches apps/v1
    Nov 30 04:01:08.237: INFO: Checking APIGroup: events.k8s.io
    Nov 30 04:01:08.238: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Nov 30 04:01:08.238: INFO: Versions found [{events.k8s.io/v1 v1}]
    Nov 30 04:01:08.238: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Nov 30 04:01:08.238: INFO: Checking APIGroup: authentication.k8s.io
    Nov 30 04:01:08.239: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Nov 30 04:01:08.239: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Nov 30 04:01:08.239: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Nov 30 04:01:08.239: INFO: Checking APIGroup: authorization.k8s.io
    Nov 30 04:01:08.240: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Nov 30 04:01:08.240: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Nov 30 04:01:08.240: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Nov 30 04:01:08.240: INFO: Checking APIGroup: autoscaling
    Nov 30 04:01:08.241: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Nov 30 04:01:08.241: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Nov 30 04:01:08.241: INFO: autoscaling/v2 matches autoscaling/v2
    Nov 30 04:01:08.241: INFO: Checking APIGroup: batch
    Nov 30 04:01:08.241: INFO: PreferredVersion.GroupVersion: batch/v1
    Nov 30 04:01:08.241: INFO: Versions found [{batch/v1 v1}]
    Nov 30 04:01:08.241: INFO: batch/v1 matches batch/v1
    Nov 30 04:01:08.241: INFO: Checking APIGroup: certificates.k8s.io
    Nov 30 04:01:08.242: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Nov 30 04:01:08.242: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Nov 30 04:01:08.242: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Nov 30 04:01:08.242: INFO: Checking APIGroup: networking.k8s.io
    Nov 30 04:01:08.243: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Nov 30 04:01:08.243: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Nov 30 04:01:08.243: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Nov 30 04:01:08.243: INFO: Checking APIGroup: policy
    Nov 30 04:01:08.244: INFO: PreferredVersion.GroupVersion: policy/v1
    Nov 30 04:01:08.244: INFO: Versions found [{policy/v1 v1}]
    Nov 30 04:01:08.244: INFO: policy/v1 matches policy/v1
    Nov 30 04:01:08.244: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Nov 30 04:01:08.245: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Nov 30 04:01:08.245: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Nov 30 04:01:08.245: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Nov 30 04:01:08.245: INFO: Checking APIGroup: storage.k8s.io
    Nov 30 04:01:08.245: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Nov 30 04:01:08.245: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Nov 30 04:01:08.245: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Nov 30 04:01:08.245: INFO: Checking APIGroup: admissionregistration.k8s.io
    Nov 30 04:01:08.246: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Nov 30 04:01:08.246: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Nov 30 04:01:08.246: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Nov 30 04:01:08.246: INFO: Checking APIGroup: apiextensions.k8s.io
    Nov 30 04:01:08.247: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Nov 30 04:01:08.247: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Nov 30 04:01:08.247: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Nov 30 04:01:08.247: INFO: Checking APIGroup: scheduling.k8s.io
    Nov 30 04:01:08.248: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Nov 30 04:01:08.248: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Nov 30 04:01:08.248: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Nov 30 04:01:08.248: INFO: Checking APIGroup: coordination.k8s.io
    Nov 30 04:01:08.249: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Nov 30 04:01:08.249: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Nov 30 04:01:08.249: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Nov 30 04:01:08.249: INFO: Checking APIGroup: node.k8s.io
    Nov 30 04:01:08.250: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Nov 30 04:01:08.250: INFO: Versions found [{node.k8s.io/v1 v1}]
    Nov 30 04:01:08.250: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Nov 30 04:01:08.250: INFO: Checking APIGroup: discovery.k8s.io
    Nov 30 04:01:08.250: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Nov 30 04:01:08.250: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Nov 30 04:01:08.250: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Nov 30 04:01:08.250: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Nov 30 04:01:08.251: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Nov 30 04:01:08.251: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Nov 30 04:01:08.251: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Nov 30 04:01:08.251: INFO: Checking APIGroup: crd.projectcalico.org
    Nov 30 04:01:08.252: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Nov 30 04:01:08.252: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Nov 30 04:01:08.252: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Nov 30 04:01:08.252: INFO: Checking APIGroup: k8s.cni.cncf.io
    Nov 30 04:01:08.253: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    Nov 30 04:01:08.253: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    Nov 30 04:01:08.253: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    Nov 30 04:01:08.253: INFO: Checking APIGroup: metrics.k8s.io
    Nov 30 04:01:08.254: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Nov 30 04:01:08.254: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Nov 30 04:01:08.254: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Nov 30 04:01:08.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-2307" for this suite. 11/30/22 04:01:08.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:08.264
Nov 30 04:01:08.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 04:01:08.265
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:08.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:08.293
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-240437d3-9285-4915-937f-1e9730a95c4b 11/30/22 04:01:08.3
STEP: Creating configMap with name cm-test-opt-upd-fd090864-c8cc-421a-ae2b-2c2fa44320d6 11/30/22 04:01:08.309
STEP: Creating the pod 11/30/22 04:01:08.312
Nov 30 04:01:08.343: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce991619-b681-431f-b2fd-14ebcef2b1df" in namespace "configmap-124" to be "running and ready"
Nov 30 04:01:08.348: INFO: Pod "pod-configmaps-ce991619-b681-431f-b2fd-14ebcef2b1df": Phase="Pending", Reason="", readiness=false. Elapsed: 5.040896ms
Nov 30 04:01:08.348: INFO: The phase of Pod pod-configmaps-ce991619-b681-431f-b2fd-14ebcef2b1df is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:01:10.355: INFO: Pod "pod-configmaps-ce991619-b681-431f-b2fd-14ebcef2b1df": Phase="Running", Reason="", readiness=true. Elapsed: 2.011814448s
Nov 30 04:01:10.355: INFO: The phase of Pod pod-configmaps-ce991619-b681-431f-b2fd-14ebcef2b1df is Running (Ready = true)
Nov 30 04:01:10.355: INFO: Pod "pod-configmaps-ce991619-b681-431f-b2fd-14ebcef2b1df" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-240437d3-9285-4915-937f-1e9730a95c4b 11/30/22 04:01:10.377
STEP: Updating configmap cm-test-opt-upd-fd090864-c8cc-421a-ae2b-2c2fa44320d6 11/30/22 04:01:10.389
STEP: Creating configMap with name cm-test-opt-create-ef0da605-2d30-4630-aad3-f805be670310 11/30/22 04:01:10.395
STEP: waiting to observe update in volume 11/30/22 04:01:10.399
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 04:01:14.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-124" for this suite. 11/30/22 04:01:14.432
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":139,"skipped":2407,"failed":0}
------------------------------
• [SLOW TEST] [6.175 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:08.264
    Nov 30 04:01:08.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 04:01:08.265
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:08.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:08.293
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-240437d3-9285-4915-937f-1e9730a95c4b 11/30/22 04:01:08.3
    STEP: Creating configMap with name cm-test-opt-upd-fd090864-c8cc-421a-ae2b-2c2fa44320d6 11/30/22 04:01:08.309
    STEP: Creating the pod 11/30/22 04:01:08.312
    Nov 30 04:01:08.343: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce991619-b681-431f-b2fd-14ebcef2b1df" in namespace "configmap-124" to be "running and ready"
    Nov 30 04:01:08.348: INFO: Pod "pod-configmaps-ce991619-b681-431f-b2fd-14ebcef2b1df": Phase="Pending", Reason="", readiness=false. Elapsed: 5.040896ms
    Nov 30 04:01:08.348: INFO: The phase of Pod pod-configmaps-ce991619-b681-431f-b2fd-14ebcef2b1df is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:01:10.355: INFO: Pod "pod-configmaps-ce991619-b681-431f-b2fd-14ebcef2b1df": Phase="Running", Reason="", readiness=true. Elapsed: 2.011814448s
    Nov 30 04:01:10.355: INFO: The phase of Pod pod-configmaps-ce991619-b681-431f-b2fd-14ebcef2b1df is Running (Ready = true)
    Nov 30 04:01:10.355: INFO: Pod "pod-configmaps-ce991619-b681-431f-b2fd-14ebcef2b1df" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-240437d3-9285-4915-937f-1e9730a95c4b 11/30/22 04:01:10.377
    STEP: Updating configmap cm-test-opt-upd-fd090864-c8cc-421a-ae2b-2c2fa44320d6 11/30/22 04:01:10.389
    STEP: Creating configMap with name cm-test-opt-create-ef0da605-2d30-4630-aad3-f805be670310 11/30/22 04:01:10.395
    STEP: waiting to observe update in volume 11/30/22 04:01:10.399
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 04:01:14.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-124" for this suite. 11/30/22 04:01:14.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:14.439
Nov 30 04:01:14.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 04:01:14.44
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:14.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:14.456
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 11/30/22 04:01:14.459
Nov 30 04:01:14.487: INFO: Waiting up to 5m0s for pod "downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333" in namespace "downward-api-1386" to be "Succeeded or Failed"
Nov 30 04:01:14.489: INFO: Pod "downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.567471ms
Nov 30 04:01:16.493: INFO: Pod "downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006017175s
Nov 30 04:01:18.493: INFO: Pod "downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006324897s
STEP: Saw pod success 11/30/22 04:01:18.493
Nov 30 04:01:18.493: INFO: Pod "downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333" satisfied condition "Succeeded or Failed"
Nov 30 04:01:18.496: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333 container client-container: <nil>
STEP: delete the pod 11/30/22 04:01:18.501
Nov 30 04:01:18.510: INFO: Waiting for pod downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333 to disappear
Nov 30 04:01:18.514: INFO: Pod downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Nov 30 04:01:18.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1386" for this suite. 11/30/22 04:01:18.518
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":140,"skipped":2418,"failed":0}
------------------------------
• [4.084 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:14.439
    Nov 30 04:01:14.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 04:01:14.44
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:14.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:14.456
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 11/30/22 04:01:14.459
    Nov 30 04:01:14.487: INFO: Waiting up to 5m0s for pod "downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333" in namespace "downward-api-1386" to be "Succeeded or Failed"
    Nov 30 04:01:14.489: INFO: Pod "downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.567471ms
    Nov 30 04:01:16.493: INFO: Pod "downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006017175s
    Nov 30 04:01:18.493: INFO: Pod "downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006324897s
    STEP: Saw pod success 11/30/22 04:01:18.493
    Nov 30 04:01:18.493: INFO: Pod "downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333" satisfied condition "Succeeded or Failed"
    Nov 30 04:01:18.496: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333 container client-container: <nil>
    STEP: delete the pod 11/30/22 04:01:18.501
    Nov 30 04:01:18.510: INFO: Waiting for pod downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333 to disappear
    Nov 30 04:01:18.514: INFO: Pod downwardapi-volume-133025ff-fd7f-46db-9bc5-60aa0460a333 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Nov 30 04:01:18.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1386" for this suite. 11/30/22 04:01:18.518
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:18.524
Nov 30 04:01:18.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 04:01:18.525
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:18.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:18.552
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 11/30/22 04:01:18.554
Nov 30 04:01:18.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-5447 create -f -'
Nov 30 04:01:19.244: INFO: stderr: ""
Nov 30 04:01:19.245: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 11/30/22 04:01:19.245
Nov 30 04:01:20.249: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 30 04:01:20.249: INFO: Found 1 / 1
Nov 30 04:01:20.249: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 11/30/22 04:01:20.249
Nov 30 04:01:20.252: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 30 04:01:20.252: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 30 04:01:20.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-5447 patch pod agnhost-primary-5hrdg -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 30 04:01:20.319: INFO: stderr: ""
Nov 30 04:01:20.319: INFO: stdout: "pod/agnhost-primary-5hrdg patched\n"
STEP: checking annotations 11/30/22 04:01:20.319
Nov 30 04:01:20.322: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 30 04:01:20.322: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 04:01:20.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5447" for this suite. 11/30/22 04:01:20.327
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":141,"skipped":2418,"failed":0}
------------------------------
• [1.822 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:18.524
    Nov 30 04:01:18.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 04:01:18.525
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:18.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:18.552
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 11/30/22 04:01:18.554
    Nov 30 04:01:18.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-5447 create -f -'
    Nov 30 04:01:19.244: INFO: stderr: ""
    Nov 30 04:01:19.245: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 11/30/22 04:01:19.245
    Nov 30 04:01:20.249: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 30 04:01:20.249: INFO: Found 1 / 1
    Nov 30 04:01:20.249: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 11/30/22 04:01:20.249
    Nov 30 04:01:20.252: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 30 04:01:20.252: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Nov 30 04:01:20.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-5447 patch pod agnhost-primary-5hrdg -p {"metadata":{"annotations":{"x":"y"}}}'
    Nov 30 04:01:20.319: INFO: stderr: ""
    Nov 30 04:01:20.319: INFO: stdout: "pod/agnhost-primary-5hrdg patched\n"
    STEP: checking annotations 11/30/22 04:01:20.319
    Nov 30 04:01:20.322: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 30 04:01:20.322: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 04:01:20.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5447" for this suite. 11/30/22 04:01:20.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:20.347
Nov 30 04:01:20.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 04:01:20.347
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:20.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:20.37
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 04:01:20.456
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:01:20.794
STEP: Deploying the webhook pod 11/30/22 04:01:20.8
STEP: Wait for the deployment to be ready 11/30/22 04:01:20.81
Nov 30 04:01:20.816: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 04:01:22.827
STEP: Verifying the service has paired with the endpoint 11/30/22 04:01:22.854
Nov 30 04:01:23.855: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Nov 30 04:01:23.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4143-crds.webhook.example.com via the AdmissionRegistration API 11/30/22 04:01:24.372
Nov 30 04:01:24.410: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook 11/30/22 04:01:24.518
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:01:27.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-111" for this suite. 11/30/22 04:01:27.106
STEP: Destroying namespace "webhook-111-markers" for this suite. 11/30/22 04:01:27.113
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":142,"skipped":2441,"failed":0}
------------------------------
• [SLOW TEST] [6.827 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:20.347
    Nov 30 04:01:20.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 04:01:20.347
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:20.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:20.37
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 04:01:20.456
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:01:20.794
    STEP: Deploying the webhook pod 11/30/22 04:01:20.8
    STEP: Wait for the deployment to be ready 11/30/22 04:01:20.81
    Nov 30 04:01:20.816: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 04:01:22.827
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:01:22.854
    Nov 30 04:01:23.855: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Nov 30 04:01:23.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4143-crds.webhook.example.com via the AdmissionRegistration API 11/30/22 04:01:24.372
    Nov 30 04:01:24.410: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a custom resource that should be mutated by the webhook 11/30/22 04:01:24.518
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:01:27.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-111" for this suite. 11/30/22 04:01:27.106
    STEP: Destroying namespace "webhook-111-markers" for this suite. 11/30/22 04:01:27.113
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:27.175
Nov 30 04:01:27.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename containers 11/30/22 04:01:27.176
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:27.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:27.216
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 11/30/22 04:01:27.219
Nov 30 04:01:27.250: INFO: Waiting up to 5m0s for pod "client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4" in namespace "containers-7161" to be "Succeeded or Failed"
Nov 30 04:01:27.259: INFO: Pod "client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.196323ms
Nov 30 04:01:29.262: INFO: Pod "client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011582692s
Nov 30 04:01:31.262: INFO: Pod "client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011599148s
STEP: Saw pod success 11/30/22 04:01:31.262
Nov 30 04:01:31.262: INFO: Pod "client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4" satisfied condition "Succeeded or Failed"
Nov 30 04:01:31.264: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4 container agnhost-container: <nil>
STEP: delete the pod 11/30/22 04:01:31.27
Nov 30 04:01:31.290: INFO: Waiting for pod client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4 to disappear
Nov 30 04:01:31.293: INFO: Pod client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Nov 30 04:01:31.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7161" for this suite. 11/30/22 04:01:31.297
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":143,"skipped":2464,"failed":0}
------------------------------
• [4.128 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:27.175
    Nov 30 04:01:27.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename containers 11/30/22 04:01:27.176
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:27.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:27.216
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 11/30/22 04:01:27.219
    Nov 30 04:01:27.250: INFO: Waiting up to 5m0s for pod "client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4" in namespace "containers-7161" to be "Succeeded or Failed"
    Nov 30 04:01:27.259: INFO: Pod "client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.196323ms
    Nov 30 04:01:29.262: INFO: Pod "client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011582692s
    Nov 30 04:01:31.262: INFO: Pod "client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011599148s
    STEP: Saw pod success 11/30/22 04:01:31.262
    Nov 30 04:01:31.262: INFO: Pod "client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4" satisfied condition "Succeeded or Failed"
    Nov 30 04:01:31.264: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4 container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 04:01:31.27
    Nov 30 04:01:31.290: INFO: Waiting for pod client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4 to disappear
    Nov 30 04:01:31.293: INFO: Pod client-containers-0392dd95-ade6-4306-9a95-3cd133ed48c4 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Nov 30 04:01:31.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7161" for this suite. 11/30/22 04:01:31.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:31.304
Nov 30 04:01:31.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename dns 11/30/22 04:01:31.305
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:31.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:31.324
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 11/30/22 04:01:31.327
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4678.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4678.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 11/30/22 04:01:31.342
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4678.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4678.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 11/30/22 04:01:31.342
STEP: creating a pod to probe DNS 11/30/22 04:01:31.342
STEP: submitting the pod to kubernetes 11/30/22 04:01:31.342
Nov 30 04:01:31.354: INFO: Waiting up to 15m0s for pod "dns-test-bee68c64-3ffc-4f6b-bad4-1ef5341d2eda" in namespace "dns-4678" to be "running"
Nov 30 04:01:31.364: INFO: Pod "dns-test-bee68c64-3ffc-4f6b-bad4-1ef5341d2eda": Phase="Pending", Reason="", readiness=false. Elapsed: 9.062303ms
Nov 30 04:01:33.367: INFO: Pod "dns-test-bee68c64-3ffc-4f6b-bad4-1ef5341d2eda": Phase="Running", Reason="", readiness=true. Elapsed: 2.012730794s
Nov 30 04:01:33.367: INFO: Pod "dns-test-bee68c64-3ffc-4f6b-bad4-1ef5341d2eda" satisfied condition "running"
STEP: retrieving the pod 11/30/22 04:01:33.367
STEP: looking for the results for each expected name from probers 11/30/22 04:01:33.37
Nov 30 04:01:33.382: INFO: DNS probes using dns-4678/dns-test-bee68c64-3ffc-4f6b-bad4-1ef5341d2eda succeeded

STEP: deleting the pod 11/30/22 04:01:33.382
STEP: deleting the test headless service 11/30/22 04:01:33.398
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Nov 30 04:01:33.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4678" for this suite. 11/30/22 04:01:33.435
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":144,"skipped":2496,"failed":0}
------------------------------
• [2.137 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:31.304
    Nov 30 04:01:31.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename dns 11/30/22 04:01:31.305
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:31.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:31.324
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 11/30/22 04:01:31.327
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4678.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4678.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     11/30/22 04:01:31.342
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4678.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4678.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     11/30/22 04:01:31.342
    STEP: creating a pod to probe DNS 11/30/22 04:01:31.342
    STEP: submitting the pod to kubernetes 11/30/22 04:01:31.342
    Nov 30 04:01:31.354: INFO: Waiting up to 15m0s for pod "dns-test-bee68c64-3ffc-4f6b-bad4-1ef5341d2eda" in namespace "dns-4678" to be "running"
    Nov 30 04:01:31.364: INFO: Pod "dns-test-bee68c64-3ffc-4f6b-bad4-1ef5341d2eda": Phase="Pending", Reason="", readiness=false. Elapsed: 9.062303ms
    Nov 30 04:01:33.367: INFO: Pod "dns-test-bee68c64-3ffc-4f6b-bad4-1ef5341d2eda": Phase="Running", Reason="", readiness=true. Elapsed: 2.012730794s
    Nov 30 04:01:33.367: INFO: Pod "dns-test-bee68c64-3ffc-4f6b-bad4-1ef5341d2eda" satisfied condition "running"
    STEP: retrieving the pod 11/30/22 04:01:33.367
    STEP: looking for the results for each expected name from probers 11/30/22 04:01:33.37
    Nov 30 04:01:33.382: INFO: DNS probes using dns-4678/dns-test-bee68c64-3ffc-4f6b-bad4-1ef5341d2eda succeeded

    STEP: deleting the pod 11/30/22 04:01:33.382
    STEP: deleting the test headless service 11/30/22 04:01:33.398
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Nov 30 04:01:33.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4678" for this suite. 11/30/22 04:01:33.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:33.441
Nov 30 04:01:33.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 04:01:33.442
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:33.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:33.459
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 11/30/22 04:01:33.461
Nov 30 04:01:33.473: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8" in namespace "downward-api-452" to be "Succeeded or Failed"
Nov 30 04:01:33.475: INFO: Pod "downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472713ms
Nov 30 04:01:35.479: INFO: Pod "downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005831287s
Nov 30 04:01:37.480: INFO: Pod "downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007094741s
STEP: Saw pod success 11/30/22 04:01:37.48
Nov 30 04:01:37.480: INFO: Pod "downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8" satisfied condition "Succeeded or Failed"
Nov 30 04:01:37.483: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8 container client-container: <nil>
STEP: delete the pod 11/30/22 04:01:37.489
Nov 30 04:01:37.503: INFO: Waiting for pod downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8 to disappear
Nov 30 04:01:37.506: INFO: Pod downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Nov 30 04:01:37.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-452" for this suite. 11/30/22 04:01:37.517
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":145,"skipped":2511,"failed":0}
------------------------------
• [4.082 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:33.441
    Nov 30 04:01:33.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 04:01:33.442
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:33.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:33.459
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 11/30/22 04:01:33.461
    Nov 30 04:01:33.473: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8" in namespace "downward-api-452" to be "Succeeded or Failed"
    Nov 30 04:01:33.475: INFO: Pod "downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472713ms
    Nov 30 04:01:35.479: INFO: Pod "downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005831287s
    Nov 30 04:01:37.480: INFO: Pod "downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007094741s
    STEP: Saw pod success 11/30/22 04:01:37.48
    Nov 30 04:01:37.480: INFO: Pod "downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8" satisfied condition "Succeeded or Failed"
    Nov 30 04:01:37.483: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8 container client-container: <nil>
    STEP: delete the pod 11/30/22 04:01:37.489
    Nov 30 04:01:37.503: INFO: Waiting for pod downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8 to disappear
    Nov 30 04:01:37.506: INFO: Pod downwardapi-volume-36f6b33f-7b87-4143-ac16-d739d7a3dfc8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Nov 30 04:01:37.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-452" for this suite. 11/30/22 04:01:37.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:37.524
Nov 30 04:01:37.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename cronjob 11/30/22 04:01:37.524
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:37.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:37.553
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 11/30/22 04:01:37.555
STEP: creating 11/30/22 04:01:37.555
STEP: getting 11/30/22 04:01:37.562
STEP: listing 11/30/22 04:01:37.567
STEP: watching 11/30/22 04:01:37.569
Nov 30 04:01:37.569: INFO: starting watch
STEP: cluster-wide listing 11/30/22 04:01:37.57
STEP: cluster-wide watching 11/30/22 04:01:37.573
Nov 30 04:01:37.573: INFO: starting watch
STEP: patching 11/30/22 04:01:37.574
STEP: updating 11/30/22 04:01:37.581
Nov 30 04:01:37.590: INFO: waiting for watch events with expected annotations
Nov 30 04:01:37.590: INFO: saw patched and updated annotations
STEP: patching /status 11/30/22 04:01:37.59
STEP: updating /status 11/30/22 04:01:37.596
STEP: get /status 11/30/22 04:01:37.603
STEP: deleting 11/30/22 04:01:37.606
STEP: deleting a collection 11/30/22 04:01:37.619
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Nov 30 04:01:37.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8324" for this suite. 11/30/22 04:01:37.638
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":146,"skipped":2522,"failed":0}
------------------------------
• [0.123 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:37.524
    Nov 30 04:01:37.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename cronjob 11/30/22 04:01:37.524
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:37.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:37.553
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 11/30/22 04:01:37.555
    STEP: creating 11/30/22 04:01:37.555
    STEP: getting 11/30/22 04:01:37.562
    STEP: listing 11/30/22 04:01:37.567
    STEP: watching 11/30/22 04:01:37.569
    Nov 30 04:01:37.569: INFO: starting watch
    STEP: cluster-wide listing 11/30/22 04:01:37.57
    STEP: cluster-wide watching 11/30/22 04:01:37.573
    Nov 30 04:01:37.573: INFO: starting watch
    STEP: patching 11/30/22 04:01:37.574
    STEP: updating 11/30/22 04:01:37.581
    Nov 30 04:01:37.590: INFO: waiting for watch events with expected annotations
    Nov 30 04:01:37.590: INFO: saw patched and updated annotations
    STEP: patching /status 11/30/22 04:01:37.59
    STEP: updating /status 11/30/22 04:01:37.596
    STEP: get /status 11/30/22 04:01:37.603
    STEP: deleting 11/30/22 04:01:37.606
    STEP: deleting a collection 11/30/22 04:01:37.619
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Nov 30 04:01:37.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8324" for this suite. 11/30/22 04:01:37.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:37.647
Nov 30 04:01:37.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 04:01:37.648
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:37.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:37.673
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 11/30/22 04:01:37.676
Nov 30 04:01:37.696: INFO: Waiting up to 5m0s for pod "downward-api-086159c1-2cdc-4394-9b40-7c611996820c" in namespace "downward-api-2596" to be "Succeeded or Failed"
Nov 30 04:01:37.706: INFO: Pod "downward-api-086159c1-2cdc-4394-9b40-7c611996820c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.719115ms
Nov 30 04:01:39.710: INFO: Pod "downward-api-086159c1-2cdc-4394-9b40-7c611996820c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014090435s
Nov 30 04:01:41.710: INFO: Pod "downward-api-086159c1-2cdc-4394-9b40-7c611996820c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0140832s
STEP: Saw pod success 11/30/22 04:01:41.71
Nov 30 04:01:41.710: INFO: Pod "downward-api-086159c1-2cdc-4394-9b40-7c611996820c" satisfied condition "Succeeded or Failed"
Nov 30 04:01:41.712: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downward-api-086159c1-2cdc-4394-9b40-7c611996820c container dapi-container: <nil>
STEP: delete the pod 11/30/22 04:01:41.719
Nov 30 04:01:41.730: INFO: Waiting for pod downward-api-086159c1-2cdc-4394-9b40-7c611996820c to disappear
Nov 30 04:01:41.733: INFO: Pod downward-api-086159c1-2cdc-4394-9b40-7c611996820c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Nov 30 04:01:41.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2596" for this suite. 11/30/22 04:01:41.736
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":147,"skipped":2534,"failed":0}
------------------------------
• [4.096 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:37.647
    Nov 30 04:01:37.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 04:01:37.648
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:37.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:37.673
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 11/30/22 04:01:37.676
    Nov 30 04:01:37.696: INFO: Waiting up to 5m0s for pod "downward-api-086159c1-2cdc-4394-9b40-7c611996820c" in namespace "downward-api-2596" to be "Succeeded or Failed"
    Nov 30 04:01:37.706: INFO: Pod "downward-api-086159c1-2cdc-4394-9b40-7c611996820c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.719115ms
    Nov 30 04:01:39.710: INFO: Pod "downward-api-086159c1-2cdc-4394-9b40-7c611996820c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014090435s
    Nov 30 04:01:41.710: INFO: Pod "downward-api-086159c1-2cdc-4394-9b40-7c611996820c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0140832s
    STEP: Saw pod success 11/30/22 04:01:41.71
    Nov 30 04:01:41.710: INFO: Pod "downward-api-086159c1-2cdc-4394-9b40-7c611996820c" satisfied condition "Succeeded or Failed"
    Nov 30 04:01:41.712: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downward-api-086159c1-2cdc-4394-9b40-7c611996820c container dapi-container: <nil>
    STEP: delete the pod 11/30/22 04:01:41.719
    Nov 30 04:01:41.730: INFO: Waiting for pod downward-api-086159c1-2cdc-4394-9b40-7c611996820c to disappear
    Nov 30 04:01:41.733: INFO: Pod downward-api-086159c1-2cdc-4394-9b40-7c611996820c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Nov 30 04:01:41.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2596" for this suite. 11/30/22 04:01:41.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:41.744
Nov 30 04:01:41.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename dns 11/30/22 04:01:41.745
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:41.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:41.768
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 11/30/22 04:01:41.77
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6041.svc.cluster.local;sleep 1; done
 11/30/22 04:01:41.779
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6041.svc.cluster.local;sleep 1; done
 11/30/22 04:01:41.779
STEP: creating a pod to probe DNS 11/30/22 04:01:41.779
STEP: submitting the pod to kubernetes 11/30/22 04:01:41.779
Nov 30 04:01:41.789: INFO: Waiting up to 15m0s for pod "dns-test-789c9269-780e-4a41-899a-715968d39445" in namespace "dns-6041" to be "running"
Nov 30 04:01:41.796: INFO: Pod "dns-test-789c9269-780e-4a41-899a-715968d39445": Phase="Pending", Reason="", readiness=false. Elapsed: 7.140118ms
Nov 30 04:01:43.800: INFO: Pod "dns-test-789c9269-780e-4a41-899a-715968d39445": Phase="Running", Reason="", readiness=true. Elapsed: 2.011133915s
Nov 30 04:01:43.800: INFO: Pod "dns-test-789c9269-780e-4a41-899a-715968d39445" satisfied condition "running"
STEP: retrieving the pod 11/30/22 04:01:43.8
STEP: looking for the results for each expected name from probers 11/30/22 04:01:43.803
Nov 30 04:01:43.806: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
Nov 30 04:01:43.809: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
Nov 30 04:01:43.812: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
Nov 30 04:01:43.815: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
Nov 30 04:01:43.817: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
Nov 30 04:01:43.820: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
Nov 30 04:01:43.822: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
Nov 30 04:01:43.825: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
Nov 30 04:01:43.825: INFO: Lookups using dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6041.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6041.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local jessie_udp@dns-test-service-2.dns-6041.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6041.svc.cluster.local]

Nov 30 04:01:48.864: INFO: DNS probes using dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445 succeeded

STEP: deleting the pod 11/30/22 04:01:48.864
STEP: deleting the test headless service 11/30/22 04:01:48.874
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Nov 30 04:01:48.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6041" for this suite. 11/30/22 04:01:48.918
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":148,"skipped":2571,"failed":0}
------------------------------
• [SLOW TEST] [7.181 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:41.744
    Nov 30 04:01:41.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename dns 11/30/22 04:01:41.745
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:41.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:41.768
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 11/30/22 04:01:41.77
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6041.svc.cluster.local;sleep 1; done
     11/30/22 04:01:41.779
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6041.svc.cluster.local;sleep 1; done
     11/30/22 04:01:41.779
    STEP: creating a pod to probe DNS 11/30/22 04:01:41.779
    STEP: submitting the pod to kubernetes 11/30/22 04:01:41.779
    Nov 30 04:01:41.789: INFO: Waiting up to 15m0s for pod "dns-test-789c9269-780e-4a41-899a-715968d39445" in namespace "dns-6041" to be "running"
    Nov 30 04:01:41.796: INFO: Pod "dns-test-789c9269-780e-4a41-899a-715968d39445": Phase="Pending", Reason="", readiness=false. Elapsed: 7.140118ms
    Nov 30 04:01:43.800: INFO: Pod "dns-test-789c9269-780e-4a41-899a-715968d39445": Phase="Running", Reason="", readiness=true. Elapsed: 2.011133915s
    Nov 30 04:01:43.800: INFO: Pod "dns-test-789c9269-780e-4a41-899a-715968d39445" satisfied condition "running"
    STEP: retrieving the pod 11/30/22 04:01:43.8
    STEP: looking for the results for each expected name from probers 11/30/22 04:01:43.803
    Nov 30 04:01:43.806: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
    Nov 30 04:01:43.809: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
    Nov 30 04:01:43.812: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
    Nov 30 04:01:43.815: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
    Nov 30 04:01:43.817: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
    Nov 30 04:01:43.820: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
    Nov 30 04:01:43.822: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
    Nov 30 04:01:43.825: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6041.svc.cluster.local from pod dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445: the server could not find the requested resource (get pods dns-test-789c9269-780e-4a41-899a-715968d39445)
    Nov 30 04:01:43.825: INFO: Lookups using dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6041.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6041.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6041.svc.cluster.local jessie_udp@dns-test-service-2.dns-6041.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6041.svc.cluster.local]

    Nov 30 04:01:48.864: INFO: DNS probes using dns-6041/dns-test-789c9269-780e-4a41-899a-715968d39445 succeeded

    STEP: deleting the pod 11/30/22 04:01:48.864
    STEP: deleting the test headless service 11/30/22 04:01:48.874
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Nov 30 04:01:48.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6041" for this suite. 11/30/22 04:01:48.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:48.926
Nov 30 04:01:48.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename custom-resource-definition 11/30/22 04:01:48.927
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:48.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:48.95
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Nov 30 04:01:48.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:01:52.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8828" for this suite. 11/30/22 04:01:52.093
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":149,"skipped":2584,"failed":0}
------------------------------
• [3.189 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:48.926
    Nov 30 04:01:48.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename custom-resource-definition 11/30/22 04:01:48.927
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:48.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:48.95
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Nov 30 04:01:48.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:01:52.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8828" for this suite. 11/30/22 04:01:52.093
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:52.115
Nov 30 04:01:52.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 04:01:52.116
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:52.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:52.141
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-d3c07efb-7df9-4748-a2c2-ccbe06ed7b4b 11/30/22 04:01:52.144
STEP: Creating a pod to test consume configMaps 11/30/22 04:01:52.15
Nov 30 04:01:52.188: INFO: Waiting up to 5m0s for pod "pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c" in namespace "configmap-8832" to be "Succeeded or Failed"
Nov 30 04:01:52.190: INFO: Pod "pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.525143ms
Nov 30 04:01:54.195: INFO: Pod "pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006996349s
Nov 30 04:01:56.194: INFO: Pod "pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005902695s
STEP: Saw pod success 11/30/22 04:01:56.194
Nov 30 04:01:56.194: INFO: Pod "pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c" satisfied condition "Succeeded or Failed"
Nov 30 04:01:56.197: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c container agnhost-container: <nil>
STEP: delete the pod 11/30/22 04:01:56.202
Nov 30 04:01:56.213: INFO: Waiting for pod pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c to disappear
Nov 30 04:01:56.216: INFO: Pod pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 04:01:56.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8832" for this suite. 11/30/22 04:01:56.221
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":150,"skipped":2586,"failed":0}
------------------------------
• [4.113 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:52.115
    Nov 30 04:01:52.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 04:01:52.116
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:52.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:52.141
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-d3c07efb-7df9-4748-a2c2-ccbe06ed7b4b 11/30/22 04:01:52.144
    STEP: Creating a pod to test consume configMaps 11/30/22 04:01:52.15
    Nov 30 04:01:52.188: INFO: Waiting up to 5m0s for pod "pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c" in namespace "configmap-8832" to be "Succeeded or Failed"
    Nov 30 04:01:52.190: INFO: Pod "pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.525143ms
    Nov 30 04:01:54.195: INFO: Pod "pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006996349s
    Nov 30 04:01:56.194: INFO: Pod "pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005902695s
    STEP: Saw pod success 11/30/22 04:01:56.194
    Nov 30 04:01:56.194: INFO: Pod "pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c" satisfied condition "Succeeded or Failed"
    Nov 30 04:01:56.197: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 04:01:56.202
    Nov 30 04:01:56.213: INFO: Waiting for pod pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c to disappear
    Nov 30 04:01:56.216: INFO: Pod pod-configmaps-555714ed-c2cc-4fc1-b711-4edc66e9ca3c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 04:01:56.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8832" for this suite. 11/30/22 04:01:56.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:01:56.229
Nov 30 04:01:56.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename svc-latency 11/30/22 04:01:56.229
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:56.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:56.248
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Nov 30 04:01:56.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: creating replication controller svc-latency-rc in namespace svc-latency-56 11/30/22 04:01:56.251
I1130 04:01:56.260204      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-56, replica count: 1
I1130 04:01:57.311220      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 30 04:01:57.437: INFO: Created: latency-svc-4mvjk
Nov 30 04:01:57.448: INFO: Got endpoints: latency-svc-4mvjk [37.128309ms]
Nov 30 04:01:57.474: INFO: Created: latency-svc-sk6d8
Nov 30 04:01:57.486: INFO: Got endpoints: latency-svc-sk6d8 [37.25479ms]
Nov 30 04:01:57.521: INFO: Created: latency-svc-pksbx
Nov 30 04:01:57.537: INFO: Got endpoints: latency-svc-pksbx [88.282559ms]
Nov 30 04:01:57.538: INFO: Created: latency-svc-swgfp
Nov 30 04:01:57.551: INFO: Got endpoints: latency-svc-swgfp [102.84626ms]
Nov 30 04:01:57.555: INFO: Created: latency-svc-nkldc
Nov 30 04:01:57.567: INFO: Got endpoints: latency-svc-nkldc [118.290273ms]
Nov 30 04:01:57.569: INFO: Created: latency-svc-lwlpf
Nov 30 04:01:57.584: INFO: Got endpoints: latency-svc-lwlpf [135.497426ms]
Nov 30 04:01:57.596: INFO: Created: latency-svc-q48gc
Nov 30 04:01:57.610: INFO: Got endpoints: latency-svc-q48gc [161.549198ms]
Nov 30 04:01:57.618: INFO: Created: latency-svc-bhsp6
Nov 30 04:01:57.650: INFO: Got endpoints: latency-svc-bhsp6 [201.233985ms]
Nov 30 04:01:57.655: INFO: Created: latency-svc-5mffj
Nov 30 04:01:57.665: INFO: Got endpoints: latency-svc-5mffj [216.373008ms]
Nov 30 04:01:57.672: INFO: Created: latency-svc-6rvlt
Nov 30 04:01:57.698: INFO: Got endpoints: latency-svc-6rvlt [249.405383ms]
Nov 30 04:01:57.711: INFO: Created: latency-svc-lwwhm
Nov 30 04:01:57.716: INFO: Got endpoints: latency-svc-lwwhm [267.721007ms]
Nov 30 04:01:57.731: INFO: Created: latency-svc-4gvcs
Nov 30 04:01:57.743: INFO: Created: latency-svc-75dwb
Nov 30 04:01:57.750: INFO: Got endpoints: latency-svc-4gvcs [300.90395ms]
Nov 30 04:01:57.775: INFO: Got endpoints: latency-svc-75dwb [325.990593ms]
Nov 30 04:01:57.780: INFO: Created: latency-svc-mhtc7
Nov 30 04:01:57.792: INFO: Got endpoints: latency-svc-mhtc7 [342.898111ms]
Nov 30 04:01:57.804: INFO: Created: latency-svc-mnpf7
Nov 30 04:01:57.825: INFO: Got endpoints: latency-svc-mnpf7 [375.77802ms]
Nov 30 04:01:57.834: INFO: Created: latency-svc-8s8q2
Nov 30 04:01:57.843: INFO: Created: latency-svc-m9wvm
Nov 30 04:01:57.867: INFO: Created: latency-svc-pzwdh
Nov 30 04:01:57.869: INFO: Got endpoints: latency-svc-m9wvm [383.556559ms]
Nov 30 04:01:57.877: INFO: Got endpoints: latency-svc-pzwdh [340.009848ms]
Nov 30 04:01:57.877: INFO: Got endpoints: latency-svc-8s8q2 [428.319247ms]
Nov 30 04:01:57.895: INFO: Created: latency-svc-jz5qb
Nov 30 04:01:57.907: INFO: Got endpoints: latency-svc-jz5qb [355.36054ms]
Nov 30 04:01:57.917: INFO: Created: latency-svc-f2rg7
Nov 30 04:01:57.930: INFO: Got endpoints: latency-svc-f2rg7 [363.54223ms]
Nov 30 04:01:57.936: INFO: Created: latency-svc-hwvv8
Nov 30 04:01:57.964: INFO: Got endpoints: latency-svc-hwvv8 [379.666714ms]
Nov 30 04:01:57.978: INFO: Created: latency-svc-mvbsb
Nov 30 04:01:57.989: INFO: Got endpoints: latency-svc-mvbsb [379.134973ms]
Nov 30 04:01:57.999: INFO: Created: latency-svc-zrbzm
Nov 30 04:01:58.007: INFO: Got endpoints: latency-svc-zrbzm [357.297914ms]
Nov 30 04:01:58.013: INFO: Created: latency-svc-ggzbf
Nov 30 04:01:58.035: INFO: Got endpoints: latency-svc-ggzbf [369.667196ms]
Nov 30 04:01:58.041: INFO: Created: latency-svc-b2whh
Nov 30 04:01:58.058: INFO: Got endpoints: latency-svc-b2whh [359.55963ms]
Nov 30 04:01:58.065: INFO: Created: latency-svc-p6j5f
Nov 30 04:01:58.078: INFO: Got endpoints: latency-svc-p6j5f [361.791727ms]
Nov 30 04:01:58.084: INFO: Created: latency-svc-zb6qb
Nov 30 04:01:58.097: INFO: Got endpoints: latency-svc-zb6qb [347.32448ms]
Nov 30 04:01:58.107: INFO: Created: latency-svc-pkhhb
Nov 30 04:01:58.118: INFO: Got endpoints: latency-svc-pkhhb [343.337487ms]
Nov 30 04:01:58.128: INFO: Created: latency-svc-2k5sw
Nov 30 04:01:58.134: INFO: Got endpoints: latency-svc-2k5sw [342.125709ms]
Nov 30 04:01:58.142: INFO: Created: latency-svc-jbx2n
Nov 30 04:01:58.163: INFO: Got endpoints: latency-svc-jbx2n [338.67226ms]
Nov 30 04:01:58.181: INFO: Created: latency-svc-pjrvw
Nov 30 04:01:58.185: INFO: Got endpoints: latency-svc-pjrvw [316.130558ms]
Nov 30 04:01:58.195: INFO: Created: latency-svc-q8gsh
Nov 30 04:01:58.228: INFO: Got endpoints: latency-svc-q8gsh [350.967556ms]
Nov 30 04:01:58.237: INFO: Created: latency-svc-sfgc9
Nov 30 04:01:58.262: INFO: Got endpoints: latency-svc-sfgc9 [384.739549ms]
Nov 30 04:01:58.278: INFO: Created: latency-svc-jzdp7
Nov 30 04:01:58.298: INFO: Got endpoints: latency-svc-jzdp7 [391.4588ms]
Nov 30 04:01:58.302: INFO: Created: latency-svc-9t9jn
Nov 30 04:01:58.319: INFO: Got endpoints: latency-svc-9t9jn [388.333576ms]
Nov 30 04:01:58.344: INFO: Created: latency-svc-rrb4q
Nov 30 04:01:58.349: INFO: Created: latency-svc-qnhvd
Nov 30 04:01:58.359: INFO: Got endpoints: latency-svc-rrb4q [395.128787ms]
Nov 30 04:01:58.369: INFO: Got endpoints: latency-svc-qnhvd [379.921147ms]
Nov 30 04:01:58.385: INFO: Created: latency-svc-db2pw
Nov 30 04:01:58.410: INFO: Got endpoints: latency-svc-db2pw [402.844478ms]
Nov 30 04:01:58.427: INFO: Created: latency-svc-stlgw
Nov 30 04:01:58.441: INFO: Got endpoints: latency-svc-stlgw [405.921537ms]
Nov 30 04:01:58.457: INFO: Created: latency-svc-fnfw7
Nov 30 04:01:58.478: INFO: Got endpoints: latency-svc-fnfw7 [420.116244ms]
Nov 30 04:01:58.488: INFO: Created: latency-svc-snncm
Nov 30 04:01:58.513: INFO: Created: latency-svc-kzv7x
Nov 30 04:01:58.516: INFO: Got endpoints: latency-svc-snncm [437.959779ms]
Nov 30 04:01:58.527: INFO: Got endpoints: latency-svc-kzv7x [429.711093ms]
Nov 30 04:01:58.534: INFO: Created: latency-svc-nr29q
Nov 30 04:01:58.547: INFO: Got endpoints: latency-svc-nr29q [428.828143ms]
Nov 30 04:01:58.562: INFO: Created: latency-svc-4g8q2
Nov 30 04:01:58.571: INFO: Got endpoints: latency-svc-4g8q2 [436.820776ms]
Nov 30 04:01:58.804: INFO: Created: latency-svc-mhc5t
Nov 30 04:01:58.807: INFO: Created: latency-svc-t65mq
Nov 30 04:01:58.807: INFO: Created: latency-svc-xf5z6
Nov 30 04:01:58.807: INFO: Created: latency-svc-nnljp
Nov 30 04:01:58.807: INFO: Created: latency-svc-nqzqh
Nov 30 04:01:58.809: INFO: Created: latency-svc-wlxdw
Nov 30 04:01:58.809: INFO: Created: latency-svc-jkdfm
Nov 30 04:01:58.810: INFO: Created: latency-svc-j8qkn
Nov 30 04:01:58.810: INFO: Created: latency-svc-tqjsv
Nov 30 04:01:58.810: INFO: Created: latency-svc-5hlqf
Nov 30 04:01:58.825: INFO: Got endpoints: latency-svc-mhc5t [639.083797ms]
Nov 30 04:01:58.827: INFO: Got endpoints: latency-svc-nqzqh [528.740708ms]
Nov 30 04:01:58.827: INFO: Got endpoints: latency-svc-xf5z6 [349.21333ms]
Nov 30 04:01:58.836: INFO: Created: latency-svc-hm969
Nov 30 04:01:58.836: INFO: Created: latency-svc-4vzpp
Nov 30 04:01:58.837: INFO: Created: latency-svc-qms65
Nov 30 04:01:58.837: INFO: Created: latency-svc-pmhhn
Nov 30 04:01:58.837: INFO: Created: latency-svc-7qhcf
Nov 30 04:01:58.858: INFO: Got endpoints: latency-svc-t65mq [310.65801ms]
Nov 30 04:01:58.858: INFO: Got endpoints: latency-svc-nnljp [488.633854ms]
Nov 30 04:01:58.858: INFO: Got endpoints: latency-svc-wlxdw [287.518292ms]
Nov 30 04:01:58.858: INFO: Got endpoints: latency-svc-hm969 [331.581513ms]
Nov 30 04:01:58.859: INFO: Got endpoints: latency-svc-5hlqf [342.371834ms]
Nov 30 04:01:58.859: INFO: Got endpoints: latency-svc-pmhhn [695.423131ms]
Nov 30 04:01:58.863: INFO: Got endpoints: latency-svc-tqjsv [634.363802ms]
Nov 30 04:01:58.863: INFO: Got endpoints: latency-svc-j8qkn [544.605297ms]
Nov 30 04:01:58.866: INFO: Got endpoints: latency-svc-qms65 [603.647807ms]
Nov 30 04:01:58.866: INFO: Got endpoints: latency-svc-7qhcf [506.535828ms]
Nov 30 04:01:58.873: INFO: Created: latency-svc-h6lj5
Nov 30 04:01:58.873: INFO: Got endpoints: latency-svc-4vzpp [432.701955ms]
Nov 30 04:01:58.901: INFO: Got endpoints: latency-svc-jkdfm [490.530076ms]
Nov 30 04:01:58.902: INFO: Created: latency-svc-ngq9h
Nov 30 04:01:58.925: INFO: Created: latency-svc-pqzf8
Nov 30 04:01:58.935: INFO: Created: latency-svc-446vh
Nov 30 04:01:58.948: INFO: Got endpoints: latency-svc-h6lj5 [123.682427ms]
Nov 30 04:01:58.974: INFO: Created: latency-svc-ww2jn
Nov 30 04:01:58.990: INFO: Created: latency-svc-t7kdj
Nov 30 04:01:59.006: INFO: Got endpoints: latency-svc-ngq9h [178.493393ms]
Nov 30 04:01:59.013: INFO: Created: latency-svc-64v2s
Nov 30 04:01:59.035: INFO: Created: latency-svc-rrphh
Nov 30 04:01:59.051: INFO: Got endpoints: latency-svc-pqzf8 [223.789312ms]
Nov 30 04:01:59.076: INFO: Created: latency-svc-kxz25
Nov 30 04:01:59.083: INFO: Created: latency-svc-t547f
Nov 30 04:01:59.095: INFO: Created: latency-svc-ddhrc
Nov 30 04:01:59.100: INFO: Got endpoints: latency-svc-446vh [242.339074ms]
Nov 30 04:01:59.111: INFO: Created: latency-svc-m99x9
Nov 30 04:01:59.139: INFO: Created: latency-svc-tdhhx
Nov 30 04:01:59.150: INFO: Got endpoints: latency-svc-ww2jn [291.656199ms]
Nov 30 04:01:59.256: INFO: Got endpoints: latency-svc-t7kdj [397.675971ms]
Nov 30 04:01:59.262: INFO: Created: latency-svc-xrjb4
Nov 30 04:01:59.262: INFO: Got endpoints: latency-svc-64v2s [403.160512ms]
Nov 30 04:01:59.284: INFO: Created: latency-svc-ntjfg
Nov 30 04:01:59.289: INFO: Created: latency-svc-k2f95
Nov 30 04:01:59.304: INFO: Got endpoints: latency-svc-rrphh [445.53575ms]
Nov 30 04:01:59.307: INFO: Created: latency-svc-5n59q
Nov 30 04:01:59.321: INFO: Created: latency-svc-4pg24
Nov 30 04:01:59.357: INFO: Got endpoints: latency-svc-kxz25 [497.930709ms]
Nov 30 04:01:59.360: INFO: Created: latency-svc-twfxz
Nov 30 04:01:59.368: INFO: Created: latency-svc-9fdsl
Nov 30 04:01:59.399: INFO: Got endpoints: latency-svc-t547f [536.703273ms]
Nov 30 04:01:59.408: INFO: Created: latency-svc-27dj6
Nov 30 04:01:59.450: INFO: Created: latency-svc-96rcf
Nov 30 04:01:59.455: INFO: Got endpoints: latency-svc-ddhrc [592.10103ms]
Nov 30 04:01:59.475: INFO: Created: latency-svc-vq2n8
Nov 30 04:01:59.489: INFO: Created: latency-svc-975dp
Nov 30 04:01:59.500: INFO: Got endpoints: latency-svc-m99x9 [633.912134ms]
Nov 30 04:01:59.502: INFO: Created: latency-svc-2zm5m
Nov 30 04:01:59.530: INFO: Created: latency-svc-tpbnh
Nov 30 04:01:59.540: INFO: Created: latency-svc-pqfqh
Nov 30 04:01:59.556: INFO: Got endpoints: latency-svc-tdhhx [690.338578ms]
Nov 30 04:01:59.589: INFO: Created: latency-svc-s2vt4
Nov 30 04:01:59.599: INFO: Got endpoints: latency-svc-xrjb4 [725.667014ms]
Nov 30 04:01:59.633: INFO: Created: latency-svc-tcdkf
Nov 30 04:01:59.649: INFO: Got endpoints: latency-svc-ntjfg [748.352954ms]
Nov 30 04:01:59.683: INFO: Created: latency-svc-kqqc2
Nov 30 04:01:59.698: INFO: Got endpoints: latency-svc-k2f95 [749.79168ms]
Nov 30 04:01:59.727: INFO: Created: latency-svc-h2mfk
Nov 30 04:01:59.749: INFO: Got endpoints: latency-svc-5n59q [743.740726ms]
Nov 30 04:01:59.794: INFO: Created: latency-svc-t5pgv
Nov 30 04:01:59.820: INFO: Got endpoints: latency-svc-4pg24 [769.197271ms]
Nov 30 04:01:59.850: INFO: Got endpoints: latency-svc-twfxz [750.221734ms]
Nov 30 04:01:59.861: INFO: Created: latency-svc-wzs46
Nov 30 04:01:59.874: INFO: Created: latency-svc-7wvq8
Nov 30 04:01:59.898: INFO: Got endpoints: latency-svc-9fdsl [748.37524ms]
Nov 30 04:01:59.924: INFO: Created: latency-svc-s9hrb
Nov 30 04:01:59.954: INFO: Got endpoints: latency-svc-27dj6 [698.130616ms]
Nov 30 04:01:59.979: INFO: Created: latency-svc-x7xlk
Nov 30 04:02:00.003: INFO: Got endpoints: latency-svc-96rcf [740.96859ms]
Nov 30 04:02:00.037: INFO: Created: latency-svc-86f4v
Nov 30 04:02:00.047: INFO: Got endpoints: latency-svc-vq2n8 [742.744409ms]
Nov 30 04:02:00.070: INFO: Created: latency-svc-qs9bd
Nov 30 04:02:00.097: INFO: Got endpoints: latency-svc-975dp [740.674142ms]
Nov 30 04:02:00.136: INFO: Created: latency-svc-876xz
Nov 30 04:02:00.148: INFO: Got endpoints: latency-svc-2zm5m [749.180706ms]
Nov 30 04:02:00.170: INFO: Created: latency-svc-tq2n8
Nov 30 04:02:00.199: INFO: Got endpoints: latency-svc-tpbnh [743.998596ms]
Nov 30 04:02:00.223: INFO: Created: latency-svc-twjxb
Nov 30 04:02:00.248: INFO: Got endpoints: latency-svc-pqfqh [748.752868ms]
Nov 30 04:02:00.269: INFO: Created: latency-svc-72m7g
Nov 30 04:02:00.297: INFO: Got endpoints: latency-svc-s2vt4 [740.434897ms]
Nov 30 04:02:00.322: INFO: Created: latency-svc-7cnjz
Nov 30 04:02:00.358: INFO: Got endpoints: latency-svc-tcdkf [758.375856ms]
Nov 30 04:02:00.378: INFO: Created: latency-svc-5b9rl
Nov 30 04:02:00.400: INFO: Got endpoints: latency-svc-kqqc2 [750.370853ms]
Nov 30 04:02:00.432: INFO: Created: latency-svc-vf8qp
Nov 30 04:02:00.443: INFO: Got endpoints: latency-svc-h2mfk [745.276003ms]
Nov 30 04:02:00.467: INFO: Created: latency-svc-wsl66
Nov 30 04:02:00.521: INFO: Got endpoints: latency-svc-t5pgv [771.655471ms]
Nov 30 04:02:00.539: INFO: Created: latency-svc-kqbks
Nov 30 04:02:00.549: INFO: Got endpoints: latency-svc-wzs46 [728.575989ms]
Nov 30 04:02:00.573: INFO: Created: latency-svc-x9k75
Nov 30 04:02:00.597: INFO: Got endpoints: latency-svc-7wvq8 [746.233985ms]
Nov 30 04:02:00.613: INFO: Created: latency-svc-cbv6f
Nov 30 04:02:00.654: INFO: Got endpoints: latency-svc-s9hrb [755.870963ms]
Nov 30 04:02:00.700: INFO: Created: latency-svc-bqwxj
Nov 30 04:02:00.701: INFO: Got endpoints: latency-svc-x7xlk [746.488144ms]
Nov 30 04:02:00.723: INFO: Created: latency-svc-hq22q
Nov 30 04:02:00.747: INFO: Got endpoints: latency-svc-86f4v [744.327293ms]
Nov 30 04:02:00.787: INFO: Created: latency-svc-mglnq
Nov 30 04:02:00.802: INFO: Got endpoints: latency-svc-qs9bd [755.55408ms]
Nov 30 04:02:00.838: INFO: Created: latency-svc-zzw7x
Nov 30 04:02:00.851: INFO: Got endpoints: latency-svc-876xz [753.401568ms]
Nov 30 04:02:00.873: INFO: Created: latency-svc-nvx8k
Nov 30 04:02:00.908: INFO: Got endpoints: latency-svc-tq2n8 [759.958701ms]
Nov 30 04:02:00.929: INFO: Created: latency-svc-jlrk4
Nov 30 04:02:00.950: INFO: Got endpoints: latency-svc-twjxb [750.304258ms]
Nov 30 04:02:00.968: INFO: Created: latency-svc-6wvhb
Nov 30 04:02:00.999: INFO: Got endpoints: latency-svc-72m7g [750.932792ms]
Nov 30 04:02:01.037: INFO: Created: latency-svc-wwmwk
Nov 30 04:02:01.057: INFO: Got endpoints: latency-svc-7cnjz [760.160463ms]
Nov 30 04:02:01.087: INFO: Created: latency-svc-9kh94
Nov 30 04:02:01.102: INFO: Got endpoints: latency-svc-5b9rl [744.211839ms]
Nov 30 04:02:01.142: INFO: Created: latency-svc-v8zxk
Nov 30 04:02:01.150: INFO: Got endpoints: latency-svc-vf8qp [750.155796ms]
Nov 30 04:02:01.176: INFO: Created: latency-svc-qbq6t
Nov 30 04:02:01.207: INFO: Got endpoints: latency-svc-wsl66 [763.352256ms]
Nov 30 04:02:01.240: INFO: Created: latency-svc-7w6ct
Nov 30 04:02:01.252: INFO: Got endpoints: latency-svc-kqbks [731.095966ms]
Nov 30 04:02:01.275: INFO: Created: latency-svc-kj2cn
Nov 30 04:02:01.300: INFO: Got endpoints: latency-svc-x9k75 [751.409471ms]
Nov 30 04:02:01.319: INFO: Created: latency-svc-vqns6
Nov 30 04:02:01.364: INFO: Got endpoints: latency-svc-cbv6f [767.839662ms]
Nov 30 04:02:01.401: INFO: Got endpoints: latency-svc-bqwxj [746.57006ms]
Nov 30 04:02:01.415: INFO: Created: latency-svc-kws87
Nov 30 04:02:01.420: INFO: Created: latency-svc-phjbq
Nov 30 04:02:01.456: INFO: Got endpoints: latency-svc-hq22q [755.196188ms]
Nov 30 04:02:01.495: INFO: Created: latency-svc-vrjft
Nov 30 04:02:01.498: INFO: Got endpoints: latency-svc-mglnq [750.61595ms]
Nov 30 04:02:01.528: INFO: Created: latency-svc-b92vp
Nov 30 04:02:01.560: INFO: Got endpoints: latency-svc-zzw7x [757.419527ms]
Nov 30 04:02:01.594: INFO: Created: latency-svc-njwtv
Nov 30 04:02:01.625: INFO: Got endpoints: latency-svc-nvx8k [773.722188ms]
Nov 30 04:02:01.666: INFO: Created: latency-svc-mg9xh
Nov 30 04:02:01.667: INFO: Got endpoints: latency-svc-jlrk4 [758.930988ms]
Nov 30 04:02:01.708: INFO: Got endpoints: latency-svc-6wvhb [757.654366ms]
Nov 30 04:02:01.708: INFO: Created: latency-svc-kn8w6
Nov 30 04:02:01.726: INFO: Created: latency-svc-k4bpj
Nov 30 04:02:01.746: INFO: Got endpoints: latency-svc-wwmwk [746.607621ms]
Nov 30 04:02:01.763: INFO: Created: latency-svc-wh6lc
Nov 30 04:02:01.800: INFO: Got endpoints: latency-svc-9kh94 [743.490783ms]
Nov 30 04:02:01.819: INFO: Created: latency-svc-xgkxq
Nov 30 04:02:01.848: INFO: Got endpoints: latency-svc-v8zxk [745.70965ms]
Nov 30 04:02:01.868: INFO: Created: latency-svc-2m24x
Nov 30 04:02:01.903: INFO: Got endpoints: latency-svc-qbq6t [753.523012ms]
Nov 30 04:02:01.933: INFO: Created: latency-svc-8ptwd
Nov 30 04:02:01.957: INFO: Got endpoints: latency-svc-7w6ct [750.183369ms]
Nov 30 04:02:02.064: INFO: Got endpoints: latency-svc-vqns6 [763.692967ms]
Nov 30 04:02:02.064: INFO: Got endpoints: latency-svc-kj2cn [811.693677ms]
Nov 30 04:02:02.077: INFO: Created: latency-svc-kx44l
Nov 30 04:02:02.103: INFO: Got endpoints: latency-svc-kws87 [738.989558ms]
Nov 30 04:02:02.106: INFO: Created: latency-svc-dht46
Nov 30 04:02:02.122: INFO: Created: latency-svc-xqck6
Nov 30 04:02:02.152: INFO: Created: latency-svc-vsznz
Nov 30 04:02:02.153: INFO: Got endpoints: latency-svc-phjbq [752.197306ms]
Nov 30 04:02:02.189: INFO: Created: latency-svc-n4th5
Nov 30 04:02:02.198: INFO: Got endpoints: latency-svc-vrjft [742.143089ms]
Nov 30 04:02:02.219: INFO: Created: latency-svc-nx7vh
Nov 30 04:02:02.261: INFO: Got endpoints: latency-svc-b92vp [763.377137ms]
Nov 30 04:02:02.289: INFO: Created: latency-svc-brw8m
Nov 30 04:02:02.311: INFO: Got endpoints: latency-svc-njwtv [751.058445ms]
Nov 30 04:02:02.327: INFO: Created: latency-svc-clzk9
Nov 30 04:02:02.351: INFO: Got endpoints: latency-svc-mg9xh [726.324856ms]
Nov 30 04:02:02.388: INFO: Created: latency-svc-9lr49
Nov 30 04:02:02.411: INFO: Got endpoints: latency-svc-kn8w6 [743.973857ms]
Nov 30 04:02:02.446: INFO: Got endpoints: latency-svc-k4bpj [738.176167ms]
Nov 30 04:02:02.446: INFO: Created: latency-svc-6s2zw
Nov 30 04:02:02.489: INFO: Created: latency-svc-7gt7w
Nov 30 04:02:02.499: INFO: Got endpoints: latency-svc-wh6lc [753.286503ms]
Nov 30 04:02:02.526: INFO: Created: latency-svc-49vw7
Nov 30 04:02:02.550: INFO: Got endpoints: latency-svc-xgkxq [749.571951ms]
Nov 30 04:02:02.572: INFO: Created: latency-svc-2w2l6
Nov 30 04:02:02.608: INFO: Got endpoints: latency-svc-2m24x [760.855247ms]
Nov 30 04:02:02.625: INFO: Created: latency-svc-4msnj
Nov 30 04:02:02.650: INFO: Got endpoints: latency-svc-8ptwd [746.615764ms]
Nov 30 04:02:02.677: INFO: Created: latency-svc-v6dsk
Nov 30 04:02:02.698: INFO: Got endpoints: latency-svc-kx44l [740.796972ms]
Nov 30 04:02:02.729: INFO: Created: latency-svc-qgksz
Nov 30 04:02:02.747: INFO: Got endpoints: latency-svc-dht46 [682.90875ms]
Nov 30 04:02:02.794: INFO: Created: latency-svc-2xzhz
Nov 30 04:02:02.839: INFO: Got endpoints: latency-svc-xqck6 [774.998807ms]
Nov 30 04:02:02.847: INFO: Got endpoints: latency-svc-vsznz [743.542375ms]
Nov 30 04:02:02.890: INFO: Created: latency-svc-qvwj7
Nov 30 04:02:02.903: INFO: Got endpoints: latency-svc-n4th5 [749.906395ms]
Nov 30 04:02:02.910: INFO: Created: latency-svc-4jnks
Nov 30 04:02:02.922: INFO: Created: latency-svc-zkxww
Nov 30 04:02:02.963: INFO: Got endpoints: latency-svc-nx7vh [764.393605ms]
Nov 30 04:02:02.985: INFO: Created: latency-svc-xw99g
Nov 30 04:02:03.005: INFO: Got endpoints: latency-svc-brw8m [744.033913ms]
Nov 30 04:02:03.025: INFO: Created: latency-svc-qqvhm
Nov 30 04:02:03.047: INFO: Got endpoints: latency-svc-clzk9 [736.218757ms]
Nov 30 04:02:03.064: INFO: Created: latency-svc-c77x6
Nov 30 04:02:03.102: INFO: Got endpoints: latency-svc-9lr49 [750.619862ms]
Nov 30 04:02:03.121: INFO: Created: latency-svc-wnl8t
Nov 30 04:02:03.154: INFO: Got endpoints: latency-svc-6s2zw [742.121409ms]
Nov 30 04:02:03.187: INFO: Created: latency-svc-25m24
Nov 30 04:02:03.197: INFO: Got endpoints: latency-svc-7gt7w [751.160541ms]
Nov 30 04:02:03.235: INFO: Created: latency-svc-dnp6r
Nov 30 04:02:03.248: INFO: Got endpoints: latency-svc-49vw7 [748.413296ms]
Nov 30 04:02:03.273: INFO: Created: latency-svc-dq6rw
Nov 30 04:02:03.303: INFO: Got endpoints: latency-svc-2w2l6 [753.324126ms]
Nov 30 04:02:03.328: INFO: Created: latency-svc-vgrmx
Nov 30 04:02:03.361: INFO: Got endpoints: latency-svc-4msnj [752.44373ms]
Nov 30 04:02:03.406: INFO: Got endpoints: latency-svc-v6dsk [755.92981ms]
Nov 30 04:02:03.406: INFO: Created: latency-svc-svdpg
Nov 30 04:02:03.425: INFO: Created: latency-svc-sw8xt
Nov 30 04:02:03.462: INFO: Got endpoints: latency-svc-qgksz [764.229587ms]
Nov 30 04:02:03.482: INFO: Created: latency-svc-64gdv
Nov 30 04:02:03.511: INFO: Got endpoints: latency-svc-2xzhz [764.0972ms]
Nov 30 04:02:03.531: INFO: Created: latency-svc-z8p8f
Nov 30 04:02:03.558: INFO: Got endpoints: latency-svc-qvwj7 [718.980744ms]
Nov 30 04:02:03.591: INFO: Created: latency-svc-xc96q
Nov 30 04:02:03.599: INFO: Got endpoints: latency-svc-4jnks [751.684653ms]
Nov 30 04:02:03.654: INFO: Got endpoints: latency-svc-zkxww [751.673237ms]
Nov 30 04:02:03.659: INFO: Created: latency-svc-hm66g
Nov 30 04:02:03.699: INFO: Got endpoints: latency-svc-xw99g [736.833185ms]
Nov 30 04:02:03.712: INFO: Created: latency-svc-pj2jn
Nov 30 04:02:03.754: INFO: Got endpoints: latency-svc-qqvhm [748.152072ms]
Nov 30 04:02:03.771: INFO: Created: latency-svc-n5k79
Nov 30 04:02:03.806: INFO: Got endpoints: latency-svc-c77x6 [758.920304ms]
Nov 30 04:02:03.807: INFO: Created: latency-svc-h6f4v
Nov 30 04:02:03.835: INFO: Created: latency-svc-vmx8n
Nov 30 04:02:03.853: INFO: Got endpoints: latency-svc-wnl8t [751.619359ms]
Nov 30 04:02:03.912: INFO: Got endpoints: latency-svc-25m24 [757.975295ms]
Nov 30 04:02:03.912: INFO: Created: latency-svc-k2lms
Nov 30 04:02:03.932: INFO: Created: latency-svc-gmcbv
Nov 30 04:02:03.947: INFO: Got endpoints: latency-svc-dnp6r [749.58493ms]
Nov 30 04:02:03.974: INFO: Created: latency-svc-7ztd8
Nov 30 04:02:04.005: INFO: Got endpoints: latency-svc-dq6rw [757.245132ms]
Nov 30 04:02:04.033: INFO: Created: latency-svc-jqn59
Nov 30 04:02:04.052: INFO: Got endpoints: latency-svc-vgrmx [748.34465ms]
Nov 30 04:02:04.069: INFO: Created: latency-svc-sqxrm
Nov 30 04:02:04.099: INFO: Got endpoints: latency-svc-svdpg [737.942842ms]
Nov 30 04:02:04.136: INFO: Created: latency-svc-vch5g
Nov 30 04:02:04.150: INFO: Got endpoints: latency-svc-sw8xt [744.280883ms]
Nov 30 04:02:04.192: INFO: Created: latency-svc-g79fm
Nov 30 04:02:04.197: INFO: Got endpoints: latency-svc-64gdv [734.600433ms]
Nov 30 04:02:04.224: INFO: Created: latency-svc-gvztb
Nov 30 04:02:04.265: INFO: Got endpoints: latency-svc-z8p8f [753.584214ms]
Nov 30 04:02:04.305: INFO: Got endpoints: latency-svc-xc96q [746.473677ms]
Nov 30 04:02:04.315: INFO: Created: latency-svc-xg48t
Nov 30 04:02:04.347: INFO: Created: latency-svc-t2d55
Nov 30 04:02:04.348: INFO: Got endpoints: latency-svc-hm66g [748.870449ms]
Nov 30 04:02:04.388: INFO: Created: latency-svc-lklf6
Nov 30 04:02:04.439: INFO: Got endpoints: latency-svc-pj2jn [784.54527ms]
Nov 30 04:02:04.451: INFO: Got endpoints: latency-svc-n5k79 [752.012708ms]
Nov 30 04:02:04.497: INFO: Created: latency-svc-xs555
Nov 30 04:02:04.499: INFO: Got endpoints: latency-svc-h6f4v [744.963844ms]
Nov 30 04:02:04.533: INFO: Created: latency-svc-s4nsg
Nov 30 04:02:04.558: INFO: Got endpoints: latency-svc-vmx8n [751.876439ms]
Nov 30 04:02:04.562: INFO: Created: latency-svc-d5b65
Nov 30 04:02:04.582: INFO: Created: latency-svc-mdsms
Nov 30 04:02:04.606: INFO: Got endpoints: latency-svc-k2lms [752.714495ms]
Nov 30 04:02:04.662: INFO: Created: latency-svc-8r4vn
Nov 30 04:02:04.664: INFO: Got endpoints: latency-svc-gmcbv [752.537255ms]
Nov 30 04:02:04.695: INFO: Created: latency-svc-xwt2j
Nov 30 04:02:04.705: INFO: Got endpoints: latency-svc-7ztd8 [758.028917ms]
Nov 30 04:02:04.723: INFO: Created: latency-svc-k9mbv
Nov 30 04:02:04.747: INFO: Got endpoints: latency-svc-jqn59 [742.139441ms]
Nov 30 04:02:04.782: INFO: Created: latency-svc-qpztt
Nov 30 04:02:04.799: INFO: Got endpoints: latency-svc-sqxrm [747.709388ms]
Nov 30 04:02:04.825: INFO: Created: latency-svc-nfhz5
Nov 30 04:02:04.856: INFO: Got endpoints: latency-svc-vch5g [757.073863ms]
Nov 30 04:02:04.897: INFO: Created: latency-svc-pcbgn
Nov 30 04:02:04.903: INFO: Got endpoints: latency-svc-g79fm [752.377793ms]
Nov 30 04:02:04.947: INFO: Got endpoints: latency-svc-gvztb [750.110567ms]
Nov 30 04:02:04.969: INFO: Created: latency-svc-xcgcl
Nov 30 04:02:04.980: INFO: Created: latency-svc-7svj4
Nov 30 04:02:05.004: INFO: Got endpoints: latency-svc-xg48t [739.777382ms]
Nov 30 04:02:05.055: INFO: Got endpoints: latency-svc-t2d55 [750.415337ms]
Nov 30 04:02:05.111: INFO: Created: latency-svc-s9vf7
Nov 30 04:02:05.111: INFO: Got endpoints: latency-svc-lklf6 [763.771219ms]
Nov 30 04:02:05.194: INFO: Got endpoints: latency-svc-xs555 [754.940309ms]
Nov 30 04:02:05.226: INFO: Got endpoints: latency-svc-s4nsg [774.960272ms]
Nov 30 04:02:05.235: INFO: Created: latency-svc-xx5tq
Nov 30 04:02:05.259: INFO: Got endpoints: latency-svc-d5b65 [760.287243ms]
Nov 30 04:02:05.260: INFO: Created: latency-svc-2mnpj
Nov 30 04:02:05.374: INFO: Got endpoints: latency-svc-8r4vn [767.562522ms]
Nov 30 04:02:05.374: INFO: Got endpoints: latency-svc-mdsms [815.880764ms]
Nov 30 04:02:05.374: INFO: Created: latency-svc-vdv89
Nov 30 04:02:05.421: INFO: Got endpoints: latency-svc-xwt2j [756.458427ms]
Nov 30 04:02:05.463: INFO: Got endpoints: latency-svc-k9mbv [758.59737ms]
Nov 30 04:02:05.476: INFO: Created: latency-svc-5wgxf
Nov 30 04:02:05.476: INFO: Created: latency-svc-8xnmj
Nov 30 04:02:05.503: INFO: Got endpoints: latency-svc-qpztt [755.793013ms]
Nov 30 04:02:05.553: INFO: Got endpoints: latency-svc-nfhz5 [753.879154ms]
Nov 30 04:02:05.649: INFO: Got endpoints: latency-svc-pcbgn [792.828973ms]
Nov 30 04:02:05.700: INFO: Got endpoints: latency-svc-7svj4 [797.705124ms]
Nov 30 04:02:05.712: INFO: Got endpoints: latency-svc-xcgcl [764.731138ms]
Nov 30 04:02:05.769: INFO: Got endpoints: latency-svc-s9vf7 [764.747512ms]
Nov 30 04:02:05.799: INFO: Got endpoints: latency-svc-2mnpj [743.896931ms]
Nov 30 04:02:05.857: INFO: Got endpoints: latency-svc-xx5tq [745.06603ms]
Nov 30 04:02:05.898: INFO: Got endpoints: latency-svc-vdv89 [704.297106ms]
Nov 30 04:02:05.955: INFO: Got endpoints: latency-svc-8xnmj [728.307765ms]
Nov 30 04:02:05.998: INFO: Got endpoints: latency-svc-5wgxf [738.741203ms]
Nov 30 04:02:05.998: INFO: Latencies: [37.25479ms 88.282559ms 102.84626ms 118.290273ms 123.682427ms 135.497426ms 161.549198ms 178.493393ms 201.233985ms 216.373008ms 223.789312ms 242.339074ms 249.405383ms 267.721007ms 287.518292ms 291.656199ms 300.90395ms 310.65801ms 316.130558ms 325.990593ms 331.581513ms 338.67226ms 340.009848ms 342.125709ms 342.371834ms 342.898111ms 343.337487ms 347.32448ms 349.21333ms 350.967556ms 355.36054ms 357.297914ms 359.55963ms 361.791727ms 363.54223ms 369.667196ms 375.77802ms 379.134973ms 379.666714ms 379.921147ms 383.556559ms 384.739549ms 388.333576ms 391.4588ms 395.128787ms 397.675971ms 402.844478ms 403.160512ms 405.921537ms 420.116244ms 428.319247ms 428.828143ms 429.711093ms 432.701955ms 436.820776ms 437.959779ms 445.53575ms 488.633854ms 490.530076ms 497.930709ms 506.535828ms 528.740708ms 536.703273ms 544.605297ms 592.10103ms 603.647807ms 633.912134ms 634.363802ms 639.083797ms 682.90875ms 690.338578ms 695.423131ms 698.130616ms 704.297106ms 718.980744ms 725.667014ms 726.324856ms 728.307765ms 728.575989ms 731.095966ms 734.600433ms 736.218757ms 736.833185ms 737.942842ms 738.176167ms 738.741203ms 738.989558ms 739.777382ms 740.434897ms 740.674142ms 740.796972ms 740.96859ms 742.121409ms 742.139441ms 742.143089ms 742.744409ms 743.490783ms 743.542375ms 743.740726ms 743.896931ms 743.973857ms 743.998596ms 744.033913ms 744.211839ms 744.280883ms 744.327293ms 744.963844ms 745.06603ms 745.276003ms 745.70965ms 746.233985ms 746.473677ms 746.488144ms 746.57006ms 746.607621ms 746.615764ms 747.709388ms 748.152072ms 748.34465ms 748.352954ms 748.37524ms 748.413296ms 748.752868ms 748.870449ms 749.180706ms 749.571951ms 749.58493ms 749.79168ms 749.906395ms 750.110567ms 750.155796ms 750.183369ms 750.221734ms 750.304258ms 750.370853ms 750.415337ms 750.61595ms 750.619862ms 750.932792ms 751.058445ms 751.160541ms 751.409471ms 751.619359ms 751.673237ms 751.684653ms 751.876439ms 752.012708ms 752.197306ms 752.377793ms 752.44373ms 752.537255ms 752.714495ms 753.286503ms 753.324126ms 753.401568ms 753.523012ms 753.584214ms 753.879154ms 754.940309ms 755.196188ms 755.55408ms 755.793013ms 755.870963ms 755.92981ms 756.458427ms 757.073863ms 757.245132ms 757.419527ms 757.654366ms 757.975295ms 758.028917ms 758.375856ms 758.59737ms 758.920304ms 758.930988ms 759.958701ms 760.160463ms 760.287243ms 760.855247ms 763.352256ms 763.377137ms 763.692967ms 763.771219ms 764.0972ms 764.229587ms 764.393605ms 764.731138ms 764.747512ms 767.562522ms 767.839662ms 769.197271ms 771.655471ms 773.722188ms 774.960272ms 774.998807ms 784.54527ms 792.828973ms 797.705124ms 811.693677ms 815.880764ms]
Nov 30 04:02:05.998: INFO: 50 %ile: 743.973857ms
Nov 30 04:02:05.998: INFO: 90 %ile: 763.377137ms
Nov 30 04:02:05.998: INFO: 99 %ile: 811.693677ms
Nov 30 04:02:05.998: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Nov 30 04:02:05.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-56" for this suite. 11/30/22 04:02:06.008
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":151,"skipped":2594,"failed":0}
------------------------------
• [SLOW TEST] [9.787 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:01:56.229
    Nov 30 04:01:56.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename svc-latency 11/30/22 04:01:56.229
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:01:56.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:01:56.248
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Nov 30 04:01:56.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-56 11/30/22 04:01:56.251
    I1130 04:01:56.260204      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-56, replica count: 1
    I1130 04:01:57.311220      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 30 04:01:57.437: INFO: Created: latency-svc-4mvjk
    Nov 30 04:01:57.448: INFO: Got endpoints: latency-svc-4mvjk [37.128309ms]
    Nov 30 04:01:57.474: INFO: Created: latency-svc-sk6d8
    Nov 30 04:01:57.486: INFO: Got endpoints: latency-svc-sk6d8 [37.25479ms]
    Nov 30 04:01:57.521: INFO: Created: latency-svc-pksbx
    Nov 30 04:01:57.537: INFO: Got endpoints: latency-svc-pksbx [88.282559ms]
    Nov 30 04:01:57.538: INFO: Created: latency-svc-swgfp
    Nov 30 04:01:57.551: INFO: Got endpoints: latency-svc-swgfp [102.84626ms]
    Nov 30 04:01:57.555: INFO: Created: latency-svc-nkldc
    Nov 30 04:01:57.567: INFO: Got endpoints: latency-svc-nkldc [118.290273ms]
    Nov 30 04:01:57.569: INFO: Created: latency-svc-lwlpf
    Nov 30 04:01:57.584: INFO: Got endpoints: latency-svc-lwlpf [135.497426ms]
    Nov 30 04:01:57.596: INFO: Created: latency-svc-q48gc
    Nov 30 04:01:57.610: INFO: Got endpoints: latency-svc-q48gc [161.549198ms]
    Nov 30 04:01:57.618: INFO: Created: latency-svc-bhsp6
    Nov 30 04:01:57.650: INFO: Got endpoints: latency-svc-bhsp6 [201.233985ms]
    Nov 30 04:01:57.655: INFO: Created: latency-svc-5mffj
    Nov 30 04:01:57.665: INFO: Got endpoints: latency-svc-5mffj [216.373008ms]
    Nov 30 04:01:57.672: INFO: Created: latency-svc-6rvlt
    Nov 30 04:01:57.698: INFO: Got endpoints: latency-svc-6rvlt [249.405383ms]
    Nov 30 04:01:57.711: INFO: Created: latency-svc-lwwhm
    Nov 30 04:01:57.716: INFO: Got endpoints: latency-svc-lwwhm [267.721007ms]
    Nov 30 04:01:57.731: INFO: Created: latency-svc-4gvcs
    Nov 30 04:01:57.743: INFO: Created: latency-svc-75dwb
    Nov 30 04:01:57.750: INFO: Got endpoints: latency-svc-4gvcs [300.90395ms]
    Nov 30 04:01:57.775: INFO: Got endpoints: latency-svc-75dwb [325.990593ms]
    Nov 30 04:01:57.780: INFO: Created: latency-svc-mhtc7
    Nov 30 04:01:57.792: INFO: Got endpoints: latency-svc-mhtc7 [342.898111ms]
    Nov 30 04:01:57.804: INFO: Created: latency-svc-mnpf7
    Nov 30 04:01:57.825: INFO: Got endpoints: latency-svc-mnpf7 [375.77802ms]
    Nov 30 04:01:57.834: INFO: Created: latency-svc-8s8q2
    Nov 30 04:01:57.843: INFO: Created: latency-svc-m9wvm
    Nov 30 04:01:57.867: INFO: Created: latency-svc-pzwdh
    Nov 30 04:01:57.869: INFO: Got endpoints: latency-svc-m9wvm [383.556559ms]
    Nov 30 04:01:57.877: INFO: Got endpoints: latency-svc-pzwdh [340.009848ms]
    Nov 30 04:01:57.877: INFO: Got endpoints: latency-svc-8s8q2 [428.319247ms]
    Nov 30 04:01:57.895: INFO: Created: latency-svc-jz5qb
    Nov 30 04:01:57.907: INFO: Got endpoints: latency-svc-jz5qb [355.36054ms]
    Nov 30 04:01:57.917: INFO: Created: latency-svc-f2rg7
    Nov 30 04:01:57.930: INFO: Got endpoints: latency-svc-f2rg7 [363.54223ms]
    Nov 30 04:01:57.936: INFO: Created: latency-svc-hwvv8
    Nov 30 04:01:57.964: INFO: Got endpoints: latency-svc-hwvv8 [379.666714ms]
    Nov 30 04:01:57.978: INFO: Created: latency-svc-mvbsb
    Nov 30 04:01:57.989: INFO: Got endpoints: latency-svc-mvbsb [379.134973ms]
    Nov 30 04:01:57.999: INFO: Created: latency-svc-zrbzm
    Nov 30 04:01:58.007: INFO: Got endpoints: latency-svc-zrbzm [357.297914ms]
    Nov 30 04:01:58.013: INFO: Created: latency-svc-ggzbf
    Nov 30 04:01:58.035: INFO: Got endpoints: latency-svc-ggzbf [369.667196ms]
    Nov 30 04:01:58.041: INFO: Created: latency-svc-b2whh
    Nov 30 04:01:58.058: INFO: Got endpoints: latency-svc-b2whh [359.55963ms]
    Nov 30 04:01:58.065: INFO: Created: latency-svc-p6j5f
    Nov 30 04:01:58.078: INFO: Got endpoints: latency-svc-p6j5f [361.791727ms]
    Nov 30 04:01:58.084: INFO: Created: latency-svc-zb6qb
    Nov 30 04:01:58.097: INFO: Got endpoints: latency-svc-zb6qb [347.32448ms]
    Nov 30 04:01:58.107: INFO: Created: latency-svc-pkhhb
    Nov 30 04:01:58.118: INFO: Got endpoints: latency-svc-pkhhb [343.337487ms]
    Nov 30 04:01:58.128: INFO: Created: latency-svc-2k5sw
    Nov 30 04:01:58.134: INFO: Got endpoints: latency-svc-2k5sw [342.125709ms]
    Nov 30 04:01:58.142: INFO: Created: latency-svc-jbx2n
    Nov 30 04:01:58.163: INFO: Got endpoints: latency-svc-jbx2n [338.67226ms]
    Nov 30 04:01:58.181: INFO: Created: latency-svc-pjrvw
    Nov 30 04:01:58.185: INFO: Got endpoints: latency-svc-pjrvw [316.130558ms]
    Nov 30 04:01:58.195: INFO: Created: latency-svc-q8gsh
    Nov 30 04:01:58.228: INFO: Got endpoints: latency-svc-q8gsh [350.967556ms]
    Nov 30 04:01:58.237: INFO: Created: latency-svc-sfgc9
    Nov 30 04:01:58.262: INFO: Got endpoints: latency-svc-sfgc9 [384.739549ms]
    Nov 30 04:01:58.278: INFO: Created: latency-svc-jzdp7
    Nov 30 04:01:58.298: INFO: Got endpoints: latency-svc-jzdp7 [391.4588ms]
    Nov 30 04:01:58.302: INFO: Created: latency-svc-9t9jn
    Nov 30 04:01:58.319: INFO: Got endpoints: latency-svc-9t9jn [388.333576ms]
    Nov 30 04:01:58.344: INFO: Created: latency-svc-rrb4q
    Nov 30 04:01:58.349: INFO: Created: latency-svc-qnhvd
    Nov 30 04:01:58.359: INFO: Got endpoints: latency-svc-rrb4q [395.128787ms]
    Nov 30 04:01:58.369: INFO: Got endpoints: latency-svc-qnhvd [379.921147ms]
    Nov 30 04:01:58.385: INFO: Created: latency-svc-db2pw
    Nov 30 04:01:58.410: INFO: Got endpoints: latency-svc-db2pw [402.844478ms]
    Nov 30 04:01:58.427: INFO: Created: latency-svc-stlgw
    Nov 30 04:01:58.441: INFO: Got endpoints: latency-svc-stlgw [405.921537ms]
    Nov 30 04:01:58.457: INFO: Created: latency-svc-fnfw7
    Nov 30 04:01:58.478: INFO: Got endpoints: latency-svc-fnfw7 [420.116244ms]
    Nov 30 04:01:58.488: INFO: Created: latency-svc-snncm
    Nov 30 04:01:58.513: INFO: Created: latency-svc-kzv7x
    Nov 30 04:01:58.516: INFO: Got endpoints: latency-svc-snncm [437.959779ms]
    Nov 30 04:01:58.527: INFO: Got endpoints: latency-svc-kzv7x [429.711093ms]
    Nov 30 04:01:58.534: INFO: Created: latency-svc-nr29q
    Nov 30 04:01:58.547: INFO: Got endpoints: latency-svc-nr29q [428.828143ms]
    Nov 30 04:01:58.562: INFO: Created: latency-svc-4g8q2
    Nov 30 04:01:58.571: INFO: Got endpoints: latency-svc-4g8q2 [436.820776ms]
    Nov 30 04:01:58.804: INFO: Created: latency-svc-mhc5t
    Nov 30 04:01:58.807: INFO: Created: latency-svc-t65mq
    Nov 30 04:01:58.807: INFO: Created: latency-svc-xf5z6
    Nov 30 04:01:58.807: INFO: Created: latency-svc-nnljp
    Nov 30 04:01:58.807: INFO: Created: latency-svc-nqzqh
    Nov 30 04:01:58.809: INFO: Created: latency-svc-wlxdw
    Nov 30 04:01:58.809: INFO: Created: latency-svc-jkdfm
    Nov 30 04:01:58.810: INFO: Created: latency-svc-j8qkn
    Nov 30 04:01:58.810: INFO: Created: latency-svc-tqjsv
    Nov 30 04:01:58.810: INFO: Created: latency-svc-5hlqf
    Nov 30 04:01:58.825: INFO: Got endpoints: latency-svc-mhc5t [639.083797ms]
    Nov 30 04:01:58.827: INFO: Got endpoints: latency-svc-nqzqh [528.740708ms]
    Nov 30 04:01:58.827: INFO: Got endpoints: latency-svc-xf5z6 [349.21333ms]
    Nov 30 04:01:58.836: INFO: Created: latency-svc-hm969
    Nov 30 04:01:58.836: INFO: Created: latency-svc-4vzpp
    Nov 30 04:01:58.837: INFO: Created: latency-svc-qms65
    Nov 30 04:01:58.837: INFO: Created: latency-svc-pmhhn
    Nov 30 04:01:58.837: INFO: Created: latency-svc-7qhcf
    Nov 30 04:01:58.858: INFO: Got endpoints: latency-svc-t65mq [310.65801ms]
    Nov 30 04:01:58.858: INFO: Got endpoints: latency-svc-nnljp [488.633854ms]
    Nov 30 04:01:58.858: INFO: Got endpoints: latency-svc-wlxdw [287.518292ms]
    Nov 30 04:01:58.858: INFO: Got endpoints: latency-svc-hm969 [331.581513ms]
    Nov 30 04:01:58.859: INFO: Got endpoints: latency-svc-5hlqf [342.371834ms]
    Nov 30 04:01:58.859: INFO: Got endpoints: latency-svc-pmhhn [695.423131ms]
    Nov 30 04:01:58.863: INFO: Got endpoints: latency-svc-tqjsv [634.363802ms]
    Nov 30 04:01:58.863: INFO: Got endpoints: latency-svc-j8qkn [544.605297ms]
    Nov 30 04:01:58.866: INFO: Got endpoints: latency-svc-qms65 [603.647807ms]
    Nov 30 04:01:58.866: INFO: Got endpoints: latency-svc-7qhcf [506.535828ms]
    Nov 30 04:01:58.873: INFO: Created: latency-svc-h6lj5
    Nov 30 04:01:58.873: INFO: Got endpoints: latency-svc-4vzpp [432.701955ms]
    Nov 30 04:01:58.901: INFO: Got endpoints: latency-svc-jkdfm [490.530076ms]
    Nov 30 04:01:58.902: INFO: Created: latency-svc-ngq9h
    Nov 30 04:01:58.925: INFO: Created: latency-svc-pqzf8
    Nov 30 04:01:58.935: INFO: Created: latency-svc-446vh
    Nov 30 04:01:58.948: INFO: Got endpoints: latency-svc-h6lj5 [123.682427ms]
    Nov 30 04:01:58.974: INFO: Created: latency-svc-ww2jn
    Nov 30 04:01:58.990: INFO: Created: latency-svc-t7kdj
    Nov 30 04:01:59.006: INFO: Got endpoints: latency-svc-ngq9h [178.493393ms]
    Nov 30 04:01:59.013: INFO: Created: latency-svc-64v2s
    Nov 30 04:01:59.035: INFO: Created: latency-svc-rrphh
    Nov 30 04:01:59.051: INFO: Got endpoints: latency-svc-pqzf8 [223.789312ms]
    Nov 30 04:01:59.076: INFO: Created: latency-svc-kxz25
    Nov 30 04:01:59.083: INFO: Created: latency-svc-t547f
    Nov 30 04:01:59.095: INFO: Created: latency-svc-ddhrc
    Nov 30 04:01:59.100: INFO: Got endpoints: latency-svc-446vh [242.339074ms]
    Nov 30 04:01:59.111: INFO: Created: latency-svc-m99x9
    Nov 30 04:01:59.139: INFO: Created: latency-svc-tdhhx
    Nov 30 04:01:59.150: INFO: Got endpoints: latency-svc-ww2jn [291.656199ms]
    Nov 30 04:01:59.256: INFO: Got endpoints: latency-svc-t7kdj [397.675971ms]
    Nov 30 04:01:59.262: INFO: Created: latency-svc-xrjb4
    Nov 30 04:01:59.262: INFO: Got endpoints: latency-svc-64v2s [403.160512ms]
    Nov 30 04:01:59.284: INFO: Created: latency-svc-ntjfg
    Nov 30 04:01:59.289: INFO: Created: latency-svc-k2f95
    Nov 30 04:01:59.304: INFO: Got endpoints: latency-svc-rrphh [445.53575ms]
    Nov 30 04:01:59.307: INFO: Created: latency-svc-5n59q
    Nov 30 04:01:59.321: INFO: Created: latency-svc-4pg24
    Nov 30 04:01:59.357: INFO: Got endpoints: latency-svc-kxz25 [497.930709ms]
    Nov 30 04:01:59.360: INFO: Created: latency-svc-twfxz
    Nov 30 04:01:59.368: INFO: Created: latency-svc-9fdsl
    Nov 30 04:01:59.399: INFO: Got endpoints: latency-svc-t547f [536.703273ms]
    Nov 30 04:01:59.408: INFO: Created: latency-svc-27dj6
    Nov 30 04:01:59.450: INFO: Created: latency-svc-96rcf
    Nov 30 04:01:59.455: INFO: Got endpoints: latency-svc-ddhrc [592.10103ms]
    Nov 30 04:01:59.475: INFO: Created: latency-svc-vq2n8
    Nov 30 04:01:59.489: INFO: Created: latency-svc-975dp
    Nov 30 04:01:59.500: INFO: Got endpoints: latency-svc-m99x9 [633.912134ms]
    Nov 30 04:01:59.502: INFO: Created: latency-svc-2zm5m
    Nov 30 04:01:59.530: INFO: Created: latency-svc-tpbnh
    Nov 30 04:01:59.540: INFO: Created: latency-svc-pqfqh
    Nov 30 04:01:59.556: INFO: Got endpoints: latency-svc-tdhhx [690.338578ms]
    Nov 30 04:01:59.589: INFO: Created: latency-svc-s2vt4
    Nov 30 04:01:59.599: INFO: Got endpoints: latency-svc-xrjb4 [725.667014ms]
    Nov 30 04:01:59.633: INFO: Created: latency-svc-tcdkf
    Nov 30 04:01:59.649: INFO: Got endpoints: latency-svc-ntjfg [748.352954ms]
    Nov 30 04:01:59.683: INFO: Created: latency-svc-kqqc2
    Nov 30 04:01:59.698: INFO: Got endpoints: latency-svc-k2f95 [749.79168ms]
    Nov 30 04:01:59.727: INFO: Created: latency-svc-h2mfk
    Nov 30 04:01:59.749: INFO: Got endpoints: latency-svc-5n59q [743.740726ms]
    Nov 30 04:01:59.794: INFO: Created: latency-svc-t5pgv
    Nov 30 04:01:59.820: INFO: Got endpoints: latency-svc-4pg24 [769.197271ms]
    Nov 30 04:01:59.850: INFO: Got endpoints: latency-svc-twfxz [750.221734ms]
    Nov 30 04:01:59.861: INFO: Created: latency-svc-wzs46
    Nov 30 04:01:59.874: INFO: Created: latency-svc-7wvq8
    Nov 30 04:01:59.898: INFO: Got endpoints: latency-svc-9fdsl [748.37524ms]
    Nov 30 04:01:59.924: INFO: Created: latency-svc-s9hrb
    Nov 30 04:01:59.954: INFO: Got endpoints: latency-svc-27dj6 [698.130616ms]
    Nov 30 04:01:59.979: INFO: Created: latency-svc-x7xlk
    Nov 30 04:02:00.003: INFO: Got endpoints: latency-svc-96rcf [740.96859ms]
    Nov 30 04:02:00.037: INFO: Created: latency-svc-86f4v
    Nov 30 04:02:00.047: INFO: Got endpoints: latency-svc-vq2n8 [742.744409ms]
    Nov 30 04:02:00.070: INFO: Created: latency-svc-qs9bd
    Nov 30 04:02:00.097: INFO: Got endpoints: latency-svc-975dp [740.674142ms]
    Nov 30 04:02:00.136: INFO: Created: latency-svc-876xz
    Nov 30 04:02:00.148: INFO: Got endpoints: latency-svc-2zm5m [749.180706ms]
    Nov 30 04:02:00.170: INFO: Created: latency-svc-tq2n8
    Nov 30 04:02:00.199: INFO: Got endpoints: latency-svc-tpbnh [743.998596ms]
    Nov 30 04:02:00.223: INFO: Created: latency-svc-twjxb
    Nov 30 04:02:00.248: INFO: Got endpoints: latency-svc-pqfqh [748.752868ms]
    Nov 30 04:02:00.269: INFO: Created: latency-svc-72m7g
    Nov 30 04:02:00.297: INFO: Got endpoints: latency-svc-s2vt4 [740.434897ms]
    Nov 30 04:02:00.322: INFO: Created: latency-svc-7cnjz
    Nov 30 04:02:00.358: INFO: Got endpoints: latency-svc-tcdkf [758.375856ms]
    Nov 30 04:02:00.378: INFO: Created: latency-svc-5b9rl
    Nov 30 04:02:00.400: INFO: Got endpoints: latency-svc-kqqc2 [750.370853ms]
    Nov 30 04:02:00.432: INFO: Created: latency-svc-vf8qp
    Nov 30 04:02:00.443: INFO: Got endpoints: latency-svc-h2mfk [745.276003ms]
    Nov 30 04:02:00.467: INFO: Created: latency-svc-wsl66
    Nov 30 04:02:00.521: INFO: Got endpoints: latency-svc-t5pgv [771.655471ms]
    Nov 30 04:02:00.539: INFO: Created: latency-svc-kqbks
    Nov 30 04:02:00.549: INFO: Got endpoints: latency-svc-wzs46 [728.575989ms]
    Nov 30 04:02:00.573: INFO: Created: latency-svc-x9k75
    Nov 30 04:02:00.597: INFO: Got endpoints: latency-svc-7wvq8 [746.233985ms]
    Nov 30 04:02:00.613: INFO: Created: latency-svc-cbv6f
    Nov 30 04:02:00.654: INFO: Got endpoints: latency-svc-s9hrb [755.870963ms]
    Nov 30 04:02:00.700: INFO: Created: latency-svc-bqwxj
    Nov 30 04:02:00.701: INFO: Got endpoints: latency-svc-x7xlk [746.488144ms]
    Nov 30 04:02:00.723: INFO: Created: latency-svc-hq22q
    Nov 30 04:02:00.747: INFO: Got endpoints: latency-svc-86f4v [744.327293ms]
    Nov 30 04:02:00.787: INFO: Created: latency-svc-mglnq
    Nov 30 04:02:00.802: INFO: Got endpoints: latency-svc-qs9bd [755.55408ms]
    Nov 30 04:02:00.838: INFO: Created: latency-svc-zzw7x
    Nov 30 04:02:00.851: INFO: Got endpoints: latency-svc-876xz [753.401568ms]
    Nov 30 04:02:00.873: INFO: Created: latency-svc-nvx8k
    Nov 30 04:02:00.908: INFO: Got endpoints: latency-svc-tq2n8 [759.958701ms]
    Nov 30 04:02:00.929: INFO: Created: latency-svc-jlrk4
    Nov 30 04:02:00.950: INFO: Got endpoints: latency-svc-twjxb [750.304258ms]
    Nov 30 04:02:00.968: INFO: Created: latency-svc-6wvhb
    Nov 30 04:02:00.999: INFO: Got endpoints: latency-svc-72m7g [750.932792ms]
    Nov 30 04:02:01.037: INFO: Created: latency-svc-wwmwk
    Nov 30 04:02:01.057: INFO: Got endpoints: latency-svc-7cnjz [760.160463ms]
    Nov 30 04:02:01.087: INFO: Created: latency-svc-9kh94
    Nov 30 04:02:01.102: INFO: Got endpoints: latency-svc-5b9rl [744.211839ms]
    Nov 30 04:02:01.142: INFO: Created: latency-svc-v8zxk
    Nov 30 04:02:01.150: INFO: Got endpoints: latency-svc-vf8qp [750.155796ms]
    Nov 30 04:02:01.176: INFO: Created: latency-svc-qbq6t
    Nov 30 04:02:01.207: INFO: Got endpoints: latency-svc-wsl66 [763.352256ms]
    Nov 30 04:02:01.240: INFO: Created: latency-svc-7w6ct
    Nov 30 04:02:01.252: INFO: Got endpoints: latency-svc-kqbks [731.095966ms]
    Nov 30 04:02:01.275: INFO: Created: latency-svc-kj2cn
    Nov 30 04:02:01.300: INFO: Got endpoints: latency-svc-x9k75 [751.409471ms]
    Nov 30 04:02:01.319: INFO: Created: latency-svc-vqns6
    Nov 30 04:02:01.364: INFO: Got endpoints: latency-svc-cbv6f [767.839662ms]
    Nov 30 04:02:01.401: INFO: Got endpoints: latency-svc-bqwxj [746.57006ms]
    Nov 30 04:02:01.415: INFO: Created: latency-svc-kws87
    Nov 30 04:02:01.420: INFO: Created: latency-svc-phjbq
    Nov 30 04:02:01.456: INFO: Got endpoints: latency-svc-hq22q [755.196188ms]
    Nov 30 04:02:01.495: INFO: Created: latency-svc-vrjft
    Nov 30 04:02:01.498: INFO: Got endpoints: latency-svc-mglnq [750.61595ms]
    Nov 30 04:02:01.528: INFO: Created: latency-svc-b92vp
    Nov 30 04:02:01.560: INFO: Got endpoints: latency-svc-zzw7x [757.419527ms]
    Nov 30 04:02:01.594: INFO: Created: latency-svc-njwtv
    Nov 30 04:02:01.625: INFO: Got endpoints: latency-svc-nvx8k [773.722188ms]
    Nov 30 04:02:01.666: INFO: Created: latency-svc-mg9xh
    Nov 30 04:02:01.667: INFO: Got endpoints: latency-svc-jlrk4 [758.930988ms]
    Nov 30 04:02:01.708: INFO: Got endpoints: latency-svc-6wvhb [757.654366ms]
    Nov 30 04:02:01.708: INFO: Created: latency-svc-kn8w6
    Nov 30 04:02:01.726: INFO: Created: latency-svc-k4bpj
    Nov 30 04:02:01.746: INFO: Got endpoints: latency-svc-wwmwk [746.607621ms]
    Nov 30 04:02:01.763: INFO: Created: latency-svc-wh6lc
    Nov 30 04:02:01.800: INFO: Got endpoints: latency-svc-9kh94 [743.490783ms]
    Nov 30 04:02:01.819: INFO: Created: latency-svc-xgkxq
    Nov 30 04:02:01.848: INFO: Got endpoints: latency-svc-v8zxk [745.70965ms]
    Nov 30 04:02:01.868: INFO: Created: latency-svc-2m24x
    Nov 30 04:02:01.903: INFO: Got endpoints: latency-svc-qbq6t [753.523012ms]
    Nov 30 04:02:01.933: INFO: Created: latency-svc-8ptwd
    Nov 30 04:02:01.957: INFO: Got endpoints: latency-svc-7w6ct [750.183369ms]
    Nov 30 04:02:02.064: INFO: Got endpoints: latency-svc-vqns6 [763.692967ms]
    Nov 30 04:02:02.064: INFO: Got endpoints: latency-svc-kj2cn [811.693677ms]
    Nov 30 04:02:02.077: INFO: Created: latency-svc-kx44l
    Nov 30 04:02:02.103: INFO: Got endpoints: latency-svc-kws87 [738.989558ms]
    Nov 30 04:02:02.106: INFO: Created: latency-svc-dht46
    Nov 30 04:02:02.122: INFO: Created: latency-svc-xqck6
    Nov 30 04:02:02.152: INFO: Created: latency-svc-vsznz
    Nov 30 04:02:02.153: INFO: Got endpoints: latency-svc-phjbq [752.197306ms]
    Nov 30 04:02:02.189: INFO: Created: latency-svc-n4th5
    Nov 30 04:02:02.198: INFO: Got endpoints: latency-svc-vrjft [742.143089ms]
    Nov 30 04:02:02.219: INFO: Created: latency-svc-nx7vh
    Nov 30 04:02:02.261: INFO: Got endpoints: latency-svc-b92vp [763.377137ms]
    Nov 30 04:02:02.289: INFO: Created: latency-svc-brw8m
    Nov 30 04:02:02.311: INFO: Got endpoints: latency-svc-njwtv [751.058445ms]
    Nov 30 04:02:02.327: INFO: Created: latency-svc-clzk9
    Nov 30 04:02:02.351: INFO: Got endpoints: latency-svc-mg9xh [726.324856ms]
    Nov 30 04:02:02.388: INFO: Created: latency-svc-9lr49
    Nov 30 04:02:02.411: INFO: Got endpoints: latency-svc-kn8w6 [743.973857ms]
    Nov 30 04:02:02.446: INFO: Got endpoints: latency-svc-k4bpj [738.176167ms]
    Nov 30 04:02:02.446: INFO: Created: latency-svc-6s2zw
    Nov 30 04:02:02.489: INFO: Created: latency-svc-7gt7w
    Nov 30 04:02:02.499: INFO: Got endpoints: latency-svc-wh6lc [753.286503ms]
    Nov 30 04:02:02.526: INFO: Created: latency-svc-49vw7
    Nov 30 04:02:02.550: INFO: Got endpoints: latency-svc-xgkxq [749.571951ms]
    Nov 30 04:02:02.572: INFO: Created: latency-svc-2w2l6
    Nov 30 04:02:02.608: INFO: Got endpoints: latency-svc-2m24x [760.855247ms]
    Nov 30 04:02:02.625: INFO: Created: latency-svc-4msnj
    Nov 30 04:02:02.650: INFO: Got endpoints: latency-svc-8ptwd [746.615764ms]
    Nov 30 04:02:02.677: INFO: Created: latency-svc-v6dsk
    Nov 30 04:02:02.698: INFO: Got endpoints: latency-svc-kx44l [740.796972ms]
    Nov 30 04:02:02.729: INFO: Created: latency-svc-qgksz
    Nov 30 04:02:02.747: INFO: Got endpoints: latency-svc-dht46 [682.90875ms]
    Nov 30 04:02:02.794: INFO: Created: latency-svc-2xzhz
    Nov 30 04:02:02.839: INFO: Got endpoints: latency-svc-xqck6 [774.998807ms]
    Nov 30 04:02:02.847: INFO: Got endpoints: latency-svc-vsznz [743.542375ms]
    Nov 30 04:02:02.890: INFO: Created: latency-svc-qvwj7
    Nov 30 04:02:02.903: INFO: Got endpoints: latency-svc-n4th5 [749.906395ms]
    Nov 30 04:02:02.910: INFO: Created: latency-svc-4jnks
    Nov 30 04:02:02.922: INFO: Created: latency-svc-zkxww
    Nov 30 04:02:02.963: INFO: Got endpoints: latency-svc-nx7vh [764.393605ms]
    Nov 30 04:02:02.985: INFO: Created: latency-svc-xw99g
    Nov 30 04:02:03.005: INFO: Got endpoints: latency-svc-brw8m [744.033913ms]
    Nov 30 04:02:03.025: INFO: Created: latency-svc-qqvhm
    Nov 30 04:02:03.047: INFO: Got endpoints: latency-svc-clzk9 [736.218757ms]
    Nov 30 04:02:03.064: INFO: Created: latency-svc-c77x6
    Nov 30 04:02:03.102: INFO: Got endpoints: latency-svc-9lr49 [750.619862ms]
    Nov 30 04:02:03.121: INFO: Created: latency-svc-wnl8t
    Nov 30 04:02:03.154: INFO: Got endpoints: latency-svc-6s2zw [742.121409ms]
    Nov 30 04:02:03.187: INFO: Created: latency-svc-25m24
    Nov 30 04:02:03.197: INFO: Got endpoints: latency-svc-7gt7w [751.160541ms]
    Nov 30 04:02:03.235: INFO: Created: latency-svc-dnp6r
    Nov 30 04:02:03.248: INFO: Got endpoints: latency-svc-49vw7 [748.413296ms]
    Nov 30 04:02:03.273: INFO: Created: latency-svc-dq6rw
    Nov 30 04:02:03.303: INFO: Got endpoints: latency-svc-2w2l6 [753.324126ms]
    Nov 30 04:02:03.328: INFO: Created: latency-svc-vgrmx
    Nov 30 04:02:03.361: INFO: Got endpoints: latency-svc-4msnj [752.44373ms]
    Nov 30 04:02:03.406: INFO: Got endpoints: latency-svc-v6dsk [755.92981ms]
    Nov 30 04:02:03.406: INFO: Created: latency-svc-svdpg
    Nov 30 04:02:03.425: INFO: Created: latency-svc-sw8xt
    Nov 30 04:02:03.462: INFO: Got endpoints: latency-svc-qgksz [764.229587ms]
    Nov 30 04:02:03.482: INFO: Created: latency-svc-64gdv
    Nov 30 04:02:03.511: INFO: Got endpoints: latency-svc-2xzhz [764.0972ms]
    Nov 30 04:02:03.531: INFO: Created: latency-svc-z8p8f
    Nov 30 04:02:03.558: INFO: Got endpoints: latency-svc-qvwj7 [718.980744ms]
    Nov 30 04:02:03.591: INFO: Created: latency-svc-xc96q
    Nov 30 04:02:03.599: INFO: Got endpoints: latency-svc-4jnks [751.684653ms]
    Nov 30 04:02:03.654: INFO: Got endpoints: latency-svc-zkxww [751.673237ms]
    Nov 30 04:02:03.659: INFO: Created: latency-svc-hm66g
    Nov 30 04:02:03.699: INFO: Got endpoints: latency-svc-xw99g [736.833185ms]
    Nov 30 04:02:03.712: INFO: Created: latency-svc-pj2jn
    Nov 30 04:02:03.754: INFO: Got endpoints: latency-svc-qqvhm [748.152072ms]
    Nov 30 04:02:03.771: INFO: Created: latency-svc-n5k79
    Nov 30 04:02:03.806: INFO: Got endpoints: latency-svc-c77x6 [758.920304ms]
    Nov 30 04:02:03.807: INFO: Created: latency-svc-h6f4v
    Nov 30 04:02:03.835: INFO: Created: latency-svc-vmx8n
    Nov 30 04:02:03.853: INFO: Got endpoints: latency-svc-wnl8t [751.619359ms]
    Nov 30 04:02:03.912: INFO: Got endpoints: latency-svc-25m24 [757.975295ms]
    Nov 30 04:02:03.912: INFO: Created: latency-svc-k2lms
    Nov 30 04:02:03.932: INFO: Created: latency-svc-gmcbv
    Nov 30 04:02:03.947: INFO: Got endpoints: latency-svc-dnp6r [749.58493ms]
    Nov 30 04:02:03.974: INFO: Created: latency-svc-7ztd8
    Nov 30 04:02:04.005: INFO: Got endpoints: latency-svc-dq6rw [757.245132ms]
    Nov 30 04:02:04.033: INFO: Created: latency-svc-jqn59
    Nov 30 04:02:04.052: INFO: Got endpoints: latency-svc-vgrmx [748.34465ms]
    Nov 30 04:02:04.069: INFO: Created: latency-svc-sqxrm
    Nov 30 04:02:04.099: INFO: Got endpoints: latency-svc-svdpg [737.942842ms]
    Nov 30 04:02:04.136: INFO: Created: latency-svc-vch5g
    Nov 30 04:02:04.150: INFO: Got endpoints: latency-svc-sw8xt [744.280883ms]
    Nov 30 04:02:04.192: INFO: Created: latency-svc-g79fm
    Nov 30 04:02:04.197: INFO: Got endpoints: latency-svc-64gdv [734.600433ms]
    Nov 30 04:02:04.224: INFO: Created: latency-svc-gvztb
    Nov 30 04:02:04.265: INFO: Got endpoints: latency-svc-z8p8f [753.584214ms]
    Nov 30 04:02:04.305: INFO: Got endpoints: latency-svc-xc96q [746.473677ms]
    Nov 30 04:02:04.315: INFO: Created: latency-svc-xg48t
    Nov 30 04:02:04.347: INFO: Created: latency-svc-t2d55
    Nov 30 04:02:04.348: INFO: Got endpoints: latency-svc-hm66g [748.870449ms]
    Nov 30 04:02:04.388: INFO: Created: latency-svc-lklf6
    Nov 30 04:02:04.439: INFO: Got endpoints: latency-svc-pj2jn [784.54527ms]
    Nov 30 04:02:04.451: INFO: Got endpoints: latency-svc-n5k79 [752.012708ms]
    Nov 30 04:02:04.497: INFO: Created: latency-svc-xs555
    Nov 30 04:02:04.499: INFO: Got endpoints: latency-svc-h6f4v [744.963844ms]
    Nov 30 04:02:04.533: INFO: Created: latency-svc-s4nsg
    Nov 30 04:02:04.558: INFO: Got endpoints: latency-svc-vmx8n [751.876439ms]
    Nov 30 04:02:04.562: INFO: Created: latency-svc-d5b65
    Nov 30 04:02:04.582: INFO: Created: latency-svc-mdsms
    Nov 30 04:02:04.606: INFO: Got endpoints: latency-svc-k2lms [752.714495ms]
    Nov 30 04:02:04.662: INFO: Created: latency-svc-8r4vn
    Nov 30 04:02:04.664: INFO: Got endpoints: latency-svc-gmcbv [752.537255ms]
    Nov 30 04:02:04.695: INFO: Created: latency-svc-xwt2j
    Nov 30 04:02:04.705: INFO: Got endpoints: latency-svc-7ztd8 [758.028917ms]
    Nov 30 04:02:04.723: INFO: Created: latency-svc-k9mbv
    Nov 30 04:02:04.747: INFO: Got endpoints: latency-svc-jqn59 [742.139441ms]
    Nov 30 04:02:04.782: INFO: Created: latency-svc-qpztt
    Nov 30 04:02:04.799: INFO: Got endpoints: latency-svc-sqxrm [747.709388ms]
    Nov 30 04:02:04.825: INFO: Created: latency-svc-nfhz5
    Nov 30 04:02:04.856: INFO: Got endpoints: latency-svc-vch5g [757.073863ms]
    Nov 30 04:02:04.897: INFO: Created: latency-svc-pcbgn
    Nov 30 04:02:04.903: INFO: Got endpoints: latency-svc-g79fm [752.377793ms]
    Nov 30 04:02:04.947: INFO: Got endpoints: latency-svc-gvztb [750.110567ms]
    Nov 30 04:02:04.969: INFO: Created: latency-svc-xcgcl
    Nov 30 04:02:04.980: INFO: Created: latency-svc-7svj4
    Nov 30 04:02:05.004: INFO: Got endpoints: latency-svc-xg48t [739.777382ms]
    Nov 30 04:02:05.055: INFO: Got endpoints: latency-svc-t2d55 [750.415337ms]
    Nov 30 04:02:05.111: INFO: Created: latency-svc-s9vf7
    Nov 30 04:02:05.111: INFO: Got endpoints: latency-svc-lklf6 [763.771219ms]
    Nov 30 04:02:05.194: INFO: Got endpoints: latency-svc-xs555 [754.940309ms]
    Nov 30 04:02:05.226: INFO: Got endpoints: latency-svc-s4nsg [774.960272ms]
    Nov 30 04:02:05.235: INFO: Created: latency-svc-xx5tq
    Nov 30 04:02:05.259: INFO: Got endpoints: latency-svc-d5b65 [760.287243ms]
    Nov 30 04:02:05.260: INFO: Created: latency-svc-2mnpj
    Nov 30 04:02:05.374: INFO: Got endpoints: latency-svc-8r4vn [767.562522ms]
    Nov 30 04:02:05.374: INFO: Got endpoints: latency-svc-mdsms [815.880764ms]
    Nov 30 04:02:05.374: INFO: Created: latency-svc-vdv89
    Nov 30 04:02:05.421: INFO: Got endpoints: latency-svc-xwt2j [756.458427ms]
    Nov 30 04:02:05.463: INFO: Got endpoints: latency-svc-k9mbv [758.59737ms]
    Nov 30 04:02:05.476: INFO: Created: latency-svc-5wgxf
    Nov 30 04:02:05.476: INFO: Created: latency-svc-8xnmj
    Nov 30 04:02:05.503: INFO: Got endpoints: latency-svc-qpztt [755.793013ms]
    Nov 30 04:02:05.553: INFO: Got endpoints: latency-svc-nfhz5 [753.879154ms]
    Nov 30 04:02:05.649: INFO: Got endpoints: latency-svc-pcbgn [792.828973ms]
    Nov 30 04:02:05.700: INFO: Got endpoints: latency-svc-7svj4 [797.705124ms]
    Nov 30 04:02:05.712: INFO: Got endpoints: latency-svc-xcgcl [764.731138ms]
    Nov 30 04:02:05.769: INFO: Got endpoints: latency-svc-s9vf7 [764.747512ms]
    Nov 30 04:02:05.799: INFO: Got endpoints: latency-svc-2mnpj [743.896931ms]
    Nov 30 04:02:05.857: INFO: Got endpoints: latency-svc-xx5tq [745.06603ms]
    Nov 30 04:02:05.898: INFO: Got endpoints: latency-svc-vdv89 [704.297106ms]
    Nov 30 04:02:05.955: INFO: Got endpoints: latency-svc-8xnmj [728.307765ms]
    Nov 30 04:02:05.998: INFO: Got endpoints: latency-svc-5wgxf [738.741203ms]
    Nov 30 04:02:05.998: INFO: Latencies: [37.25479ms 88.282559ms 102.84626ms 118.290273ms 123.682427ms 135.497426ms 161.549198ms 178.493393ms 201.233985ms 216.373008ms 223.789312ms 242.339074ms 249.405383ms 267.721007ms 287.518292ms 291.656199ms 300.90395ms 310.65801ms 316.130558ms 325.990593ms 331.581513ms 338.67226ms 340.009848ms 342.125709ms 342.371834ms 342.898111ms 343.337487ms 347.32448ms 349.21333ms 350.967556ms 355.36054ms 357.297914ms 359.55963ms 361.791727ms 363.54223ms 369.667196ms 375.77802ms 379.134973ms 379.666714ms 379.921147ms 383.556559ms 384.739549ms 388.333576ms 391.4588ms 395.128787ms 397.675971ms 402.844478ms 403.160512ms 405.921537ms 420.116244ms 428.319247ms 428.828143ms 429.711093ms 432.701955ms 436.820776ms 437.959779ms 445.53575ms 488.633854ms 490.530076ms 497.930709ms 506.535828ms 528.740708ms 536.703273ms 544.605297ms 592.10103ms 603.647807ms 633.912134ms 634.363802ms 639.083797ms 682.90875ms 690.338578ms 695.423131ms 698.130616ms 704.297106ms 718.980744ms 725.667014ms 726.324856ms 728.307765ms 728.575989ms 731.095966ms 734.600433ms 736.218757ms 736.833185ms 737.942842ms 738.176167ms 738.741203ms 738.989558ms 739.777382ms 740.434897ms 740.674142ms 740.796972ms 740.96859ms 742.121409ms 742.139441ms 742.143089ms 742.744409ms 743.490783ms 743.542375ms 743.740726ms 743.896931ms 743.973857ms 743.998596ms 744.033913ms 744.211839ms 744.280883ms 744.327293ms 744.963844ms 745.06603ms 745.276003ms 745.70965ms 746.233985ms 746.473677ms 746.488144ms 746.57006ms 746.607621ms 746.615764ms 747.709388ms 748.152072ms 748.34465ms 748.352954ms 748.37524ms 748.413296ms 748.752868ms 748.870449ms 749.180706ms 749.571951ms 749.58493ms 749.79168ms 749.906395ms 750.110567ms 750.155796ms 750.183369ms 750.221734ms 750.304258ms 750.370853ms 750.415337ms 750.61595ms 750.619862ms 750.932792ms 751.058445ms 751.160541ms 751.409471ms 751.619359ms 751.673237ms 751.684653ms 751.876439ms 752.012708ms 752.197306ms 752.377793ms 752.44373ms 752.537255ms 752.714495ms 753.286503ms 753.324126ms 753.401568ms 753.523012ms 753.584214ms 753.879154ms 754.940309ms 755.196188ms 755.55408ms 755.793013ms 755.870963ms 755.92981ms 756.458427ms 757.073863ms 757.245132ms 757.419527ms 757.654366ms 757.975295ms 758.028917ms 758.375856ms 758.59737ms 758.920304ms 758.930988ms 759.958701ms 760.160463ms 760.287243ms 760.855247ms 763.352256ms 763.377137ms 763.692967ms 763.771219ms 764.0972ms 764.229587ms 764.393605ms 764.731138ms 764.747512ms 767.562522ms 767.839662ms 769.197271ms 771.655471ms 773.722188ms 774.960272ms 774.998807ms 784.54527ms 792.828973ms 797.705124ms 811.693677ms 815.880764ms]
    Nov 30 04:02:05.998: INFO: 50 %ile: 743.973857ms
    Nov 30 04:02:05.998: INFO: 90 %ile: 763.377137ms
    Nov 30 04:02:05.998: INFO: 99 %ile: 811.693677ms
    Nov 30 04:02:05.998: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Nov 30 04:02:05.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-56" for this suite. 11/30/22 04:02:06.008
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:02:06.016
Nov 30 04:02:06.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 04:02:06.017
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:02:06.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:02:06.074
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-10bfc531-3a40-4928-a991-287ecae92643 11/30/22 04:02:06.077
STEP: Creating a pod to test consume secrets 11/30/22 04:02:06.108
Nov 30 04:02:06.154: INFO: Waiting up to 5m0s for pod "pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda" in namespace "secrets-5218" to be "Succeeded or Failed"
Nov 30 04:02:06.158: INFO: Pod "pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda": Phase="Pending", Reason="", readiness=false. Elapsed: 3.734073ms
Nov 30 04:02:08.162: INFO: Pod "pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007719775s
Nov 30 04:02:10.165: INFO: Pod "pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010809835s
STEP: Saw pod success 11/30/22 04:02:10.165
Nov 30 04:02:10.165: INFO: Pod "pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda" satisfied condition "Succeeded or Failed"
Nov 30 04:02:10.168: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda container secret-env-test: <nil>
STEP: delete the pod 11/30/22 04:02:10.174
Nov 30 04:02:10.190: INFO: Waiting for pod pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda to disappear
Nov 30 04:02:10.194: INFO: Pod pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Nov 30 04:02:10.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5218" for this suite. 11/30/22 04:02:10.199
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":152,"skipped":2594,"failed":0}
------------------------------
• [4.195 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:02:06.016
    Nov 30 04:02:06.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 04:02:06.017
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:02:06.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:02:06.074
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-10bfc531-3a40-4928-a991-287ecae92643 11/30/22 04:02:06.077
    STEP: Creating a pod to test consume secrets 11/30/22 04:02:06.108
    Nov 30 04:02:06.154: INFO: Waiting up to 5m0s for pod "pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda" in namespace "secrets-5218" to be "Succeeded or Failed"
    Nov 30 04:02:06.158: INFO: Pod "pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda": Phase="Pending", Reason="", readiness=false. Elapsed: 3.734073ms
    Nov 30 04:02:08.162: INFO: Pod "pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007719775s
    Nov 30 04:02:10.165: INFO: Pod "pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010809835s
    STEP: Saw pod success 11/30/22 04:02:10.165
    Nov 30 04:02:10.165: INFO: Pod "pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda" satisfied condition "Succeeded or Failed"
    Nov 30 04:02:10.168: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda container secret-env-test: <nil>
    STEP: delete the pod 11/30/22 04:02:10.174
    Nov 30 04:02:10.190: INFO: Waiting for pod pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda to disappear
    Nov 30 04:02:10.194: INFO: Pod pod-secrets-06da5b30-8a57-4c5d-81a9-54ecd29eafda no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 04:02:10.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5218" for this suite. 11/30/22 04:02:10.199
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:02:10.212
Nov 30 04:02:10.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename job 11/30/22 04:02:10.213
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:02:10.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:02:10.239
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 11/30/22 04:02:10.242
STEP: Ensuring active pods == parallelism 11/30/22 04:02:10.252
STEP: delete a job 11/30/22 04:02:12.257
STEP: deleting Job.batch foo in namespace job-5136, will wait for the garbage collector to delete the pods 11/30/22 04:02:12.257
Nov 30 04:02:12.321: INFO: Deleting Job.batch foo took: 10.243573ms
Nov 30 04:02:12.421: INFO: Terminating Job.batch foo pods took: 100.363264ms
STEP: Ensuring job was deleted 11/30/22 04:02:44.522
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Nov 30 04:02:44.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5136" for this suite. 11/30/22 04:02:44.529
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":153,"skipped":2596,"failed":0}
------------------------------
• [SLOW TEST] [34.324 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:02:10.212
    Nov 30 04:02:10.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename job 11/30/22 04:02:10.213
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:02:10.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:02:10.239
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 11/30/22 04:02:10.242
    STEP: Ensuring active pods == parallelism 11/30/22 04:02:10.252
    STEP: delete a job 11/30/22 04:02:12.257
    STEP: deleting Job.batch foo in namespace job-5136, will wait for the garbage collector to delete the pods 11/30/22 04:02:12.257
    Nov 30 04:02:12.321: INFO: Deleting Job.batch foo took: 10.243573ms
    Nov 30 04:02:12.421: INFO: Terminating Job.batch foo pods took: 100.363264ms
    STEP: Ensuring job was deleted 11/30/22 04:02:44.522
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Nov 30 04:02:44.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5136" for this suite. 11/30/22 04:02:44.529
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:02:44.536
Nov 30 04:02:44.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename cronjob 11/30/22 04:02:44.536
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:02:44.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:02:44.555
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 11/30/22 04:02:44.563
STEP: Ensuring a job is scheduled 11/30/22 04:02:44.568
STEP: Ensuring exactly one is scheduled 11/30/22 04:03:00.573
STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/30/22 04:03:00.578
STEP: Ensuring no more jobs are scheduled 11/30/22 04:03:00.581
STEP: Removing cronjob 11/30/22 04:08:00.589
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Nov 30 04:08:00.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3563" for this suite. 11/30/22 04:08:00.601
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":154,"skipped":2600,"failed":0}
------------------------------
• [SLOW TEST] [316.080 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:02:44.536
    Nov 30 04:02:44.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename cronjob 11/30/22 04:02:44.536
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:02:44.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:02:44.555
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 11/30/22 04:02:44.563
    STEP: Ensuring a job is scheduled 11/30/22 04:02:44.568
    STEP: Ensuring exactly one is scheduled 11/30/22 04:03:00.573
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/30/22 04:03:00.578
    STEP: Ensuring no more jobs are scheduled 11/30/22 04:03:00.581
    STEP: Removing cronjob 11/30/22 04:08:00.589
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Nov 30 04:08:00.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3563" for this suite. 11/30/22 04:08:00.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:08:00.616
Nov 30 04:08:00.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 04:08:00.617
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:08:00.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:08:00.654
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 04:08:00.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4141" for this suite. 11/30/22 04:08:00.667
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":155,"skipped":2618,"failed":0}
------------------------------
• [0.064 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:08:00.616
    Nov 30 04:08:00.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 04:08:00.617
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:08:00.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:08:00.654
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 04:08:00.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4141" for this suite. 11/30/22 04:08:00.667
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:08:00.681
Nov 30 04:08:00.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 04:08:00.682
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:08:00.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:08:00.704
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 11/30/22 04:08:00.707
Nov 30 04:08:00.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-1469 api-versions'
Nov 30 04:08:00.763: INFO: stderr: ""
Nov 30 04:08:00.763: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nk8s.cni.cncf.io/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 04:08:00.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1469" for this suite. 11/30/22 04:08:00.767
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":156,"skipped":2629,"failed":0}
------------------------------
• [0.095 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:08:00.681
    Nov 30 04:08:00.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 04:08:00.682
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:08:00.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:08:00.704
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 11/30/22 04:08:00.707
    Nov 30 04:08:00.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-1469 api-versions'
    Nov 30 04:08:00.763: INFO: stderr: ""
    Nov 30 04:08:00.763: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nk8s.cni.cncf.io/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 04:08:00.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1469" for this suite. 11/30/22 04:08:00.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:08:00.777
Nov 30 04:08:00.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 04:08:00.778
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:08:00.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:08:00.803
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 11/30/22 04:08:00.805
Nov 30 04:08:00.843: INFO: Waiting up to 5m0s for pod "downward-api-f2aad34d-fd18-4327-860f-128fc2534031" in namespace "downward-api-1064" to be "Succeeded or Failed"
Nov 30 04:08:00.846: INFO: Pod "downward-api-f2aad34d-fd18-4327-860f-128fc2534031": Phase="Pending", Reason="", readiness=false. Elapsed: 2.674638ms
Nov 30 04:08:02.849: INFO: Pod "downward-api-f2aad34d-fd18-4327-860f-128fc2534031": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006195754s
Nov 30 04:08:04.850: INFO: Pod "downward-api-f2aad34d-fd18-4327-860f-128fc2534031": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006818737s
STEP: Saw pod success 11/30/22 04:08:04.85
Nov 30 04:08:04.850: INFO: Pod "downward-api-f2aad34d-fd18-4327-860f-128fc2534031" satisfied condition "Succeeded or Failed"
Nov 30 04:08:04.853: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downward-api-f2aad34d-fd18-4327-860f-128fc2534031 container dapi-container: <nil>
STEP: delete the pod 11/30/22 04:08:04.86
Nov 30 04:08:04.886: INFO: Waiting for pod downward-api-f2aad34d-fd18-4327-860f-128fc2534031 to disappear
Nov 30 04:08:04.888: INFO: Pod downward-api-f2aad34d-fd18-4327-860f-128fc2534031 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Nov 30 04:08:04.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1064" for this suite. 11/30/22 04:08:04.893
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":157,"skipped":2664,"failed":0}
------------------------------
• [4.127 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:08:00.777
    Nov 30 04:08:00.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 04:08:00.778
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:08:00.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:08:00.803
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 11/30/22 04:08:00.805
    Nov 30 04:08:00.843: INFO: Waiting up to 5m0s for pod "downward-api-f2aad34d-fd18-4327-860f-128fc2534031" in namespace "downward-api-1064" to be "Succeeded or Failed"
    Nov 30 04:08:00.846: INFO: Pod "downward-api-f2aad34d-fd18-4327-860f-128fc2534031": Phase="Pending", Reason="", readiness=false. Elapsed: 2.674638ms
    Nov 30 04:08:02.849: INFO: Pod "downward-api-f2aad34d-fd18-4327-860f-128fc2534031": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006195754s
    Nov 30 04:08:04.850: INFO: Pod "downward-api-f2aad34d-fd18-4327-860f-128fc2534031": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006818737s
    STEP: Saw pod success 11/30/22 04:08:04.85
    Nov 30 04:08:04.850: INFO: Pod "downward-api-f2aad34d-fd18-4327-860f-128fc2534031" satisfied condition "Succeeded or Failed"
    Nov 30 04:08:04.853: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downward-api-f2aad34d-fd18-4327-860f-128fc2534031 container dapi-container: <nil>
    STEP: delete the pod 11/30/22 04:08:04.86
    Nov 30 04:08:04.886: INFO: Waiting for pod downward-api-f2aad34d-fd18-4327-860f-128fc2534031 to disappear
    Nov 30 04:08:04.888: INFO: Pod downward-api-f2aad34d-fd18-4327-860f-128fc2534031 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Nov 30 04:08:04.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1064" for this suite. 11/30/22 04:08:04.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:08:04.904
Nov 30 04:08:04.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename hostport 11/30/22 04:08:04.905
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:08:04.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:08:04.933
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 11/30/22 04:08:04.951
W1130 04:08:04.962568      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
Nov 30 04:08:04.962: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9856" to be "running and ready"
Nov 30 04:08:04.965: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.643754ms
Nov 30 04:08:04.965: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:08:06.969: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006422224s
Nov 30 04:08:06.969: INFO: The phase of Pod pod1 is Running (Ready = true)
Nov 30 04:08:06.969: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.10.8 on the node which pod1 resides and expect scheduled 11/30/22 04:08:06.969
W1130 04:08:06.977942      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
Nov 30 04:08:06.978: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9856" to be "running and ready"
Nov 30 04:08:06.981: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.372548ms
Nov 30 04:08:06.981: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:08:08.984: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006461299s
Nov 30 04:08:08.984: INFO: The phase of Pod pod2 is Running (Ready = true)
Nov 30 04:08:08.984: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.10.8 but use UDP protocol on the node which pod2 resides 11/30/22 04:08:08.984
W1130 04:08:08.993346      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
Nov 30 04:08:08.993: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9856" to be "running and ready"
Nov 30 04:08:08.995: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.559716ms
Nov 30 04:08:08.996: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:08:11.000: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.00667576s
Nov 30 04:08:11.000: INFO: The phase of Pod pod3 is Running (Ready = false)
Nov 30 04:08:12.999: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.006422308s
Nov 30 04:08:12.999: INFO: The phase of Pod pod3 is Running (Ready = true)
Nov 30 04:08:12.999: INFO: Pod "pod3" satisfied condition "running and ready"
W1130 04:08:13.008050      20 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Nov 30 04:08:13.008: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9856" to be "running and ready"
Nov 30 04:08:13.010: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.726454ms
Nov 30 04:08:13.010: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:08:15.018: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.010483333s
Nov 30 04:08:15.018: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Nov 30 04:08:15.018: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 11/30/22 04:08:15.021
Nov 30 04:08:15.021: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.10.8 http://127.0.0.1:54323/hostname] Namespace:hostport-9856 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:08:15.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:08:15.021: INFO: ExecWithOptions: Clientset creation
Nov 30 04:08:15.021: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9856/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.10.8+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.8, port: 54323 11/30/22 04:08:15.126
Nov 30 04:08:15.126: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.10.8:54323/hostname] Namespace:hostport-9856 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:08:15.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:08:15.127: INFO: ExecWithOptions: Clientset creation
Nov 30 04:08:15.127: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9856/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.10.8%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.8, port: 54323 UDP 11/30/22 04:08:15.189
Nov 30 04:08:15.189: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.10.8 54323] Namespace:hostport-9856 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:08:15.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:08:15.189: INFO: ExecWithOptions: Clientset creation
Nov 30 04:08:15.189: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9856/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.10.8+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Nov 30 04:08:20.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-9856" for this suite. 11/30/22 04:08:20.256
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":158,"skipped":2677,"failed":0}
------------------------------
• [SLOW TEST] [15.357 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:08:04.904
    Nov 30 04:08:04.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename hostport 11/30/22 04:08:04.905
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:08:04.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:08:04.933
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 11/30/22 04:08:04.951
    W1130 04:08:04.962568      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
    Nov 30 04:08:04.962: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9856" to be "running and ready"
    Nov 30 04:08:04.965: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.643754ms
    Nov 30 04:08:04.965: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:08:06.969: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006422224s
    Nov 30 04:08:06.969: INFO: The phase of Pod pod1 is Running (Ready = true)
    Nov 30 04:08:06.969: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.10.8 on the node which pod1 resides and expect scheduled 11/30/22 04:08:06.969
    W1130 04:08:06.977942      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
    Nov 30 04:08:06.978: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9856" to be "running and ready"
    Nov 30 04:08:06.981: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.372548ms
    Nov 30 04:08:06.981: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:08:08.984: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006461299s
    Nov 30 04:08:08.984: INFO: The phase of Pod pod2 is Running (Ready = true)
    Nov 30 04:08:08.984: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.10.8 but use UDP protocol on the node which pod2 resides 11/30/22 04:08:08.984
    W1130 04:08:08.993346      20 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
    Nov 30 04:08:08.993: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9856" to be "running and ready"
    Nov 30 04:08:08.995: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.559716ms
    Nov 30 04:08:08.996: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:08:11.000: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.00667576s
    Nov 30 04:08:11.000: INFO: The phase of Pod pod3 is Running (Ready = false)
    Nov 30 04:08:12.999: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.006422308s
    Nov 30 04:08:12.999: INFO: The phase of Pod pod3 is Running (Ready = true)
    Nov 30 04:08:12.999: INFO: Pod "pod3" satisfied condition "running and ready"
    W1130 04:08:13.008050      20 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Nov 30 04:08:13.008: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9856" to be "running and ready"
    Nov 30 04:08:13.010: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.726454ms
    Nov 30 04:08:13.010: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:08:15.018: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.010483333s
    Nov 30 04:08:15.018: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Nov 30 04:08:15.018: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 11/30/22 04:08:15.021
    Nov 30 04:08:15.021: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.10.8 http://127.0.0.1:54323/hostname] Namespace:hostport-9856 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:08:15.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:08:15.021: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:08:15.021: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9856/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.10.8+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.8, port: 54323 11/30/22 04:08:15.126
    Nov 30 04:08:15.126: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.10.8:54323/hostname] Namespace:hostport-9856 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:08:15.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:08:15.127: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:08:15.127: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9856/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.10.8%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.8, port: 54323 UDP 11/30/22 04:08:15.189
    Nov 30 04:08:15.189: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.10.8 54323] Namespace:hostport-9856 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:08:15.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:08:15.189: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:08:15.189: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9856/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.10.8+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Nov 30 04:08:20.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-9856" for this suite. 11/30/22 04:08:20.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:08:20.262
Nov 30 04:08:20.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-probe 11/30/22 04:08:20.263
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:08:20.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:08:20.287
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0 in namespace container-probe-6511 11/30/22 04:08:20.289
Nov 30 04:08:20.326: INFO: Waiting up to 5m0s for pod "test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0" in namespace "container-probe-6511" to be "not pending"
Nov 30 04:08:20.330: INFO: Pod "test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.576324ms
Nov 30 04:08:22.334: INFO: Pod "test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.007667344s
Nov 30 04:08:22.334: INFO: Pod "test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0" satisfied condition "not pending"
Nov 30 04:08:22.334: INFO: Started pod test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0 in namespace container-probe-6511
STEP: checking the pod's current state and verifying that restartCount is present 11/30/22 04:08:22.334
Nov 30 04:08:22.337: INFO: Initial restart count of pod test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0 is 0
STEP: deleting the pod 11/30/22 04:12:22.841
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Nov 30 04:12:22.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6511" for this suite. 11/30/22 04:12:22.861
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":159,"skipped":2692,"failed":0}
------------------------------
• [SLOW TEST] [242.606 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:08:20.262
    Nov 30 04:08:20.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-probe 11/30/22 04:08:20.263
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:08:20.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:08:20.287
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0 in namespace container-probe-6511 11/30/22 04:08:20.289
    Nov 30 04:08:20.326: INFO: Waiting up to 5m0s for pod "test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0" in namespace "container-probe-6511" to be "not pending"
    Nov 30 04:08:20.330: INFO: Pod "test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.576324ms
    Nov 30 04:08:22.334: INFO: Pod "test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.007667344s
    Nov 30 04:08:22.334: INFO: Pod "test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0" satisfied condition "not pending"
    Nov 30 04:08:22.334: INFO: Started pod test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0 in namespace container-probe-6511
    STEP: checking the pod's current state and verifying that restartCount is present 11/30/22 04:08:22.334
    Nov 30 04:08:22.337: INFO: Initial restart count of pod test-webserver-37a0de0b-9254-49db-ac04-e505e48df5b0 is 0
    STEP: deleting the pod 11/30/22 04:12:22.841
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Nov 30 04:12:22.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6511" for this suite. 11/30/22 04:12:22.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:12:22.869
Nov 30 04:12:22.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubelet-test 11/30/22 04:12:22.869
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:12:22.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:12:22.9
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Nov 30 04:12:22.936: INFO: Waiting up to 5m0s for pod "busybox-scheduling-a77dfcda-0787-4b00-a634-57afd0f454b4" in namespace "kubelet-test-5301" to be "running and ready"
Nov 30 04:12:22.945: INFO: Pod "busybox-scheduling-a77dfcda-0787-4b00-a634-57afd0f454b4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.287459ms
Nov 30 04:12:22.945: INFO: The phase of Pod busybox-scheduling-a77dfcda-0787-4b00-a634-57afd0f454b4 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:12:24.953: INFO: Pod "busybox-scheduling-a77dfcda-0787-4b00-a634-57afd0f454b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.01765444s
Nov 30 04:12:24.954: INFO: The phase of Pod busybox-scheduling-a77dfcda-0787-4b00-a634-57afd0f454b4 is Running (Ready = true)
Nov 30 04:12:24.954: INFO: Pod "busybox-scheduling-a77dfcda-0787-4b00-a634-57afd0f454b4" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Nov 30 04:12:24.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5301" for this suite. 11/30/22 04:12:24.966
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":160,"skipped":2721,"failed":0}
------------------------------
• [2.104 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:12:22.869
    Nov 30 04:12:22.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubelet-test 11/30/22 04:12:22.869
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:12:22.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:12:22.9
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Nov 30 04:12:22.936: INFO: Waiting up to 5m0s for pod "busybox-scheduling-a77dfcda-0787-4b00-a634-57afd0f454b4" in namespace "kubelet-test-5301" to be "running and ready"
    Nov 30 04:12:22.945: INFO: Pod "busybox-scheduling-a77dfcda-0787-4b00-a634-57afd0f454b4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.287459ms
    Nov 30 04:12:22.945: INFO: The phase of Pod busybox-scheduling-a77dfcda-0787-4b00-a634-57afd0f454b4 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:12:24.953: INFO: Pod "busybox-scheduling-a77dfcda-0787-4b00-a634-57afd0f454b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.01765444s
    Nov 30 04:12:24.954: INFO: The phase of Pod busybox-scheduling-a77dfcda-0787-4b00-a634-57afd0f454b4 is Running (Ready = true)
    Nov 30 04:12:24.954: INFO: Pod "busybox-scheduling-a77dfcda-0787-4b00-a634-57afd0f454b4" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Nov 30 04:12:24.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5301" for this suite. 11/30/22 04:12:24.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:12:24.974
Nov 30 04:12:24.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename cronjob 11/30/22 04:12:24.974
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:12:24.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:12:24.999
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 11/30/22 04:12:25.004
STEP: Ensuring more than one job is running at a time 11/30/22 04:12:25.011
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 11/30/22 04:14:01.015
STEP: Removing cronjob 11/30/22 04:14:01.018
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Nov 30 04:14:01.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4327" for this suite. 11/30/22 04:14:01.027
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":161,"skipped":2744,"failed":0}
------------------------------
• [SLOW TEST] [96.062 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:12:24.974
    Nov 30 04:12:24.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename cronjob 11/30/22 04:12:24.974
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:12:24.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:12:24.999
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 11/30/22 04:12:25.004
    STEP: Ensuring more than one job is running at a time 11/30/22 04:12:25.011
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 11/30/22 04:14:01.015
    STEP: Removing cronjob 11/30/22 04:14:01.018
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Nov 30 04:14:01.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4327" for this suite. 11/30/22 04:14:01.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:14:01.037
Nov 30 04:14:01.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename podtemplate 11/30/22 04:14:01.038
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:01.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:01.085
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Nov 30 04:14:01.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6309" for this suite. 11/30/22 04:14:01.12
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":162,"skipped":2787,"failed":0}
------------------------------
• [0.090 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:14:01.037
    Nov 30 04:14:01.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename podtemplate 11/30/22 04:14:01.038
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:01.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:01.085
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Nov 30 04:14:01.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-6309" for this suite. 11/30/22 04:14:01.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:14:01.128
Nov 30 04:14:01.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename endpointslice 11/30/22 04:14:01.128
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:01.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:01.148
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 11/30/22 04:14:01.15
STEP: getting /apis/discovery.k8s.io 11/30/22 04:14:01.152
STEP: getting /apis/discovery.k8s.iov1 11/30/22 04:14:01.153
STEP: creating 11/30/22 04:14:01.154
STEP: getting 11/30/22 04:14:01.168
STEP: listing 11/30/22 04:14:01.17
STEP: watching 11/30/22 04:14:01.173
Nov 30 04:14:01.173: INFO: starting watch
STEP: cluster-wide listing 11/30/22 04:14:01.174
STEP: cluster-wide watching 11/30/22 04:14:01.177
Nov 30 04:14:01.177: INFO: starting watch
STEP: patching 11/30/22 04:14:01.178
STEP: updating 11/30/22 04:14:01.187
Nov 30 04:14:01.192: INFO: waiting for watch events with expected annotations
Nov 30 04:14:01.192: INFO: saw patched and updated annotations
STEP: deleting 11/30/22 04:14:01.192
STEP: deleting a collection 11/30/22 04:14:01.201
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Nov 30 04:14:01.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8031" for this suite. 11/30/22 04:14:01.219
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":163,"skipped":2817,"failed":0}
------------------------------
• [0.096 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:14:01.128
    Nov 30 04:14:01.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename endpointslice 11/30/22 04:14:01.128
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:01.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:01.148
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 11/30/22 04:14:01.15
    STEP: getting /apis/discovery.k8s.io 11/30/22 04:14:01.152
    STEP: getting /apis/discovery.k8s.iov1 11/30/22 04:14:01.153
    STEP: creating 11/30/22 04:14:01.154
    STEP: getting 11/30/22 04:14:01.168
    STEP: listing 11/30/22 04:14:01.17
    STEP: watching 11/30/22 04:14:01.173
    Nov 30 04:14:01.173: INFO: starting watch
    STEP: cluster-wide listing 11/30/22 04:14:01.174
    STEP: cluster-wide watching 11/30/22 04:14:01.177
    Nov 30 04:14:01.177: INFO: starting watch
    STEP: patching 11/30/22 04:14:01.178
    STEP: updating 11/30/22 04:14:01.187
    Nov 30 04:14:01.192: INFO: waiting for watch events with expected annotations
    Nov 30 04:14:01.192: INFO: saw patched and updated annotations
    STEP: deleting 11/30/22 04:14:01.192
    STEP: deleting a collection 11/30/22 04:14:01.201
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Nov 30 04:14:01.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8031" for this suite. 11/30/22 04:14:01.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:14:01.225
Nov 30 04:14:01.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 04:14:01.226
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:01.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:01.245
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 04:14:01.27
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:14:01.647
STEP: Deploying the webhook pod 11/30/22 04:14:01.653
STEP: Wait for the deployment to be ready 11/30/22 04:14:01.67
Nov 30 04:14:01.678: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 04:14:03.687
STEP: Verifying the service has paired with the endpoint 11/30/22 04:14:03.7
Nov 30 04:14:04.700: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 11/30/22 04:14:04.704
STEP: Registering slow webhook via the AdmissionRegistration API 11/30/22 04:14:04.704
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 11/30/22 04:14:04.718
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 11/30/22 04:14:05.727
STEP: Registering slow webhook via the AdmissionRegistration API 11/30/22 04:14:05.728
STEP: Having no error when timeout is longer than webhook latency 11/30/22 04:14:06.753
STEP: Registering slow webhook via the AdmissionRegistration API 11/30/22 04:14:06.753
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 11/30/22 04:14:11.787
STEP: Registering slow webhook via the AdmissionRegistration API 11/30/22 04:14:11.788
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:14:16.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7925" for this suite. 11/30/22 04:14:16.818
STEP: Destroying namespace "webhook-7925-markers" for this suite. 11/30/22 04:14:16.823
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":164,"skipped":2836,"failed":0}
------------------------------
• [SLOW TEST] [15.682 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:14:01.225
    Nov 30 04:14:01.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 04:14:01.226
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:01.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:01.245
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 04:14:01.27
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:14:01.647
    STEP: Deploying the webhook pod 11/30/22 04:14:01.653
    STEP: Wait for the deployment to be ready 11/30/22 04:14:01.67
    Nov 30 04:14:01.678: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 04:14:03.687
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:14:03.7
    Nov 30 04:14:04.700: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 11/30/22 04:14:04.704
    STEP: Registering slow webhook via the AdmissionRegistration API 11/30/22 04:14:04.704
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 11/30/22 04:14:04.718
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 11/30/22 04:14:05.727
    STEP: Registering slow webhook via the AdmissionRegistration API 11/30/22 04:14:05.728
    STEP: Having no error when timeout is longer than webhook latency 11/30/22 04:14:06.753
    STEP: Registering slow webhook via the AdmissionRegistration API 11/30/22 04:14:06.753
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 11/30/22 04:14:11.787
    STEP: Registering slow webhook via the AdmissionRegistration API 11/30/22 04:14:11.788
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:14:16.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7925" for this suite. 11/30/22 04:14:16.818
    STEP: Destroying namespace "webhook-7925-markers" for this suite. 11/30/22 04:14:16.823
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:14:16.908
Nov 30 04:14:16.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:14:16.909
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:16.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:16.995
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 11/30/22 04:14:16.998
Nov 30 04:14:17.038: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad" in namespace "projected-520" to be "Succeeded or Failed"
Nov 30 04:14:17.042: INFO: Pod "downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.882283ms
Nov 30 04:14:19.046: INFO: Pod "downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad": Phase="Running", Reason="", readiness=false. Elapsed: 2.007718083s
Nov 30 04:14:21.047: INFO: Pod "downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008646527s
STEP: Saw pod success 11/30/22 04:14:21.047
Nov 30 04:14:21.047: INFO: Pod "downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad" satisfied condition "Succeeded or Failed"
Nov 30 04:14:21.049: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad container client-container: <nil>
STEP: delete the pod 11/30/22 04:14:21.056
Nov 30 04:14:21.068: INFO: Waiting for pod downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad to disappear
Nov 30 04:14:21.070: INFO: Pod downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Nov 30 04:14:21.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-520" for this suite. 11/30/22 04:14:21.074
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":165,"skipped":2852,"failed":0}
------------------------------
• [4.172 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:14:16.908
    Nov 30 04:14:16.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:14:16.909
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:16.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:16.995
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 11/30/22 04:14:16.998
    Nov 30 04:14:17.038: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad" in namespace "projected-520" to be "Succeeded or Failed"
    Nov 30 04:14:17.042: INFO: Pod "downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.882283ms
    Nov 30 04:14:19.046: INFO: Pod "downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad": Phase="Running", Reason="", readiness=false. Elapsed: 2.007718083s
    Nov 30 04:14:21.047: INFO: Pod "downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008646527s
    STEP: Saw pod success 11/30/22 04:14:21.047
    Nov 30 04:14:21.047: INFO: Pod "downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad" satisfied condition "Succeeded or Failed"
    Nov 30 04:14:21.049: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad container client-container: <nil>
    STEP: delete the pod 11/30/22 04:14:21.056
    Nov 30 04:14:21.068: INFO: Waiting for pod downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad to disappear
    Nov 30 04:14:21.070: INFO: Pod downwardapi-volume-2064c5d6-ad4e-4054-8595-6909965dbaad no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Nov 30 04:14:21.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-520" for this suite. 11/30/22 04:14:21.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:14:21.081
Nov 30 04:14:21.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename svcaccounts 11/30/22 04:14:21.082
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:21.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:21.102
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Nov 30 04:14:21.122: INFO: Waiting up to 5m0s for pod "pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4" in namespace "svcaccounts-3876" to be "running"
Nov 30 04:14:21.127: INFO: Pod "pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.697429ms
Nov 30 04:14:23.130: INFO: Pod "pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00834573s
Nov 30 04:14:23.130: INFO: Pod "pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4" satisfied condition "running"
STEP: reading a file in the container 11/30/22 04:14:23.13
Nov 30 04:14:23.130: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3876 pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 11/30/22 04:14:23.265
Nov 30 04:14:23.265: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3876 pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 11/30/22 04:14:23.403
Nov 30 04:14:23.403: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3876 pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Nov 30 04:14:23.557: INFO: Got root ca configmap in namespace "svcaccounts-3876"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Nov 30 04:14:23.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3876" for this suite. 11/30/22 04:14:23.563
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":166,"skipped":2920,"failed":0}
------------------------------
• [2.486 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:14:21.081
    Nov 30 04:14:21.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename svcaccounts 11/30/22 04:14:21.082
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:21.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:21.102
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Nov 30 04:14:21.122: INFO: Waiting up to 5m0s for pod "pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4" in namespace "svcaccounts-3876" to be "running"
    Nov 30 04:14:21.127: INFO: Pod "pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.697429ms
    Nov 30 04:14:23.130: INFO: Pod "pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00834573s
    Nov 30 04:14:23.130: INFO: Pod "pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4" satisfied condition "running"
    STEP: reading a file in the container 11/30/22 04:14:23.13
    Nov 30 04:14:23.130: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3876 pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 11/30/22 04:14:23.265
    Nov 30 04:14:23.265: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3876 pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 11/30/22 04:14:23.403
    Nov 30 04:14:23.403: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3876 pod-service-account-7c27ddda-3a98-4236-8ead-4263a71123e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Nov 30 04:14:23.557: INFO: Got root ca configmap in namespace "svcaccounts-3876"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Nov 30 04:14:23.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3876" for this suite. 11/30/22 04:14:23.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:14:23.568
Nov 30 04:14:23.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename statefulset 11/30/22 04:14:23.569
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:23.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:23.593
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5940 11/30/22 04:14:23.596
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Nov 30 04:14:23.613: INFO: Found 0 stateful pods, waiting for 1
Nov 30 04:14:33.617: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 11/30/22 04:14:33.622
W1130 04:14:33.634165      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Nov 30 04:14:33.639: INFO: Found 1 stateful pods, waiting for 2
Nov 30 04:14:43.643: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 30 04:14:43.643: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 11/30/22 04:14:43.648
STEP: Delete all of the StatefulSets 11/30/22 04:14:43.65
STEP: Verify that StatefulSets have been deleted 11/30/22 04:14:43.656
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Nov 30 04:14:43.659: INFO: Deleting all statefulset in ns statefulset-5940
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Nov 30 04:14:43.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5940" for this suite. 11/30/22 04:14:43.67
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":167,"skipped":2948,"failed":0}
------------------------------
• [SLOW TEST] [20.108 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:14:23.568
    Nov 30 04:14:23.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename statefulset 11/30/22 04:14:23.569
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:23.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:23.593
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5940 11/30/22 04:14:23.596
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Nov 30 04:14:23.613: INFO: Found 0 stateful pods, waiting for 1
    Nov 30 04:14:33.617: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 11/30/22 04:14:33.622
    W1130 04:14:33.634165      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Nov 30 04:14:33.639: INFO: Found 1 stateful pods, waiting for 2
    Nov 30 04:14:43.643: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 30 04:14:43.643: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 11/30/22 04:14:43.648
    STEP: Delete all of the StatefulSets 11/30/22 04:14:43.65
    STEP: Verify that StatefulSets have been deleted 11/30/22 04:14:43.656
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Nov 30 04:14:43.659: INFO: Deleting all statefulset in ns statefulset-5940
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Nov 30 04:14:43.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5940" for this suite. 11/30/22 04:14:43.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:14:43.677
Nov 30 04:14:43.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename deployment 11/30/22 04:14:43.678
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:43.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:43.702
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Nov 30 04:14:43.704: INFO: Creating deployment "test-recreate-deployment"
Nov 30 04:14:43.715: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 30 04:14:43.723: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 30 04:14:45.729: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 30 04:14:45.731: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 30 04:14:45.740: INFO: Updating deployment test-recreate-deployment
Nov 30 04:14:45.740: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 30 04:14:45.905: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1811  18041c11-850c-42fe-92cf-e7262b5bc0f3 42300 2 2022-11-30 04:14:43 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00540b578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-11-30 04:14:45 +0000 UTC,LastTransitionTime:2022-11-30 04:14:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-58957df4d6" is progressing.,LastUpdateTime:2022-11-30 04:14:45 +0000 UTC,LastTransitionTime:2022-11-30 04:14:43 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 30 04:14:45.908: INFO: New ReplicaSet "test-recreate-deployment-58957df4d6" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-58957df4d6  deployment-1811  a0054d98-3386-4140-9002-be4cc144234c 42299 1 2022-11-30 04:14:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:58957df4d6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 18041c11-850c-42fe-92cf-e7262b5bc0f3 0xc003d25f57 0xc003d25f58}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18041c11-850c-42fe-92cf-e7262b5bc0f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 58957df4d6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:58957df4d6] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d25ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 30 04:14:45.908: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 30 04:14:45.908: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-54dcf7499b  deployment-1811  e1039546-d089-40e1-94bf-b1741ff480c1 42288 2 2022-11-30 04:14:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54dcf7499b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 18041c11-850c-42fe-92cf-e7262b5bc0f3 0xc003d25e37 0xc003d25e38}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18041c11-850c-42fe-92cf-e7262b5bc0f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54dcf7499b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54dcf7499b] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d25ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 30 04:14:45.910: INFO: Pod "test-recreate-deployment-58957df4d6-glpxn" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-58957df4d6-glpxn test-recreate-deployment-58957df4d6- deployment-1811  f4f339d3-5670-4841-8223-afb46caa5b59 42298 0 2022-11-30 04:14:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:58957df4d6] map[] [{apps/v1 ReplicaSet test-recreate-deployment-58957df4d6 a0054d98-3386-4140-9002-be4cc144234c 0xc002ac58a7 0xc002ac58a8}] [] [{kube-controller-manager Update v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0054d98-3386-4140-9002-be4cc144234c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqczv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqczv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:14:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:14:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:14:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:14:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:,StartTime:2022-11-30 04:14:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Nov 30 04:14:45.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1811" for this suite. 11/30/22 04:14:45.914
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":168,"skipped":2959,"failed":0}
------------------------------
• [2.242 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:14:43.677
    Nov 30 04:14:43.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename deployment 11/30/22 04:14:43.678
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:43.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:43.702
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Nov 30 04:14:43.704: INFO: Creating deployment "test-recreate-deployment"
    Nov 30 04:14:43.715: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Nov 30 04:14:43.723: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Nov 30 04:14:45.729: INFO: Waiting deployment "test-recreate-deployment" to complete
    Nov 30 04:14:45.731: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Nov 30 04:14:45.740: INFO: Updating deployment test-recreate-deployment
    Nov 30 04:14:45.740: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 30 04:14:45.905: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1811  18041c11-850c-42fe-92cf-e7262b5bc0f3 42300 2 2022-11-30 04:14:43 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00540b578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-11-30 04:14:45 +0000 UTC,LastTransitionTime:2022-11-30 04:14:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-58957df4d6" is progressing.,LastUpdateTime:2022-11-30 04:14:45 +0000 UTC,LastTransitionTime:2022-11-30 04:14:43 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Nov 30 04:14:45.908: INFO: New ReplicaSet "test-recreate-deployment-58957df4d6" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-58957df4d6  deployment-1811  a0054d98-3386-4140-9002-be4cc144234c 42299 1 2022-11-30 04:14:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:58957df4d6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 18041c11-850c-42fe-92cf-e7262b5bc0f3 0xc003d25f57 0xc003d25f58}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18041c11-850c-42fe-92cf-e7262b5bc0f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 58957df4d6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:58957df4d6] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d25ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 30 04:14:45.908: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Nov 30 04:14:45.908: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-54dcf7499b  deployment-1811  e1039546-d089-40e1-94bf-b1741ff480c1 42288 2 2022-11-30 04:14:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54dcf7499b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 18041c11-850c-42fe-92cf-e7262b5bc0f3 0xc003d25e37 0xc003d25e38}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18041c11-850c-42fe-92cf-e7262b5bc0f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54dcf7499b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54dcf7499b] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d25ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 30 04:14:45.910: INFO: Pod "test-recreate-deployment-58957df4d6-glpxn" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-58957df4d6-glpxn test-recreate-deployment-58957df4d6- deployment-1811  f4f339d3-5670-4841-8223-afb46caa5b59 42298 0 2022-11-30 04:14:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:58957df4d6] map[] [{apps/v1 ReplicaSet test-recreate-deployment-58957df4d6 a0054d98-3386-4140-9002-be4cc144234c 0xc002ac58a7 0xc002ac58a8}] [] [{kube-controller-manager Update v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0054d98-3386-4140-9002-be4cc144234c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:14:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqczv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqczv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:14:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:14:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:14:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:14:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:,StartTime:2022-11-30 04:14:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Nov 30 04:14:45.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1811" for this suite. 11/30/22 04:14:45.914
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:14:45.919
Nov 30 04:14:45.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 04:14:45.919
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:45.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:45.942
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-7258 11/30/22 04:14:45.945
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7258 to expose endpoints map[] 11/30/22 04:14:45.958
Nov 30 04:14:45.961: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Nov 30 04:14:46.969: INFO: successfully validated that service endpoint-test2 in namespace services-7258 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7258 11/30/22 04:14:46.969
Nov 30 04:14:46.998: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7258" to be "running and ready"
Nov 30 04:14:47.001: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.881614ms
Nov 30 04:14:47.001: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:14:49.004: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006170057s
Nov 30 04:14:49.004: INFO: The phase of Pod pod1 is Running (Ready = true)
Nov 30 04:14:49.004: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7258 to expose endpoints map[pod1:[80]] 11/30/22 04:14:49.021
Nov 30 04:14:49.041: INFO: successfully validated that service endpoint-test2 in namespace services-7258 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 11/30/22 04:14:49.041
Nov 30 04:14:49.041: INFO: Creating new exec pod
Nov 30 04:14:49.048: INFO: Waiting up to 5m0s for pod "execpodnrdcz" in namespace "services-7258" to be "running"
Nov 30 04:14:49.052: INFO: Pod "execpodnrdcz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.418172ms
Nov 30 04:14:51.056: INFO: Pod "execpodnrdcz": Phase="Running", Reason="", readiness=true. Elapsed: 2.007591818s
Nov 30 04:14:51.056: INFO: Pod "execpodnrdcz" satisfied condition "running"
Nov 30 04:14:52.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7258 exec execpodnrdcz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Nov 30 04:14:52.175: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 30 04:14:52.175: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 04:14:52.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7258 exec execpodnrdcz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.54.191 80'
Nov 30 04:14:52.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.54.191 80\nConnection to 10.103.54.191 80 port [tcp/http] succeeded!\n"
Nov 30 04:14:52.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-7258 11/30/22 04:14:52.329
Nov 30 04:14:52.336: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7258" to be "running and ready"
Nov 30 04:14:52.341: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.385536ms
Nov 30 04:14:52.341: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:14:54.344: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007849518s
Nov 30 04:14:54.344: INFO: The phase of Pod pod2 is Running (Ready = true)
Nov 30 04:14:54.344: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7258 to expose endpoints map[pod1:[80] pod2:[80]] 11/30/22 04:14:54.347
Nov 30 04:14:54.356: INFO: successfully validated that service endpoint-test2 in namespace services-7258 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 11/30/22 04:14:54.356
Nov 30 04:14:55.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7258 exec execpodnrdcz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Nov 30 04:14:55.484: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 30 04:14:55.484: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 04:14:55.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7258 exec execpodnrdcz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.54.191 80'
Nov 30 04:14:55.674: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.54.191 80\nConnection to 10.103.54.191 80 port [tcp/http] succeeded!\n"
Nov 30 04:14:55.674: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7258 11/30/22 04:14:55.674
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7258 to expose endpoints map[pod2:[80]] 11/30/22 04:14:55.696
Nov 30 04:14:56.722: INFO: successfully validated that service endpoint-test2 in namespace services-7258 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 11/30/22 04:14:56.722
Nov 30 04:14:57.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7258 exec execpodnrdcz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Nov 30 04:14:57.866: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 30 04:14:57.866: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 04:14:57.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7258 exec execpodnrdcz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.54.191 80'
Nov 30 04:14:57.999: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 10.103.54.191 80\nConnection to 10.103.54.191 80 port [tcp/http] succeeded!\n"
Nov 30 04:14:57.999: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-7258 11/30/22 04:14:57.999
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7258 to expose endpoints map[] 11/30/22 04:14:58.018
Nov 30 04:14:58.028: INFO: successfully validated that service endpoint-test2 in namespace services-7258 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 04:14:58.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7258" for this suite. 11/30/22 04:14:58.059
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":169,"skipped":2961,"failed":0}
------------------------------
• [SLOW TEST] [12.150 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:14:45.919
    Nov 30 04:14:45.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 04:14:45.919
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:45.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:45.942
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-7258 11/30/22 04:14:45.945
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7258 to expose endpoints map[] 11/30/22 04:14:45.958
    Nov 30 04:14:45.961: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Nov 30 04:14:46.969: INFO: successfully validated that service endpoint-test2 in namespace services-7258 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7258 11/30/22 04:14:46.969
    Nov 30 04:14:46.998: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7258" to be "running and ready"
    Nov 30 04:14:47.001: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.881614ms
    Nov 30 04:14:47.001: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:14:49.004: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006170057s
    Nov 30 04:14:49.004: INFO: The phase of Pod pod1 is Running (Ready = true)
    Nov 30 04:14:49.004: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7258 to expose endpoints map[pod1:[80]] 11/30/22 04:14:49.021
    Nov 30 04:14:49.041: INFO: successfully validated that service endpoint-test2 in namespace services-7258 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 11/30/22 04:14:49.041
    Nov 30 04:14:49.041: INFO: Creating new exec pod
    Nov 30 04:14:49.048: INFO: Waiting up to 5m0s for pod "execpodnrdcz" in namespace "services-7258" to be "running"
    Nov 30 04:14:49.052: INFO: Pod "execpodnrdcz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.418172ms
    Nov 30 04:14:51.056: INFO: Pod "execpodnrdcz": Phase="Running", Reason="", readiness=true. Elapsed: 2.007591818s
    Nov 30 04:14:51.056: INFO: Pod "execpodnrdcz" satisfied condition "running"
    Nov 30 04:14:52.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7258 exec execpodnrdcz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Nov 30 04:14:52.175: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Nov 30 04:14:52.175: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 04:14:52.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7258 exec execpodnrdcz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.54.191 80'
    Nov 30 04:14:52.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.54.191 80\nConnection to 10.103.54.191 80 port [tcp/http] succeeded!\n"
    Nov 30 04:14:52.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-7258 11/30/22 04:14:52.329
    Nov 30 04:14:52.336: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7258" to be "running and ready"
    Nov 30 04:14:52.341: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.385536ms
    Nov 30 04:14:52.341: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:14:54.344: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007849518s
    Nov 30 04:14:54.344: INFO: The phase of Pod pod2 is Running (Ready = true)
    Nov 30 04:14:54.344: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7258 to expose endpoints map[pod1:[80] pod2:[80]] 11/30/22 04:14:54.347
    Nov 30 04:14:54.356: INFO: successfully validated that service endpoint-test2 in namespace services-7258 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 11/30/22 04:14:54.356
    Nov 30 04:14:55.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7258 exec execpodnrdcz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Nov 30 04:14:55.484: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Nov 30 04:14:55.484: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 04:14:55.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7258 exec execpodnrdcz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.54.191 80'
    Nov 30 04:14:55.674: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.54.191 80\nConnection to 10.103.54.191 80 port [tcp/http] succeeded!\n"
    Nov 30 04:14:55.674: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-7258 11/30/22 04:14:55.674
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7258 to expose endpoints map[pod2:[80]] 11/30/22 04:14:55.696
    Nov 30 04:14:56.722: INFO: successfully validated that service endpoint-test2 in namespace services-7258 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 11/30/22 04:14:56.722
    Nov 30 04:14:57.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7258 exec execpodnrdcz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Nov 30 04:14:57.866: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Nov 30 04:14:57.866: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 04:14:57.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7258 exec execpodnrdcz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.54.191 80'
    Nov 30 04:14:57.999: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 10.103.54.191 80\nConnection to 10.103.54.191 80 port [tcp/http] succeeded!\n"
    Nov 30 04:14:57.999: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-7258 11/30/22 04:14:57.999
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7258 to expose endpoints map[] 11/30/22 04:14:58.018
    Nov 30 04:14:58.028: INFO: successfully validated that service endpoint-test2 in namespace services-7258 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 04:14:58.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7258" for this suite. 11/30/22 04:14:58.059
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:14:58.069
Nov 30 04:14:58.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename var-expansion 11/30/22 04:14:58.07
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:58.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:58.096
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 11/30/22 04:14:58.099
Nov 30 04:14:58.136: INFO: Waiting up to 5m0s for pod "var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad" in namespace "var-expansion-7182" to be "Succeeded or Failed"
Nov 30 04:14:58.149: INFO: Pod "var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad": Phase="Pending", Reason="", readiness=false. Elapsed: 13.360022ms
Nov 30 04:15:00.153: INFO: Pod "var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017322186s
Nov 30 04:15:02.153: INFO: Pod "var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017342398s
STEP: Saw pod success 11/30/22 04:15:02.153
Nov 30 04:15:02.153: INFO: Pod "var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad" satisfied condition "Succeeded or Failed"
Nov 30 04:15:02.156: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad container dapi-container: <nil>
STEP: delete the pod 11/30/22 04:15:02.162
Nov 30 04:15:02.176: INFO: Waiting for pod var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad to disappear
Nov 30 04:15:02.179: INFO: Pod var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Nov 30 04:15:02.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7182" for this suite. 11/30/22 04:15:02.183
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":170,"skipped":2968,"failed":0}
------------------------------
• [4.122 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:14:58.069
    Nov 30 04:14:58.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename var-expansion 11/30/22 04:14:58.07
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:14:58.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:14:58.096
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 11/30/22 04:14:58.099
    Nov 30 04:14:58.136: INFO: Waiting up to 5m0s for pod "var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad" in namespace "var-expansion-7182" to be "Succeeded or Failed"
    Nov 30 04:14:58.149: INFO: Pod "var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad": Phase="Pending", Reason="", readiness=false. Elapsed: 13.360022ms
    Nov 30 04:15:00.153: INFO: Pod "var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017322186s
    Nov 30 04:15:02.153: INFO: Pod "var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017342398s
    STEP: Saw pod success 11/30/22 04:15:02.153
    Nov 30 04:15:02.153: INFO: Pod "var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad" satisfied condition "Succeeded or Failed"
    Nov 30 04:15:02.156: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad container dapi-container: <nil>
    STEP: delete the pod 11/30/22 04:15:02.162
    Nov 30 04:15:02.176: INFO: Waiting for pod var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad to disappear
    Nov 30 04:15:02.179: INFO: Pod var-expansion-f1a9b014-da09-4f8d-8789-76e5fe1e99ad no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Nov 30 04:15:02.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7182" for this suite. 11/30/22 04:15:02.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:15:02.192
Nov 30 04:15:02.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename resourcequota 11/30/22 04:15:02.193
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:15:02.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:15:02.216
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 11/30/22 04:15:02.22
STEP: Creating a ResourceQuota 11/30/22 04:15:07.223
STEP: Ensuring resource quota status is calculated 11/30/22 04:15:07.228
STEP: Creating a Pod that fits quota 11/30/22 04:15:09.231
STEP: Ensuring ResourceQuota status captures the pod usage 11/30/22 04:15:09.272
STEP: Not allowing a pod to be created that exceeds remaining quota 11/30/22 04:15:11.284
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 11/30/22 04:15:11.292
STEP: Ensuring a pod cannot update its resource requirements 11/30/22 04:15:11.296
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 11/30/22 04:15:11.3
STEP: Deleting the pod 11/30/22 04:15:13.304
STEP: Ensuring resource quota status released the pod usage 11/30/22 04:15:13.315
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Nov 30 04:15:15.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6640" for this suite. 11/30/22 04:15:15.325
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":171,"skipped":2977,"failed":0}
------------------------------
• [SLOW TEST] [13.138 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:15:02.192
    Nov 30 04:15:02.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename resourcequota 11/30/22 04:15:02.193
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:15:02.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:15:02.216
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 11/30/22 04:15:02.22
    STEP: Creating a ResourceQuota 11/30/22 04:15:07.223
    STEP: Ensuring resource quota status is calculated 11/30/22 04:15:07.228
    STEP: Creating a Pod that fits quota 11/30/22 04:15:09.231
    STEP: Ensuring ResourceQuota status captures the pod usage 11/30/22 04:15:09.272
    STEP: Not allowing a pod to be created that exceeds remaining quota 11/30/22 04:15:11.284
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 11/30/22 04:15:11.292
    STEP: Ensuring a pod cannot update its resource requirements 11/30/22 04:15:11.296
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 11/30/22 04:15:11.3
    STEP: Deleting the pod 11/30/22 04:15:13.304
    STEP: Ensuring resource quota status released the pod usage 11/30/22 04:15:13.315
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Nov 30 04:15:15.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6640" for this suite. 11/30/22 04:15:15.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:15:15.33
Nov 30 04:15:15.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename job 11/30/22 04:15:15.331
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:15:15.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:15:15.354
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 11/30/22 04:15:15.357
STEP: Ensuring job reaches completions 11/30/22 04:15:15.364
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Nov 30 04:15:25.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6417" for this suite. 11/30/22 04:15:25.374
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":172,"skipped":2985,"failed":0}
------------------------------
• [SLOW TEST] [10.050 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:15:15.33
    Nov 30 04:15:15.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename job 11/30/22 04:15:15.331
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:15:15.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:15:15.354
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 11/30/22 04:15:15.357
    STEP: Ensuring job reaches completions 11/30/22 04:15:15.364
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Nov 30 04:15:25.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6417" for this suite. 11/30/22 04:15:25.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:15:25.381
Nov 30 04:15:25.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 04:15:25.382
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:15:25.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:15:25.402
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 11/30/22 04:15:25.405
Nov 30 04:15:25.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-6411 run e2e-test-httpd-pod --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 30 04:15:25.522: INFO: stderr: ""
Nov 30 04:15:25.522: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 11/30/22 04:15:25.522
Nov 30 04:15:25.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-6411 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-2"}]}} --dry-run=server'
Nov 30 04:15:26.266: INFO: stderr: ""
Nov 30 04:15:26.266: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 11/30/22 04:15:26.266
Nov 30 04:15:26.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-6411 delete pods e2e-test-httpd-pod'
Nov 30 04:15:29.116: INFO: stderr: ""
Nov 30 04:15:29.116: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 04:15:29.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6411" for this suite. 11/30/22 04:15:29.121
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":173,"skipped":3001,"failed":0}
------------------------------
• [3.746 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:15:25.381
    Nov 30 04:15:25.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 04:15:25.382
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:15:25.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:15:25.402
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 11/30/22 04:15:25.405
    Nov 30 04:15:25.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-6411 run e2e-test-httpd-pod --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Nov 30 04:15:25.522: INFO: stderr: ""
    Nov 30 04:15:25.522: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 11/30/22 04:15:25.522
    Nov 30 04:15:25.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-6411 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-2"}]}} --dry-run=server'
    Nov 30 04:15:26.266: INFO: stderr: ""
    Nov 30 04:15:26.266: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 11/30/22 04:15:26.266
    Nov 30 04:15:26.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-6411 delete pods e2e-test-httpd-pod'
    Nov 30 04:15:29.116: INFO: stderr: ""
    Nov 30 04:15:29.116: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 04:15:29.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6411" for this suite. 11/30/22 04:15:29.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:15:29.128
Nov 30 04:15:29.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename statefulset 11/30/22 04:15:29.129
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:15:29.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:15:29.171
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4488 11/30/22 04:15:29.173
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 11/30/22 04:15:29.178
Nov 30 04:15:29.193: INFO: Found 0 stateful pods, waiting for 3
Nov 30 04:15:39.197: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 30 04:15:39.197: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 30 04:15:39.197: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 30 04:15:39.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-4488 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 30 04:15:39.426: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 30 04:15:39.426: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 30 04:15:39.426: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 to armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.39-2 11/30/22 04:15:49.44
Nov 30 04:15:49.461: INFO: Updating stateful set ss2
STEP: Creating a new revision 11/30/22 04:15:49.461
STEP: Updating Pods in reverse ordinal order 11/30/22 04:15:59.476
Nov 30 04:15:59.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-4488 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 30 04:15:59.611: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 30 04:15:59.611: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 30 04:15:59.611: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 11/30/22 04:16:19.629
Nov 30 04:16:19.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-4488 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 30 04:16:19.763: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 30 04:16:19.763: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 30 04:16:19.763: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 30 04:16:29.794: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 11/30/22 04:16:39.807
Nov 30 04:16:39.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-4488 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 30 04:16:39.930: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 30 04:16:39.930: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 30 04:16:39.930: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Nov 30 04:16:49.948: INFO: Deleting all statefulset in ns statefulset-4488
Nov 30 04:16:49.951: INFO: Scaling statefulset ss2 to 0
Nov 30 04:16:59.968: INFO: Waiting for statefulset status.replicas updated to 0
Nov 30 04:16:59.970: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Nov 30 04:16:59.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4488" for this suite. 11/30/22 04:16:59.983
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":174,"skipped":3021,"failed":0}
------------------------------
• [SLOW TEST] [90.863 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:15:29.128
    Nov 30 04:15:29.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename statefulset 11/30/22 04:15:29.129
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:15:29.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:15:29.171
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4488 11/30/22 04:15:29.173
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 11/30/22 04:15:29.178
    Nov 30 04:15:29.193: INFO: Found 0 stateful pods, waiting for 3
    Nov 30 04:15:39.197: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 30 04:15:39.197: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 30 04:15:39.197: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Nov 30 04:15:39.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-4488 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 30 04:15:39.426: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 30 04:15:39.426: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 30 04:15:39.426: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 to armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.39-2 11/30/22 04:15:49.44
    Nov 30 04:15:49.461: INFO: Updating stateful set ss2
    STEP: Creating a new revision 11/30/22 04:15:49.461
    STEP: Updating Pods in reverse ordinal order 11/30/22 04:15:59.476
    Nov 30 04:15:59.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-4488 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 30 04:15:59.611: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 30 04:15:59.611: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 30 04:15:59.611: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 11/30/22 04:16:19.629
    Nov 30 04:16:19.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-4488 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 30 04:16:19.763: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 30 04:16:19.763: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 30 04:16:19.763: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 30 04:16:29.794: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 11/30/22 04:16:39.807
    Nov 30 04:16:39.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-4488 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 30 04:16:39.930: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 30 04:16:39.930: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 30 04:16:39.930: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Nov 30 04:16:49.948: INFO: Deleting all statefulset in ns statefulset-4488
    Nov 30 04:16:49.951: INFO: Scaling statefulset ss2 to 0
    Nov 30 04:16:59.968: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 30 04:16:59.970: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Nov 30 04:16:59.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4488" for this suite. 11/30/22 04:16:59.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:16:59.992
Nov 30 04:16:59.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 04:16:59.992
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:17:00.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:17:00.012
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 11/30/22 04:17:00.063
Nov 30 04:17:00.089: INFO: Waiting up to 5m0s for pod "downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc" in namespace "downward-api-153" to be "Succeeded or Failed"
Nov 30 04:17:00.092: INFO: Pod "downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.970894ms
Nov 30 04:17:02.097: INFO: Pod "downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007623458s
Nov 30 04:17:04.098: INFO: Pod "downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00848113s
STEP: Saw pod success 11/30/22 04:17:04.098
Nov 30 04:17:04.098: INFO: Pod "downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc" satisfied condition "Succeeded or Failed"
Nov 30 04:17:04.101: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc container dapi-container: <nil>
STEP: delete the pod 11/30/22 04:17:04.108
Nov 30 04:17:04.128: INFO: Waiting for pod downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc to disappear
Nov 30 04:17:04.131: INFO: Pod downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Nov 30 04:17:04.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-153" for this suite. 11/30/22 04:17:04.136
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":175,"skipped":3026,"failed":0}
------------------------------
• [4.152 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:16:59.992
    Nov 30 04:16:59.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 04:16:59.992
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:17:00.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:17:00.012
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 11/30/22 04:17:00.063
    Nov 30 04:17:00.089: INFO: Waiting up to 5m0s for pod "downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc" in namespace "downward-api-153" to be "Succeeded or Failed"
    Nov 30 04:17:00.092: INFO: Pod "downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.970894ms
    Nov 30 04:17:02.097: INFO: Pod "downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007623458s
    Nov 30 04:17:04.098: INFO: Pod "downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00848113s
    STEP: Saw pod success 11/30/22 04:17:04.098
    Nov 30 04:17:04.098: INFO: Pod "downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc" satisfied condition "Succeeded or Failed"
    Nov 30 04:17:04.101: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc container dapi-container: <nil>
    STEP: delete the pod 11/30/22 04:17:04.108
    Nov 30 04:17:04.128: INFO: Waiting for pod downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc to disappear
    Nov 30 04:17:04.131: INFO: Pod downward-api-a21ee334-0dce-4954-b8ef-2993bcb9f0bc no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Nov 30 04:17:04.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-153" for this suite. 11/30/22 04:17:04.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:17:04.144
Nov 30 04:17:04.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename events 11/30/22 04:17:04.145
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:17:04.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:17:04.177
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 11/30/22 04:17:04.18
STEP: listing events in all namespaces 11/30/22 04:17:04.191
STEP: listing events in test namespace 11/30/22 04:17:04.2
STEP: listing events with field selection filtering on source 11/30/22 04:17:04.203
STEP: listing events with field selection filtering on reportingController 11/30/22 04:17:04.206
STEP: getting the test event 11/30/22 04:17:04.209
STEP: patching the test event 11/30/22 04:17:04.212
STEP: getting the test event 11/30/22 04:17:04.222
STEP: updating the test event 11/30/22 04:17:04.226
STEP: getting the test event 11/30/22 04:17:04.232
STEP: deleting the test event 11/30/22 04:17:04.234
STEP: listing events in all namespaces 11/30/22 04:17:04.241
STEP: listing events in test namespace 11/30/22 04:17:04.248
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Nov 30 04:17:04.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2968" for this suite. 11/30/22 04:17:04.258
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":176,"skipped":3065,"failed":0}
------------------------------
• [0.120 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:17:04.144
    Nov 30 04:17:04.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename events 11/30/22 04:17:04.145
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:17:04.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:17:04.177
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 11/30/22 04:17:04.18
    STEP: listing events in all namespaces 11/30/22 04:17:04.191
    STEP: listing events in test namespace 11/30/22 04:17:04.2
    STEP: listing events with field selection filtering on source 11/30/22 04:17:04.203
    STEP: listing events with field selection filtering on reportingController 11/30/22 04:17:04.206
    STEP: getting the test event 11/30/22 04:17:04.209
    STEP: patching the test event 11/30/22 04:17:04.212
    STEP: getting the test event 11/30/22 04:17:04.222
    STEP: updating the test event 11/30/22 04:17:04.226
    STEP: getting the test event 11/30/22 04:17:04.232
    STEP: deleting the test event 11/30/22 04:17:04.234
    STEP: listing events in all namespaces 11/30/22 04:17:04.241
    STEP: listing events in test namespace 11/30/22 04:17:04.248
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Nov 30 04:17:04.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-2968" for this suite. 11/30/22 04:17:04.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:17:04.265
Nov 30 04:17:04.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 04:17:04.265
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:17:04.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:17:04.282
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 11/30/22 04:17:04.285
Nov 30 04:17:04.299: INFO: Waiting up to 5m0s for pod "pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4" in namespace "emptydir-2148" to be "Succeeded or Failed"
Nov 30 04:17:04.302: INFO: Pod "pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.556148ms
Nov 30 04:17:06.307: INFO: Pod "pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007556974s
Nov 30 04:17:08.306: INFO: Pod "pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006343231s
STEP: Saw pod success 11/30/22 04:17:08.306
Nov 30 04:17:08.306: INFO: Pod "pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4" satisfied condition "Succeeded or Failed"
Nov 30 04:17:08.308: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4 container test-container: <nil>
STEP: delete the pod 11/30/22 04:17:08.313
Nov 30 04:17:08.326: INFO: Waiting for pod pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4 to disappear
Nov 30 04:17:08.329: INFO: Pod pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 04:17:08.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2148" for this suite. 11/30/22 04:17:08.333
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":177,"skipped":3083,"failed":0}
------------------------------
• [4.074 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:17:04.265
    Nov 30 04:17:04.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 04:17:04.265
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:17:04.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:17:04.282
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 11/30/22 04:17:04.285
    Nov 30 04:17:04.299: INFO: Waiting up to 5m0s for pod "pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4" in namespace "emptydir-2148" to be "Succeeded or Failed"
    Nov 30 04:17:04.302: INFO: Pod "pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.556148ms
    Nov 30 04:17:06.307: INFO: Pod "pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007556974s
    Nov 30 04:17:08.306: INFO: Pod "pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006343231s
    STEP: Saw pod success 11/30/22 04:17:08.306
    Nov 30 04:17:08.306: INFO: Pod "pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4" satisfied condition "Succeeded or Failed"
    Nov 30 04:17:08.308: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4 container test-container: <nil>
    STEP: delete the pod 11/30/22 04:17:08.313
    Nov 30 04:17:08.326: INFO: Waiting for pod pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4 to disappear
    Nov 30 04:17:08.329: INFO: Pod pod-8b01b02e-4e65-4ab9-91c8-35e992c792c4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 04:17:08.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2148" for this suite. 11/30/22 04:17:08.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:17:08.339
Nov 30 04:17:08.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename disruption 11/30/22 04:17:08.34
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:17:08.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:17:08.362
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 11/30/22 04:17:08.366
STEP: Waiting for the pdb to be processed 11/30/22 04:17:08.37
STEP: updating the pdb 11/30/22 04:17:10.385
STEP: Waiting for the pdb to be processed 11/30/22 04:17:10.397
STEP: patching the pdb 11/30/22 04:17:10.4
STEP: Waiting for the pdb to be processed 11/30/22 04:17:10.414
STEP: Waiting for the pdb to be deleted 11/30/22 04:17:10.431
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Nov 30 04:17:10.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2753" for this suite. 11/30/22 04:17:10.439
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":178,"skipped":3098,"failed":0}
------------------------------
• [2.109 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:17:08.339
    Nov 30 04:17:08.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename disruption 11/30/22 04:17:08.34
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:17:08.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:17:08.362
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 11/30/22 04:17:08.366
    STEP: Waiting for the pdb to be processed 11/30/22 04:17:08.37
    STEP: updating the pdb 11/30/22 04:17:10.385
    STEP: Waiting for the pdb to be processed 11/30/22 04:17:10.397
    STEP: patching the pdb 11/30/22 04:17:10.4
    STEP: Waiting for the pdb to be processed 11/30/22 04:17:10.414
    STEP: Waiting for the pdb to be deleted 11/30/22 04:17:10.431
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Nov 30 04:17:10.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2753" for this suite. 11/30/22 04:17:10.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:17:10.452
Nov 30 04:17:10.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename svcaccounts 11/30/22 04:17:10.452
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:17:10.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:17:10.476
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Nov 30 04:17:10.518: INFO: created pod
Nov 30 04:17:10.518: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5379" to be "Succeeded or Failed"
Nov 30 04:17:10.521: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.626421ms
Nov 30 04:17:12.525: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007084788s
Nov 30 04:17:14.524: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00583178s
STEP: Saw pod success 11/30/22 04:17:14.524
Nov 30 04:17:14.524: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Nov 30 04:17:44.524: INFO: polling logs
Nov 30 04:17:44.530: INFO: Pod logs: 
I1130 04:17:11.269382       1 log.go:195] OK: Got token
I1130 04:17:11.269421       1 log.go:195] validating with in-cluster discovery
I1130 04:17:11.269752       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I1130 04:17:11.269787       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5379:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1669782430, NotBefore:1669781830, IssuedAt:1669781830, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5379", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0f35609b-0c5c-42a0-af82-7f443d6027f7"}}}
I1130 04:17:11.281592       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I1130 04:17:11.287247       1 log.go:195] OK: Validated signature on JWT
I1130 04:17:11.287369       1 log.go:195] OK: Got valid claims from token!
I1130 04:17:11.287401       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5379:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1669782430, NotBefore:1669781830, IssuedAt:1669781830, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5379", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0f35609b-0c5c-42a0-af82-7f443d6027f7"}}}

Nov 30 04:17:44.530: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Nov 30 04:17:44.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5379" for this suite. 11/30/22 04:17:44.542
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":179,"skipped":3154,"failed":0}
------------------------------
• [SLOW TEST] [34.097 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:17:10.452
    Nov 30 04:17:10.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename svcaccounts 11/30/22 04:17:10.452
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:17:10.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:17:10.476
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Nov 30 04:17:10.518: INFO: created pod
    Nov 30 04:17:10.518: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5379" to be "Succeeded or Failed"
    Nov 30 04:17:10.521: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.626421ms
    Nov 30 04:17:12.525: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007084788s
    Nov 30 04:17:14.524: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00583178s
    STEP: Saw pod success 11/30/22 04:17:14.524
    Nov 30 04:17:14.524: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Nov 30 04:17:44.524: INFO: polling logs
    Nov 30 04:17:44.530: INFO: Pod logs: 
    I1130 04:17:11.269382       1 log.go:195] OK: Got token
    I1130 04:17:11.269421       1 log.go:195] validating with in-cluster discovery
    I1130 04:17:11.269752       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I1130 04:17:11.269787       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5379:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1669782430, NotBefore:1669781830, IssuedAt:1669781830, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5379", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0f35609b-0c5c-42a0-af82-7f443d6027f7"}}}
    I1130 04:17:11.281592       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I1130 04:17:11.287247       1 log.go:195] OK: Validated signature on JWT
    I1130 04:17:11.287369       1 log.go:195] OK: Got valid claims from token!
    I1130 04:17:11.287401       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5379:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1669782430, NotBefore:1669781830, IssuedAt:1669781830, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5379", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0f35609b-0c5c-42a0-af82-7f443d6027f7"}}}

    Nov 30 04:17:44.530: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Nov 30 04:17:44.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5379" for this suite. 11/30/22 04:17:44.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:17:44.549
Nov 30 04:17:44.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename sched-preemption 11/30/22 04:17:44.55
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:17:44.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:17:44.571
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Nov 30 04:17:44.590: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 30 04:18:44.653: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 11/30/22 04:18:44.656
Nov 30 04:18:44.700: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 30 04:18:44.711: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 30 04:18:44.731: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 30 04:18:44.741: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Nov 30 04:18:44.766: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Nov 30 04:18:44.782: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Nov 30 04:18:44.803: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Nov 30 04:18:44.815: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 11/30/22 04:18:44.815
Nov 30 04:18:44.815: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6061" to be "running"
Nov 30 04:18:44.828: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.467601ms
Nov 30 04:18:46.832: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016976298s
Nov 30 04:18:48.832: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016340728s
Nov 30 04:18:50.833: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018099084s
Nov 30 04:18:52.832: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.016456519s
Nov 30 04:18:52.832: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Nov 30 04:18:52.832: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
Nov 30 04:18:52.835: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.945259ms
Nov 30 04:18:52.835: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 30 04:18:52.835: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
Nov 30 04:18:52.838: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.3207ms
Nov 30 04:18:54.842: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007741478s
Nov 30 04:18:54.842: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 30 04:18:54.842: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
Nov 30 04:18:54.845: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.675855ms
Nov 30 04:18:54.845: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 30 04:18:54.845: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
Nov 30 04:18:54.848: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.415622ms
Nov 30 04:18:54.848: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 30 04:18:54.848: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
Nov 30 04:18:54.850: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.502079ms
Nov 30 04:18:54.850: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 30 04:18:54.850: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
Nov 30 04:18:54.853: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.777839ms
Nov 30 04:18:56.860: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009732401s
Nov 30 04:18:56.860: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 30 04:18:56.860: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
Nov 30 04:18:56.863: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.978362ms
Nov 30 04:18:56.863: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 11/30/22 04:18:56.863
Nov 30 04:18:56.874: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Nov 30 04:18:56.876: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368963ms
Nov 30 04:18:58.881: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007380493s
Nov 30 04:19:00.888: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.013740294s
Nov 30 04:19:00.888: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Nov 30 04:19:00.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6061" for this suite. 11/30/22 04:19:00.941
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":180,"skipped":3187,"failed":0}
------------------------------
• [SLOW TEST] [76.452 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:17:44.549
    Nov 30 04:17:44.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename sched-preemption 11/30/22 04:17:44.55
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:17:44.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:17:44.571
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Nov 30 04:17:44.590: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 30 04:18:44.653: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 11/30/22 04:18:44.656
    Nov 30 04:18:44.700: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Nov 30 04:18:44.711: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Nov 30 04:18:44.731: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Nov 30 04:18:44.741: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Nov 30 04:18:44.766: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Nov 30 04:18:44.782: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Nov 30 04:18:44.803: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Nov 30 04:18:44.815: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 11/30/22 04:18:44.815
    Nov 30 04:18:44.815: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6061" to be "running"
    Nov 30 04:18:44.828: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.467601ms
    Nov 30 04:18:46.832: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016976298s
    Nov 30 04:18:48.832: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016340728s
    Nov 30 04:18:50.833: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018099084s
    Nov 30 04:18:52.832: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.016456519s
    Nov 30 04:18:52.832: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Nov 30 04:18:52.832: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
    Nov 30 04:18:52.835: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.945259ms
    Nov 30 04:18:52.835: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 30 04:18:52.835: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
    Nov 30 04:18:52.838: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.3207ms
    Nov 30 04:18:54.842: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007741478s
    Nov 30 04:18:54.842: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 30 04:18:54.842: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
    Nov 30 04:18:54.845: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.675855ms
    Nov 30 04:18:54.845: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 30 04:18:54.845: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
    Nov 30 04:18:54.848: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.415622ms
    Nov 30 04:18:54.848: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 30 04:18:54.848: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
    Nov 30 04:18:54.850: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.502079ms
    Nov 30 04:18:54.850: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 30 04:18:54.850: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
    Nov 30 04:18:54.853: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.777839ms
    Nov 30 04:18:56.860: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009732401s
    Nov 30 04:18:56.860: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 30 04:18:56.860: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-6061" to be "running"
    Nov 30 04:18:56.863: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.978362ms
    Nov 30 04:18:56.863: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 11/30/22 04:18:56.863
    Nov 30 04:18:56.874: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Nov 30 04:18:56.876: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368963ms
    Nov 30 04:18:58.881: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007380493s
    Nov 30 04:19:00.888: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.013740294s
    Nov 30 04:19:00.888: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 04:19:00.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6061" for this suite. 11/30/22 04:19:00.941
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:19:01.002
Nov 30 04:19:01.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename endpointslice 11/30/22 04:19:01.003
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:19:01.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:19:01.029
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 11/30/22 04:19:06.145
STEP: referencing matching pods with named port 11/30/22 04:19:11.152
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 11/30/22 04:19:16.161
STEP: recreating EndpointSlices after they've been deleted 11/30/22 04:19:21.172
Nov 30 04:19:21.192: INFO: EndpointSlice for Service endpointslice-2484/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Nov 30 04:19:31.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2484" for this suite. 11/30/22 04:19:31.209
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":181,"skipped":3193,"failed":0}
------------------------------
• [SLOW TEST] [30.213 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:19:01.002
    Nov 30 04:19:01.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename endpointslice 11/30/22 04:19:01.003
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:19:01.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:19:01.029
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 11/30/22 04:19:06.145
    STEP: referencing matching pods with named port 11/30/22 04:19:11.152
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 11/30/22 04:19:16.161
    STEP: recreating EndpointSlices after they've been deleted 11/30/22 04:19:21.172
    Nov 30 04:19:21.192: INFO: EndpointSlice for Service endpointslice-2484/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Nov 30 04:19:31.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2484" for this suite. 11/30/22 04:19:31.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:19:31.216
Nov 30 04:19:31.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename subpath 11/30/22 04:19:31.216
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:19:31.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:19:31.238
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/30/22 04:19:31.24
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-bfqf 11/30/22 04:19:31.252
STEP: Creating a pod to test atomic-volume-subpath 11/30/22 04:19:31.252
Nov 30 04:19:31.289: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-bfqf" in namespace "subpath-8279" to be "Succeeded or Failed"
Nov 30 04:19:31.292: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48737ms
Nov 30 04:19:33.295: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006453848s
Nov 30 04:19:35.295: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 4.006535818s
Nov 30 04:19:37.296: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 6.007363314s
Nov 30 04:19:39.296: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 8.006812537s
Nov 30 04:19:41.297: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 10.008575808s
Nov 30 04:19:43.297: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 12.007811401s
Nov 30 04:19:45.295: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 14.006482864s
Nov 30 04:19:47.296: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 16.007468217s
Nov 30 04:19:49.297: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 18.007871602s
Nov 30 04:19:51.296: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 20.006666208s
Nov 30 04:19:53.297: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=false. Elapsed: 22.008268726s
Nov 30 04:19:55.304: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015601608s
STEP: Saw pod success 11/30/22 04:19:55.305
Nov 30 04:19:55.305: INFO: Pod "pod-subpath-test-downwardapi-bfqf" satisfied condition "Succeeded or Failed"
Nov 30 04:19:55.308: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-subpath-test-downwardapi-bfqf container test-container-subpath-downwardapi-bfqf: <nil>
STEP: delete the pod 11/30/22 04:19:55.32
Nov 30 04:19:55.339: INFO: Waiting for pod pod-subpath-test-downwardapi-bfqf to disappear
Nov 30 04:19:55.342: INFO: Pod pod-subpath-test-downwardapi-bfqf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-bfqf 11/30/22 04:19:55.342
Nov 30 04:19:55.342: INFO: Deleting pod "pod-subpath-test-downwardapi-bfqf" in namespace "subpath-8279"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Nov 30 04:19:55.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8279" for this suite. 11/30/22 04:19:55.35
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":182,"skipped":3203,"failed":0}
------------------------------
• [SLOW TEST] [24.143 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:19:31.216
    Nov 30 04:19:31.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename subpath 11/30/22 04:19:31.216
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:19:31.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:19:31.238
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/30/22 04:19:31.24
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-bfqf 11/30/22 04:19:31.252
    STEP: Creating a pod to test atomic-volume-subpath 11/30/22 04:19:31.252
    Nov 30 04:19:31.289: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-bfqf" in namespace "subpath-8279" to be "Succeeded or Failed"
    Nov 30 04:19:31.292: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48737ms
    Nov 30 04:19:33.295: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006453848s
    Nov 30 04:19:35.295: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 4.006535818s
    Nov 30 04:19:37.296: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 6.007363314s
    Nov 30 04:19:39.296: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 8.006812537s
    Nov 30 04:19:41.297: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 10.008575808s
    Nov 30 04:19:43.297: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 12.007811401s
    Nov 30 04:19:45.295: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 14.006482864s
    Nov 30 04:19:47.296: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 16.007468217s
    Nov 30 04:19:49.297: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 18.007871602s
    Nov 30 04:19:51.296: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=true. Elapsed: 20.006666208s
    Nov 30 04:19:53.297: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Running", Reason="", readiness=false. Elapsed: 22.008268726s
    Nov 30 04:19:55.304: INFO: Pod "pod-subpath-test-downwardapi-bfqf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015601608s
    STEP: Saw pod success 11/30/22 04:19:55.305
    Nov 30 04:19:55.305: INFO: Pod "pod-subpath-test-downwardapi-bfqf" satisfied condition "Succeeded or Failed"
    Nov 30 04:19:55.308: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-subpath-test-downwardapi-bfqf container test-container-subpath-downwardapi-bfqf: <nil>
    STEP: delete the pod 11/30/22 04:19:55.32
    Nov 30 04:19:55.339: INFO: Waiting for pod pod-subpath-test-downwardapi-bfqf to disappear
    Nov 30 04:19:55.342: INFO: Pod pod-subpath-test-downwardapi-bfqf no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-bfqf 11/30/22 04:19:55.342
    Nov 30 04:19:55.342: INFO: Deleting pod "pod-subpath-test-downwardapi-bfqf" in namespace "subpath-8279"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Nov 30 04:19:55.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8279" for this suite. 11/30/22 04:19:55.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:19:55.359
Nov 30 04:19:55.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:19:55.359
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:19:55.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:19:55.389
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-d1161018-bb0d-471f-b909-96d4ab2fc8c5 11/30/22 04:19:55.399
STEP: Creating secret with name s-test-opt-upd-698d8065-df3a-42c0-9878-7d334ec11a2f 11/30/22 04:19:55.405
STEP: Creating the pod 11/30/22 04:19:55.41
Nov 30 04:19:55.452: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2af2da6b-296f-495f-8d7a-6cdd611f3338" in namespace "projected-6602" to be "running and ready"
Nov 30 04:19:55.456: INFO: Pod "pod-projected-secrets-2af2da6b-296f-495f-8d7a-6cdd611f3338": Phase="Pending", Reason="", readiness=false. Elapsed: 3.480246ms
Nov 30 04:19:55.456: INFO: The phase of Pod pod-projected-secrets-2af2da6b-296f-495f-8d7a-6cdd611f3338 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:19:57.459: INFO: Pod "pod-projected-secrets-2af2da6b-296f-495f-8d7a-6cdd611f3338": Phase="Running", Reason="", readiness=true. Elapsed: 2.00737088s
Nov 30 04:19:57.459: INFO: The phase of Pod pod-projected-secrets-2af2da6b-296f-495f-8d7a-6cdd611f3338 is Running (Ready = true)
Nov 30 04:19:57.459: INFO: Pod "pod-projected-secrets-2af2da6b-296f-495f-8d7a-6cdd611f3338" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-d1161018-bb0d-471f-b909-96d4ab2fc8c5 11/30/22 04:19:57.476
STEP: Updating secret s-test-opt-upd-698d8065-df3a-42c0-9878-7d334ec11a2f 11/30/22 04:19:57.482
STEP: Creating secret with name s-test-opt-create-5838ec2b-e2d0-42fe-acaf-09781148b3bc 11/30/22 04:19:57.487
STEP: waiting to observe update in volume 11/30/22 04:19:57.492
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Nov 30 04:19:59.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6602" for this suite. 11/30/22 04:19:59.52
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":183,"skipped":3208,"failed":0}
------------------------------
• [4.166 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:19:55.359
    Nov 30 04:19:55.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:19:55.359
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:19:55.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:19:55.389
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-d1161018-bb0d-471f-b909-96d4ab2fc8c5 11/30/22 04:19:55.399
    STEP: Creating secret with name s-test-opt-upd-698d8065-df3a-42c0-9878-7d334ec11a2f 11/30/22 04:19:55.405
    STEP: Creating the pod 11/30/22 04:19:55.41
    Nov 30 04:19:55.452: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2af2da6b-296f-495f-8d7a-6cdd611f3338" in namespace "projected-6602" to be "running and ready"
    Nov 30 04:19:55.456: INFO: Pod "pod-projected-secrets-2af2da6b-296f-495f-8d7a-6cdd611f3338": Phase="Pending", Reason="", readiness=false. Elapsed: 3.480246ms
    Nov 30 04:19:55.456: INFO: The phase of Pod pod-projected-secrets-2af2da6b-296f-495f-8d7a-6cdd611f3338 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:19:57.459: INFO: Pod "pod-projected-secrets-2af2da6b-296f-495f-8d7a-6cdd611f3338": Phase="Running", Reason="", readiness=true. Elapsed: 2.00737088s
    Nov 30 04:19:57.459: INFO: The phase of Pod pod-projected-secrets-2af2da6b-296f-495f-8d7a-6cdd611f3338 is Running (Ready = true)
    Nov 30 04:19:57.459: INFO: Pod "pod-projected-secrets-2af2da6b-296f-495f-8d7a-6cdd611f3338" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-d1161018-bb0d-471f-b909-96d4ab2fc8c5 11/30/22 04:19:57.476
    STEP: Updating secret s-test-opt-upd-698d8065-df3a-42c0-9878-7d334ec11a2f 11/30/22 04:19:57.482
    STEP: Creating secret with name s-test-opt-create-5838ec2b-e2d0-42fe-acaf-09781148b3bc 11/30/22 04:19:57.487
    STEP: waiting to observe update in volume 11/30/22 04:19:57.492
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Nov 30 04:19:59.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6602" for this suite. 11/30/22 04:19:59.52
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:19:59.525
Nov 30 04:19:59.525: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:19:59.526
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:19:59.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:19:59.551
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-b25ee064-a82d-4a8f-8f03-f76a716cd017 11/30/22 04:19:59.553
STEP: Creating a pod to test consume configMaps 11/30/22 04:19:59.56
Nov 30 04:19:59.572: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f" in namespace "projected-9006" to be "Succeeded or Failed"
Nov 30 04:19:59.575: INFO: Pod "pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.8099ms
Nov 30 04:20:01.579: INFO: Pod "pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006115378s
Nov 30 04:20:03.579: INFO: Pod "pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006556281s
STEP: Saw pod success 11/30/22 04:20:03.579
Nov 30 04:20:03.579: INFO: Pod "pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f" satisfied condition "Succeeded or Failed"
Nov 30 04:20:03.582: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f container agnhost-container: <nil>
STEP: delete the pod 11/30/22 04:20:03.589
Nov 30 04:20:03.605: INFO: Waiting for pod pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f to disappear
Nov 30 04:20:03.608: INFO: Pod pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Nov 30 04:20:03.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9006" for this suite. 11/30/22 04:20:03.613
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":184,"skipped":3208,"failed":0}
------------------------------
• [4.096 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:19:59.525
    Nov 30 04:19:59.525: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:19:59.526
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:19:59.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:19:59.551
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-b25ee064-a82d-4a8f-8f03-f76a716cd017 11/30/22 04:19:59.553
    STEP: Creating a pod to test consume configMaps 11/30/22 04:19:59.56
    Nov 30 04:19:59.572: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f" in namespace "projected-9006" to be "Succeeded or Failed"
    Nov 30 04:19:59.575: INFO: Pod "pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.8099ms
    Nov 30 04:20:01.579: INFO: Pod "pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006115378s
    Nov 30 04:20:03.579: INFO: Pod "pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006556281s
    STEP: Saw pod success 11/30/22 04:20:03.579
    Nov 30 04:20:03.579: INFO: Pod "pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f" satisfied condition "Succeeded or Failed"
    Nov 30 04:20:03.582: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 04:20:03.589
    Nov 30 04:20:03.605: INFO: Waiting for pod pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f to disappear
    Nov 30 04:20:03.608: INFO: Pod pod-projected-configmaps-461ad733-d95f-47ea-888a-51e87713b94f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Nov 30 04:20:03.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9006" for this suite. 11/30/22 04:20:03.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:20:03.622
Nov 30 04:20:03.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename init-container 11/30/22 04:20:03.623
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:03.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:03.648
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 11/30/22 04:20:03.65
Nov 30 04:20:03.650: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Nov 30 04:20:08.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-812" for this suite. 11/30/22 04:20:08.848
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":185,"skipped":3257,"failed":0}
------------------------------
• [SLOW TEST] [5.235 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:20:03.622
    Nov 30 04:20:03.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename init-container 11/30/22 04:20:03.623
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:03.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:03.648
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 11/30/22 04:20:03.65
    Nov 30 04:20:03.650: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Nov 30 04:20:08.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-812" for this suite. 11/30/22 04:20:08.848
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:20:08.858
Nov 30 04:20:08.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 04:20:08.858
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:08.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:08.898
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-9518 11/30/22 04:20:08.901
STEP: creating replication controller nodeport-test in namespace services-9518 11/30/22 04:20:08.936
I1130 04:20:08.948624      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9518, replica count: 2
I1130 04:20:11.999239      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 30 04:20:11.999: INFO: Creating new exec pod
Nov 30 04:20:12.027: INFO: Waiting up to 5m0s for pod "execpod4mvhh" in namespace "services-9518" to be "running"
Nov 30 04:20:12.030: INFO: Pod "execpod4mvhh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.074019ms
Nov 30 04:20:14.034: INFO: Pod "execpod4mvhh": Phase="Running", Reason="", readiness=true. Elapsed: 2.007109799s
Nov 30 04:20:14.034: INFO: Pod "execpod4mvhh" satisfied condition "running"
Nov 30 04:20:15.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9518 exec execpod4mvhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Nov 30 04:20:15.187: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 30 04:20:15.187: INFO: stdout: "nodeport-test-kh4ft"
Nov 30 04:20:15.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9518 exec execpod4mvhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.145.32 80'
Nov 30 04:20:15.321: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.145.32 80\nConnection to 10.100.145.32 80 port [tcp/http] succeeded!\n"
Nov 30 04:20:15.321: INFO: stdout: "nodeport-test-97tdx"
Nov 30 04:20:15.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9518 exec execpod4mvhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.8 30112'
Nov 30 04:20:15.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.8 30112\nConnection to 10.0.10.8 30112 port [tcp/*] succeeded!\n"
Nov 30 04:20:15.440: INFO: stdout: "nodeport-test-kh4ft"
Nov 30 04:20:15.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9518 exec execpod4mvhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.2 30112'
Nov 30 04:20:15.579: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.2 30112\nConnection to 10.0.10.2 30112 port [tcp/*] succeeded!\n"
Nov 30 04:20:15.580: INFO: stdout: ""
Nov 30 04:20:16.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9518 exec execpod4mvhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.2 30112'
Nov 30 04:20:16.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.2 30112\nConnection to 10.0.10.2 30112 port [tcp/*] succeeded!\n"
Nov 30 04:20:16.707: INFO: stdout: ""
Nov 30 04:20:17.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9518 exec execpod4mvhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.2 30112'
Nov 30 04:20:17.730: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.2 30112\nConnection to 10.0.10.2 30112 port [tcp/*] succeeded!\n"
Nov 30 04:20:17.730: INFO: stdout: "nodeport-test-kh4ft"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 04:20:17.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9518" for this suite. 11/30/22 04:20:17.735
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":186,"skipped":3260,"failed":0}
------------------------------
• [SLOW TEST] [8.884 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:20:08.858
    Nov 30 04:20:08.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 04:20:08.858
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:08.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:08.898
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-9518 11/30/22 04:20:08.901
    STEP: creating replication controller nodeport-test in namespace services-9518 11/30/22 04:20:08.936
    I1130 04:20:08.948624      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9518, replica count: 2
    I1130 04:20:11.999239      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 30 04:20:11.999: INFO: Creating new exec pod
    Nov 30 04:20:12.027: INFO: Waiting up to 5m0s for pod "execpod4mvhh" in namespace "services-9518" to be "running"
    Nov 30 04:20:12.030: INFO: Pod "execpod4mvhh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.074019ms
    Nov 30 04:20:14.034: INFO: Pod "execpod4mvhh": Phase="Running", Reason="", readiness=true. Elapsed: 2.007109799s
    Nov 30 04:20:14.034: INFO: Pod "execpod4mvhh" satisfied condition "running"
    Nov 30 04:20:15.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9518 exec execpod4mvhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Nov 30 04:20:15.187: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Nov 30 04:20:15.187: INFO: stdout: "nodeport-test-kh4ft"
    Nov 30 04:20:15.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9518 exec execpod4mvhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.145.32 80'
    Nov 30 04:20:15.321: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.145.32 80\nConnection to 10.100.145.32 80 port [tcp/http] succeeded!\n"
    Nov 30 04:20:15.321: INFO: stdout: "nodeport-test-97tdx"
    Nov 30 04:20:15.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9518 exec execpod4mvhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.8 30112'
    Nov 30 04:20:15.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.8 30112\nConnection to 10.0.10.8 30112 port [tcp/*] succeeded!\n"
    Nov 30 04:20:15.440: INFO: stdout: "nodeport-test-kh4ft"
    Nov 30 04:20:15.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9518 exec execpod4mvhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.2 30112'
    Nov 30 04:20:15.579: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.2 30112\nConnection to 10.0.10.2 30112 port [tcp/*] succeeded!\n"
    Nov 30 04:20:15.580: INFO: stdout: ""
    Nov 30 04:20:16.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9518 exec execpod4mvhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.2 30112'
    Nov 30 04:20:16.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.2 30112\nConnection to 10.0.10.2 30112 port [tcp/*] succeeded!\n"
    Nov 30 04:20:16.707: INFO: stdout: ""
    Nov 30 04:20:17.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-9518 exec execpod4mvhh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.2 30112'
    Nov 30 04:20:17.730: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.2 30112\nConnection to 10.0.10.2 30112 port [tcp/*] succeeded!\n"
    Nov 30 04:20:17.730: INFO: stdout: "nodeport-test-kh4ft"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 04:20:17.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9518" for this suite. 11/30/22 04:20:17.735
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:20:17.747
Nov 30 04:20:17.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 04:20:17.748
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:17.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:17.775
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 11/30/22 04:20:17.777
Nov 30 04:20:17.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3944 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2'
Nov 30 04:20:17.883: INFO: stderr: ""
Nov 30 04:20:17.883: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 11/30/22 04:20:17.883
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Nov 30 04:20:17.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3944 delete pods e2e-test-httpd-pod'
Nov 30 04:20:19.879: INFO: stderr: ""
Nov 30 04:20:19.879: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 04:20:19.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3944" for this suite. 11/30/22 04:20:19.883
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":187,"skipped":3288,"failed":0}
------------------------------
• [2.143 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:20:17.747
    Nov 30 04:20:17.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 04:20:17.748
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:17.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:17.775
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 11/30/22 04:20:17.777
    Nov 30 04:20:17.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3944 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2'
    Nov 30 04:20:17.883: INFO: stderr: ""
    Nov 30 04:20:17.883: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 11/30/22 04:20:17.883
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Nov 30 04:20:17.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3944 delete pods e2e-test-httpd-pod'
    Nov 30 04:20:19.879: INFO: stderr: ""
    Nov 30 04:20:19.879: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 04:20:19.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3944" for this suite. 11/30/22 04:20:19.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:20:19.891
Nov 30 04:20:19.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename svcaccounts 11/30/22 04:20:19.892
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:19.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:19.909
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Nov 30 04:20:19.914: INFO: Got root ca configmap in namespace "svcaccounts-6699"
Nov 30 04:20:19.920: INFO: Deleted root ca configmap in namespace "svcaccounts-6699"
STEP: waiting for a new root ca configmap created 11/30/22 04:20:20.422
Nov 30 04:20:20.425: INFO: Recreated root ca configmap in namespace "svcaccounts-6699"
Nov 30 04:20:20.430: INFO: Updated root ca configmap in namespace "svcaccounts-6699"
STEP: waiting for the root ca configmap reconciled 11/30/22 04:20:20.93
Nov 30 04:20:20.933: INFO: Reconciled root ca configmap in namespace "svcaccounts-6699"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Nov 30 04:20:20.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6699" for this suite. 11/30/22 04:20:20.939
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":188,"skipped":3297,"failed":0}
------------------------------
• [1.055 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:20:19.891
    Nov 30 04:20:19.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename svcaccounts 11/30/22 04:20:19.892
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:19.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:19.909
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Nov 30 04:20:19.914: INFO: Got root ca configmap in namespace "svcaccounts-6699"
    Nov 30 04:20:19.920: INFO: Deleted root ca configmap in namespace "svcaccounts-6699"
    STEP: waiting for a new root ca configmap created 11/30/22 04:20:20.422
    Nov 30 04:20:20.425: INFO: Recreated root ca configmap in namespace "svcaccounts-6699"
    Nov 30 04:20:20.430: INFO: Updated root ca configmap in namespace "svcaccounts-6699"
    STEP: waiting for the root ca configmap reconciled 11/30/22 04:20:20.93
    Nov 30 04:20:20.933: INFO: Reconciled root ca configmap in namespace "svcaccounts-6699"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Nov 30 04:20:20.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6699" for this suite. 11/30/22 04:20:20.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:20:20.947
Nov 30 04:20:20.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename sched-pred 11/30/22 04:20:20.948
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:20.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:20.966
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Nov 30 04:20:20.968: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 30 04:20:20.982: INFO: Waiting for terminating namespaces to be deleted...
Nov 30 04:20:20.985: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins before test
Nov 30 04:20:20.996: INFO: calico-node-9bcg6 from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:20.996: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 04:20:20.996: INFO: csi-cinder-nodeplugin-hnpf8 from kube-system started at 2022-11-30 02:52:37 +0000 UTC (3 container statuses recorded)
Nov 30 04:20:20.996: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 04:20:20.996: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 04:20:20.996: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 04:20:20.996: INFO: eric-tm-external-connectivity-frontend-speaker-wdz6h from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:20.996: INFO: 	Container speaker ready: true, restart count 0
Nov 30 04:20:20.996: INFO: kube-multus-ds-amd64-nw2qp from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:20.996: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 04:20:20.996: INFO: kube-proxy-stdnr from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:20.996: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 30 04:20:20.996: INFO: kucero-qz2ld from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:20.996: INFO: 	Container kucero ready: true, restart count 0
Nov 30 04:20:20.996: INFO: node-local-dns-wvqkc from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:20.996: INFO: 	Container node-cache ready: true, restart count 1
Nov 30 04:20:20.996: INFO: eric-pm-node-exporter-rlfnb from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:20.996: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 04:20:20.996: INFO: node-cert-exporter-hn5jk from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:20.996: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 04:20:20.996: INFO: execpod4mvhh from services-9518 started at 2022-11-30 04:20:12 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:20.996: INFO: 	Container agnhost-container ready: true, restart count 0
Nov 30 04:20:20.996: INFO: nodeport-test-97tdx from services-9518 started at 2022-11-30 04:20:09 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:20.996: INFO: 	Container nodeport-test ready: true, restart count 0
Nov 30 04:20:20.996: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-6bvhh from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 04:20:20.996: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 04:20:20.996: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 30 04:20:20.996: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins before test
Nov 30 04:20:21.010: INFO: default-http-backend-6f4f64db57-dnh7m from ingress-nginx started at 2022-11-30 02:55:20 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container default-http-backend ready: true, restart count 0
Nov 30 04:20:21.010: INFO: nginx-ingress-controller-844dff9bc5-684fv from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Nov 30 04:20:21.010: INFO: calico-node-thlq4 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 04:20:21.010: INFO: calicoctl-c47c68f5-42nfq from kube-system started at 2022-11-30 03:07:12 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container calicoctl ready: true, restart count 0
Nov 30 04:20:21.010: INFO: csi-cinder-nodeplugin-5hx92 from kube-system started at 2022-11-30 02:52:16 +0000 UTC (3 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 04:20:21.010: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 04:20:21.010: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 04:20:21.010: INFO: eric-app-sys-info-handler-754d6dcc6b-5xw88 from kube-system started at 2022-11-30 03:02:36 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
Nov 30 04:20:21.010: INFO: eric-data-document-database-pg-0 from kube-system started at 2022-11-30 03:03:48 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
Nov 30 04:20:21.010: INFO: eric-tm-external-connectivity-frontend-controller-689dd9fdkths4 from kube-system started at 2022-11-30 03:57:29 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container controller ready: true, restart count 0
Nov 30 04:20:21.010: INFO: eric-tm-external-connectivity-frontend-speaker-vkmgp from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container speaker ready: true, restart count 0
Nov 30 04:20:21.010: INFO: kube-multus-ds-amd64-lhrwf from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 04:20:21.010: INFO: kube-proxy-g68qp from kube-system started at 2022-11-30 02:51:41 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container kube-proxy ready: true, restart count 1
Nov 30 04:20:21.010: INFO: kucero-29zhp from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container kucero ready: true, restart count 0
Nov 30 04:20:21.010: INFO: network-resources-injector-6dfc58d4f-rm8m2 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container webhook-server ready: true, restart count 0
Nov 30 04:20:21.010: INFO: node-local-dns-4fvm6 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container node-cache ready: true, restart count 0
Nov 30 04:20:21.010: INFO: eric-pm-alertmanager-76c454d9f7-d2tvr from monitoring started at 2022-11-30 03:57:29 +0000 UTC (2 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
Nov 30 04:20:21.010: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
Nov 30 04:20:21.010: INFO: eric-pm-kube-state-metrics-6696677569-fnnhj from monitoring started at 2022-11-30 02:58:51 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
Nov 30 04:20:21.010: INFO: eric-pm-node-exporter-dhtts from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 04:20:21.010: INFO: eric-pm-pushgateway-6db47dc9d7-fbx4n from monitoring started at 2022-11-30 02:59:19 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container pushgateway ready: true, restart count 0
Nov 30 04:20:21.010: INFO: eric-victoria-metrics-agent-c9f978858-cmz9r from monitoring started at 2022-11-30 02:57:48 +0000 UTC (2 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
Nov 30 04:20:21.010: INFO: 	Container vmagent-config-reload ready: true, restart count 0
Nov 30 04:20:21.010: INFO: node-cert-exporter-jbl7k from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 04:20:21.010: INFO: nodeport-test-kh4ft from services-9518 started at 2022-11-30 04:20:09 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container nodeport-test ready: true, restart count 0
Nov 30 04:20:21.010: INFO: sonobuoy-e2e-job-681d55e1c5a74dc7 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container e2e ready: true, restart count 0
Nov 30 04:20:21.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 04:20:21.010: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-nbjs4 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 04:20:21.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 04:20:21.010: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 30 04:20:21.010: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins before test
Nov 30 04:20:21.021: INFO: nginx-ingress-controller-844dff9bc5-7std7 from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Nov 30 04:20:21.021: INFO: calico-node-c6ckn from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 04:20:21.021: INFO: csi-cinder-nodeplugin-xvtjm from kube-system started at 2022-11-30 02:52:53 +0000 UTC (3 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 04:20:21.021: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 04:20:21.021: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 04:20:21.021: INFO: eric-lm-combined-server-license-server-client-d9b78bd6b-5r5rz from kube-system started at 2022-11-30 03:04:36 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Nov 30 04:20:21.021: INFO: eric-tm-external-connectivity-frontend-speaker-9k8gj from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container speaker ready: true, restart count 0
Nov 30 04:20:21.021: INFO: kube-multus-ds-amd64-rk7dk from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 04:20:21.021: INFO: kube-proxy-2zzdw from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 30 04:20:21.021: INFO: kucero-nds94 from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container kucero ready: true, restart count 0
Nov 30 04:20:21.021: INFO: node-local-dns-4bsnx from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container node-cache ready: true, restart count 1
Nov 30 04:20:21.021: INFO: eric-pm-node-exporter-xs7lt from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 04:20:21.021: INFO: eric-pm-server-utils-7585bb6b5d-7btvg from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
Nov 30 04:20:21.021: INFO: eric-victoria-metrics-alert-server-54c5c5474c-mhjjn from monitoring started at 2022-11-30 03:38:30 +0000 UTC (2 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
Nov 30 04:20:21.021: INFO: 	Container vmalert-config-reload ready: true, restart count 0
Nov 30 04:20:21.021: INFO: eric-victoria-metrics-cluster-vmselect-55c4b48dfb-lnx7j from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
Nov 30 04:20:21.021: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2022-11-30 02:57:08 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
Nov 30 04:20:21.021: INFO: node-cert-exporter-l77s4 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 04:20:21.021: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-88nb5 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 04:20:21.021: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 04:20:21.021: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 30 04:20:21.021: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins before test
Nov 30 04:20:21.033: INFO: calico-node-klj9m from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 04:20:21.033: INFO: ccd-license-consumer-5949d66498-mjgr8 from kube-system started at 2022-11-30 03:05:25 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Nov 30 04:20:21.033: INFO: csi-cinder-nodeplugin-sw4wz from kube-system started at 2022-11-30 02:52:31 +0000 UTC (3 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 04:20:21.033: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 04:20:21.033: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 04:20:21.033: INFO: eric-lcm-container-registry-registry-7f7856985-4rfb5 from kube-system started at 2022-11-30 02:56:13 +0000 UTC (3 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
Nov 30 04:20:21.033: INFO: 	Container registry ready: true, restart count 0
Nov 30 04:20:21.033: INFO: 	Container sidecar ready: true, restart count 0
Nov 30 04:20:21.033: INFO: eric-lm-combined-server-license-consumer-handler-75bfcf7bc2tbcb from kube-system started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Nov 30 04:20:21.033: INFO: eric-tm-external-connectivity-frontend-speaker-vmfpl from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container speaker ready: true, restart count 0
Nov 30 04:20:21.033: INFO: kube-multus-ds-amd64-cf4v4 from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 04:20:21.033: INFO: kube-proxy-6s7bb from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 30 04:20:21.033: INFO: kucero-5bwbh from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container kucero ready: true, restart count 0
Nov 30 04:20:21.033: INFO: metrics-server-7567d6784b-dw2c8 from kube-system started at 2022-11-30 03:01:29 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container metrics-server ready: true, restart count 0
Nov 30 04:20:21.033: INFO: network-resources-injector-6dfc58d4f-nnqj7 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container webhook-server ready: true, restart count 0
Nov 30 04:20:21.033: INFO: node-local-dns-4hp9c from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container node-cache ready: true, restart count 1
Nov 30 04:20:21.033: INFO: eric-pm-node-exporter-lmw6v from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 04:20:21.033: INFO: eric-victoria-metrics-cluster-vminsert-6c76f96c54-jsw5z from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
Nov 30 04:20:21.033: INFO: isp-logger-5b4d57f796-5ncwr from monitoring started at 2022-11-30 03:06:55 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container isp-logger ready: true, restart count 0
Nov 30 04:20:21.033: INFO: node-cert-exporter-v97f6 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 04:20:21.033: INFO: sonobuoy from sonobuoy started at 2022-11-30 03:19:54 +0000 UTC (1 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 30 04:20:21.033: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-z79zw from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 04:20:21.033: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 04:20:21.033: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins 11/30/22 04:20:21.065
STEP: verifying the node has the label node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins 11/30/22 04:20:21.083
STEP: verifying the node has the label node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins 11/30/22 04:20:21.098
STEP: verifying the node has the label node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins 11/30/22 04:20:21.115
Nov 30 04:20:21.144: INFO: Pod default-http-backend-6f4f64db57-dnh7m requesting resource cpu=10m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod nginx-ingress-controller-844dff9bc5-684fv requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod nginx-ingress-controller-844dff9bc5-7std7 requesting resource cpu=0m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod calico-node-9bcg6 requesting resource cpu=250m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod calico-node-c6ckn requesting resource cpu=250m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod calico-node-klj9m requesting resource cpu=250m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod calico-node-thlq4 requesting resource cpu=250m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod calicoctl-c47c68f5-42nfq requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod ccd-license-consumer-5949d66498-mjgr8 requesting resource cpu=10m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod csi-cinder-nodeplugin-5hx92 requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod csi-cinder-nodeplugin-hnpf8 requesting resource cpu=0m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod csi-cinder-nodeplugin-sw4wz requesting resource cpu=0m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod csi-cinder-nodeplugin-xvtjm requesting resource cpu=0m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-app-sys-info-handler-754d6dcc6b-5xw88 requesting resource cpu=50m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-data-document-database-pg-0 requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-lcm-container-registry-registry-7f7856985-4rfb5 requesting resource cpu=400m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-lm-combined-server-license-consumer-handler-75bfcf7bc2tbcb requesting resource cpu=100m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-lm-combined-server-license-server-client-d9b78bd6b-5r5rz requesting resource cpu=100m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-tm-external-connectivity-frontend-controller-689dd9fdkths4 requesting resource cpu=50m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-tm-external-connectivity-frontend-speaker-9k8gj requesting resource cpu=50m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-tm-external-connectivity-frontend-speaker-vkmgp requesting resource cpu=50m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-tm-external-connectivity-frontend-speaker-vmfpl requesting resource cpu=50m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-tm-external-connectivity-frontend-speaker-wdz6h requesting resource cpu=50m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod kube-multus-ds-amd64-cf4v4 requesting resource cpu=100m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod kube-multus-ds-amd64-lhrwf requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod kube-multus-ds-amd64-nw2qp requesting resource cpu=100m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod kube-multus-ds-amd64-rk7dk requesting resource cpu=100m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod kube-proxy-2zzdw requesting resource cpu=0m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod kube-proxy-6s7bb requesting resource cpu=0m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod kube-proxy-g68qp requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod kube-proxy-stdnr requesting resource cpu=0m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod kucero-29zhp requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod kucero-5bwbh requesting resource cpu=100m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod kucero-nds94 requesting resource cpu=100m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod kucero-qz2ld requesting resource cpu=100m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod metrics-server-7567d6784b-dw2c8 requesting resource cpu=0m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod network-resources-injector-6dfc58d4f-nnqj7 requesting resource cpu=250m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod network-resources-injector-6dfc58d4f-rm8m2 requesting resource cpu=250m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod node-local-dns-4bsnx requesting resource cpu=25m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod node-local-dns-4fvm6 requesting resource cpu=25m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod node-local-dns-4hp9c requesting resource cpu=25m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod node-local-dns-wvqkc requesting resource cpu=25m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-pm-alertmanager-76c454d9f7-d2tvr requesting resource cpu=110m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-pm-kube-state-metrics-6696677569-fnnhj requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-pm-node-exporter-dhtts requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-pm-node-exporter-lmw6v requesting resource cpu=100m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-pm-node-exporter-rlfnb requesting resource cpu=100m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-pm-node-exporter-xs7lt requesting resource cpu=100m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-pm-pushgateway-6db47dc9d7-fbx4n requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-pm-server-utils-7585bb6b5d-7btvg requesting resource cpu=0m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-victoria-metrics-agent-c9f978858-cmz9r requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-victoria-metrics-alert-server-54c5c5474c-mhjjn requesting resource cpu=100m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-victoria-metrics-cluster-vminsert-6c76f96c54-jsw5z requesting resource cpu=50m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-victoria-metrics-cluster-vmselect-55c4b48dfb-lnx7j requesting resource cpu=50m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod eric-victoria-metrics-cluster-vmstorage-0 requesting resource cpu=500m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod isp-logger-5b4d57f796-5ncwr requesting resource cpu=0m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod node-cert-exporter-hn5jk requesting resource cpu=100m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod node-cert-exporter-jbl7k requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod node-cert-exporter-l77s4 requesting resource cpu=100m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod node-cert-exporter-v97f6 requesting resource cpu=100m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod execpod4mvhh requesting resource cpu=0m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod nodeport-test-97tdx requesting resource cpu=0m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod nodeport-test-kh4ft requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod sonobuoy-e2e-job-681d55e1c5a74dc7 requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-6bvhh requesting resource cpu=0m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-88nb5 requesting resource cpu=0m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-nbjs4 requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.144: INFO: Pod sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-z79zw requesting resource cpu=0m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
STEP: Starting Pods to consume most of the cluster CPU. 11/30/22 04:20:21.144
Nov 30 04:20:21.144: INFO: Creating a pod which consumes cpu=1053m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.174: INFO: Creating a pod which consumes cpu=1067m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.182: INFO: Creating a pod which consumes cpu=1025m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.195: INFO: Creating a pod which consumes cpu=1592m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
Nov 30 04:20:21.211: INFO: Waiting up to 5m0s for pod "filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62" in namespace "sched-pred-3564" to be "running"
Nov 30 04:20:21.214: INFO: Pod "filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197617ms
Nov 30 04:20:23.217: INFO: Pod "filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62": Phase="Running", Reason="", readiness=true. Elapsed: 2.006593655s
Nov 30 04:20:23.218: INFO: Pod "filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62" satisfied condition "running"
Nov 30 04:20:23.218: INFO: Waiting up to 5m0s for pod "filler-pod-42130418-29c4-4e1b-8d67-da8025852344" in namespace "sched-pred-3564" to be "running"
Nov 30 04:20:23.221: INFO: Pod "filler-pod-42130418-29c4-4e1b-8d67-da8025852344": Phase="Running", Reason="", readiness=true. Elapsed: 3.590357ms
Nov 30 04:20:23.221: INFO: Pod "filler-pod-42130418-29c4-4e1b-8d67-da8025852344" satisfied condition "running"
Nov 30 04:20:23.221: INFO: Waiting up to 5m0s for pod "filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad" in namespace "sched-pred-3564" to be "running"
Nov 30 04:20:23.223: INFO: Pod "filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.329891ms
Nov 30 04:20:23.224: INFO: Pod "filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad" satisfied condition "running"
Nov 30 04:20:23.224: INFO: Waiting up to 5m0s for pod "filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7" in namespace "sched-pred-3564" to be "running"
Nov 30 04:20:23.226: INFO: Pod "filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7": Phase="Running", Reason="", readiness=true. Elapsed: 2.279562ms
Nov 30 04:20:23.226: INFO: Pod "filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 11/30/22 04:20:23.226
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7.172c41e6c3ac53a4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3564/filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7 to worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7.172c41e6e441e670], Reason = [AddedInterface], Message = [Add eth0 [192.168.12.231/32] from k8s-pod-network] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7.172c41e6ec7d0d7e], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.8" already present on machine] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7.172c41e6ee26bfb5], Reason = [Created], Message = [Created container filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7.172c41e6f3e63b4e], Reason = [Started], Message = [Started container filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42130418-29c4-4e1b-8d67-da8025852344.172c41e6c1caacd5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3564/filler-pod-42130418-29c4-4e1b-8d67-da8025852344 to worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42130418-29c4-4e1b-8d67-da8025852344.172c41e6e0b08bef], Reason = [AddedInterface], Message = [Add eth0 [192.168.251.191/32] from k8s-pod-network] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42130418-29c4-4e1b-8d67-da8025852344.172c41e6e8c2d394], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.8" already present on machine] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42130418-29c4-4e1b-8d67-da8025852344.172c41e6e9c64a52], Reason = [Created], Message = [Created container filler-pod-42130418-29c4-4e1b-8d67-da8025852344] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42130418-29c4-4e1b-8d67-da8025852344.172c41e6f05a1329], Reason = [Started], Message = [Started container filler-pod-42130418-29c4-4e1b-8d67-da8025852344] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62.172c41e6c0c96035], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3564/filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62 to worker-pool1-ok72912g-n92-ci-ibd-23-jenkins] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62.172c41e6df388549], Reason = [AddedInterface], Message = [Add eth0 [192.168.228.146/32] from k8s-pod-network] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62.172c41e6e603c38c], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.8" already present on machine] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62.172c41e6e7124acf], Reason = [Created], Message = [Created container filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62.172c41e6ec5cc32f], Reason = [Started], Message = [Started container filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad.172c41e6c2b44beb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3564/filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad to worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad.172c41e6e1550b48], Reason = [AddedInterface], Message = [Add eth0 [192.168.254.58/32] from k8s-pod-network] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad.172c41e6e8a2da90], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.8" already present on machine] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad.172c41e6ea1266f2], Reason = [Created], Message = [Created container filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad.172c41e6f0aebe75], Reason = [Started], Message = [Started container filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad] 11/30/22 04:20:23.229
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.172c41e73b503ab0], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 7 Insufficient cpu. preemption: 0/7 nodes are available: 3 Preemption is not helpful for scheduling, 4 No preemption victims found for incoming pod.] 11/30/22 04:20:23.245
STEP: removing the label node off the node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins 11/30/22 04:20:24.242
STEP: verifying the node doesn't have the label node 11/30/22 04:20:24.256
STEP: removing the label node off the node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins 11/30/22 04:20:24.26
STEP: verifying the node doesn't have the label node 11/30/22 04:20:24.284
STEP: removing the label node off the node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins 11/30/22 04:20:24.289
STEP: verifying the node doesn't have the label node 11/30/22 04:20:24.305
STEP: removing the label node off the node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins 11/30/22 04:20:24.308
STEP: verifying the node doesn't have the label node 11/30/22 04:20:24.327
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Nov 30 04:20:24.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3564" for this suite. 11/30/22 04:20:24.341
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":189,"skipped":3334,"failed":0}
------------------------------
• [3.402 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:20:20.947
    Nov 30 04:20:20.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename sched-pred 11/30/22 04:20:20.948
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:20.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:20.966
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Nov 30 04:20:20.968: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 30 04:20:20.982: INFO: Waiting for terminating namespaces to be deleted...
    Nov 30 04:20:20.985: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins before test
    Nov 30 04:20:20.996: INFO: calico-node-9bcg6 from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:20.996: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: csi-cinder-nodeplugin-hnpf8 from kube-system started at 2022-11-30 02:52:37 +0000 UTC (3 container statuses recorded)
    Nov 30 04:20:20.996: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: eric-tm-external-connectivity-frontend-speaker-wdz6h from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:20.996: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: kube-multus-ds-amd64-nw2qp from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:20.996: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: kube-proxy-stdnr from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:20.996: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: kucero-qz2ld from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:20.996: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: node-local-dns-wvqkc from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:20.996: INFO: 	Container node-cache ready: true, restart count 1
    Nov 30 04:20:20.996: INFO: eric-pm-node-exporter-rlfnb from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:20.996: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: node-cert-exporter-hn5jk from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:20.996: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: execpod4mvhh from services-9518 started at 2022-11-30 04:20:12 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:20.996: INFO: 	Container agnhost-container ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: nodeport-test-97tdx from services-9518 started at 2022-11-30 04:20:09 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:20.996: INFO: 	Container nodeport-test ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-6bvhh from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 04:20:20.996: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 30 04:20:20.996: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins before test
    Nov 30 04:20:21.010: INFO: default-http-backend-6f4f64db57-dnh7m from ingress-nginx started at 2022-11-30 02:55:20 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container default-http-backend ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: nginx-ingress-controller-844dff9bc5-684fv from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: calico-node-thlq4 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: calicoctl-c47c68f5-42nfq from kube-system started at 2022-11-30 03:07:12 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container calicoctl ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: csi-cinder-nodeplugin-5hx92 from kube-system started at 2022-11-30 02:52:16 +0000 UTC (3 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: eric-app-sys-info-handler-754d6dcc6b-5xw88 from kube-system started at 2022-11-30 03:02:36 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: eric-data-document-database-pg-0 from kube-system started at 2022-11-30 03:03:48 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: eric-tm-external-connectivity-frontend-controller-689dd9fdkths4 from kube-system started at 2022-11-30 03:57:29 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container controller ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: eric-tm-external-connectivity-frontend-speaker-vkmgp from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: kube-multus-ds-amd64-lhrwf from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: kube-proxy-g68qp from kube-system started at 2022-11-30 02:51:41 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container kube-proxy ready: true, restart count 1
    Nov 30 04:20:21.010: INFO: kucero-29zhp from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: network-resources-injector-6dfc58d4f-rm8m2 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container webhook-server ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: node-local-dns-4fvm6 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container node-cache ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: eric-pm-alertmanager-76c454d9f7-d2tvr from monitoring started at 2022-11-30 03:57:29 +0000 UTC (2 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: eric-pm-kube-state-metrics-6696677569-fnnhj from monitoring started at 2022-11-30 02:58:51 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: eric-pm-node-exporter-dhtts from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: eric-pm-pushgateway-6db47dc9d7-fbx4n from monitoring started at 2022-11-30 02:59:19 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container pushgateway ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: eric-victoria-metrics-agent-c9f978858-cmz9r from monitoring started at 2022-11-30 02:57:48 +0000 UTC (2 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: 	Container vmagent-config-reload ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: node-cert-exporter-jbl7k from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: nodeport-test-kh4ft from services-9518 started at 2022-11-30 04:20:09 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container nodeport-test ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: sonobuoy-e2e-job-681d55e1c5a74dc7 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container e2e ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-nbjs4 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 04:20:21.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 30 04:20:21.010: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins before test
    Nov 30 04:20:21.021: INFO: nginx-ingress-controller-844dff9bc5-7std7 from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: calico-node-c6ckn from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: csi-cinder-nodeplugin-xvtjm from kube-system started at 2022-11-30 02:52:53 +0000 UTC (3 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: eric-lm-combined-server-license-server-client-d9b78bd6b-5r5rz from kube-system started at 2022-11-30 03:04:36 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: eric-tm-external-connectivity-frontend-speaker-9k8gj from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: kube-multus-ds-amd64-rk7dk from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: kube-proxy-2zzdw from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: kucero-nds94 from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: node-local-dns-4bsnx from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container node-cache ready: true, restart count 1
    Nov 30 04:20:21.021: INFO: eric-pm-node-exporter-xs7lt from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: eric-pm-server-utils-7585bb6b5d-7btvg from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: eric-victoria-metrics-alert-server-54c5c5474c-mhjjn from monitoring started at 2022-11-30 03:38:30 +0000 UTC (2 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: 	Container vmalert-config-reload ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: eric-victoria-metrics-cluster-vmselect-55c4b48dfb-lnx7j from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2022-11-30 02:57:08 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: node-cert-exporter-l77s4 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-88nb5 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 04:20:21.021: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 30 04:20:21.021: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins before test
    Nov 30 04:20:21.033: INFO: calico-node-klj9m from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: ccd-license-consumer-5949d66498-mjgr8 from kube-system started at 2022-11-30 03:05:25 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container ccd-license-consumer ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: csi-cinder-nodeplugin-sw4wz from kube-system started at 2022-11-30 02:52:31 +0000 UTC (3 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: eric-lcm-container-registry-registry-7f7856985-4rfb5 from kube-system started at 2022-11-30 02:56:13 +0000 UTC (3 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: 	Container registry ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: 	Container sidecar ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: eric-lm-combined-server-license-consumer-handler-75bfcf7bc2tbcb from kube-system started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: eric-tm-external-connectivity-frontend-speaker-vmfpl from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: kube-multus-ds-amd64-cf4v4 from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: kube-proxy-6s7bb from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: kucero-5bwbh from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: metrics-server-7567d6784b-dw2c8 from kube-system started at 2022-11-30 03:01:29 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container metrics-server ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: network-resources-injector-6dfc58d4f-nnqj7 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container webhook-server ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: node-local-dns-4hp9c from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container node-cache ready: true, restart count 1
    Nov 30 04:20:21.033: INFO: eric-pm-node-exporter-lmw6v from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: eric-victoria-metrics-cluster-vminsert-6c76f96c54-jsw5z from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: isp-logger-5b4d57f796-5ncwr from monitoring started at 2022-11-30 03:06:55 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container isp-logger ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: node-cert-exporter-v97f6 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: sonobuoy from sonobuoy started at 2022-11-30 03:19:54 +0000 UTC (1 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-z79zw from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 04:20:21.033: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 04:20:21.033: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins 11/30/22 04:20:21.065
    STEP: verifying the node has the label node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins 11/30/22 04:20:21.083
    STEP: verifying the node has the label node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins 11/30/22 04:20:21.098
    STEP: verifying the node has the label node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins 11/30/22 04:20:21.115
    Nov 30 04:20:21.144: INFO: Pod default-http-backend-6f4f64db57-dnh7m requesting resource cpu=10m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod nginx-ingress-controller-844dff9bc5-684fv requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod nginx-ingress-controller-844dff9bc5-7std7 requesting resource cpu=0m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod calico-node-9bcg6 requesting resource cpu=250m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod calico-node-c6ckn requesting resource cpu=250m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod calico-node-klj9m requesting resource cpu=250m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod calico-node-thlq4 requesting resource cpu=250m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod calicoctl-c47c68f5-42nfq requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod ccd-license-consumer-5949d66498-mjgr8 requesting resource cpu=10m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod csi-cinder-nodeplugin-5hx92 requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod csi-cinder-nodeplugin-hnpf8 requesting resource cpu=0m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod csi-cinder-nodeplugin-sw4wz requesting resource cpu=0m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod csi-cinder-nodeplugin-xvtjm requesting resource cpu=0m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-app-sys-info-handler-754d6dcc6b-5xw88 requesting resource cpu=50m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-data-document-database-pg-0 requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-lcm-container-registry-registry-7f7856985-4rfb5 requesting resource cpu=400m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-lm-combined-server-license-consumer-handler-75bfcf7bc2tbcb requesting resource cpu=100m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-lm-combined-server-license-server-client-d9b78bd6b-5r5rz requesting resource cpu=100m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-tm-external-connectivity-frontend-controller-689dd9fdkths4 requesting resource cpu=50m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-tm-external-connectivity-frontend-speaker-9k8gj requesting resource cpu=50m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-tm-external-connectivity-frontend-speaker-vkmgp requesting resource cpu=50m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-tm-external-connectivity-frontend-speaker-vmfpl requesting resource cpu=50m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-tm-external-connectivity-frontend-speaker-wdz6h requesting resource cpu=50m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod kube-multus-ds-amd64-cf4v4 requesting resource cpu=100m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod kube-multus-ds-amd64-lhrwf requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod kube-multus-ds-amd64-nw2qp requesting resource cpu=100m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod kube-multus-ds-amd64-rk7dk requesting resource cpu=100m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod kube-proxy-2zzdw requesting resource cpu=0m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod kube-proxy-6s7bb requesting resource cpu=0m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod kube-proxy-g68qp requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod kube-proxy-stdnr requesting resource cpu=0m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod kucero-29zhp requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod kucero-5bwbh requesting resource cpu=100m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod kucero-nds94 requesting resource cpu=100m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod kucero-qz2ld requesting resource cpu=100m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod metrics-server-7567d6784b-dw2c8 requesting resource cpu=0m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod network-resources-injector-6dfc58d4f-nnqj7 requesting resource cpu=250m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod network-resources-injector-6dfc58d4f-rm8m2 requesting resource cpu=250m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod node-local-dns-4bsnx requesting resource cpu=25m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod node-local-dns-4fvm6 requesting resource cpu=25m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod node-local-dns-4hp9c requesting resource cpu=25m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod node-local-dns-wvqkc requesting resource cpu=25m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-pm-alertmanager-76c454d9f7-d2tvr requesting resource cpu=110m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-pm-kube-state-metrics-6696677569-fnnhj requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-pm-node-exporter-dhtts requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-pm-node-exporter-lmw6v requesting resource cpu=100m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-pm-node-exporter-rlfnb requesting resource cpu=100m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-pm-node-exporter-xs7lt requesting resource cpu=100m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-pm-pushgateway-6db47dc9d7-fbx4n requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-pm-server-utils-7585bb6b5d-7btvg requesting resource cpu=0m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-victoria-metrics-agent-c9f978858-cmz9r requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-victoria-metrics-alert-server-54c5c5474c-mhjjn requesting resource cpu=100m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-victoria-metrics-cluster-vminsert-6c76f96c54-jsw5z requesting resource cpu=50m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-victoria-metrics-cluster-vmselect-55c4b48dfb-lnx7j requesting resource cpu=50m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod eric-victoria-metrics-cluster-vmstorage-0 requesting resource cpu=500m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod isp-logger-5b4d57f796-5ncwr requesting resource cpu=0m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod node-cert-exporter-hn5jk requesting resource cpu=100m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod node-cert-exporter-jbl7k requesting resource cpu=100m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod node-cert-exporter-l77s4 requesting resource cpu=100m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod node-cert-exporter-v97f6 requesting resource cpu=100m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod execpod4mvhh requesting resource cpu=0m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod nodeport-test-97tdx requesting resource cpu=0m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod nodeport-test-kh4ft requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod sonobuoy-e2e-job-681d55e1c5a74dc7 requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-6bvhh requesting resource cpu=0m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-88nb5 requesting resource cpu=0m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-nbjs4 requesting resource cpu=0m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.144: INFO: Pod sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-z79zw requesting resource cpu=0m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    STEP: Starting Pods to consume most of the cluster CPU. 11/30/22 04:20:21.144
    Nov 30 04:20:21.144: INFO: Creating a pod which consumes cpu=1053m on Node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.174: INFO: Creating a pod which consumes cpu=1067m on Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.182: INFO: Creating a pod which consumes cpu=1025m on Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.195: INFO: Creating a pod which consumes cpu=1592m on Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    Nov 30 04:20:21.211: INFO: Waiting up to 5m0s for pod "filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62" in namespace "sched-pred-3564" to be "running"
    Nov 30 04:20:21.214: INFO: Pod "filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197617ms
    Nov 30 04:20:23.217: INFO: Pod "filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62": Phase="Running", Reason="", readiness=true. Elapsed: 2.006593655s
    Nov 30 04:20:23.218: INFO: Pod "filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62" satisfied condition "running"
    Nov 30 04:20:23.218: INFO: Waiting up to 5m0s for pod "filler-pod-42130418-29c4-4e1b-8d67-da8025852344" in namespace "sched-pred-3564" to be "running"
    Nov 30 04:20:23.221: INFO: Pod "filler-pod-42130418-29c4-4e1b-8d67-da8025852344": Phase="Running", Reason="", readiness=true. Elapsed: 3.590357ms
    Nov 30 04:20:23.221: INFO: Pod "filler-pod-42130418-29c4-4e1b-8d67-da8025852344" satisfied condition "running"
    Nov 30 04:20:23.221: INFO: Waiting up to 5m0s for pod "filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad" in namespace "sched-pred-3564" to be "running"
    Nov 30 04:20:23.223: INFO: Pod "filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.329891ms
    Nov 30 04:20:23.224: INFO: Pod "filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad" satisfied condition "running"
    Nov 30 04:20:23.224: INFO: Waiting up to 5m0s for pod "filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7" in namespace "sched-pred-3564" to be "running"
    Nov 30 04:20:23.226: INFO: Pod "filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7": Phase="Running", Reason="", readiness=true. Elapsed: 2.279562ms
    Nov 30 04:20:23.226: INFO: Pod "filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 11/30/22 04:20:23.226
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7.172c41e6c3ac53a4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3564/filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7 to worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7.172c41e6e441e670], Reason = [AddedInterface], Message = [Add eth0 [192.168.12.231/32] from k8s-pod-network] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7.172c41e6ec7d0d7e], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.8" already present on machine] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7.172c41e6ee26bfb5], Reason = [Created], Message = [Created container filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7.172c41e6f3e63b4e], Reason = [Started], Message = [Started container filler-pod-03639ca5-3675-4de0-b51e-2adbb2dbfac7] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-42130418-29c4-4e1b-8d67-da8025852344.172c41e6c1caacd5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3564/filler-pod-42130418-29c4-4e1b-8d67-da8025852344 to worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-42130418-29c4-4e1b-8d67-da8025852344.172c41e6e0b08bef], Reason = [AddedInterface], Message = [Add eth0 [192.168.251.191/32] from k8s-pod-network] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-42130418-29c4-4e1b-8d67-da8025852344.172c41e6e8c2d394], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.8" already present on machine] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-42130418-29c4-4e1b-8d67-da8025852344.172c41e6e9c64a52], Reason = [Created], Message = [Created container filler-pod-42130418-29c4-4e1b-8d67-da8025852344] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-42130418-29c4-4e1b-8d67-da8025852344.172c41e6f05a1329], Reason = [Started], Message = [Started container filler-pod-42130418-29c4-4e1b-8d67-da8025852344] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62.172c41e6c0c96035], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3564/filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62 to worker-pool1-ok72912g-n92-ci-ibd-23-jenkins] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62.172c41e6df388549], Reason = [AddedInterface], Message = [Add eth0 [192.168.228.146/32] from k8s-pod-network] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62.172c41e6e603c38c], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.8" already present on machine] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62.172c41e6e7124acf], Reason = [Created], Message = [Created container filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62.172c41e6ec5cc32f], Reason = [Started], Message = [Started container filler-pod-b66c3685-295a-4b6b-a1cf-43d64d2d3c62] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad.172c41e6c2b44beb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3564/filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad to worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad.172c41e6e1550b48], Reason = [AddedInterface], Message = [Add eth0 [192.168.254.58/32] from k8s-pod-network] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad.172c41e6e8a2da90], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.8" already present on machine] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad.172c41e6ea1266f2], Reason = [Created], Message = [Created container filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad.172c41e6f0aebe75], Reason = [Started], Message = [Started container filler-pod-c44be35d-119d-4187-91eb-fb897206f9ad] 11/30/22 04:20:23.229
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.172c41e73b503ab0], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 7 Insufficient cpu. preemption: 0/7 nodes are available: 3 Preemption is not helpful for scheduling, 4 No preemption victims found for incoming pod.] 11/30/22 04:20:23.245
    STEP: removing the label node off the node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins 11/30/22 04:20:24.242
    STEP: verifying the node doesn't have the label node 11/30/22 04:20:24.256
    STEP: removing the label node off the node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins 11/30/22 04:20:24.26
    STEP: verifying the node doesn't have the label node 11/30/22 04:20:24.284
    STEP: removing the label node off the node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins 11/30/22 04:20:24.289
    STEP: verifying the node doesn't have the label node 11/30/22 04:20:24.305
    STEP: removing the label node off the node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins 11/30/22 04:20:24.308
    STEP: verifying the node doesn't have the label node 11/30/22 04:20:24.327
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 04:20:24.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3564" for this suite. 11/30/22 04:20:24.341
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:20:24.35
Nov 30 04:20:24.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 04:20:24.351
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:24.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:24.371
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 04:20:24.388
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:20:25.052
STEP: Deploying the webhook pod 11/30/22 04:20:25.082
STEP: Wait for the deployment to be ready 11/30/22 04:20:25.096
Nov 30 04:20:25.111: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 04:20:27.119
STEP: Verifying the service has paired with the endpoint 11/30/22 04:20:27.132
Nov 30 04:20:28.132: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 11/30/22 04:20:28.135
STEP: Creating a configMap that does not comply to the validation webhook rules 11/30/22 04:20:28.149
STEP: Updating a validating webhook configuration's rules to not include the create operation 11/30/22 04:20:28.155
STEP: Creating a configMap that does not comply to the validation webhook rules 11/30/22 04:20:28.164
STEP: Patching a validating webhook configuration's rules to include the create operation 11/30/22 04:20:28.173
STEP: Creating a configMap that does not comply to the validation webhook rules 11/30/22 04:20:28.181
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:20:28.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8132" for this suite. 11/30/22 04:20:28.193
STEP: Destroying namespace "webhook-8132-markers" for this suite. 11/30/22 04:20:28.199
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":190,"skipped":3342,"failed":0}
------------------------------
• [3.953 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:20:24.35
    Nov 30 04:20:24.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 04:20:24.351
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:24.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:24.371
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 04:20:24.388
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:20:25.052
    STEP: Deploying the webhook pod 11/30/22 04:20:25.082
    STEP: Wait for the deployment to be ready 11/30/22 04:20:25.096
    Nov 30 04:20:25.111: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 04:20:27.119
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:20:27.132
    Nov 30 04:20:28.132: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 11/30/22 04:20:28.135
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/30/22 04:20:28.149
    STEP: Updating a validating webhook configuration's rules to not include the create operation 11/30/22 04:20:28.155
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/30/22 04:20:28.164
    STEP: Patching a validating webhook configuration's rules to include the create operation 11/30/22 04:20:28.173
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/30/22 04:20:28.181
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:20:28.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8132" for this suite. 11/30/22 04:20:28.193
    STEP: Destroying namespace "webhook-8132-markers" for this suite. 11/30/22 04:20:28.199
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:20:28.303
Nov 30 04:20:28.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pods 11/30/22 04:20:28.306
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:28.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:28.35
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 11/30/22 04:20:28.352
STEP: setting up watch 11/30/22 04:20:28.352
STEP: submitting the pod to kubernetes 11/30/22 04:20:28.456
STEP: verifying the pod is in kubernetes 11/30/22 04:20:28.487
STEP: verifying pod creation was observed 11/30/22 04:20:28.49
Nov 30 04:20:28.490: INFO: Waiting up to 5m0s for pod "pod-submit-remove-72234c6a-4880-4488-a08b-2a01ad25c777" in namespace "pods-3134" to be "running"
Nov 30 04:20:28.494: INFO: Pod "pod-submit-remove-72234c6a-4880-4488-a08b-2a01ad25c777": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171665ms
Nov 30 04:20:30.499: INFO: Pod "pod-submit-remove-72234c6a-4880-4488-a08b-2a01ad25c777": Phase="Running", Reason="", readiness=true. Elapsed: 2.009241373s
Nov 30 04:20:30.499: INFO: Pod "pod-submit-remove-72234c6a-4880-4488-a08b-2a01ad25c777" satisfied condition "running"
STEP: deleting the pod gracefully 11/30/22 04:20:30.526
STEP: verifying pod deletion was observed 11/30/22 04:20:30.534
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Nov 30 04:20:32.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3134" for this suite. 11/30/22 04:20:32.949
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":191,"skipped":3347,"failed":0}
------------------------------
• [4.652 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:20:28.303
    Nov 30 04:20:28.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pods 11/30/22 04:20:28.306
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:28.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:28.35
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 11/30/22 04:20:28.352
    STEP: setting up watch 11/30/22 04:20:28.352
    STEP: submitting the pod to kubernetes 11/30/22 04:20:28.456
    STEP: verifying the pod is in kubernetes 11/30/22 04:20:28.487
    STEP: verifying pod creation was observed 11/30/22 04:20:28.49
    Nov 30 04:20:28.490: INFO: Waiting up to 5m0s for pod "pod-submit-remove-72234c6a-4880-4488-a08b-2a01ad25c777" in namespace "pods-3134" to be "running"
    Nov 30 04:20:28.494: INFO: Pod "pod-submit-remove-72234c6a-4880-4488-a08b-2a01ad25c777": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171665ms
    Nov 30 04:20:30.499: INFO: Pod "pod-submit-remove-72234c6a-4880-4488-a08b-2a01ad25c777": Phase="Running", Reason="", readiness=true. Elapsed: 2.009241373s
    Nov 30 04:20:30.499: INFO: Pod "pod-submit-remove-72234c6a-4880-4488-a08b-2a01ad25c777" satisfied condition "running"
    STEP: deleting the pod gracefully 11/30/22 04:20:30.526
    STEP: verifying pod deletion was observed 11/30/22 04:20:30.534
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Nov 30 04:20:32.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3134" for this suite. 11/30/22 04:20:32.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:20:32.958
Nov 30 04:20:32.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-probe 11/30/22 04:20:32.959
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:32.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:32.986
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Nov 30 04:21:33.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9126" for this suite. 11/30/22 04:21:33.009
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":192,"skipped":3425,"failed":0}
------------------------------
• [SLOW TEST] [60.056 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:20:32.958
    Nov 30 04:20:32.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-probe 11/30/22 04:20:32.959
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:20:32.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:20:32.986
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Nov 30 04:21:33.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9126" for this suite. 11/30/22 04:21:33.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:21:33.014
Nov 30 04:21:33.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 04:21:33.015
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:21:33.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:21:33.068
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Nov 30 04:21:33.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-7476 version'
Nov 30 04:21:33.122: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Nov 30 04:21:33.122: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.3\", GitCommit:\"434bfd82814af038ad94d62ebe59b133fcb50506\", GitTreeState:\"clean\", BuildDate:\"2022-10-12T10:57:26Z\", GoVersion:\"go1.19.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.3\", GitCommit:\"c37ac32c86e6cc346a92f9aeb62a0620c84ae047\", GitTreeState:\"clean\", BuildDate:\"2022-10-17T04:31:37Z\", GoVersion:\"go1.19.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 04:21:33.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7476" for this suite. 11/30/22 04:21:33.127
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":193,"skipped":3449,"failed":0}
------------------------------
• [0.119 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:21:33.014
    Nov 30 04:21:33.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 04:21:33.015
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:21:33.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:21:33.068
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Nov 30 04:21:33.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-7476 version'
    Nov 30 04:21:33.122: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Nov 30 04:21:33.122: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.3\", GitCommit:\"434bfd82814af038ad94d62ebe59b133fcb50506\", GitTreeState:\"clean\", BuildDate:\"2022-10-12T10:57:26Z\", GoVersion:\"go1.19.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.3\", GitCommit:\"c37ac32c86e6cc346a92f9aeb62a0620c84ae047\", GitTreeState:\"clean\", BuildDate:\"2022-10-17T04:31:37Z\", GoVersion:\"go1.19.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 04:21:33.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7476" for this suite. 11/30/22 04:21:33.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:21:33.134
Nov 30 04:21:33.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 04:21:33.135
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:21:33.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:21:33.153
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 11/30/22 04:21:33.156
Nov 30 04:21:33.188: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a" in namespace "downward-api-6799" to be "Succeeded or Failed"
Nov 30 04:21:33.191: INFO: Pod "downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.87996ms
Nov 30 04:21:35.195: INFO: Pod "downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006781598s
Nov 30 04:21:37.194: INFO: Pod "downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005846978s
STEP: Saw pod success 11/30/22 04:21:37.194
Nov 30 04:21:37.194: INFO: Pod "downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a" satisfied condition "Succeeded or Failed"
Nov 30 04:21:37.197: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a container client-container: <nil>
STEP: delete the pod 11/30/22 04:21:37.202
Nov 30 04:21:37.222: INFO: Waiting for pod downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a to disappear
Nov 30 04:21:37.225: INFO: Pod downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Nov 30 04:21:37.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6799" for this suite. 11/30/22 04:21:37.229
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":194,"skipped":3462,"failed":0}
------------------------------
• [4.101 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:21:33.134
    Nov 30 04:21:33.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 04:21:33.135
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:21:33.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:21:33.153
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 11/30/22 04:21:33.156
    Nov 30 04:21:33.188: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a" in namespace "downward-api-6799" to be "Succeeded or Failed"
    Nov 30 04:21:33.191: INFO: Pod "downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.87996ms
    Nov 30 04:21:35.195: INFO: Pod "downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006781598s
    Nov 30 04:21:37.194: INFO: Pod "downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005846978s
    STEP: Saw pod success 11/30/22 04:21:37.194
    Nov 30 04:21:37.194: INFO: Pod "downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a" satisfied condition "Succeeded or Failed"
    Nov 30 04:21:37.197: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a container client-container: <nil>
    STEP: delete the pod 11/30/22 04:21:37.202
    Nov 30 04:21:37.222: INFO: Waiting for pod downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a to disappear
    Nov 30 04:21:37.225: INFO: Pod downwardapi-volume-fd014a3f-f32a-43b8-bac5-a6b39507618a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Nov 30 04:21:37.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6799" for this suite. 11/30/22 04:21:37.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:21:37.236
Nov 30 04:21:37.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 04:21:37.237
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:21:37.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:21:37.257
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-6af93d09-61c7-4868-b843-ada60c7e6b8a 11/30/22 04:21:37.259
STEP: Creating a pod to test consume secrets 11/30/22 04:21:37.267
Nov 30 04:21:37.278: INFO: Waiting up to 5m0s for pod "pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6" in namespace "secrets-1778" to be "Succeeded or Failed"
Nov 30 04:21:37.281: INFO: Pod "pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.255419ms
Nov 30 04:21:39.285: INFO: Pod "pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6": Phase="Running", Reason="", readiness=false. Elapsed: 2.006880858s
Nov 30 04:21:41.286: INFO: Pod "pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007882808s
STEP: Saw pod success 11/30/22 04:21:41.286
Nov 30 04:21:41.286: INFO: Pod "pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6" satisfied condition "Succeeded or Failed"
Nov 30 04:21:41.289: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6 container secret-volume-test: <nil>
STEP: delete the pod 11/30/22 04:21:41.298
Nov 30 04:21:41.310: INFO: Waiting for pod pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6 to disappear
Nov 30 04:21:41.315: INFO: Pod pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Nov 30 04:21:41.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1778" for this suite. 11/30/22 04:21:41.319
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":195,"skipped":3478,"failed":0}
------------------------------
• [4.101 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:21:37.236
    Nov 30 04:21:37.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 04:21:37.237
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:21:37.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:21:37.257
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-6af93d09-61c7-4868-b843-ada60c7e6b8a 11/30/22 04:21:37.259
    STEP: Creating a pod to test consume secrets 11/30/22 04:21:37.267
    Nov 30 04:21:37.278: INFO: Waiting up to 5m0s for pod "pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6" in namespace "secrets-1778" to be "Succeeded or Failed"
    Nov 30 04:21:37.281: INFO: Pod "pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.255419ms
    Nov 30 04:21:39.285: INFO: Pod "pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6": Phase="Running", Reason="", readiness=false. Elapsed: 2.006880858s
    Nov 30 04:21:41.286: INFO: Pod "pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007882808s
    STEP: Saw pod success 11/30/22 04:21:41.286
    Nov 30 04:21:41.286: INFO: Pod "pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6" satisfied condition "Succeeded or Failed"
    Nov 30 04:21:41.289: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6 container secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 04:21:41.298
    Nov 30 04:21:41.310: INFO: Waiting for pod pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6 to disappear
    Nov 30 04:21:41.315: INFO: Pod pod-secrets-503be6a9-2d23-40e9-b29d-00a50e42a7b6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 04:21:41.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1778" for this suite. 11/30/22 04:21:41.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:21:41.338
Nov 30 04:21:41.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pod-network-test 11/30/22 04:21:41.338
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:21:41.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:21:41.358
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-8376 11/30/22 04:21:41.36
STEP: creating a selector 11/30/22 04:21:41.36
STEP: Creating the service pods in kubernetes 11/30/22 04:21:41.36
Nov 30 04:21:41.360: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 30 04:21:41.484: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8376" to be "running and ready"
Nov 30 04:21:41.489: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.595292ms
Nov 30 04:21:41.489: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:21:43.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008874361s
Nov 30 04:21:43.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:21:45.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009727918s
Nov 30 04:21:45.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:21:47.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009264343s
Nov 30 04:21:47.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:21:49.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008428422s
Nov 30 04:21:49.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:21:51.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008304842s
Nov 30 04:21:51.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:21:53.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008929887s
Nov 30 04:21:53.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:21:55.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011695253s
Nov 30 04:21:55.496: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:21:57.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008417504s
Nov 30 04:21:57.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:21:59.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.008262691s
Nov 30 04:21:59.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:22:01.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.012115016s
Nov 30 04:22:01.496: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:22:03.498: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013883299s
Nov 30 04:22:03.498: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 30 04:22:03.498: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 30 04:22:03.501: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8376" to be "running and ready"
Nov 30 04:22:03.503: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.542677ms
Nov 30 04:22:03.503: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 30 04:22:03.503: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 30 04:22:03.506: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8376" to be "running and ready"
Nov 30 04:22:03.508: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.036791ms
Nov 30 04:22:03.508: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 30 04:22:03.508: INFO: Pod "netserver-2" satisfied condition "running and ready"
Nov 30 04:22:03.510: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8376" to be "running and ready"
Nov 30 04:22:03.516: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.963432ms
Nov 30 04:22:03.516: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Nov 30 04:22:03.516: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 11/30/22 04:22:03.521
W1130 04:22:03.563345      20 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Nov 30 04:22:03.563: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8376" to be "running"
Nov 30 04:22:03.566: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.912192ms
Nov 30 04:22:05.569: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006253961s
Nov 30 04:22:05.569: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 30 04:22:05.573: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8376" to be "running"
Nov 30 04:22:05.582: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.315246ms
Nov 30 04:22:05.582: INFO: Pod "host-test-container-pod" satisfied condition "running"
Nov 30 04:22:05.600: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Nov 30 04:22:05.600: INFO: Going to poll 192.168.12.244 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Nov 30 04:22:05.618: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.12.244 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8376 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:22:05.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:22:05.619: INFO: ExecWithOptions: Clientset creation
Nov 30 04:22:05.619: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8376/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.12.244+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 30 04:22:06.691: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 30 04:22:06.691: INFO: Going to poll 192.168.228.165 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Nov 30 04:22:06.694: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.228.165 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8376 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:22:06.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:22:06.694: INFO: ExecWithOptions: Clientset creation
Nov 30 04:22:06.694: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8376/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.228.165+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 30 04:22:07.768: INFO: Found all 1 expected endpoints: [netserver-1]
Nov 30 04:22:07.768: INFO: Going to poll 192.168.251.155 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Nov 30 04:22:07.771: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.251.155 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8376 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:22:07.771: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:22:07.772: INFO: ExecWithOptions: Clientset creation
Nov 30 04:22:07.772: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8376/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.251.155+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 30 04:22:08.842: INFO: Found all 1 expected endpoints: [netserver-2]
Nov 30 04:22:08.842: INFO: Going to poll 192.168.254.59 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Nov 30 04:22:08.846: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.254.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8376 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:22:08.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:22:08.846: INFO: ExecWithOptions: Clientset creation
Nov 30 04:22:08.846: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8376/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.254.59+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 30 04:22:09.913: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Nov 30 04:22:09.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8376" for this suite. 11/30/22 04:22:09.918
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":196,"skipped":3494,"failed":0}
------------------------------
• [SLOW TEST] [28.587 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:21:41.338
    Nov 30 04:21:41.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pod-network-test 11/30/22 04:21:41.338
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:21:41.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:21:41.358
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-8376 11/30/22 04:21:41.36
    STEP: creating a selector 11/30/22 04:21:41.36
    STEP: Creating the service pods in kubernetes 11/30/22 04:21:41.36
    Nov 30 04:21:41.360: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 30 04:21:41.484: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8376" to be "running and ready"
    Nov 30 04:21:41.489: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.595292ms
    Nov 30 04:21:41.489: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:21:43.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008874361s
    Nov 30 04:21:43.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:21:45.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009727918s
    Nov 30 04:21:45.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:21:47.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009264343s
    Nov 30 04:21:47.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:21:49.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008428422s
    Nov 30 04:21:49.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:21:51.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008304842s
    Nov 30 04:21:51.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:21:53.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008929887s
    Nov 30 04:21:53.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:21:55.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011695253s
    Nov 30 04:21:55.496: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:21:57.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008417504s
    Nov 30 04:21:57.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:21:59.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.008262691s
    Nov 30 04:21:59.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:22:01.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.012115016s
    Nov 30 04:22:01.496: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:22:03.498: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013883299s
    Nov 30 04:22:03.498: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 30 04:22:03.498: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 30 04:22:03.501: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8376" to be "running and ready"
    Nov 30 04:22:03.503: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.542677ms
    Nov 30 04:22:03.503: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 30 04:22:03.503: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 30 04:22:03.506: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8376" to be "running and ready"
    Nov 30 04:22:03.508: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.036791ms
    Nov 30 04:22:03.508: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 30 04:22:03.508: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Nov 30 04:22:03.510: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8376" to be "running and ready"
    Nov 30 04:22:03.516: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.963432ms
    Nov 30 04:22:03.516: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Nov 30 04:22:03.516: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 11/30/22 04:22:03.521
    W1130 04:22:03.563345      20 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Nov 30 04:22:03.563: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8376" to be "running"
    Nov 30 04:22:03.566: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.912192ms
    Nov 30 04:22:05.569: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006253961s
    Nov 30 04:22:05.569: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 30 04:22:05.573: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8376" to be "running"
    Nov 30 04:22:05.582: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.315246ms
    Nov 30 04:22:05.582: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Nov 30 04:22:05.600: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Nov 30 04:22:05.600: INFO: Going to poll 192.168.12.244 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Nov 30 04:22:05.618: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.12.244 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8376 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:22:05.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:22:05.619: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:22:05.619: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8376/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.12.244+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 30 04:22:06.691: INFO: Found all 1 expected endpoints: [netserver-0]
    Nov 30 04:22:06.691: INFO: Going to poll 192.168.228.165 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Nov 30 04:22:06.694: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.228.165 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8376 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:22:06.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:22:06.694: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:22:06.694: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8376/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.228.165+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 30 04:22:07.768: INFO: Found all 1 expected endpoints: [netserver-1]
    Nov 30 04:22:07.768: INFO: Going to poll 192.168.251.155 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Nov 30 04:22:07.771: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.251.155 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8376 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:22:07.771: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:22:07.772: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:22:07.772: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8376/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.251.155+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 30 04:22:08.842: INFO: Found all 1 expected endpoints: [netserver-2]
    Nov 30 04:22:08.842: INFO: Going to poll 192.168.254.59 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Nov 30 04:22:08.846: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.254.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8376 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:22:08.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:22:08.846: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:22:08.846: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8376/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.254.59+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 30 04:22:09.913: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Nov 30 04:22:09.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8376" for this suite. 11/30/22 04:22:09.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:22:09.925
Nov 30 04:22:09.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:22:09.925
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:09.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:09.947
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-e168eec4-ba4b-4e99-989c-e948267a1228 11/30/22 04:22:09.951
STEP: Creating a pod to test consume secrets 11/30/22 04:22:09.958
Nov 30 04:22:09.990: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265" in namespace "projected-3310" to be "Succeeded or Failed"
Nov 30 04:22:09.993: INFO: Pod "pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.507698ms
Nov 30 04:22:11.997: INFO: Pod "pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00672859s
Nov 30 04:22:14.000: INFO: Pod "pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009548529s
STEP: Saw pod success 11/30/22 04:22:14
Nov 30 04:22:14.000: INFO: Pod "pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265" satisfied condition "Succeeded or Failed"
Nov 30 04:22:14.002: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/30/22 04:22:14.007
Nov 30 04:22:14.019: INFO: Waiting for pod pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265 to disappear
Nov 30 04:22:14.021: INFO: Pod pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Nov 30 04:22:14.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3310" for this suite. 11/30/22 04:22:14.025
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":197,"skipped":3509,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:22:09.925
    Nov 30 04:22:09.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:22:09.925
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:09.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:09.947
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-e168eec4-ba4b-4e99-989c-e948267a1228 11/30/22 04:22:09.951
    STEP: Creating a pod to test consume secrets 11/30/22 04:22:09.958
    Nov 30 04:22:09.990: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265" in namespace "projected-3310" to be "Succeeded or Failed"
    Nov 30 04:22:09.993: INFO: Pod "pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.507698ms
    Nov 30 04:22:11.997: INFO: Pod "pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00672859s
    Nov 30 04:22:14.000: INFO: Pod "pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009548529s
    STEP: Saw pod success 11/30/22 04:22:14
    Nov 30 04:22:14.000: INFO: Pod "pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265" satisfied condition "Succeeded or Failed"
    Nov 30 04:22:14.002: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 04:22:14.007
    Nov 30 04:22:14.019: INFO: Waiting for pod pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265 to disappear
    Nov 30 04:22:14.021: INFO: Pod pod-projected-secrets-d6a4fc0a-cdc5-4f38-9bbd-d2735a789265 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Nov 30 04:22:14.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3310" for this suite. 11/30/22 04:22:14.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:22:14.035
Nov 30 04:22:14.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename resourcequota 11/30/22 04:22:14.036
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:14.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:14.052
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 11/30/22 04:22:14.055
STEP: Creating a ResourceQuota 11/30/22 04:22:19.063
STEP: Ensuring resource quota status is calculated 11/30/22 04:22:19.074
STEP: Creating a Service 11/30/22 04:22:21.078
STEP: Creating a NodePort Service 11/30/22 04:22:21.096
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 11/30/22 04:22:21.122
STEP: Ensuring resource quota status captures service creation 11/30/22 04:22:21.171
STEP: Deleting Services 11/30/22 04:22:23.174
STEP: Ensuring resource quota status released usage 11/30/22 04:22:23.252
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Nov 30 04:22:25.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2368" for this suite. 11/30/22 04:22:25.264
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":198,"skipped":3562,"failed":0}
------------------------------
• [SLOW TEST] [11.235 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:22:14.035
    Nov 30 04:22:14.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename resourcequota 11/30/22 04:22:14.036
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:14.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:14.052
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 11/30/22 04:22:14.055
    STEP: Creating a ResourceQuota 11/30/22 04:22:19.063
    STEP: Ensuring resource quota status is calculated 11/30/22 04:22:19.074
    STEP: Creating a Service 11/30/22 04:22:21.078
    STEP: Creating a NodePort Service 11/30/22 04:22:21.096
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 11/30/22 04:22:21.122
    STEP: Ensuring resource quota status captures service creation 11/30/22 04:22:21.171
    STEP: Deleting Services 11/30/22 04:22:23.174
    STEP: Ensuring resource quota status released usage 11/30/22 04:22:23.252
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Nov 30 04:22:25.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2368" for this suite. 11/30/22 04:22:25.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:22:25.271
Nov 30 04:22:25.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename replicaset 11/30/22 04:22:25.272
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:25.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:25.296
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 11/30/22 04:22:25.299
Nov 30 04:22:25.308: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 30 04:22:30.315: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/30/22 04:22:30.315
STEP: getting scale subresource 11/30/22 04:22:30.315
STEP: updating a scale subresource 11/30/22 04:22:30.317
STEP: verifying the replicaset Spec.Replicas was modified 11/30/22 04:22:30.337
STEP: Patch a scale subresource 11/30/22 04:22:30.34
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Nov 30 04:22:30.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3501" for this suite. 11/30/22 04:22:30.381
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":199,"skipped":3592,"failed":0}
------------------------------
• [SLOW TEST] [5.116 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:22:25.271
    Nov 30 04:22:25.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename replicaset 11/30/22 04:22:25.272
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:25.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:25.296
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 11/30/22 04:22:25.299
    Nov 30 04:22:25.308: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 30 04:22:30.315: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/30/22 04:22:30.315
    STEP: getting scale subresource 11/30/22 04:22:30.315
    STEP: updating a scale subresource 11/30/22 04:22:30.317
    STEP: verifying the replicaset Spec.Replicas was modified 11/30/22 04:22:30.337
    STEP: Patch a scale subresource 11/30/22 04:22:30.34
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Nov 30 04:22:30.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3501" for this suite. 11/30/22 04:22:30.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:22:30.387
Nov 30 04:22:30.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename resourcequota 11/30/22 04:22:30.388
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:30.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:30.413
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 11/30/22 04:22:30.416
STEP: Ensuring ResourceQuota status is calculated 11/30/22 04:22:30.427
STEP: Creating a ResourceQuota with not terminating scope 11/30/22 04:22:32.431
STEP: Ensuring ResourceQuota status is calculated 11/30/22 04:22:32.435
STEP: Creating a long running pod 11/30/22 04:22:34.438
STEP: Ensuring resource quota with not terminating scope captures the pod usage 11/30/22 04:22:34.473
STEP: Ensuring resource quota with terminating scope ignored the pod usage 11/30/22 04:22:36.476
STEP: Deleting the pod 11/30/22 04:22:38.481
STEP: Ensuring resource quota status released the pod usage 11/30/22 04:22:38.499
STEP: Creating a terminating pod 11/30/22 04:22:40.503
STEP: Ensuring resource quota with terminating scope captures the pod usage 11/30/22 04:22:40.538
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 11/30/22 04:22:42.542
STEP: Deleting the pod 11/30/22 04:22:44.545
STEP: Ensuring resource quota status released the pod usage 11/30/22 04:22:44.558
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Nov 30 04:22:46.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6614" for this suite. 11/30/22 04:22:46.569
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":200,"skipped":3597,"failed":0}
------------------------------
• [SLOW TEST] [16.187 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:22:30.387
    Nov 30 04:22:30.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename resourcequota 11/30/22 04:22:30.388
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:30.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:30.413
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 11/30/22 04:22:30.416
    STEP: Ensuring ResourceQuota status is calculated 11/30/22 04:22:30.427
    STEP: Creating a ResourceQuota with not terminating scope 11/30/22 04:22:32.431
    STEP: Ensuring ResourceQuota status is calculated 11/30/22 04:22:32.435
    STEP: Creating a long running pod 11/30/22 04:22:34.438
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 11/30/22 04:22:34.473
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 11/30/22 04:22:36.476
    STEP: Deleting the pod 11/30/22 04:22:38.481
    STEP: Ensuring resource quota status released the pod usage 11/30/22 04:22:38.499
    STEP: Creating a terminating pod 11/30/22 04:22:40.503
    STEP: Ensuring resource quota with terminating scope captures the pod usage 11/30/22 04:22:40.538
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 11/30/22 04:22:42.542
    STEP: Deleting the pod 11/30/22 04:22:44.545
    STEP: Ensuring resource quota status released the pod usage 11/30/22 04:22:44.558
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Nov 30 04:22:46.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6614" for this suite. 11/30/22 04:22:46.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:22:46.576
Nov 30 04:22:46.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename events 11/30/22 04:22:46.577
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:46.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:46.595
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 11/30/22 04:22:46.597
STEP: listing all events in all namespaces 11/30/22 04:22:46.606
STEP: patching the test event 11/30/22 04:22:46.61
STEP: fetching the test event 11/30/22 04:22:46.616
STEP: updating the test event 11/30/22 04:22:46.618
STEP: getting the test event 11/30/22 04:22:46.625
STEP: deleting the test event 11/30/22 04:22:46.628
STEP: listing all events in all namespaces 11/30/22 04:22:46.636
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Nov 30 04:22:46.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5467" for this suite. 11/30/22 04:22:46.643
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":201,"skipped":3670,"failed":0}
------------------------------
• [0.073 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:22:46.576
    Nov 30 04:22:46.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename events 11/30/22 04:22:46.577
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:46.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:46.595
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 11/30/22 04:22:46.597
    STEP: listing all events in all namespaces 11/30/22 04:22:46.606
    STEP: patching the test event 11/30/22 04:22:46.61
    STEP: fetching the test event 11/30/22 04:22:46.616
    STEP: updating the test event 11/30/22 04:22:46.618
    STEP: getting the test event 11/30/22 04:22:46.625
    STEP: deleting the test event 11/30/22 04:22:46.628
    STEP: listing all events in all namespaces 11/30/22 04:22:46.636
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Nov 30 04:22:46.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5467" for this suite. 11/30/22 04:22:46.643
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:22:46.649
Nov 30 04:22:46.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:22:46.649
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:46.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:46.667
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-acf407eb-1142-4a6b-b56b-b2a4418fd7f5 11/30/22 04:22:46.67
STEP: Creating a pod to test consume secrets 11/30/22 04:22:46.676
Nov 30 04:22:46.709: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28" in namespace "projected-1341" to be "Succeeded or Failed"
Nov 30 04:22:46.716: INFO: Pod "pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28": Phase="Pending", Reason="", readiness=false. Elapsed: 7.089572ms
Nov 30 04:22:48.720: INFO: Pod "pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011192493s
Nov 30 04:22:50.720: INFO: Pod "pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01070176s
STEP: Saw pod success 11/30/22 04:22:50.72
Nov 30 04:22:50.720: INFO: Pod "pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28" satisfied condition "Succeeded or Failed"
Nov 30 04:22:50.722: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/30/22 04:22:50.727
Nov 30 04:22:50.736: INFO: Waiting for pod pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28 to disappear
Nov 30 04:22:50.738: INFO: Pod pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Nov 30 04:22:50.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1341" for this suite. 11/30/22 04:22:50.743
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":202,"skipped":3670,"failed":0}
------------------------------
• [4.103 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:22:46.649
    Nov 30 04:22:46.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:22:46.649
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:46.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:46.667
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-acf407eb-1142-4a6b-b56b-b2a4418fd7f5 11/30/22 04:22:46.67
    STEP: Creating a pod to test consume secrets 11/30/22 04:22:46.676
    Nov 30 04:22:46.709: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28" in namespace "projected-1341" to be "Succeeded or Failed"
    Nov 30 04:22:46.716: INFO: Pod "pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28": Phase="Pending", Reason="", readiness=false. Elapsed: 7.089572ms
    Nov 30 04:22:48.720: INFO: Pod "pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011192493s
    Nov 30 04:22:50.720: INFO: Pod "pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01070176s
    STEP: Saw pod success 11/30/22 04:22:50.72
    Nov 30 04:22:50.720: INFO: Pod "pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28" satisfied condition "Succeeded or Failed"
    Nov 30 04:22:50.722: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 04:22:50.727
    Nov 30 04:22:50.736: INFO: Waiting for pod pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28 to disappear
    Nov 30 04:22:50.738: INFO: Pod pod-projected-secrets-e68fe2ab-929b-425f-a50a-ecd0da883d28 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Nov 30 04:22:50.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1341" for this suite. 11/30/22 04:22:50.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:22:50.752
Nov 30 04:22:50.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 04:22:50.753
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:50.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:50.773
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Nov 30 04:22:50.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 11/30/22 04:22:54.308
Nov 30 04:22:54.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 create -f -'
Nov 30 04:22:54.899: INFO: stderr: ""
Nov 30 04:22:54.899: INFO: stdout: "e2e-test-crd-publish-openapi-22-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 30 04:22:54.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 delete e2e-test-crd-publish-openapi-22-crds test-foo'
Nov 30 04:22:55.005: INFO: stderr: ""
Nov 30 04:22:55.005: INFO: stdout: "e2e-test-crd-publish-openapi-22-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 30 04:22:55.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 apply -f -'
Nov 30 04:22:55.650: INFO: stderr: ""
Nov 30 04:22:55.650: INFO: stdout: "e2e-test-crd-publish-openapi-22-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 30 04:22:55.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 delete e2e-test-crd-publish-openapi-22-crds test-foo'
Nov 30 04:22:55.741: INFO: stderr: ""
Nov 30 04:22:55.741: INFO: stdout: "e2e-test-crd-publish-openapi-22-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 11/30/22 04:22:55.741
Nov 30 04:22:55.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 create -f -'
Nov 30 04:22:56.196: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 11/30/22 04:22:56.196
Nov 30 04:22:56.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 create -f -'
Nov 30 04:22:56.350: INFO: rc: 1
Nov 30 04:22:56.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 apply -f -'
Nov 30 04:22:56.554: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 11/30/22 04:22:56.554
Nov 30 04:22:56.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 create -f -'
Nov 30 04:22:56.705: INFO: rc: 1
Nov 30 04:22:56.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 apply -f -'
Nov 30 04:22:56.865: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 11/30/22 04:22:56.865
Nov 30 04:22:56.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 explain e2e-test-crd-publish-openapi-22-crds'
Nov 30 04:22:57.017: INFO: stderr: ""
Nov 30 04:22:57.017: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-22-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 11/30/22 04:22:57.017
Nov 30 04:22:57.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 explain e2e-test-crd-publish-openapi-22-crds.metadata'
Nov 30 04:22:57.172: INFO: stderr: ""
Nov 30 04:22:57.172: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-22-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 30 04:22:57.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 explain e2e-test-crd-publish-openapi-22-crds.spec'
Nov 30 04:22:57.316: INFO: stderr: ""
Nov 30 04:22:57.316: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-22-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 30 04:22:57.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 explain e2e-test-crd-publish-openapi-22-crds.spec.bars'
Nov 30 04:22:57.466: INFO: stderr: ""
Nov 30 04:22:57.466: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-22-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 11/30/22 04:22:57.466
Nov 30 04:22:57.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 explain e2e-test-crd-publish-openapi-22-crds.spec.bars2'
Nov 30 04:22:57.615: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:23:01.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5315" for this suite. 11/30/22 04:23:01.094
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":203,"skipped":3675,"failed":0}
------------------------------
• [SLOW TEST] [10.347 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:22:50.752
    Nov 30 04:22:50.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 04:22:50.753
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:22:50.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:22:50.773
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Nov 30 04:22:50.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 11/30/22 04:22:54.308
    Nov 30 04:22:54.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 create -f -'
    Nov 30 04:22:54.899: INFO: stderr: ""
    Nov 30 04:22:54.899: INFO: stdout: "e2e-test-crd-publish-openapi-22-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Nov 30 04:22:54.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 delete e2e-test-crd-publish-openapi-22-crds test-foo'
    Nov 30 04:22:55.005: INFO: stderr: ""
    Nov 30 04:22:55.005: INFO: stdout: "e2e-test-crd-publish-openapi-22-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Nov 30 04:22:55.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 apply -f -'
    Nov 30 04:22:55.650: INFO: stderr: ""
    Nov 30 04:22:55.650: INFO: stdout: "e2e-test-crd-publish-openapi-22-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Nov 30 04:22:55.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 delete e2e-test-crd-publish-openapi-22-crds test-foo'
    Nov 30 04:22:55.741: INFO: stderr: ""
    Nov 30 04:22:55.741: INFO: stdout: "e2e-test-crd-publish-openapi-22-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 11/30/22 04:22:55.741
    Nov 30 04:22:55.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 create -f -'
    Nov 30 04:22:56.196: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 11/30/22 04:22:56.196
    Nov 30 04:22:56.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 create -f -'
    Nov 30 04:22:56.350: INFO: rc: 1
    Nov 30 04:22:56.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 apply -f -'
    Nov 30 04:22:56.554: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 11/30/22 04:22:56.554
    Nov 30 04:22:56.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 create -f -'
    Nov 30 04:22:56.705: INFO: rc: 1
    Nov 30 04:22:56.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 --namespace=crd-publish-openapi-5315 apply -f -'
    Nov 30 04:22:56.865: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 11/30/22 04:22:56.865
    Nov 30 04:22:56.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 explain e2e-test-crd-publish-openapi-22-crds'
    Nov 30 04:22:57.017: INFO: stderr: ""
    Nov 30 04:22:57.017: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-22-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 11/30/22 04:22:57.017
    Nov 30 04:22:57.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 explain e2e-test-crd-publish-openapi-22-crds.metadata'
    Nov 30 04:22:57.172: INFO: stderr: ""
    Nov 30 04:22:57.172: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-22-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Nov 30 04:22:57.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 explain e2e-test-crd-publish-openapi-22-crds.spec'
    Nov 30 04:22:57.316: INFO: stderr: ""
    Nov 30 04:22:57.316: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-22-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Nov 30 04:22:57.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 explain e2e-test-crd-publish-openapi-22-crds.spec.bars'
    Nov 30 04:22:57.466: INFO: stderr: ""
    Nov 30 04:22:57.466: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-22-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 11/30/22 04:22:57.466
    Nov 30 04:22:57.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-5315 explain e2e-test-crd-publish-openapi-22-crds.spec.bars2'
    Nov 30 04:22:57.615: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:23:01.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5315" for this suite. 11/30/22 04:23:01.094
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:01.099
Nov 30 04:23:01.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 04:23:01.1
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:01.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:01.119
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-3684 11/30/22 04:23:01.121
STEP: creating service affinity-nodeport-transition in namespace services-3684 11/30/22 04:23:01.121
STEP: creating replication controller affinity-nodeport-transition in namespace services-3684 11/30/22 04:23:01.14
I1130 04:23:01.149597      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3684, replica count: 3
I1130 04:23:04.200600      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 30 04:23:04.210: INFO: Creating new exec pod
Nov 30 04:23:04.240: INFO: Waiting up to 5m0s for pod "execpod-affinity9h78n" in namespace "services-3684" to be "running"
Nov 30 04:23:04.243: INFO: Pod "execpod-affinity9h78n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.265575ms
Nov 30 04:23:06.247: INFO: Pod "execpod-affinity9h78n": Phase="Running", Reason="", readiness=true. Elapsed: 2.006587291s
Nov 30 04:23:06.247: INFO: Pod "execpod-affinity9h78n" satisfied condition "running"
Nov 30 04:23:07.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3684 exec execpod-affinity9h78n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Nov 30 04:23:07.387: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Nov 30 04:23:07.387: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 04:23:07.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3684 exec execpod-affinity9h78n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.101.2 80'
Nov 30 04:23:07.513: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.101.2 80\nConnection to 10.102.101.2 80 port [tcp/http] succeeded!\n"
Nov 30 04:23:07.513: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 04:23:07.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3684 exec execpod-affinity9h78n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.26 31607'
Nov 30 04:23:07.630: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.26 31607\nConnection to 10.0.10.26 31607 port [tcp/*] succeeded!\n"
Nov 30 04:23:07.630: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 04:23:07.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3684 exec execpod-affinity9h78n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.2 31607'
Nov 30 04:23:07.772: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.2 31607\nConnection to 10.0.10.2 31607 port [tcp/*] succeeded!\n"
Nov 30 04:23:07.772: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 04:23:07.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3684 exec execpod-affinity9h78n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.8:31607/ ; done'
Nov 30 04:23:07.993: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n"
Nov 30 04:23:07.993: INFO: stdout: "\naffinity-nodeport-transition-k2l5w\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-lgpbc\naffinity-nodeport-transition-k2l5w\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-lgpbc\naffinity-nodeport-transition-k2l5w\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-lgpbc\naffinity-nodeport-transition-k2l5w\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-lgpbc\naffinity-nodeport-transition-k2l5w\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-lgpbc\naffinity-nodeport-transition-k2l5w"
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-k2l5w
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-lgpbc
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-k2l5w
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-lgpbc
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-k2l5w
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-lgpbc
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-k2l5w
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-lgpbc
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-k2l5w
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-lgpbc
Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-k2l5w
Nov 30 04:23:08.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3684 exec execpod-affinity9h78n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.8:31607/ ; done'
Nov 30 04:23:08.219: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n"
Nov 30 04:23:08.219: INFO: stdout: "\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9"
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
Nov 30 04:23:08.219: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3684, will wait for the garbage collector to delete the pods 11/30/22 04:23:08.235
Nov 30 04:23:08.296: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.206637ms
Nov 30 04:23:08.397: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.311989ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 04:23:10.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3684" for this suite. 11/30/22 04:23:10.555
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":204,"skipped":3675,"failed":0}
------------------------------
• [SLOW TEST] [9.469 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:01.099
    Nov 30 04:23:01.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 04:23:01.1
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:01.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:01.119
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-3684 11/30/22 04:23:01.121
    STEP: creating service affinity-nodeport-transition in namespace services-3684 11/30/22 04:23:01.121
    STEP: creating replication controller affinity-nodeport-transition in namespace services-3684 11/30/22 04:23:01.14
    I1130 04:23:01.149597      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3684, replica count: 3
    I1130 04:23:04.200600      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 30 04:23:04.210: INFO: Creating new exec pod
    Nov 30 04:23:04.240: INFO: Waiting up to 5m0s for pod "execpod-affinity9h78n" in namespace "services-3684" to be "running"
    Nov 30 04:23:04.243: INFO: Pod "execpod-affinity9h78n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.265575ms
    Nov 30 04:23:06.247: INFO: Pod "execpod-affinity9h78n": Phase="Running", Reason="", readiness=true. Elapsed: 2.006587291s
    Nov 30 04:23:06.247: INFO: Pod "execpod-affinity9h78n" satisfied condition "running"
    Nov 30 04:23:07.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3684 exec execpod-affinity9h78n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Nov 30 04:23:07.387: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Nov 30 04:23:07.387: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 04:23:07.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3684 exec execpod-affinity9h78n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.101.2 80'
    Nov 30 04:23:07.513: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.101.2 80\nConnection to 10.102.101.2 80 port [tcp/http] succeeded!\n"
    Nov 30 04:23:07.513: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 04:23:07.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3684 exec execpod-affinity9h78n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.26 31607'
    Nov 30 04:23:07.630: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.26 31607\nConnection to 10.0.10.26 31607 port [tcp/*] succeeded!\n"
    Nov 30 04:23:07.630: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 04:23:07.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3684 exec execpod-affinity9h78n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.10.2 31607'
    Nov 30 04:23:07.772: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.10.2 31607\nConnection to 10.0.10.2 31607 port [tcp/*] succeeded!\n"
    Nov 30 04:23:07.772: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 04:23:07.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3684 exec execpod-affinity9h78n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.8:31607/ ; done'
    Nov 30 04:23:07.993: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n"
    Nov 30 04:23:07.993: INFO: stdout: "\naffinity-nodeport-transition-k2l5w\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-lgpbc\naffinity-nodeport-transition-k2l5w\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-lgpbc\naffinity-nodeport-transition-k2l5w\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-lgpbc\naffinity-nodeport-transition-k2l5w\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-lgpbc\naffinity-nodeport-transition-k2l5w\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-lgpbc\naffinity-nodeport-transition-k2l5w"
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-k2l5w
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-lgpbc
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-k2l5w
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-lgpbc
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-k2l5w
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-lgpbc
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-k2l5w
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-lgpbc
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-k2l5w
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-lgpbc
    Nov 30 04:23:07.993: INFO: Received response from host: affinity-nodeport-transition-k2l5w
    Nov 30 04:23:08.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3684 exec execpod-affinity9h78n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.8:31607/ ; done'
    Nov 30 04:23:08.219: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.8:31607/\n"
    Nov 30 04:23:08.219: INFO: stdout: "\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9\naffinity-nodeport-transition-tqbg9"
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Received response from host: affinity-nodeport-transition-tqbg9
    Nov 30 04:23:08.219: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3684, will wait for the garbage collector to delete the pods 11/30/22 04:23:08.235
    Nov 30 04:23:08.296: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.206637ms
    Nov 30 04:23:08.397: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.311989ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 04:23:10.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3684" for this suite. 11/30/22 04:23:10.555
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:10.571
Nov 30 04:23:10.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 04:23:10.571
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:10.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:10.595
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 04:23:10.614
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:23:11.02
STEP: Deploying the webhook pod 11/30/22 04:23:11.027
STEP: Wait for the deployment to be ready 11/30/22 04:23:11.043
Nov 30 04:23:11.054: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 04:23:13.069
STEP: Verifying the service has paired with the endpoint 11/30/22 04:23:13.081
Nov 30 04:23:14.082: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 11/30/22 04:23:14.086
STEP: Updating a mutating webhook configuration's rules to not include the create operation 11/30/22 04:23:14.102
STEP: Creating a configMap that should not be mutated 11/30/22 04:23:14.116
STEP: Patching a mutating webhook configuration's rules to include the create operation 11/30/22 04:23:14.145
STEP: Creating a configMap that should be mutated 11/30/22 04:23:14.154
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:23:14.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7597" for this suite. 11/30/22 04:23:14.176
STEP: Destroying namespace "webhook-7597-markers" for this suite. 11/30/22 04:23:14.181
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":205,"skipped":3712,"failed":0}
------------------------------
• [3.683 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:10.571
    Nov 30 04:23:10.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 04:23:10.571
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:10.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:10.595
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 04:23:10.614
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:23:11.02
    STEP: Deploying the webhook pod 11/30/22 04:23:11.027
    STEP: Wait for the deployment to be ready 11/30/22 04:23:11.043
    Nov 30 04:23:11.054: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 04:23:13.069
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:23:13.081
    Nov 30 04:23:14.082: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 11/30/22 04:23:14.086
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 11/30/22 04:23:14.102
    STEP: Creating a configMap that should not be mutated 11/30/22 04:23:14.116
    STEP: Patching a mutating webhook configuration's rules to include the create operation 11/30/22 04:23:14.145
    STEP: Creating a configMap that should be mutated 11/30/22 04:23:14.154
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:23:14.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7597" for this suite. 11/30/22 04:23:14.176
    STEP: Destroying namespace "webhook-7597-markers" for this suite. 11/30/22 04:23:14.181
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:14.254
Nov 30 04:23:14.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename ingressclass 11/30/22 04:23:14.255
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:14.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:14.291
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 11/30/22 04:23:14.295
STEP: getting /apis/networking.k8s.io 11/30/22 04:23:14.297
STEP: getting /apis/networking.k8s.iov1 11/30/22 04:23:14.298
STEP: creating 11/30/22 04:23:14.299
STEP: getting 11/30/22 04:23:14.32
STEP: listing 11/30/22 04:23:14.33
STEP: watching 11/30/22 04:23:14.334
Nov 30 04:23:14.334: INFO: starting watch
STEP: patching 11/30/22 04:23:14.335
STEP: updating 11/30/22 04:23:14.343
Nov 30 04:23:14.349: INFO: waiting for watch events with expected annotations
Nov 30 04:23:14.349: INFO: saw patched and updated annotations
STEP: deleting 11/30/22 04:23:14.349
STEP: deleting a collection 11/30/22 04:23:14.367
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Nov 30 04:23:14.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1078" for this suite. 11/30/22 04:23:14.385
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":206,"skipped":3731,"failed":0}
------------------------------
• [0.137 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:14.254
    Nov 30 04:23:14.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename ingressclass 11/30/22 04:23:14.255
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:14.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:14.291
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 11/30/22 04:23:14.295
    STEP: getting /apis/networking.k8s.io 11/30/22 04:23:14.297
    STEP: getting /apis/networking.k8s.iov1 11/30/22 04:23:14.298
    STEP: creating 11/30/22 04:23:14.299
    STEP: getting 11/30/22 04:23:14.32
    STEP: listing 11/30/22 04:23:14.33
    STEP: watching 11/30/22 04:23:14.334
    Nov 30 04:23:14.334: INFO: starting watch
    STEP: patching 11/30/22 04:23:14.335
    STEP: updating 11/30/22 04:23:14.343
    Nov 30 04:23:14.349: INFO: waiting for watch events with expected annotations
    Nov 30 04:23:14.349: INFO: saw patched and updated annotations
    STEP: deleting 11/30/22 04:23:14.349
    STEP: deleting a collection 11/30/22 04:23:14.367
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Nov 30 04:23:14.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-1078" for this suite. 11/30/22 04:23:14.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:14.392
Nov 30 04:23:14.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename init-container 11/30/22 04:23:14.392
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:14.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:14.414
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 11/30/22 04:23:14.416
Nov 30 04:23:14.416: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Nov 30 04:23:18.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5610" for this suite. 11/30/22 04:23:18.375
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":207,"skipped":3749,"failed":0}
------------------------------
• [3.990 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:14.392
    Nov 30 04:23:14.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename init-container 11/30/22 04:23:14.392
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:14.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:14.414
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 11/30/22 04:23:14.416
    Nov 30 04:23:14.416: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Nov 30 04:23:18.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5610" for this suite. 11/30/22 04:23:18.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:18.383
Nov 30 04:23:18.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename runtimeclass 11/30/22 04:23:18.383
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:18.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:18.405
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 11/30/22 04:23:18.407
STEP: getting /apis/node.k8s.io 11/30/22 04:23:18.41
STEP: getting /apis/node.k8s.io/v1 11/30/22 04:23:18.411
STEP: creating 11/30/22 04:23:18.412
STEP: watching 11/30/22 04:23:18.437
Nov 30 04:23:18.437: INFO: starting watch
STEP: getting 11/30/22 04:23:18.441
STEP: listing 11/30/22 04:23:18.444
STEP: patching 11/30/22 04:23:18.447
STEP: updating 11/30/22 04:23:18.452
Nov 30 04:23:18.457: INFO: waiting for watch events with expected annotations
STEP: deleting 11/30/22 04:23:18.457
STEP: deleting a collection 11/30/22 04:23:18.466
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Nov 30 04:23:18.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4334" for this suite. 11/30/22 04:23:18.482
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":208,"skipped":3779,"failed":0}
------------------------------
• [0.114 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:18.383
    Nov 30 04:23:18.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename runtimeclass 11/30/22 04:23:18.383
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:18.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:18.405
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 11/30/22 04:23:18.407
    STEP: getting /apis/node.k8s.io 11/30/22 04:23:18.41
    STEP: getting /apis/node.k8s.io/v1 11/30/22 04:23:18.411
    STEP: creating 11/30/22 04:23:18.412
    STEP: watching 11/30/22 04:23:18.437
    Nov 30 04:23:18.437: INFO: starting watch
    STEP: getting 11/30/22 04:23:18.441
    STEP: listing 11/30/22 04:23:18.444
    STEP: patching 11/30/22 04:23:18.447
    STEP: updating 11/30/22 04:23:18.452
    Nov 30 04:23:18.457: INFO: waiting for watch events with expected annotations
    STEP: deleting 11/30/22 04:23:18.457
    STEP: deleting a collection 11/30/22 04:23:18.466
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Nov 30 04:23:18.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4334" for this suite. 11/30/22 04:23:18.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:18.498
Nov 30 04:23:18.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename deployment 11/30/22 04:23:18.499
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:18.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:18.529
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Nov 30 04:23:18.542: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 30 04:23:23.545: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/30/22 04:23:23.545
Nov 30 04:23:23.545: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 11/30/22 04:23:23.557
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 30 04:23:23.569: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4378  f1016a62-2ac8-4024-b6a2-4d2d4961db5e 47587 1 2022-11-30 04:23:23 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-11-30 04:23:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d22ac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Nov 30 04:23:23.571: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Nov 30 04:23:23.571: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Nov 30 04:23:23.572: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4378  5a787d85-8a1a-478b-bf33-9f723f7c09bd 47588 1 2022-11-30 04:23:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment f1016a62-2ac8-4024-b6a2-4d2d4961db5e 0xc004da1657 0xc004da1658}] [] [{e2e.test Update apps/v1 2022-11-30 04:23:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:23:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-11-30 04:23:23 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"f1016a62-2ac8-4024-b6a2-4d2d4961db5e\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004da1718 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 30 04:23:23.575: INFO: Pod "test-cleanup-controller-jfsh9" is available:
&Pod{ObjectMeta:{test-cleanup-controller-jfsh9 test-cleanup-controller- deployment-4378  33e64e97-335a-43d4-b087-9083d253f77d 47543 0 2022-11-30 04:23:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.250"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.250"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-cleanup-controller 5a787d85-8a1a-478b-bf33-9f723f7c09bd 0xc004d22e07 0xc004d22e08}] [] [{kube-controller-manager Update v1 2022-11-30 04:23:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a787d85-8a1a-478b-bf33-9f723f7c09bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:23:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:23:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4pwxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4pwxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.250,StartTime:2022-11-30 04:23:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:23:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://237fa5bc83825fa06073f7baf1951d7721d011e5366f4971c74a5236c2b2f7aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Nov 30 04:23:23.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4378" for this suite. 11/30/22 04:23:23.579
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":209,"skipped":3791,"failed":0}
------------------------------
• [SLOW TEST] [5.089 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:18.498
    Nov 30 04:23:18.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename deployment 11/30/22 04:23:18.499
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:18.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:18.529
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Nov 30 04:23:18.542: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Nov 30 04:23:23.545: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/30/22 04:23:23.545
    Nov 30 04:23:23.545: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 11/30/22 04:23:23.557
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 30 04:23:23.569: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4378  f1016a62-2ac8-4024-b6a2-4d2d4961db5e 47587 1 2022-11-30 04:23:23 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-11-30 04:23:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d22ac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Nov 30 04:23:23.571: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Nov 30 04:23:23.571: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Nov 30 04:23:23.572: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4378  5a787d85-8a1a-478b-bf33-9f723f7c09bd 47588 1 2022-11-30 04:23:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment f1016a62-2ac8-4024-b6a2-4d2d4961db5e 0xc004da1657 0xc004da1658}] [] [{e2e.test Update apps/v1 2022-11-30 04:23:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:23:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-11-30 04:23:23 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"f1016a62-2ac8-4024-b6a2-4d2d4961db5e\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004da1718 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 30 04:23:23.575: INFO: Pod "test-cleanup-controller-jfsh9" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-jfsh9 test-cleanup-controller- deployment-4378  33e64e97-335a-43d4-b087-9083d253f77d 47543 0 2022-11-30 04:23:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.250"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.250"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-cleanup-controller 5a787d85-8a1a-478b-bf33-9f723f7c09bd 0xc004d22e07 0xc004d22e08}] [] [{kube-controller-manager Update v1 2022-11-30 04:23:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a787d85-8a1a-478b-bf33-9f723f7c09bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:23:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:23:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4pwxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4pwxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.250,StartTime:2022-11-30 04:23:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:23:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://237fa5bc83825fa06073f7baf1951d7721d011e5366f4971c74a5236c2b2f7aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Nov 30 04:23:23.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4378" for this suite. 11/30/22 04:23:23.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:23.587
Nov 30 04:23:23.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename endpointslice 11/30/22 04:23:23.587
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:23.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:23.631
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Nov 30 04:23:23.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-638" for this suite. 11/30/22 04:23:23.74
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":210,"skipped":3806,"failed":0}
------------------------------
• [0.169 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:23.587
    Nov 30 04:23:23.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename endpointslice 11/30/22 04:23:23.587
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:23.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:23.631
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Nov 30 04:23:23.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-638" for this suite. 11/30/22 04:23:23.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:23.757
Nov 30 04:23:23.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename deployment 11/30/22 04:23:23.758
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:23.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:23.782
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Nov 30 04:23:23.798: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 30 04:23:28.803: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/30/22 04:23:28.803
Nov 30 04:23:28.804: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 30 04:23:30.807: INFO: Creating deployment "test-rollover-deployment"
Nov 30 04:23:30.815: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 30 04:23:32.821: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 30 04:23:32.827: INFO: Ensure that both replica sets have 1 created replica
Nov 30 04:23:32.832: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 30 04:23:32.840: INFO: Updating deployment test-rollover-deployment
Nov 30 04:23:32.840: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 30 04:23:34.846: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 30 04:23:34.853: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 30 04:23:34.857: INFO: all replica sets need to contain the pod-template-hash label
Nov 30 04:23:34.857: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-bff85f7bc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:23:36.867: INFO: all replica sets need to contain the pod-template-hash label
Nov 30 04:23:36.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-bff85f7bc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:23:38.867: INFO: all replica sets need to contain the pod-template-hash label
Nov 30 04:23:38.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-bff85f7bc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:23:40.865: INFO: all replica sets need to contain the pod-template-hash label
Nov 30 04:23:40.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-bff85f7bc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:23:42.894: INFO: all replica sets need to contain the pod-template-hash label
Nov 30 04:23:42.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-bff85f7bc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:23:44.865: INFO: 
Nov 30 04:23:44.865: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 30 04:23:44.873: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4087  abad5503-1051-407a-927e-67f76ba3a7c6 47848 2 2022-11-30 04:23:30 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-11-30 04:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:23:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e33148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-11-30 04:23:30 +0000 UTC,LastTransitionTime:2022-11-30 04:23:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-bff85f7bc" has successfully progressed.,LastUpdateTime:2022-11-30 04:23:44 +0000 UTC,LastTransitionTime:2022-11-30 04:23:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 30 04:23:44.876: INFO: New ReplicaSet "test-rollover-deployment-bff85f7bc" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-bff85f7bc  deployment-4087  5aa11069-4fcd-4842-9e47-efe67c26c4e8 47838 2 2022-11-30 04:23:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:bff85f7bc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment abad5503-1051-407a-927e-67f76ba3a7c6 0xc003a83710 0xc003a83711}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abad5503-1051-407a-927e-67f76ba3a7c6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:23:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: bff85f7bc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:bff85f7bc] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a837a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 30 04:23:44.876: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 30 04:23:44.876: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4087  88d978fa-e0b8-48a1-bf90-c214380ba2a4 47847 2 2022-11-30 04:23:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment abad5503-1051-407a-927e-67f76ba3a7c6 0xc003a834c7 0xc003a834c8}] [] [{e2e.test Update apps/v1 2022-11-30 04:23:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:23:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abad5503-1051-407a-927e-67f76ba3a7c6\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:23:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a83588 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 30 04:23:44.876: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-4087  ff10cf05-a6c8-4a97-b6a8-19325e502aa1 47764 2 2022-11-30 04:23:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment abad5503-1051-407a-927e-67f76ba3a7c6 0xc003a835f7 0xc003a835f8}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abad5503-1051-407a-927e-67f76ba3a7c6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:23:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a836a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 30 04:23:44.879: INFO: Pod "test-rollover-deployment-bff85f7bc-j4whr" is available:
&Pod{ObjectMeta:{test-rollover-deployment-bff85f7bc-j4whr test-rollover-deployment-bff85f7bc- deployment-4087  34cf58d3-d63a-4f8a-82c2-d05121600ef3 47789 0 2022-11-30 04:23:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:bff85f7bc] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.252"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.252"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rollover-deployment-bff85f7bc 5aa11069-4fcd-4842-9e47-efe67c26c4e8 0xc003a83d10 0xc003a83d11}] [] [{kube-controller-manager Update v1 2022-11-30 04:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5aa11069-4fcd-4842-9e47-efe67c26c4e8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:23:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:23:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dp57q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dp57q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.252,StartTime:2022-11-30 04:23:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:23:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:1c9665737fa6e8ea5fa5031c5053c4bc36943d087d5d1d3926df50505b6236a8,ContainerID:containerd://d59a739b18212b1c3774c66098d16c53cd3d532512fd563b142490fd6312ad7c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Nov 30 04:23:44.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4087" for this suite. 11/30/22 04:23:44.883
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":211,"skipped":3832,"failed":0}
------------------------------
• [SLOW TEST] [21.131 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:23.757
    Nov 30 04:23:23.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename deployment 11/30/22 04:23:23.758
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:23.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:23.782
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Nov 30 04:23:23.798: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Nov 30 04:23:28.803: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/30/22 04:23:28.803
    Nov 30 04:23:28.804: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Nov 30 04:23:30.807: INFO: Creating deployment "test-rollover-deployment"
    Nov 30 04:23:30.815: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Nov 30 04:23:32.821: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Nov 30 04:23:32.827: INFO: Ensure that both replica sets have 1 created replica
    Nov 30 04:23:32.832: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Nov 30 04:23:32.840: INFO: Updating deployment test-rollover-deployment
    Nov 30 04:23:32.840: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Nov 30 04:23:34.846: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Nov 30 04:23:34.853: INFO: Make sure deployment "test-rollover-deployment" is complete
    Nov 30 04:23:34.857: INFO: all replica sets need to contain the pod-template-hash label
    Nov 30 04:23:34.857: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-bff85f7bc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:23:36.867: INFO: all replica sets need to contain the pod-template-hash label
    Nov 30 04:23:36.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-bff85f7bc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:23:38.867: INFO: all replica sets need to contain the pod-template-hash label
    Nov 30 04:23:38.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-bff85f7bc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:23:40.865: INFO: all replica sets need to contain the pod-template-hash label
    Nov 30 04:23:40.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-bff85f7bc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:23:42.894: INFO: all replica sets need to contain the pod-template-hash label
    Nov 30 04:23:42.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 23, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 23, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-bff85f7bc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:23:44.865: INFO: 
    Nov 30 04:23:44.865: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 30 04:23:44.873: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4087  abad5503-1051-407a-927e-67f76ba3a7c6 47848 2 2022-11-30 04:23:30 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-11-30 04:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:23:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e33148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-11-30 04:23:30 +0000 UTC,LastTransitionTime:2022-11-30 04:23:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-bff85f7bc" has successfully progressed.,LastUpdateTime:2022-11-30 04:23:44 +0000 UTC,LastTransitionTime:2022-11-30 04:23:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 30 04:23:44.876: INFO: New ReplicaSet "test-rollover-deployment-bff85f7bc" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-bff85f7bc  deployment-4087  5aa11069-4fcd-4842-9e47-efe67c26c4e8 47838 2 2022-11-30 04:23:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:bff85f7bc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment abad5503-1051-407a-927e-67f76ba3a7c6 0xc003a83710 0xc003a83711}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abad5503-1051-407a-927e-67f76ba3a7c6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:23:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: bff85f7bc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:bff85f7bc] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a837a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 30 04:23:44.876: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Nov 30 04:23:44.876: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4087  88d978fa-e0b8-48a1-bf90-c214380ba2a4 47847 2 2022-11-30 04:23:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment abad5503-1051-407a-927e-67f76ba3a7c6 0xc003a834c7 0xc003a834c8}] [] [{e2e.test Update apps/v1 2022-11-30 04:23:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:23:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abad5503-1051-407a-927e-67f76ba3a7c6\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:23:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a83588 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 30 04:23:44.876: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-4087  ff10cf05-a6c8-4a97-b6a8-19325e502aa1 47764 2 2022-11-30 04:23:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment abad5503-1051-407a-927e-67f76ba3a7c6 0xc003a835f7 0xc003a835f8}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abad5503-1051-407a-927e-67f76ba3a7c6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:23:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a836a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 30 04:23:44.879: INFO: Pod "test-rollover-deployment-bff85f7bc-j4whr" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-bff85f7bc-j4whr test-rollover-deployment-bff85f7bc- deployment-4087  34cf58d3-d63a-4f8a-82c2-d05121600ef3 47789 0 2022-11-30 04:23:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:bff85f7bc] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.252"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.252"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-rollover-deployment-bff85f7bc 5aa11069-4fcd-4842-9e47-efe67c26c4e8 0xc003a83d10 0xc003a83d11}] [] [{kube-controller-manager Update v1 2022-11-30 04:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5aa11069-4fcd-4842-9e47-efe67c26c4e8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:23:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:23:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dp57q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dp57q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:23:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.252,StartTime:2022-11-30 04:23:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:23:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:1c9665737fa6e8ea5fa5031c5053c4bc36943d087d5d1d3926df50505b6236a8,ContainerID:containerd://d59a739b18212b1c3774c66098d16c53cd3d532512fd563b142490fd6312ad7c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Nov 30 04:23:44.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4087" for this suite. 11/30/22 04:23:44.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:44.888
Nov 30 04:23:44.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 04:23:44.889
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:44.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:44.917
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-3c62617e-fa91-430e-ae11-5c77990f4391 11/30/22 04:23:44.92
STEP: Creating a pod to test consume configMaps 11/30/22 04:23:44.929
Nov 30 04:23:44.962: INFO: Waiting up to 5m0s for pod "pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c" in namespace "configmap-6238" to be "Succeeded or Failed"
Nov 30 04:23:44.965: INFO: Pod "pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.834758ms
Nov 30 04:23:46.969: INFO: Pod "pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007061829s
Nov 30 04:23:48.968: INFO: Pod "pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006167666s
STEP: Saw pod success 11/30/22 04:23:48.968
Nov 30 04:23:48.968: INFO: Pod "pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c" satisfied condition "Succeeded or Failed"
Nov 30 04:23:48.971: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c container agnhost-container: <nil>
STEP: delete the pod 11/30/22 04:23:48.977
Nov 30 04:23:48.993: INFO: Waiting for pod pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c to disappear
Nov 30 04:23:48.995: INFO: Pod pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 04:23:48.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6238" for this suite. 11/30/22 04:23:49
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":212,"skipped":3843,"failed":0}
------------------------------
• [4.119 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:44.888
    Nov 30 04:23:44.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 04:23:44.889
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:44.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:44.917
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-3c62617e-fa91-430e-ae11-5c77990f4391 11/30/22 04:23:44.92
    STEP: Creating a pod to test consume configMaps 11/30/22 04:23:44.929
    Nov 30 04:23:44.962: INFO: Waiting up to 5m0s for pod "pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c" in namespace "configmap-6238" to be "Succeeded or Failed"
    Nov 30 04:23:44.965: INFO: Pod "pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.834758ms
    Nov 30 04:23:46.969: INFO: Pod "pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007061829s
    Nov 30 04:23:48.968: INFO: Pod "pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006167666s
    STEP: Saw pod success 11/30/22 04:23:48.968
    Nov 30 04:23:48.968: INFO: Pod "pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c" satisfied condition "Succeeded or Failed"
    Nov 30 04:23:48.971: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 04:23:48.977
    Nov 30 04:23:48.993: INFO: Waiting for pod pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c to disappear
    Nov 30 04:23:48.995: INFO: Pod pod-configmaps-97b4ac78-75dc-4592-a70c-a5267a66098c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 04:23:48.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6238" for this suite. 11/30/22 04:23:49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:49.008
Nov 30 04:23:49.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 04:23:49.009
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:49.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:49.035
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-a3a5b314-7358-40df-8d53-8b995e938d8b 11/30/22 04:23:49.038
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Nov 30 04:23:49.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-825" for this suite. 11/30/22 04:23:49.046
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":213,"skipped":3877,"failed":0}
------------------------------
• [0.043 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:49.008
    Nov 30 04:23:49.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 04:23:49.009
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:49.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:49.035
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-a3a5b314-7358-40df-8d53-8b995e938d8b 11/30/22 04:23:49.038
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 04:23:49.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-825" for this suite. 11/30/22 04:23:49.046
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:49.052
Nov 30 04:23:49.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 04:23:49.053
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:49.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:49.076
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 11/30/22 04:23:49.08
Nov 30 04:23:49.080: INFO: Creating e2e-svc-a-cwj5c
Nov 30 04:23:49.102: INFO: Creating e2e-svc-b-pbzx5
Nov 30 04:23:49.139: INFO: Creating e2e-svc-c-qmcmk
STEP: deleting service collection 11/30/22 04:23:49.163
Nov 30 04:23:49.219: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 04:23:49.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-954" for this suite. 11/30/22 04:23:49.224
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":214,"skipped":3877,"failed":0}
------------------------------
• [0.180 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:49.052
    Nov 30 04:23:49.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 04:23:49.053
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:49.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:49.076
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 11/30/22 04:23:49.08
    Nov 30 04:23:49.080: INFO: Creating e2e-svc-a-cwj5c
    Nov 30 04:23:49.102: INFO: Creating e2e-svc-b-pbzx5
    Nov 30 04:23:49.139: INFO: Creating e2e-svc-c-qmcmk
    STEP: deleting service collection 11/30/22 04:23:49.163
    Nov 30 04:23:49.219: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 04:23:49.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-954" for this suite. 11/30/22 04:23:49.224
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:49.233
Nov 30 04:23:49.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 04:23:49.233
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:49.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:49.272
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 11/30/22 04:23:49.275
Nov 30 04:23:49.275: INFO: namespace kubectl-4891
Nov 30 04:23:49.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4891 create -f -'
Nov 30 04:23:49.850: INFO: stderr: ""
Nov 30 04:23:49.850: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 11/30/22 04:23:49.85
Nov 30 04:23:50.854: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 30 04:23:50.854: INFO: Found 0 / 1
Nov 30 04:23:51.853: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 30 04:23:51.853: INFO: Found 1 / 1
Nov 30 04:23:51.853: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 30 04:23:51.856: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 30 04:23:51.856: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 30 04:23:51.856: INFO: wait on agnhost-primary startup in kubectl-4891 
Nov 30 04:23:51.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4891 logs agnhost-primary-588h6 agnhost-primary'
Nov 30 04:23:51.923: INFO: stderr: ""
Nov 30 04:23:51.924: INFO: stdout: "Paused\n"
STEP: exposing RC 11/30/22 04:23:51.924
Nov 30 04:23:51.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4891 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Nov 30 04:23:52.003: INFO: stderr: ""
Nov 30 04:23:52.003: INFO: stdout: "service/rm2 exposed\n"
Nov 30 04:23:52.008: INFO: Service rm2 in namespace kubectl-4891 found.
STEP: exposing service 11/30/22 04:23:54.018
Nov 30 04:23:54.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4891 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Nov 30 04:23:54.091: INFO: stderr: ""
Nov 30 04:23:54.091: INFO: stdout: "service/rm3 exposed\n"
Nov 30 04:23:54.105: INFO: Service rm3 in namespace kubectl-4891 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 04:23:56.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4891" for this suite. 11/30/22 04:23:56.115
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":215,"skipped":3883,"failed":0}
------------------------------
• [SLOW TEST] [6.888 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:49.233
    Nov 30 04:23:49.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 04:23:49.233
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:49.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:49.272
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 11/30/22 04:23:49.275
    Nov 30 04:23:49.275: INFO: namespace kubectl-4891
    Nov 30 04:23:49.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4891 create -f -'
    Nov 30 04:23:49.850: INFO: stderr: ""
    Nov 30 04:23:49.850: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 11/30/22 04:23:49.85
    Nov 30 04:23:50.854: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 30 04:23:50.854: INFO: Found 0 / 1
    Nov 30 04:23:51.853: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 30 04:23:51.853: INFO: Found 1 / 1
    Nov 30 04:23:51.853: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Nov 30 04:23:51.856: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 30 04:23:51.856: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Nov 30 04:23:51.856: INFO: wait on agnhost-primary startup in kubectl-4891 
    Nov 30 04:23:51.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4891 logs agnhost-primary-588h6 agnhost-primary'
    Nov 30 04:23:51.923: INFO: stderr: ""
    Nov 30 04:23:51.924: INFO: stdout: "Paused\n"
    STEP: exposing RC 11/30/22 04:23:51.924
    Nov 30 04:23:51.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4891 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Nov 30 04:23:52.003: INFO: stderr: ""
    Nov 30 04:23:52.003: INFO: stdout: "service/rm2 exposed\n"
    Nov 30 04:23:52.008: INFO: Service rm2 in namespace kubectl-4891 found.
    STEP: exposing service 11/30/22 04:23:54.018
    Nov 30 04:23:54.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-4891 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Nov 30 04:23:54.091: INFO: stderr: ""
    Nov 30 04:23:54.091: INFO: stdout: "service/rm3 exposed\n"
    Nov 30 04:23:54.105: INFO: Service rm3 in namespace kubectl-4891 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 04:23:56.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4891" for this suite. 11/30/22 04:23:56.115
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:56.121
Nov 30 04:23:56.121: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pods 11/30/22 04:23:56.122
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:56.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:56.147
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 11/30/22 04:23:56.149
Nov 30 04:23:56.180: INFO: Waiting up to 5m0s for pod "pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0" in namespace "pods-1832" to be "running and ready"
Nov 30 04:23:56.183: INFO: Pod "pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.105026ms
Nov 30 04:23:56.183: INFO: The phase of Pod pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:23:58.188: INFO: Pod "pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.008026375s
Nov 30 04:23:58.188: INFO: The phase of Pod pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0 is Running (Ready = true)
Nov 30 04:23:58.188: INFO: Pod "pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0" satisfied condition "running and ready"
Nov 30 04:23:58.193: INFO: Pod pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0 has hostIP: 10.0.10.8
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Nov 30 04:23:58.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1832" for this suite. 11/30/22 04:23:58.198
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":216,"skipped":3884,"failed":0}
------------------------------
• [2.082 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:56.121
    Nov 30 04:23:56.121: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pods 11/30/22 04:23:56.122
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:56.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:56.147
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 11/30/22 04:23:56.149
    Nov 30 04:23:56.180: INFO: Waiting up to 5m0s for pod "pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0" in namespace "pods-1832" to be "running and ready"
    Nov 30 04:23:56.183: INFO: Pod "pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.105026ms
    Nov 30 04:23:56.183: INFO: The phase of Pod pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:23:58.188: INFO: Pod "pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.008026375s
    Nov 30 04:23:58.188: INFO: The phase of Pod pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0 is Running (Ready = true)
    Nov 30 04:23:58.188: INFO: Pod "pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0" satisfied condition "running and ready"
    Nov 30 04:23:58.193: INFO: Pod pod-hostip-452f9400-f548-4f11-acb4-1b62180cb0d0 has hostIP: 10.0.10.8
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Nov 30 04:23:58.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1832" for this suite. 11/30/22 04:23:58.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:23:58.204
Nov 30 04:23:58.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename security-context 11/30/22 04:23:58.205
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:58.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:58.224
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/30/22 04:23:58.227
Nov 30 04:23:58.239: INFO: Waiting up to 5m0s for pod "security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693" in namespace "security-context-9240" to be "Succeeded or Failed"
Nov 30 04:23:58.242: INFO: Pod "security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197645ms
Nov 30 04:24:00.247: INFO: Pod "security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008041148s
Nov 30 04:24:02.246: INFO: Pod "security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007346757s
STEP: Saw pod success 11/30/22 04:24:02.246
Nov 30 04:24:02.246: INFO: Pod "security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693" satisfied condition "Succeeded or Failed"
Nov 30 04:24:02.249: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693 container test-container: <nil>
STEP: delete the pod 11/30/22 04:24:02.255
Nov 30 04:24:02.267: INFO: Waiting for pod security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693 to disappear
Nov 30 04:24:02.270: INFO: Pod security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Nov 30 04:24:02.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9240" for this suite. 11/30/22 04:24:02.274
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":217,"skipped":3899,"failed":0}
------------------------------
• [4.075 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:23:58.204
    Nov 30 04:23:58.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename security-context 11/30/22 04:23:58.205
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:23:58.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:23:58.224
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/30/22 04:23:58.227
    Nov 30 04:23:58.239: INFO: Waiting up to 5m0s for pod "security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693" in namespace "security-context-9240" to be "Succeeded or Failed"
    Nov 30 04:23:58.242: INFO: Pod "security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197645ms
    Nov 30 04:24:00.247: INFO: Pod "security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008041148s
    Nov 30 04:24:02.246: INFO: Pod "security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007346757s
    STEP: Saw pod success 11/30/22 04:24:02.246
    Nov 30 04:24:02.246: INFO: Pod "security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693" satisfied condition "Succeeded or Failed"
    Nov 30 04:24:02.249: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693 container test-container: <nil>
    STEP: delete the pod 11/30/22 04:24:02.255
    Nov 30 04:24:02.267: INFO: Waiting for pod security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693 to disappear
    Nov 30 04:24:02.270: INFO: Pod security-context-75e3b00b-2097-4161-bc1f-80cbee2e3693 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Nov 30 04:24:02.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-9240" for this suite. 11/30/22 04:24:02.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:24:02.279
Nov 30 04:24:02.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 04:24:02.28
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:02.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:02.299
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 11/30/22 04:24:02.305
STEP: waiting for available Endpoint 11/30/22 04:24:02.31
STEP: listing all Endpoints 11/30/22 04:24:02.311
STEP: updating the Endpoint 11/30/22 04:24:02.314
STEP: fetching the Endpoint 11/30/22 04:24:02.32
STEP: patching the Endpoint 11/30/22 04:24:02.322
STEP: fetching the Endpoint 11/30/22 04:24:02.33
STEP: deleting the Endpoint by Collection 11/30/22 04:24:02.333
STEP: waiting for Endpoint deletion 11/30/22 04:24:02.339
STEP: fetching the Endpoint 11/30/22 04:24:02.34
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 04:24:02.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7565" for this suite. 11/30/22 04:24:02.346
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":218,"skipped":3920,"failed":0}
------------------------------
• [0.075 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:24:02.279
    Nov 30 04:24:02.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 04:24:02.28
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:02.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:02.299
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 11/30/22 04:24:02.305
    STEP: waiting for available Endpoint 11/30/22 04:24:02.31
    STEP: listing all Endpoints 11/30/22 04:24:02.311
    STEP: updating the Endpoint 11/30/22 04:24:02.314
    STEP: fetching the Endpoint 11/30/22 04:24:02.32
    STEP: patching the Endpoint 11/30/22 04:24:02.322
    STEP: fetching the Endpoint 11/30/22 04:24:02.33
    STEP: deleting the Endpoint by Collection 11/30/22 04:24:02.333
    STEP: waiting for Endpoint deletion 11/30/22 04:24:02.339
    STEP: fetching the Endpoint 11/30/22 04:24:02.34
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 04:24:02.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7565" for this suite. 11/30/22 04:24:02.346
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:24:02.355
Nov 30 04:24:02.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename resourcequota 11/30/22 04:24:02.356
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:02.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:02.375
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 11/30/22 04:24:02.377
STEP: Creating a ResourceQuota 11/30/22 04:24:07.381
STEP: Ensuring resource quota status is calculated 11/30/22 04:24:07.387
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Nov 30 04:24:09.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6010" for this suite. 11/30/22 04:24:09.395
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":219,"skipped":3940,"failed":0}
------------------------------
• [SLOW TEST] [7.045 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:24:02.355
    Nov 30 04:24:02.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename resourcequota 11/30/22 04:24:02.356
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:02.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:02.375
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 11/30/22 04:24:02.377
    STEP: Creating a ResourceQuota 11/30/22 04:24:07.381
    STEP: Ensuring resource quota status is calculated 11/30/22 04:24:07.387
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Nov 30 04:24:09.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6010" for this suite. 11/30/22 04:24:09.395
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:24:09.4
Nov 30 04:24:09.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename job 11/30/22 04:24:09.401
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:09.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:09.422
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 11/30/22 04:24:09.425
STEP: Ensuring active pods == parallelism 11/30/22 04:24:09.431
STEP: Orphaning one of the Job's Pods 11/30/22 04:24:11.434
Nov 30 04:24:11.947: INFO: Successfully updated pod "adopt-release-4rgz9"
STEP: Checking that the Job readopts the Pod 11/30/22 04:24:11.947
Nov 30 04:24:11.947: INFO: Waiting up to 15m0s for pod "adopt-release-4rgz9" in namespace "job-6884" to be "adopted"
Nov 30 04:24:11.949: INFO: Pod "adopt-release-4rgz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.518765ms
Nov 30 04:24:13.953: INFO: Pod "adopt-release-4rgz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00642338s
Nov 30 04:24:13.953: INFO: Pod "adopt-release-4rgz9" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 11/30/22 04:24:13.953
Nov 30 04:24:14.469: INFO: Successfully updated pod "adopt-release-4rgz9"
STEP: Checking that the Job releases the Pod 11/30/22 04:24:14.469
Nov 30 04:24:14.469: INFO: Waiting up to 15m0s for pod "adopt-release-4rgz9" in namespace "job-6884" to be "released"
Nov 30 04:24:14.472: INFO: Pod "adopt-release-4rgz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.822011ms
Nov 30 04:24:16.476: INFO: Pod "adopt-release-4rgz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007050053s
Nov 30 04:24:16.476: INFO: Pod "adopt-release-4rgz9" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Nov 30 04:24:16.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6884" for this suite. 11/30/22 04:24:16.48
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":220,"skipped":3943,"failed":0}
------------------------------
• [SLOW TEST] [7.087 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:24:09.4
    Nov 30 04:24:09.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename job 11/30/22 04:24:09.401
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:09.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:09.422
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 11/30/22 04:24:09.425
    STEP: Ensuring active pods == parallelism 11/30/22 04:24:09.431
    STEP: Orphaning one of the Job's Pods 11/30/22 04:24:11.434
    Nov 30 04:24:11.947: INFO: Successfully updated pod "adopt-release-4rgz9"
    STEP: Checking that the Job readopts the Pod 11/30/22 04:24:11.947
    Nov 30 04:24:11.947: INFO: Waiting up to 15m0s for pod "adopt-release-4rgz9" in namespace "job-6884" to be "adopted"
    Nov 30 04:24:11.949: INFO: Pod "adopt-release-4rgz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.518765ms
    Nov 30 04:24:13.953: INFO: Pod "adopt-release-4rgz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00642338s
    Nov 30 04:24:13.953: INFO: Pod "adopt-release-4rgz9" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 11/30/22 04:24:13.953
    Nov 30 04:24:14.469: INFO: Successfully updated pod "adopt-release-4rgz9"
    STEP: Checking that the Job releases the Pod 11/30/22 04:24:14.469
    Nov 30 04:24:14.469: INFO: Waiting up to 15m0s for pod "adopt-release-4rgz9" in namespace "job-6884" to be "released"
    Nov 30 04:24:14.472: INFO: Pod "adopt-release-4rgz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.822011ms
    Nov 30 04:24:16.476: INFO: Pod "adopt-release-4rgz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007050053s
    Nov 30 04:24:16.476: INFO: Pod "adopt-release-4rgz9" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Nov 30 04:24:16.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6884" for this suite. 11/30/22 04:24:16.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:24:16.488
Nov 30 04:24:16.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-runtime 11/30/22 04:24:16.489
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:16.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:16.508
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 11/30/22 04:24:16.51
STEP: wait for the container to reach Succeeded 11/30/22 04:24:16.547
STEP: get the container status 11/30/22 04:24:20.566
STEP: the container should be terminated 11/30/22 04:24:20.569
STEP: the termination message should be set 11/30/22 04:24:20.569
Nov 30 04:24:20.569: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 11/30/22 04:24:20.569
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Nov 30 04:24:20.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5280" for this suite. 11/30/22 04:24:20.586
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":221,"skipped":3956,"failed":0}
------------------------------
• [4.105 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:24:16.488
    Nov 30 04:24:16.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-runtime 11/30/22 04:24:16.489
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:16.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:16.508
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 11/30/22 04:24:16.51
    STEP: wait for the container to reach Succeeded 11/30/22 04:24:16.547
    STEP: get the container status 11/30/22 04:24:20.566
    STEP: the container should be terminated 11/30/22 04:24:20.569
    STEP: the termination message should be set 11/30/22 04:24:20.569
    Nov 30 04:24:20.569: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 11/30/22 04:24:20.569
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Nov 30 04:24:20.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5280" for this suite. 11/30/22 04:24:20.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:24:20.594
Nov 30 04:24:20.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename svcaccounts 11/30/22 04:24:20.595
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:20.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:20.62
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 11/30/22 04:24:20.623
STEP: watching for the ServiceAccount to be added 11/30/22 04:24:20.635
STEP: patching the ServiceAccount 11/30/22 04:24:20.636
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 11/30/22 04:24:20.649
STEP: deleting the ServiceAccount 11/30/22 04:24:20.654
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Nov 30 04:24:20.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4883" for this suite. 11/30/22 04:24:20.693
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":222,"skipped":3980,"failed":0}
------------------------------
• [0.114 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:24:20.594
    Nov 30 04:24:20.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename svcaccounts 11/30/22 04:24:20.595
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:20.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:20.62
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 11/30/22 04:24:20.623
    STEP: watching for the ServiceAccount to be added 11/30/22 04:24:20.635
    STEP: patching the ServiceAccount 11/30/22 04:24:20.636
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 11/30/22 04:24:20.649
    STEP: deleting the ServiceAccount 11/30/22 04:24:20.654
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Nov 30 04:24:20.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4883" for this suite. 11/30/22 04:24:20.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:24:20.709
Nov 30 04:24:20.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 04:24:20.709
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:20.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:20.727
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 11/30/22 04:24:20.729
STEP: listing secrets in all namespaces to ensure that there are more than zero 11/30/22 04:24:20.736
STEP: patching the secret 11/30/22 04:24:20.747
STEP: deleting the secret using a LabelSelector 11/30/22 04:24:20.758
STEP: listing secrets in all namespaces, searching for label name and value in patch 11/30/22 04:24:20.764
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Nov 30 04:24:20.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6079" for this suite. 11/30/22 04:24:20.778
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":223,"skipped":4020,"failed":0}
------------------------------
• [0.075 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:24:20.709
    Nov 30 04:24:20.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 04:24:20.709
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:20.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:20.727
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 11/30/22 04:24:20.729
    STEP: listing secrets in all namespaces to ensure that there are more than zero 11/30/22 04:24:20.736
    STEP: patching the secret 11/30/22 04:24:20.747
    STEP: deleting the secret using a LabelSelector 11/30/22 04:24:20.758
    STEP: listing secrets in all namespaces, searching for label name and value in patch 11/30/22 04:24:20.764
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 04:24:20.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6079" for this suite. 11/30/22 04:24:20.778
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:24:20.784
Nov 30 04:24:20.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 04:24:20.785
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:20.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:20.803
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-79c94d36-ab7d-47c4-8dfd-82a91e883fed 11/30/22 04:24:20.805
STEP: Creating a pod to test consume configMaps 11/30/22 04:24:20.811
Nov 30 04:24:20.849: INFO: Waiting up to 5m0s for pod "pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b" in namespace "configmap-2238" to be "Succeeded or Failed"
Nov 30 04:24:20.892: INFO: Pod "pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b": Phase="Pending", Reason="", readiness=false. Elapsed: 43.151528ms
Nov 30 04:24:22.896: INFO: Pod "pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047068111s
Nov 30 04:24:24.895: INFO: Pod "pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04671192s
STEP: Saw pod success 11/30/22 04:24:24.895
Nov 30 04:24:24.895: INFO: Pod "pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b" satisfied condition "Succeeded or Failed"
Nov 30 04:24:24.898: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b container agnhost-container: <nil>
STEP: delete the pod 11/30/22 04:24:24.904
Nov 30 04:24:24.917: INFO: Waiting for pod pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b to disappear
Nov 30 04:24:24.919: INFO: Pod pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 04:24:24.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2238" for this suite. 11/30/22 04:24:24.924
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":224,"skipped":4024,"failed":0}
------------------------------
• [4.147 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:24:20.784
    Nov 30 04:24:20.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 04:24:20.785
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:20.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:20.803
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-79c94d36-ab7d-47c4-8dfd-82a91e883fed 11/30/22 04:24:20.805
    STEP: Creating a pod to test consume configMaps 11/30/22 04:24:20.811
    Nov 30 04:24:20.849: INFO: Waiting up to 5m0s for pod "pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b" in namespace "configmap-2238" to be "Succeeded or Failed"
    Nov 30 04:24:20.892: INFO: Pod "pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b": Phase="Pending", Reason="", readiness=false. Elapsed: 43.151528ms
    Nov 30 04:24:22.896: INFO: Pod "pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047068111s
    Nov 30 04:24:24.895: INFO: Pod "pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04671192s
    STEP: Saw pod success 11/30/22 04:24:24.895
    Nov 30 04:24:24.895: INFO: Pod "pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b" satisfied condition "Succeeded or Failed"
    Nov 30 04:24:24.898: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 04:24:24.904
    Nov 30 04:24:24.917: INFO: Waiting for pod pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b to disappear
    Nov 30 04:24:24.919: INFO: Pod pod-configmaps-3fb08e05-40a5-4207-b1e8-31c4ee652f7b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 04:24:24.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2238" for this suite. 11/30/22 04:24:24.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:24:24.932
Nov 30 04:24:24.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename statefulset 11/30/22 04:24:24.933
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:24.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:24.96
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-195 11/30/22 04:24:24.963
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 11/30/22 04:24:24.977
Nov 30 04:24:24.987: INFO: Found 0 stateful pods, waiting for 3
Nov 30 04:24:34.992: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 30 04:24:34.992: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 30 04:24:34.992: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 to armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.39-2 11/30/22 04:24:35
Nov 30 04:24:35.023: INFO: Updating stateful set ss2
STEP: Creating a new revision 11/30/22 04:24:35.023
STEP: Not applying an update when the partition is greater than the number of replicas 11/30/22 04:24:45.037
STEP: Performing a canary update 11/30/22 04:24:45.037
Nov 30 04:24:45.083: INFO: Updating stateful set ss2
Nov 30 04:24:45.104: INFO: Waiting for Pod statefulset-195/ss2-2 to have revision ss2-7847bcd8f update revision ss2-68b674bdd8
STEP: Restoring Pods to the correct revision when they are deleted 11/30/22 04:24:55.12
Nov 30 04:24:55.194: INFO: Found 1 stateful pods, waiting for 3
Nov 30 04:25:05.202: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 30 04:25:05.202: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 30 04:25:05.202: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 11/30/22 04:25:05.207
Nov 30 04:25:05.228: INFO: Updating stateful set ss2
Nov 30 04:25:05.237: INFO: Waiting for Pod statefulset-195/ss2-1 to have revision ss2-7847bcd8f update revision ss2-68b674bdd8
Nov 30 04:25:15.268: INFO: Updating stateful set ss2
Nov 30 04:25:15.275: INFO: Waiting for StatefulSet statefulset-195/ss2 to complete update
Nov 30 04:25:15.275: INFO: Waiting for Pod statefulset-195/ss2-0 to have revision ss2-7847bcd8f update revision ss2-68b674bdd8
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Nov 30 04:25:25.281: INFO: Deleting all statefulset in ns statefulset-195
Nov 30 04:25:25.283: INFO: Scaling statefulset ss2 to 0
Nov 30 04:25:35.298: INFO: Waiting for statefulset status.replicas updated to 0
Nov 30 04:25:35.301: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Nov 30 04:25:35.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-195" for this suite. 11/30/22 04:25:35.316
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":225,"skipped":4037,"failed":0}
------------------------------
• [SLOW TEST] [70.392 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:24:24.932
    Nov 30 04:24:24.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename statefulset 11/30/22 04:24:24.933
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:24:24.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:24:24.96
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-195 11/30/22 04:24:24.963
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 11/30/22 04:24:24.977
    Nov 30 04:24:24.987: INFO: Found 0 stateful pods, waiting for 3
    Nov 30 04:24:34.992: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 30 04:24:34.992: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 30 04:24:34.992: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 to armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.39-2 11/30/22 04:24:35
    Nov 30 04:24:35.023: INFO: Updating stateful set ss2
    STEP: Creating a new revision 11/30/22 04:24:35.023
    STEP: Not applying an update when the partition is greater than the number of replicas 11/30/22 04:24:45.037
    STEP: Performing a canary update 11/30/22 04:24:45.037
    Nov 30 04:24:45.083: INFO: Updating stateful set ss2
    Nov 30 04:24:45.104: INFO: Waiting for Pod statefulset-195/ss2-2 to have revision ss2-7847bcd8f update revision ss2-68b674bdd8
    STEP: Restoring Pods to the correct revision when they are deleted 11/30/22 04:24:55.12
    Nov 30 04:24:55.194: INFO: Found 1 stateful pods, waiting for 3
    Nov 30 04:25:05.202: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 30 04:25:05.202: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 30 04:25:05.202: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 11/30/22 04:25:05.207
    Nov 30 04:25:05.228: INFO: Updating stateful set ss2
    Nov 30 04:25:05.237: INFO: Waiting for Pod statefulset-195/ss2-1 to have revision ss2-7847bcd8f update revision ss2-68b674bdd8
    Nov 30 04:25:15.268: INFO: Updating stateful set ss2
    Nov 30 04:25:15.275: INFO: Waiting for StatefulSet statefulset-195/ss2 to complete update
    Nov 30 04:25:15.275: INFO: Waiting for Pod statefulset-195/ss2-0 to have revision ss2-7847bcd8f update revision ss2-68b674bdd8
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Nov 30 04:25:25.281: INFO: Deleting all statefulset in ns statefulset-195
    Nov 30 04:25:25.283: INFO: Scaling statefulset ss2 to 0
    Nov 30 04:25:35.298: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 30 04:25:35.301: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Nov 30 04:25:35.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-195" for this suite. 11/30/22 04:25:35.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:25:35.325
Nov 30 04:25:35.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename events 11/30/22 04:25:35.326
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:25:35.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:25:35.352
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 11/30/22 04:25:35.354
STEP: get a list of Events with a label in the current namespace 11/30/22 04:25:35.373
STEP: delete a list of events 11/30/22 04:25:35.376
Nov 30 04:25:35.376: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 11/30/22 04:25:35.393
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Nov 30 04:25:35.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4610" for this suite. 11/30/22 04:25:35.399
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":226,"skipped":4061,"failed":0}
------------------------------
• [0.079 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:25:35.325
    Nov 30 04:25:35.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename events 11/30/22 04:25:35.326
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:25:35.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:25:35.352
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 11/30/22 04:25:35.354
    STEP: get a list of Events with a label in the current namespace 11/30/22 04:25:35.373
    STEP: delete a list of events 11/30/22 04:25:35.376
    Nov 30 04:25:35.376: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 11/30/22 04:25:35.393
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Nov 30 04:25:35.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4610" for this suite. 11/30/22 04:25:35.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:25:35.405
Nov 30 04:25:35.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pods 11/30/22 04:25:35.406
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:25:35.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:25:35.426
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 11/30/22 04:25:35.44
STEP: watching for Pod to be ready 11/30/22 04:25:35.477
Nov 30 04:25:35.479: INFO: observed Pod pod-test in namespace pods-5342 in phase Pending with labels: map[test-pod-static:true] & conditions []
Nov 30 04:25:35.483: INFO: observed Pod pod-test in namespace pods-5342 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  }]
Nov 30 04:25:35.497: INFO: observed Pod pod-test in namespace pods-5342 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  }]
Nov 30 04:25:36.021: INFO: observed Pod pod-test in namespace pods-5342 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  }]
Nov 30 04:25:36.853: INFO: Found Pod pod-test in namespace pods-5342 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 11/30/22 04:25:36.856
STEP: getting the Pod and ensuring that it's patched 11/30/22 04:25:36.866
STEP: replacing the Pod's status Ready condition to False 11/30/22 04:25:36.872
STEP: check the Pod again to ensure its Ready conditions are False 11/30/22 04:25:36.884
STEP: deleting the Pod via a Collection with a LabelSelector 11/30/22 04:25:36.885
STEP: watching for the Pod to be deleted 11/30/22 04:25:36.893
Nov 30 04:25:36.895: INFO: observed event type MODIFIED
Nov 30 04:25:37.258: INFO: observed event type MODIFIED
Nov 30 04:25:39.867: INFO: observed event type MODIFIED
Nov 30 04:25:39.876: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Nov 30 04:25:39.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5342" for this suite. 11/30/22 04:25:39.891
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":227,"skipped":4079,"failed":0}
------------------------------
• [4.491 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:25:35.405
    Nov 30 04:25:35.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pods 11/30/22 04:25:35.406
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:25:35.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:25:35.426
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 11/30/22 04:25:35.44
    STEP: watching for Pod to be ready 11/30/22 04:25:35.477
    Nov 30 04:25:35.479: INFO: observed Pod pod-test in namespace pods-5342 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Nov 30 04:25:35.483: INFO: observed Pod pod-test in namespace pods-5342 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  }]
    Nov 30 04:25:35.497: INFO: observed Pod pod-test in namespace pods-5342 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  }]
    Nov 30 04:25:36.021: INFO: observed Pod pod-test in namespace pods-5342 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  }]
    Nov 30 04:25:36.853: INFO: Found Pod pod-test in namespace pods-5342 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:25:35 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 11/30/22 04:25:36.856
    STEP: getting the Pod and ensuring that it's patched 11/30/22 04:25:36.866
    STEP: replacing the Pod's status Ready condition to False 11/30/22 04:25:36.872
    STEP: check the Pod again to ensure its Ready conditions are False 11/30/22 04:25:36.884
    STEP: deleting the Pod via a Collection with a LabelSelector 11/30/22 04:25:36.885
    STEP: watching for the Pod to be deleted 11/30/22 04:25:36.893
    Nov 30 04:25:36.895: INFO: observed event type MODIFIED
    Nov 30 04:25:37.258: INFO: observed event type MODIFIED
    Nov 30 04:25:39.867: INFO: observed event type MODIFIED
    Nov 30 04:25:39.876: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Nov 30 04:25:39.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5342" for this suite. 11/30/22 04:25:39.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:25:39.899
Nov 30 04:25:39.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename podtemplate 11/30/22 04:25:39.899
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:25:39.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:25:39.917
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 11/30/22 04:25:39.919
STEP: Replace a pod template 11/30/22 04:25:39.924
Nov 30 04:25:39.937: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Nov 30 04:25:39.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3956" for this suite. 11/30/22 04:25:39.942
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":228,"skipped":4095,"failed":0}
------------------------------
• [0.049 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:25:39.899
    Nov 30 04:25:39.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename podtemplate 11/30/22 04:25:39.899
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:25:39.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:25:39.917
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 11/30/22 04:25:39.919
    STEP: Replace a pod template 11/30/22 04:25:39.924
    Nov 30 04:25:39.937: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Nov 30 04:25:39.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3956" for this suite. 11/30/22 04:25:39.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:25:39.949
Nov 30 04:25:39.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename custom-resource-definition 11/30/22 04:25:39.95
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:25:39.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:25:39.967
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 11/30/22 04:25:39.969
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 11/30/22 04:25:39.97
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 11/30/22 04:25:39.97
STEP: fetching the /apis/apiextensions.k8s.io discovery document 11/30/22 04:25:39.97
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 11/30/22 04:25:39.971
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 11/30/22 04:25:39.971
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 11/30/22 04:25:39.972
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:25:39.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7051" for this suite. 11/30/22 04:25:39.978
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":229,"skipped":4139,"failed":0}
------------------------------
• [0.038 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:25:39.949
    Nov 30 04:25:39.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename custom-resource-definition 11/30/22 04:25:39.95
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:25:39.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:25:39.967
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 11/30/22 04:25:39.969
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 11/30/22 04:25:39.97
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 11/30/22 04:25:39.97
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 11/30/22 04:25:39.97
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 11/30/22 04:25:39.971
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 11/30/22 04:25:39.971
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 11/30/22 04:25:39.972
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:25:39.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7051" for this suite. 11/30/22 04:25:39.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:25:39.987
Nov 30 04:25:39.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 04:25:39.988
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:25:40.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:25:40.01
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 11/30/22 04:25:40.012
Nov 30 04:25:40.027: INFO: Waiting up to 5m0s for pod "pod-b64669c8-2cee-4d22-8ef4-088139ac9355" in namespace "emptydir-2314" to be "Succeeded or Failed"
Nov 30 04:25:40.034: INFO: Pod "pod-b64669c8-2cee-4d22-8ef4-088139ac9355": Phase="Pending", Reason="", readiness=false. Elapsed: 6.715013ms
Nov 30 04:25:42.037: INFO: Pod "pod-b64669c8-2cee-4d22-8ef4-088139ac9355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009960704s
Nov 30 04:25:44.037: INFO: Pod "pod-b64669c8-2cee-4d22-8ef4-088139ac9355": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009788828s
STEP: Saw pod success 11/30/22 04:25:44.037
Nov 30 04:25:44.037: INFO: Pod "pod-b64669c8-2cee-4d22-8ef4-088139ac9355" satisfied condition "Succeeded or Failed"
Nov 30 04:25:44.040: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-b64669c8-2cee-4d22-8ef4-088139ac9355 container test-container: <nil>
STEP: delete the pod 11/30/22 04:25:44.046
Nov 30 04:25:44.059: INFO: Waiting for pod pod-b64669c8-2cee-4d22-8ef4-088139ac9355 to disappear
Nov 30 04:25:44.062: INFO: Pod pod-b64669c8-2cee-4d22-8ef4-088139ac9355 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 04:25:44.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2314" for this suite. 11/30/22 04:25:44.065
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":230,"skipped":4145,"failed":0}
------------------------------
• [4.083 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:25:39.987
    Nov 30 04:25:39.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 04:25:39.988
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:25:40.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:25:40.01
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 11/30/22 04:25:40.012
    Nov 30 04:25:40.027: INFO: Waiting up to 5m0s for pod "pod-b64669c8-2cee-4d22-8ef4-088139ac9355" in namespace "emptydir-2314" to be "Succeeded or Failed"
    Nov 30 04:25:40.034: INFO: Pod "pod-b64669c8-2cee-4d22-8ef4-088139ac9355": Phase="Pending", Reason="", readiness=false. Elapsed: 6.715013ms
    Nov 30 04:25:42.037: INFO: Pod "pod-b64669c8-2cee-4d22-8ef4-088139ac9355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009960704s
    Nov 30 04:25:44.037: INFO: Pod "pod-b64669c8-2cee-4d22-8ef4-088139ac9355": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009788828s
    STEP: Saw pod success 11/30/22 04:25:44.037
    Nov 30 04:25:44.037: INFO: Pod "pod-b64669c8-2cee-4d22-8ef4-088139ac9355" satisfied condition "Succeeded or Failed"
    Nov 30 04:25:44.040: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-b64669c8-2cee-4d22-8ef4-088139ac9355 container test-container: <nil>
    STEP: delete the pod 11/30/22 04:25:44.046
    Nov 30 04:25:44.059: INFO: Waiting for pod pod-b64669c8-2cee-4d22-8ef4-088139ac9355 to disappear
    Nov 30 04:25:44.062: INFO: Pod pod-b64669c8-2cee-4d22-8ef4-088139ac9355 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 04:25:44.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2314" for this suite. 11/30/22 04:25:44.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:25:44.071
Nov 30 04:25:44.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename statefulset 11/30/22 04:25:44.072
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:25:44.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:25:44.087
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2014 11/30/22 04:25:44.09
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-2014 11/30/22 04:25:44.095
Nov 30 04:25:44.102: INFO: Found 0 stateful pods, waiting for 1
Nov 30 04:25:54.106: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 11/30/22 04:25:54.111
STEP: updating a scale subresource 11/30/22 04:25:54.114
STEP: verifying the statefulset Spec.Replicas was modified 11/30/22 04:25:54.121
STEP: Patch a scale subresource 11/30/22 04:25:54.123
STEP: verifying the statefulset Spec.Replicas was modified 11/30/22 04:25:54.13
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Nov 30 04:25:54.132: INFO: Deleting all statefulset in ns statefulset-2014
Nov 30 04:25:54.134: INFO: Scaling statefulset ss to 0
Nov 30 04:26:04.153: INFO: Waiting for statefulset status.replicas updated to 0
Nov 30 04:26:04.156: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Nov 30 04:26:04.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2014" for this suite. 11/30/22 04:26:04.174
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":231,"skipped":4163,"failed":0}
------------------------------
• [SLOW TEST] [20.110 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:25:44.071
    Nov 30 04:25:44.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename statefulset 11/30/22 04:25:44.072
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:25:44.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:25:44.087
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2014 11/30/22 04:25:44.09
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-2014 11/30/22 04:25:44.095
    Nov 30 04:25:44.102: INFO: Found 0 stateful pods, waiting for 1
    Nov 30 04:25:54.106: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 11/30/22 04:25:54.111
    STEP: updating a scale subresource 11/30/22 04:25:54.114
    STEP: verifying the statefulset Spec.Replicas was modified 11/30/22 04:25:54.121
    STEP: Patch a scale subresource 11/30/22 04:25:54.123
    STEP: verifying the statefulset Spec.Replicas was modified 11/30/22 04:25:54.13
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Nov 30 04:25:54.132: INFO: Deleting all statefulset in ns statefulset-2014
    Nov 30 04:25:54.134: INFO: Scaling statefulset ss to 0
    Nov 30 04:26:04.153: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 30 04:26:04.156: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Nov 30 04:26:04.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2014" for this suite. 11/30/22 04:26:04.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:26:04.182
Nov 30 04:26:04.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubelet-test 11/30/22 04:26:04.182
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:26:04.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:26:04.205
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Nov 30 04:26:08.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4032" for this suite. 11/30/22 04:26:08.246
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":232,"skipped":4214,"failed":0}
------------------------------
• [4.070 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:26:04.182
    Nov 30 04:26:04.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubelet-test 11/30/22 04:26:04.182
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:26:04.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:26:04.205
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Nov 30 04:26:08.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4032" for this suite. 11/30/22 04:26:08.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:26:08.252
Nov 30 04:26:08.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-probe 11/30/22 04:26:08.253
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:26:08.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:26:08.345
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-631f25e3-1599-4103-a5f4-b35e2da26259 in namespace container-probe-687 11/30/22 04:26:08.347
Nov 30 04:26:08.371: INFO: Waiting up to 5m0s for pod "liveness-631f25e3-1599-4103-a5f4-b35e2da26259" in namespace "container-probe-687" to be "not pending"
Nov 30 04:26:08.375: INFO: Pod "liveness-631f25e3-1599-4103-a5f4-b35e2da26259": Phase="Pending", Reason="", readiness=false. Elapsed: 3.792731ms
Nov 30 04:26:10.379: INFO: Pod "liveness-631f25e3-1599-4103-a5f4-b35e2da26259": Phase="Running", Reason="", readiness=true. Elapsed: 2.007490571s
Nov 30 04:26:10.379: INFO: Pod "liveness-631f25e3-1599-4103-a5f4-b35e2da26259" satisfied condition "not pending"
Nov 30 04:26:10.379: INFO: Started pod liveness-631f25e3-1599-4103-a5f4-b35e2da26259 in namespace container-probe-687
STEP: checking the pod's current state and verifying that restartCount is present 11/30/22 04:26:10.379
Nov 30 04:26:10.382: INFO: Initial restart count of pod liveness-631f25e3-1599-4103-a5f4-b35e2da26259 is 0
Nov 30 04:26:30.428: INFO: Restart count of pod container-probe-687/liveness-631f25e3-1599-4103-a5f4-b35e2da26259 is now 1 (20.045704921s elapsed)
Nov 30 04:26:50.466: INFO: Restart count of pod container-probe-687/liveness-631f25e3-1599-4103-a5f4-b35e2da26259 is now 2 (40.083629359s elapsed)
Nov 30 04:27:10.505: INFO: Restart count of pod container-probe-687/liveness-631f25e3-1599-4103-a5f4-b35e2da26259 is now 3 (1m0.122789651s elapsed)
Nov 30 04:27:30.550: INFO: Restart count of pod container-probe-687/liveness-631f25e3-1599-4103-a5f4-b35e2da26259 is now 4 (1m20.167846501s elapsed)
Nov 30 04:28:38.696: INFO: Restart count of pod container-probe-687/liveness-631f25e3-1599-4103-a5f4-b35e2da26259 is now 5 (2m28.314482865s elapsed)
STEP: deleting the pod 11/30/22 04:28:38.696
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Nov 30 04:28:38.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-687" for this suite. 11/30/22 04:28:38.714
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":233,"skipped":4236,"failed":0}
------------------------------
• [SLOW TEST] [150.470 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:26:08.252
    Nov 30 04:26:08.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-probe 11/30/22 04:26:08.253
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:26:08.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:26:08.345
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-631f25e3-1599-4103-a5f4-b35e2da26259 in namespace container-probe-687 11/30/22 04:26:08.347
    Nov 30 04:26:08.371: INFO: Waiting up to 5m0s for pod "liveness-631f25e3-1599-4103-a5f4-b35e2da26259" in namespace "container-probe-687" to be "not pending"
    Nov 30 04:26:08.375: INFO: Pod "liveness-631f25e3-1599-4103-a5f4-b35e2da26259": Phase="Pending", Reason="", readiness=false. Elapsed: 3.792731ms
    Nov 30 04:26:10.379: INFO: Pod "liveness-631f25e3-1599-4103-a5f4-b35e2da26259": Phase="Running", Reason="", readiness=true. Elapsed: 2.007490571s
    Nov 30 04:26:10.379: INFO: Pod "liveness-631f25e3-1599-4103-a5f4-b35e2da26259" satisfied condition "not pending"
    Nov 30 04:26:10.379: INFO: Started pod liveness-631f25e3-1599-4103-a5f4-b35e2da26259 in namespace container-probe-687
    STEP: checking the pod's current state and verifying that restartCount is present 11/30/22 04:26:10.379
    Nov 30 04:26:10.382: INFO: Initial restart count of pod liveness-631f25e3-1599-4103-a5f4-b35e2da26259 is 0
    Nov 30 04:26:30.428: INFO: Restart count of pod container-probe-687/liveness-631f25e3-1599-4103-a5f4-b35e2da26259 is now 1 (20.045704921s elapsed)
    Nov 30 04:26:50.466: INFO: Restart count of pod container-probe-687/liveness-631f25e3-1599-4103-a5f4-b35e2da26259 is now 2 (40.083629359s elapsed)
    Nov 30 04:27:10.505: INFO: Restart count of pod container-probe-687/liveness-631f25e3-1599-4103-a5f4-b35e2da26259 is now 3 (1m0.122789651s elapsed)
    Nov 30 04:27:30.550: INFO: Restart count of pod container-probe-687/liveness-631f25e3-1599-4103-a5f4-b35e2da26259 is now 4 (1m20.167846501s elapsed)
    Nov 30 04:28:38.696: INFO: Restart count of pod container-probe-687/liveness-631f25e3-1599-4103-a5f4-b35e2da26259 is now 5 (2m28.314482865s elapsed)
    STEP: deleting the pod 11/30/22 04:28:38.696
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Nov 30 04:28:38.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-687" for this suite. 11/30/22 04:28:38.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:28:38.722
Nov 30 04:28:38.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename limitrange 11/30/22 04:28:38.723
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:38.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:38.749
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 11/30/22 04:28:38.752
STEP: Setting up watch 11/30/22 04:28:38.752
STEP: Submitting a LimitRange 11/30/22 04:28:38.86
STEP: Verifying LimitRange creation was observed 11/30/22 04:28:38.872
STEP: Fetching the LimitRange to ensure it has proper values 11/30/22 04:28:38.872
Nov 30 04:28:38.874: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 30 04:28:38.874: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 11/30/22 04:28:38.874
STEP: Ensuring Pod has resource requirements applied from LimitRange 11/30/22 04:28:38.904
Nov 30 04:28:38.909: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 30 04:28:38.909: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 11/30/22 04:28:38.909
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 11/30/22 04:28:38.916
Nov 30 04:28:38.920: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov 30 04:28:38.920: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 11/30/22 04:28:38.92
STEP: Failing to create a Pod with more than max resources 11/30/22 04:28:38.924
STEP: Updating a LimitRange 11/30/22 04:28:38.929
STEP: Verifying LimitRange updating is effective 11/30/22 04:28:38.935
STEP: Creating a Pod with less than former min resources 11/30/22 04:28:40.939
STEP: Failing to create a Pod with more than max resources 11/30/22 04:28:40.947
STEP: Deleting a LimitRange 11/30/22 04:28:40.95
STEP: Verifying the LimitRange was deleted 11/30/22 04:28:40.957
Nov 30 04:28:45.966: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 11/30/22 04:28:45.966
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Nov 30 04:28:45.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2029" for this suite. 11/30/22 04:28:46.002
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":234,"skipped":4263,"failed":0}
------------------------------
• [SLOW TEST] [7.289 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:28:38.722
    Nov 30 04:28:38.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename limitrange 11/30/22 04:28:38.723
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:38.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:38.749
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 11/30/22 04:28:38.752
    STEP: Setting up watch 11/30/22 04:28:38.752
    STEP: Submitting a LimitRange 11/30/22 04:28:38.86
    STEP: Verifying LimitRange creation was observed 11/30/22 04:28:38.872
    STEP: Fetching the LimitRange to ensure it has proper values 11/30/22 04:28:38.872
    Nov 30 04:28:38.874: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Nov 30 04:28:38.874: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 11/30/22 04:28:38.874
    STEP: Ensuring Pod has resource requirements applied from LimitRange 11/30/22 04:28:38.904
    Nov 30 04:28:38.909: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Nov 30 04:28:38.909: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 11/30/22 04:28:38.909
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 11/30/22 04:28:38.916
    Nov 30 04:28:38.920: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Nov 30 04:28:38.920: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 11/30/22 04:28:38.92
    STEP: Failing to create a Pod with more than max resources 11/30/22 04:28:38.924
    STEP: Updating a LimitRange 11/30/22 04:28:38.929
    STEP: Verifying LimitRange updating is effective 11/30/22 04:28:38.935
    STEP: Creating a Pod with less than former min resources 11/30/22 04:28:40.939
    STEP: Failing to create a Pod with more than max resources 11/30/22 04:28:40.947
    STEP: Deleting a LimitRange 11/30/22 04:28:40.95
    STEP: Verifying the LimitRange was deleted 11/30/22 04:28:40.957
    Nov 30 04:28:45.966: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 11/30/22 04:28:45.966
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Nov 30 04:28:45.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-2029" for this suite. 11/30/22 04:28:46.002
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:28:46.011
Nov 30 04:28:46.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 04:28:46.013
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:46.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:46.032
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 11/30/22 04:28:46.035
Nov 30 04:28:46.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9248 cluster-info'
Nov 30 04:28:46.117: INFO: stderr: ""
Nov 30 04:28:46.117: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 04:28:46.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9248" for this suite. 11/30/22 04:28:46.122
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":235,"skipped":4265,"failed":0}
------------------------------
• [0.116 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:28:46.011
    Nov 30 04:28:46.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 04:28:46.013
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:46.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:46.032
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 11/30/22 04:28:46.035
    Nov 30 04:28:46.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9248 cluster-info'
    Nov 30 04:28:46.117: INFO: stderr: ""
    Nov 30 04:28:46.117: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 04:28:46.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9248" for this suite. 11/30/22 04:28:46.122
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:28:46.127
Nov 30 04:28:46.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 04:28:46.128
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:46.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:46.148
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-334e92e8-9f55-47cb-8dbf-541136740c78 11/30/22 04:28:46.15
STEP: Creating a pod to test consume configMaps 11/30/22 04:28:46.154
Nov 30 04:28:46.165: INFO: Waiting up to 5m0s for pod "pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e" in namespace "configmap-4772" to be "Succeeded or Failed"
Nov 30 04:28:46.172: INFO: Pod "pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.722262ms
Nov 30 04:28:48.176: INFO: Pod "pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011277134s
Nov 30 04:28:50.175: INFO: Pod "pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009954644s
STEP: Saw pod success 11/30/22 04:28:50.175
Nov 30 04:28:50.175: INFO: Pod "pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e" satisfied condition "Succeeded or Failed"
Nov 30 04:28:50.178: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e container configmap-volume-test: <nil>
STEP: delete the pod 11/30/22 04:28:50.183
Nov 30 04:28:50.196: INFO: Waiting for pod pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e to disappear
Nov 30 04:28:50.198: INFO: Pod pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 04:28:50.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4772" for this suite. 11/30/22 04:28:50.202
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":236,"skipped":4268,"failed":0}
------------------------------
• [4.080 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:28:46.127
    Nov 30 04:28:46.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 04:28:46.128
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:46.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:46.148
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-334e92e8-9f55-47cb-8dbf-541136740c78 11/30/22 04:28:46.15
    STEP: Creating a pod to test consume configMaps 11/30/22 04:28:46.154
    Nov 30 04:28:46.165: INFO: Waiting up to 5m0s for pod "pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e" in namespace "configmap-4772" to be "Succeeded or Failed"
    Nov 30 04:28:46.172: INFO: Pod "pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.722262ms
    Nov 30 04:28:48.176: INFO: Pod "pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011277134s
    Nov 30 04:28:50.175: INFO: Pod "pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009954644s
    STEP: Saw pod success 11/30/22 04:28:50.175
    Nov 30 04:28:50.175: INFO: Pod "pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e" satisfied condition "Succeeded or Failed"
    Nov 30 04:28:50.178: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e container configmap-volume-test: <nil>
    STEP: delete the pod 11/30/22 04:28:50.183
    Nov 30 04:28:50.196: INFO: Waiting for pod pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e to disappear
    Nov 30 04:28:50.198: INFO: Pod pod-configmaps-65e3b66f-add9-417e-b4b6-599c52cb398e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 04:28:50.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4772" for this suite. 11/30/22 04:28:50.202
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:28:50.208
Nov 30 04:28:50.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename watch 11/30/22 04:28:50.208
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:50.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:50.229
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 11/30/22 04:28:50.233
STEP: creating a new configmap 11/30/22 04:28:50.234
STEP: modifying the configmap once 11/30/22 04:28:50.24
STEP: closing the watch once it receives two notifications 11/30/22 04:28:50.248
Nov 30 04:28:50.248: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9321  f55abc93-2c22-4b06-bfd6-b6e69fe4d0ed 50542 0 2022-11-30 04:28:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-11-30 04:28:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:28:50.248: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9321  f55abc93-2c22-4b06-bfd6-b6e69fe4d0ed 50543 0 2022-11-30 04:28:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-11-30 04:28:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 11/30/22 04:28:50.248
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 11/30/22 04:28:50.256
STEP: deleting the configmap 11/30/22 04:28:50.257
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 11/30/22 04:28:50.263
Nov 30 04:28:50.263: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9321  f55abc93-2c22-4b06-bfd6-b6e69fe4d0ed 50544 0 2022-11-30 04:28:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-11-30 04:28:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:28:50.263: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9321  f55abc93-2c22-4b06-bfd6-b6e69fe4d0ed 50545 0 2022-11-30 04:28:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-11-30 04:28:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Nov 30 04:28:50.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9321" for this suite. 11/30/22 04:28:50.268
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":237,"skipped":4270,"failed":0}
------------------------------
• [0.067 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:28:50.208
    Nov 30 04:28:50.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename watch 11/30/22 04:28:50.208
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:50.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:50.229
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 11/30/22 04:28:50.233
    STEP: creating a new configmap 11/30/22 04:28:50.234
    STEP: modifying the configmap once 11/30/22 04:28:50.24
    STEP: closing the watch once it receives two notifications 11/30/22 04:28:50.248
    Nov 30 04:28:50.248: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9321  f55abc93-2c22-4b06-bfd6-b6e69fe4d0ed 50542 0 2022-11-30 04:28:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-11-30 04:28:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:28:50.248: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9321  f55abc93-2c22-4b06-bfd6-b6e69fe4d0ed 50543 0 2022-11-30 04:28:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-11-30 04:28:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 11/30/22 04:28:50.248
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 11/30/22 04:28:50.256
    STEP: deleting the configmap 11/30/22 04:28:50.257
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 11/30/22 04:28:50.263
    Nov 30 04:28:50.263: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9321  f55abc93-2c22-4b06-bfd6-b6e69fe4d0ed 50544 0 2022-11-30 04:28:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-11-30 04:28:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:28:50.263: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9321  f55abc93-2c22-4b06-bfd6-b6e69fe4d0ed 50545 0 2022-11-30 04:28:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-11-30 04:28:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Nov 30 04:28:50.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9321" for this suite. 11/30/22 04:28:50.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:28:50.275
Nov 30 04:28:50.275: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 04:28:50.276
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:50.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:50.297
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 11/30/22 04:28:50.299
Nov 30 04:28:50.314: INFO: Waiting up to 5m0s for pod "downward-api-40b04349-9049-49de-a73b-51c5b16cac8d" in namespace "downward-api-1495" to be "Succeeded or Failed"
Nov 30 04:28:50.318: INFO: Pod "downward-api-40b04349-9049-49de-a73b-51c5b16cac8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035864ms
Nov 30 04:28:52.322: INFO: Pod "downward-api-40b04349-9049-49de-a73b-51c5b16cac8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007893695s
Nov 30 04:28:54.323: INFO: Pod "downward-api-40b04349-9049-49de-a73b-51c5b16cac8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009163462s
STEP: Saw pod success 11/30/22 04:28:54.323
Nov 30 04:28:54.323: INFO: Pod "downward-api-40b04349-9049-49de-a73b-51c5b16cac8d" satisfied condition "Succeeded or Failed"
Nov 30 04:28:54.327: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downward-api-40b04349-9049-49de-a73b-51c5b16cac8d container dapi-container: <nil>
STEP: delete the pod 11/30/22 04:28:54.332
Nov 30 04:28:54.342: INFO: Waiting for pod downward-api-40b04349-9049-49de-a73b-51c5b16cac8d to disappear
Nov 30 04:28:54.345: INFO: Pod downward-api-40b04349-9049-49de-a73b-51c5b16cac8d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Nov 30 04:28:54.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1495" for this suite. 11/30/22 04:28:54.349
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":238,"skipped":4289,"failed":0}
------------------------------
• [4.079 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:28:50.275
    Nov 30 04:28:50.275: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 04:28:50.276
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:50.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:50.297
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 11/30/22 04:28:50.299
    Nov 30 04:28:50.314: INFO: Waiting up to 5m0s for pod "downward-api-40b04349-9049-49de-a73b-51c5b16cac8d" in namespace "downward-api-1495" to be "Succeeded or Failed"
    Nov 30 04:28:50.318: INFO: Pod "downward-api-40b04349-9049-49de-a73b-51c5b16cac8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035864ms
    Nov 30 04:28:52.322: INFO: Pod "downward-api-40b04349-9049-49de-a73b-51c5b16cac8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007893695s
    Nov 30 04:28:54.323: INFO: Pod "downward-api-40b04349-9049-49de-a73b-51c5b16cac8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009163462s
    STEP: Saw pod success 11/30/22 04:28:54.323
    Nov 30 04:28:54.323: INFO: Pod "downward-api-40b04349-9049-49de-a73b-51c5b16cac8d" satisfied condition "Succeeded or Failed"
    Nov 30 04:28:54.327: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downward-api-40b04349-9049-49de-a73b-51c5b16cac8d container dapi-container: <nil>
    STEP: delete the pod 11/30/22 04:28:54.332
    Nov 30 04:28:54.342: INFO: Waiting for pod downward-api-40b04349-9049-49de-a73b-51c5b16cac8d to disappear
    Nov 30 04:28:54.345: INFO: Pod downward-api-40b04349-9049-49de-a73b-51c5b16cac8d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Nov 30 04:28:54.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1495" for this suite. 11/30/22 04:28:54.349
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:28:54.354
Nov 30 04:28:54.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename ingress 11/30/22 04:28:54.355
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:54.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:54.372
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 11/30/22 04:28:54.374
STEP: getting /apis/networking.k8s.io 11/30/22 04:28:54.375
STEP: getting /apis/networking.k8s.iov1 11/30/22 04:28:54.376
STEP: creating 11/30/22 04:28:54.377
STEP: getting 11/30/22 04:28:54.398
STEP: listing 11/30/22 04:28:54.4
STEP: watching 11/30/22 04:28:54.411
Nov 30 04:28:54.411: INFO: starting watch
STEP: cluster-wide listing 11/30/22 04:28:54.412
STEP: cluster-wide watching 11/30/22 04:28:54.416
Nov 30 04:28:54.416: INFO: starting watch
STEP: patching 11/30/22 04:28:54.418
STEP: updating 11/30/22 04:28:54.423
Nov 30 04:28:54.432: INFO: waiting for watch events with expected annotations
Nov 30 04:28:54.432: INFO: saw patched and updated annotations
STEP: patching /status 11/30/22 04:28:54.432
STEP: updating /status 11/30/22 04:28:54.446
STEP: get /status 11/30/22 04:28:54.457
STEP: deleting 11/30/22 04:28:54.462
STEP: deleting a collection 11/30/22 04:28:54.477
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Nov 30 04:28:54.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2467" for this suite. 11/30/22 04:28:54.499
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":239,"skipped":4289,"failed":0}
------------------------------
• [0.150 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:28:54.354
    Nov 30 04:28:54.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename ingress 11/30/22 04:28:54.355
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:54.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:54.372
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 11/30/22 04:28:54.374
    STEP: getting /apis/networking.k8s.io 11/30/22 04:28:54.375
    STEP: getting /apis/networking.k8s.iov1 11/30/22 04:28:54.376
    STEP: creating 11/30/22 04:28:54.377
    STEP: getting 11/30/22 04:28:54.398
    STEP: listing 11/30/22 04:28:54.4
    STEP: watching 11/30/22 04:28:54.411
    Nov 30 04:28:54.411: INFO: starting watch
    STEP: cluster-wide listing 11/30/22 04:28:54.412
    STEP: cluster-wide watching 11/30/22 04:28:54.416
    Nov 30 04:28:54.416: INFO: starting watch
    STEP: patching 11/30/22 04:28:54.418
    STEP: updating 11/30/22 04:28:54.423
    Nov 30 04:28:54.432: INFO: waiting for watch events with expected annotations
    Nov 30 04:28:54.432: INFO: saw patched and updated annotations
    STEP: patching /status 11/30/22 04:28:54.432
    STEP: updating /status 11/30/22 04:28:54.446
    STEP: get /status 11/30/22 04:28:54.457
    STEP: deleting 11/30/22 04:28:54.462
    STEP: deleting a collection 11/30/22 04:28:54.477
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Nov 30 04:28:54.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-2467" for this suite. 11/30/22 04:28:54.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:28:54.506
Nov 30 04:28:54.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename security-context-test 11/30/22 04:28:54.508
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:54.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:54.527
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Nov 30 04:28:54.546: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-0ab06ca0-aef2-4c86-ba73-3bedc26144b5" in namespace "security-context-test-392" to be "Succeeded or Failed"
Nov 30 04:28:54.549: INFO: Pod "busybox-privileged-false-0ab06ca0-aef2-4c86-ba73-3bedc26144b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.993065ms
Nov 30 04:28:56.553: INFO: Pod "busybox-privileged-false-0ab06ca0-aef2-4c86-ba73-3bedc26144b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006693662s
Nov 30 04:28:58.556: INFO: Pod "busybox-privileged-false-0ab06ca0-aef2-4c86-ba73-3bedc26144b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010109384s
Nov 30 04:28:58.556: INFO: Pod "busybox-privileged-false-0ab06ca0-aef2-4c86-ba73-3bedc26144b5" satisfied condition "Succeeded or Failed"
Nov 30 04:28:58.560: INFO: Got logs for pod "busybox-privileged-false-0ab06ca0-aef2-4c86-ba73-3bedc26144b5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Nov 30 04:28:58.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-392" for this suite. 11/30/22 04:28:58.565
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":240,"skipped":4336,"failed":0}
------------------------------
• [4.064 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:28:54.506
    Nov 30 04:28:54.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename security-context-test 11/30/22 04:28:54.508
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:54.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:54.527
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Nov 30 04:28:54.546: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-0ab06ca0-aef2-4c86-ba73-3bedc26144b5" in namespace "security-context-test-392" to be "Succeeded or Failed"
    Nov 30 04:28:54.549: INFO: Pod "busybox-privileged-false-0ab06ca0-aef2-4c86-ba73-3bedc26144b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.993065ms
    Nov 30 04:28:56.553: INFO: Pod "busybox-privileged-false-0ab06ca0-aef2-4c86-ba73-3bedc26144b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006693662s
    Nov 30 04:28:58.556: INFO: Pod "busybox-privileged-false-0ab06ca0-aef2-4c86-ba73-3bedc26144b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010109384s
    Nov 30 04:28:58.556: INFO: Pod "busybox-privileged-false-0ab06ca0-aef2-4c86-ba73-3bedc26144b5" satisfied condition "Succeeded or Failed"
    Nov 30 04:28:58.560: INFO: Got logs for pod "busybox-privileged-false-0ab06ca0-aef2-4c86-ba73-3bedc26144b5": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Nov 30 04:28:58.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-392" for this suite. 11/30/22 04:28:58.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:28:58.572
Nov 30 04:28:58.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 04:28:58.573
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:58.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:58.591
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 11/30/22 04:28:58.594
Nov 30 04:28:58.614: INFO: Waiting up to 5m0s for pod "pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13" in namespace "emptydir-1563" to be "Succeeded or Failed"
Nov 30 04:28:58.626: INFO: Pod "pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13": Phase="Pending", Reason="", readiness=false. Elapsed: 11.564051ms
Nov 30 04:29:00.630: INFO: Pod "pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015399164s
Nov 30 04:29:02.630: INFO: Pod "pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015741313s
STEP: Saw pod success 11/30/22 04:29:02.63
Nov 30 04:29:02.630: INFO: Pod "pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13" satisfied condition "Succeeded or Failed"
Nov 30 04:29:02.632: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13 container test-container: <nil>
STEP: delete the pod 11/30/22 04:29:02.637
Nov 30 04:29:02.651: INFO: Waiting for pod pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13 to disappear
Nov 30 04:29:02.653: INFO: Pod pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 04:29:02.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1563" for this suite. 11/30/22 04:29:02.658
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":241,"skipped":4375,"failed":0}
------------------------------
• [4.095 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:28:58.572
    Nov 30 04:28:58.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 04:28:58.573
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:28:58.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:28:58.591
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 11/30/22 04:28:58.594
    Nov 30 04:28:58.614: INFO: Waiting up to 5m0s for pod "pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13" in namespace "emptydir-1563" to be "Succeeded or Failed"
    Nov 30 04:28:58.626: INFO: Pod "pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13": Phase="Pending", Reason="", readiness=false. Elapsed: 11.564051ms
    Nov 30 04:29:00.630: INFO: Pod "pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015399164s
    Nov 30 04:29:02.630: INFO: Pod "pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015741313s
    STEP: Saw pod success 11/30/22 04:29:02.63
    Nov 30 04:29:02.630: INFO: Pod "pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13" satisfied condition "Succeeded or Failed"
    Nov 30 04:29:02.632: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13 container test-container: <nil>
    STEP: delete the pod 11/30/22 04:29:02.637
    Nov 30 04:29:02.651: INFO: Waiting for pod pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13 to disappear
    Nov 30 04:29:02.653: INFO: Pod pod-eebd4ccb-551c-4638-a4fd-6784cf2e1b13 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 04:29:02.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1563" for this suite. 11/30/22 04:29:02.658
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:29:02.667
Nov 30 04:29:02.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename cronjob 11/30/22 04:29:02.667
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:29:02.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:29:02.694
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 11/30/22 04:29:02.696
STEP: Ensuring no jobs are scheduled 11/30/22 04:29:02.71
STEP: Ensuring no job exists by listing jobs explicitly 11/30/22 04:34:02.725
STEP: Removing cronjob 11/30/22 04:34:02.727
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Nov 30 04:34:02.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6948" for this suite. 11/30/22 04:34:02.736
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":242,"skipped":4377,"failed":0}
------------------------------
• [SLOW TEST] [300.074 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:29:02.667
    Nov 30 04:29:02.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename cronjob 11/30/22 04:29:02.667
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:29:02.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:29:02.694
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 11/30/22 04:29:02.696
    STEP: Ensuring no jobs are scheduled 11/30/22 04:29:02.71
    STEP: Ensuring no job exists by listing jobs explicitly 11/30/22 04:34:02.725
    STEP: Removing cronjob 11/30/22 04:34:02.727
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Nov 30 04:34:02.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6948" for this suite. 11/30/22 04:34:02.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:34:02.741
Nov 30 04:34:02.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename crd-webhook 11/30/22 04:34:02.742
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:34:02.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:34:02.763
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 11/30/22 04:34:02.765
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/30/22 04:34:03.213
STEP: Deploying the custom resource conversion webhook pod 11/30/22 04:34:03.22
STEP: Wait for the deployment to be ready 11/30/22 04:34:03.233
Nov 30 04:34:03.242: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 04:34:05.255
STEP: Verifying the service has paired with the endpoint 11/30/22 04:34:05.281
Nov 30 04:34:06.281: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Nov 30 04:34:06.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Creating a v1 custom resource 11/30/22 04:34:08.865
STEP: Create a v2 custom resource 11/30/22 04:34:08.884
STEP: List CRs in v1 11/30/22 04:34:08.935
STEP: List CRs in v2 11/30/22 04:34:08.942
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:34:09.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7790" for this suite. 11/30/22 04:34:09.463
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":243,"skipped":4385,"failed":0}
------------------------------
• [SLOW TEST] [6.858 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:34:02.741
    Nov 30 04:34:02.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename crd-webhook 11/30/22 04:34:02.742
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:34:02.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:34:02.763
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 11/30/22 04:34:02.765
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/30/22 04:34:03.213
    STEP: Deploying the custom resource conversion webhook pod 11/30/22 04:34:03.22
    STEP: Wait for the deployment to be ready 11/30/22 04:34:03.233
    Nov 30 04:34:03.242: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 04:34:05.255
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:34:05.281
    Nov 30 04:34:06.281: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Nov 30 04:34:06.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Creating a v1 custom resource 11/30/22 04:34:08.865
    STEP: Create a v2 custom resource 11/30/22 04:34:08.884
    STEP: List CRs in v1 11/30/22 04:34:08.935
    STEP: List CRs in v2 11/30/22 04:34:08.942
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:34:09.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-7790" for this suite. 11/30/22 04:34:09.463
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:34:09.6
Nov 30 04:34:09.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename sysctl 11/30/22 04:34:09.601
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:34:09.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:34:09.642
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 11/30/22 04:34:09.644
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Nov 30 04:34:09.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8547" for this suite. 11/30/22 04:34:09.686
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":244,"skipped":4392,"failed":0}
------------------------------
• [0.092 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:34:09.6
    Nov 30 04:34:09.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename sysctl 11/30/22 04:34:09.601
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:34:09.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:34:09.642
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 11/30/22 04:34:09.644
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Nov 30 04:34:09.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-8547" for this suite. 11/30/22 04:34:09.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:34:09.693
Nov 30 04:34:09.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename statefulset 11/30/22 04:34:09.694
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:34:09.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:34:09.717
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-354 11/30/22 04:34:09.72
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-354 11/30/22 04:34:09.727
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-354 11/30/22 04:34:09.732
Nov 30 04:34:09.736: INFO: Found 0 stateful pods, waiting for 1
Nov 30 04:34:19.740: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 11/30/22 04:34:19.74
Nov 30 04:34:19.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 30 04:34:19.881: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 30 04:34:19.881: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 30 04:34:19.881: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 30 04:34:19.886: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 30 04:34:29.889: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 30 04:34:29.889: INFO: Waiting for statefulset status.replicas updated to 0
Nov 30 04:34:29.902: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Nov 30 04:34:29.902: INFO: ss-0  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:09 +0000 UTC  }]
Nov 30 04:34:29.902: INFO: 
Nov 30 04:34:29.902: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 30 04:34:30.907: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995447113s
Nov 30 04:34:31.910: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991177679s
Nov 30 04:34:32.914: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987106001s
Nov 30 04:34:33.918: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983909756s
Nov 30 04:34:34.922: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980288796s
Nov 30 04:34:35.926: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975631379s
Nov 30 04:34:36.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972281568s
Nov 30 04:34:37.933: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969014321s
Nov 30 04:34:38.936: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.721754ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-354 11/30/22 04:34:39.936
Nov 30 04:34:39.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 30 04:34:40.062: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 30 04:34:40.062: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 30 04:34:40.062: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 30 04:34:40.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 30 04:34:40.210: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 30 04:34:40.210: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 30 04:34:40.210: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 30 04:34:40.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 30 04:34:40.338: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 30 04:34:40.339: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 30 04:34:40.339: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 30 04:34:40.341: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 30 04:34:40.342: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 30 04:34:40.342: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 11/30/22 04:34:40.342
Nov 30 04:34:40.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 30 04:34:40.469: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 30 04:34:40.469: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 30 04:34:40.469: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 30 04:34:40.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 30 04:34:40.595: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 30 04:34:40.595: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 30 04:34:40.595: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 30 04:34:40.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 30 04:34:40.733: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 30 04:34:40.733: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 30 04:34:40.733: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 30 04:34:40.733: INFO: Waiting for statefulset status.replicas updated to 0
Nov 30 04:34:40.736: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 30 04:34:50.745: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 30 04:34:50.745: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 30 04:34:50.745: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 30 04:34:50.756: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Nov 30 04:34:50.756: INFO: ss-0  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:09 +0000 UTC  }]
Nov 30 04:34:50.756: INFO: ss-1  worker-pool1-ok72912g-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:29 +0000 UTC  }]
Nov 30 04:34:50.756: INFO: ss-2  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:29 +0000 UTC  }]
Nov 30 04:34:50.756: INFO: 
Nov 30 04:34:50.756: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 30 04:34:51.760: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Nov 30 04:34:51.760: INFO: ss-0  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:09 +0000 UTC  }]
Nov 30 04:34:51.760: INFO: ss-2  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:29 +0000 UTC  }]
Nov 30 04:34:51.760: INFO: 
Nov 30 04:34:51.760: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 30 04:34:52.764: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992117879s
Nov 30 04:34:53.767: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989100022s
Nov 30 04:34:54.770: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.985811168s
Nov 30 04:34:55.774: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.982506978s
Nov 30 04:34:56.777: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.979014992s
Nov 30 04:34:57.780: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.97576877s
Nov 30 04:34:58.783: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.972587718s
Nov 30 04:34:59.787: INFO: Verifying statefulset ss doesn't scale past 0 for another 969.528658ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-354 11/30/22 04:35:00.787
Nov 30 04:35:00.791: INFO: Scaling statefulset ss to 0
Nov 30 04:35:00.801: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Nov 30 04:35:00.803: INFO: Deleting all statefulset in ns statefulset-354
Nov 30 04:35:00.806: INFO: Scaling statefulset ss to 0
Nov 30 04:35:00.816: INFO: Waiting for statefulset status.replicas updated to 0
Nov 30 04:35:00.818: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Nov 30 04:35:00.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-354" for this suite. 11/30/22 04:35:00.833
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":245,"skipped":4397,"failed":0}
------------------------------
• [SLOW TEST] [51.147 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:34:09.693
    Nov 30 04:34:09.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename statefulset 11/30/22 04:34:09.694
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:34:09.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:34:09.717
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-354 11/30/22 04:34:09.72
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-354 11/30/22 04:34:09.727
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-354 11/30/22 04:34:09.732
    Nov 30 04:34:09.736: INFO: Found 0 stateful pods, waiting for 1
    Nov 30 04:34:19.740: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 11/30/22 04:34:19.74
    Nov 30 04:34:19.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 30 04:34:19.881: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 30 04:34:19.881: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 30 04:34:19.881: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 30 04:34:19.886: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Nov 30 04:34:29.889: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 30 04:34:29.889: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 30 04:34:29.902: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Nov 30 04:34:29.902: INFO: ss-0  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:09 +0000 UTC  }]
    Nov 30 04:34:29.902: INFO: 
    Nov 30 04:34:29.902: INFO: StatefulSet ss has not reached scale 3, at 1
    Nov 30 04:34:30.907: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995447113s
    Nov 30 04:34:31.910: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991177679s
    Nov 30 04:34:32.914: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987106001s
    Nov 30 04:34:33.918: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983909756s
    Nov 30 04:34:34.922: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980288796s
    Nov 30 04:34:35.926: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975631379s
    Nov 30 04:34:36.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972281568s
    Nov 30 04:34:37.933: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969014321s
    Nov 30 04:34:38.936: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.721754ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-354 11/30/22 04:34:39.936
    Nov 30 04:34:39.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 30 04:34:40.062: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 30 04:34:40.062: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 30 04:34:40.062: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 30 04:34:40.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 30 04:34:40.210: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Nov 30 04:34:40.210: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 30 04:34:40.210: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 30 04:34:40.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 30 04:34:40.338: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Nov 30 04:34:40.339: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 30 04:34:40.339: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 30 04:34:40.341: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 30 04:34:40.342: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 30 04:34:40.342: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 11/30/22 04:34:40.342
    Nov 30 04:34:40.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 30 04:34:40.469: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 30 04:34:40.469: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 30 04:34:40.469: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 30 04:34:40.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 30 04:34:40.595: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 30 04:34:40.595: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 30 04:34:40.595: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 30 04:34:40.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=statefulset-354 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 30 04:34:40.733: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 30 04:34:40.733: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 30 04:34:40.733: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 30 04:34:40.733: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 30 04:34:40.736: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Nov 30 04:34:50.745: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 30 04:34:50.745: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Nov 30 04:34:50.745: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Nov 30 04:34:50.756: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Nov 30 04:34:50.756: INFO: ss-0  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:09 +0000 UTC  }]
    Nov 30 04:34:50.756: INFO: ss-1  worker-pool1-ok72912g-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:29 +0000 UTC  }]
    Nov 30 04:34:50.756: INFO: ss-2  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:29 +0000 UTC  }]
    Nov 30 04:34:50.756: INFO: 
    Nov 30 04:34:50.756: INFO: StatefulSet ss has not reached scale 0, at 3
    Nov 30 04:34:51.760: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Nov 30 04:34:51.760: INFO: ss-0  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:09 +0000 UTC  }]
    Nov 30 04:34:51.760: INFO: ss-2  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:34:29 +0000 UTC  }]
    Nov 30 04:34:51.760: INFO: 
    Nov 30 04:34:51.760: INFO: StatefulSet ss has not reached scale 0, at 2
    Nov 30 04:34:52.764: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992117879s
    Nov 30 04:34:53.767: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989100022s
    Nov 30 04:34:54.770: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.985811168s
    Nov 30 04:34:55.774: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.982506978s
    Nov 30 04:34:56.777: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.979014992s
    Nov 30 04:34:57.780: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.97576877s
    Nov 30 04:34:58.783: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.972587718s
    Nov 30 04:34:59.787: INFO: Verifying statefulset ss doesn't scale past 0 for another 969.528658ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-354 11/30/22 04:35:00.787
    Nov 30 04:35:00.791: INFO: Scaling statefulset ss to 0
    Nov 30 04:35:00.801: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Nov 30 04:35:00.803: INFO: Deleting all statefulset in ns statefulset-354
    Nov 30 04:35:00.806: INFO: Scaling statefulset ss to 0
    Nov 30 04:35:00.816: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 30 04:35:00.818: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Nov 30 04:35:00.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-354" for this suite. 11/30/22 04:35:00.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:35:00.84
Nov 30 04:35:00.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pods 11/30/22 04:35:00.841
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:00.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:00.863
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Nov 30 04:35:00.900: INFO: Waiting up to 5m0s for pod "server-envvars-edf58dd7-b84d-4914-a48e-bc05a81e60da" in namespace "pods-9532" to be "running and ready"
Nov 30 04:35:00.903: INFO: Pod "server-envvars-edf58dd7-b84d-4914-a48e-bc05a81e60da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.782396ms
Nov 30 04:35:00.903: INFO: The phase of Pod server-envvars-edf58dd7-b84d-4914-a48e-bc05a81e60da is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:35:02.907: INFO: Pod "server-envvars-edf58dd7-b84d-4914-a48e-bc05a81e60da": Phase="Running", Reason="", readiness=true. Elapsed: 2.007328478s
Nov 30 04:35:02.907: INFO: The phase of Pod server-envvars-edf58dd7-b84d-4914-a48e-bc05a81e60da is Running (Ready = true)
Nov 30 04:35:02.907: INFO: Pod "server-envvars-edf58dd7-b84d-4914-a48e-bc05a81e60da" satisfied condition "running and ready"
Nov 30 04:35:02.934: INFO: Waiting up to 5m0s for pod "client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417" in namespace "pods-9532" to be "Succeeded or Failed"
Nov 30 04:35:02.939: INFO: Pod "client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417": Phase="Pending", Reason="", readiness=false. Elapsed: 5.283162ms
Nov 30 04:35:04.944: INFO: Pod "client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010057377s
Nov 30 04:35:06.942: INFO: Pod "client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008787026s
STEP: Saw pod success 11/30/22 04:35:06.942
Nov 30 04:35:06.942: INFO: Pod "client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417" satisfied condition "Succeeded or Failed"
Nov 30 04:35:06.949: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417 container env3cont: <nil>
STEP: delete the pod 11/30/22 04:35:06.957
Nov 30 04:35:06.971: INFO: Waiting for pod client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417 to disappear
Nov 30 04:35:06.974: INFO: Pod client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Nov 30 04:35:06.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9532" for this suite. 11/30/22 04:35:06.978
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":246,"skipped":4423,"failed":0}
------------------------------
• [SLOW TEST] [6.142 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:35:00.84
    Nov 30 04:35:00.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pods 11/30/22 04:35:00.841
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:00.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:00.863
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Nov 30 04:35:00.900: INFO: Waiting up to 5m0s for pod "server-envvars-edf58dd7-b84d-4914-a48e-bc05a81e60da" in namespace "pods-9532" to be "running and ready"
    Nov 30 04:35:00.903: INFO: Pod "server-envvars-edf58dd7-b84d-4914-a48e-bc05a81e60da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.782396ms
    Nov 30 04:35:00.903: INFO: The phase of Pod server-envvars-edf58dd7-b84d-4914-a48e-bc05a81e60da is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:35:02.907: INFO: Pod "server-envvars-edf58dd7-b84d-4914-a48e-bc05a81e60da": Phase="Running", Reason="", readiness=true. Elapsed: 2.007328478s
    Nov 30 04:35:02.907: INFO: The phase of Pod server-envvars-edf58dd7-b84d-4914-a48e-bc05a81e60da is Running (Ready = true)
    Nov 30 04:35:02.907: INFO: Pod "server-envvars-edf58dd7-b84d-4914-a48e-bc05a81e60da" satisfied condition "running and ready"
    Nov 30 04:35:02.934: INFO: Waiting up to 5m0s for pod "client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417" in namespace "pods-9532" to be "Succeeded or Failed"
    Nov 30 04:35:02.939: INFO: Pod "client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417": Phase="Pending", Reason="", readiness=false. Elapsed: 5.283162ms
    Nov 30 04:35:04.944: INFO: Pod "client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010057377s
    Nov 30 04:35:06.942: INFO: Pod "client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008787026s
    STEP: Saw pod success 11/30/22 04:35:06.942
    Nov 30 04:35:06.942: INFO: Pod "client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417" satisfied condition "Succeeded or Failed"
    Nov 30 04:35:06.949: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417 container env3cont: <nil>
    STEP: delete the pod 11/30/22 04:35:06.957
    Nov 30 04:35:06.971: INFO: Waiting for pod client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417 to disappear
    Nov 30 04:35:06.974: INFO: Pod client-envvars-95de09dd-d88e-40a9-8896-4c76d913a417 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Nov 30 04:35:06.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9532" for this suite. 11/30/22 04:35:06.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:35:06.983
Nov 30 04:35:06.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 04:35:06.984
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:07.003
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 11/30/22 04:35:07.005
Nov 30 04:35:07.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-7795 run e2e-test-httpd-pod --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 30 04:35:07.099: INFO: stderr: ""
Nov 30 04:35:07.099: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 11/30/22 04:35:07.099
STEP: verifying the pod e2e-test-httpd-pod was created 11/30/22 04:35:12.15
Nov 30 04:35:12.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-7795 get pod e2e-test-httpd-pod -o json'
Nov 30 04:35:12.211: INFO: stderr: ""
Nov 30 04:35:12.211: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.12.211\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.12.211\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2022-11-30T04:35:07Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7795\",\n        \"resourceVersion\": \"52777\",\n        \"uid\": \"0dc13ae3-5f03-44da-93d3-4ead7936cb2b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-9hm2j\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-9hm2j\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-11-30T04:35:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-11-30T04:35:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-11-30T04:35:08Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-11-30T04:35:07Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://e7d61c5a69bbb024940e6db2a8f4eecd48ed7ac4cd6667f86e41ad486299f291\",\n                \"image\": \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2\",\n                \"imageID\": \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-11-30T04:35:07Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.10.8\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.12.211\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.12.211\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-11-30T04:35:07Z\"\n    }\n}\n"
STEP: replace the image in the pod 11/30/22 04:35:12.212
Nov 30 04:35:12.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-7795 replace -f -'
Nov 30 04:35:12.912: INFO: stderr: ""
Nov 30 04:35:12.912: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-2 11/30/22 04:35:12.912
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Nov 30 04:35:12.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-7795 delete pods e2e-test-httpd-pod'
Nov 30 04:35:15.115: INFO: stderr: ""
Nov 30 04:35:15.115: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 04:35:15.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7795" for this suite. 11/30/22 04:35:15.119
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":247,"skipped":4446,"failed":0}
------------------------------
• [SLOW TEST] [8.141 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:35:06.983
    Nov 30 04:35:06.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 04:35:06.984
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:07.003
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 11/30/22 04:35:07.005
    Nov 30 04:35:07.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-7795 run e2e-test-httpd-pod --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Nov 30 04:35:07.099: INFO: stderr: ""
    Nov 30 04:35:07.099: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 11/30/22 04:35:07.099
    STEP: verifying the pod e2e-test-httpd-pod was created 11/30/22 04:35:12.15
    Nov 30 04:35:12.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-7795 get pod e2e-test-httpd-pod -o json'
    Nov 30 04:35:12.211: INFO: stderr: ""
    Nov 30 04:35:12.211: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.12.211\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.12.211\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2022-11-30T04:35:07Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7795\",\n        \"resourceVersion\": \"52777\",\n        \"uid\": \"0dc13ae3-5f03-44da-93d3-4ead7936cb2b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-9hm2j\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-9hm2j\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-11-30T04:35:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-11-30T04:35:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-11-30T04:35:08Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-11-30T04:35:07Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://e7d61c5a69bbb024940e6db2a8f4eecd48ed7ac4cd6667f86e41ad486299f291\",\n                \"image\": \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2\",\n                \"imageID\": \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-11-30T04:35:07Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.10.8\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.12.211\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.12.211\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-11-30T04:35:07Z\"\n    }\n}\n"
    STEP: replace the image in the pod 11/30/22 04:35:12.212
    Nov 30 04:35:12.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-7795 replace -f -'
    Nov 30 04:35:12.912: INFO: stderr: ""
    Nov 30 04:35:12.912: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-2 11/30/22 04:35:12.912
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Nov 30 04:35:12.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-7795 delete pods e2e-test-httpd-pod'
    Nov 30 04:35:15.115: INFO: stderr: ""
    Nov 30 04:35:15.115: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 04:35:15.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7795" for this suite. 11/30/22 04:35:15.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:35:15.125
Nov 30 04:35:15.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:35:15.126
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:15.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:15.143
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 11/30/22 04:35:15.148
Nov 30 04:35:15.176: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9" in namespace "projected-7812" to be "Succeeded or Failed"
Nov 30 04:35:15.181: INFO: Pod "downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.740301ms
Nov 30 04:35:17.185: INFO: Pod "downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008018193s
Nov 30 04:35:19.187: INFO: Pod "downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01056235s
STEP: Saw pod success 11/30/22 04:35:19.187
Nov 30 04:35:19.187: INFO: Pod "downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9" satisfied condition "Succeeded or Failed"
Nov 30 04:35:19.190: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9 container client-container: <nil>
STEP: delete the pod 11/30/22 04:35:19.194
Nov 30 04:35:19.207: INFO: Waiting for pod downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9 to disappear
Nov 30 04:35:19.209: INFO: Pod downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Nov 30 04:35:19.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7812" for this suite. 11/30/22 04:35:19.213
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":248,"skipped":4470,"failed":0}
------------------------------
• [4.092 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:35:15.125
    Nov 30 04:35:15.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:35:15.126
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:15.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:15.143
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 11/30/22 04:35:15.148
    Nov 30 04:35:15.176: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9" in namespace "projected-7812" to be "Succeeded or Failed"
    Nov 30 04:35:15.181: INFO: Pod "downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.740301ms
    Nov 30 04:35:17.185: INFO: Pod "downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008018193s
    Nov 30 04:35:19.187: INFO: Pod "downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01056235s
    STEP: Saw pod success 11/30/22 04:35:19.187
    Nov 30 04:35:19.187: INFO: Pod "downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9" satisfied condition "Succeeded or Failed"
    Nov 30 04:35:19.190: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9 container client-container: <nil>
    STEP: delete the pod 11/30/22 04:35:19.194
    Nov 30 04:35:19.207: INFO: Waiting for pod downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9 to disappear
    Nov 30 04:35:19.209: INFO: Pod downwardapi-volume-80782002-f480-4f17-a87d-6d977bbd39e9 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Nov 30 04:35:19.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7812" for this suite. 11/30/22 04:35:19.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:35:19.218
Nov 30 04:35:19.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename containers 11/30/22 04:35:19.218
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:19.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:19.238
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 11/30/22 04:35:19.24
Nov 30 04:35:19.250: INFO: Waiting up to 5m0s for pod "client-containers-958f6784-4644-489a-8eef-961b769f7f10" in namespace "containers-515" to be "Succeeded or Failed"
Nov 30 04:35:19.253: INFO: Pod "client-containers-958f6784-4644-489a-8eef-961b769f7f10": Phase="Pending", Reason="", readiness=false. Elapsed: 3.201624ms
Nov 30 04:35:21.257: INFO: Pod "client-containers-958f6784-4644-489a-8eef-961b769f7f10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006810759s
Nov 30 04:35:23.257: INFO: Pod "client-containers-958f6784-4644-489a-8eef-961b769f7f10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00721643s
STEP: Saw pod success 11/30/22 04:35:23.257
Nov 30 04:35:23.257: INFO: Pod "client-containers-958f6784-4644-489a-8eef-961b769f7f10" satisfied condition "Succeeded or Failed"
Nov 30 04:35:23.260: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod client-containers-958f6784-4644-489a-8eef-961b769f7f10 container agnhost-container: <nil>
STEP: delete the pod 11/30/22 04:35:23.265
Nov 30 04:35:23.278: INFO: Waiting for pod client-containers-958f6784-4644-489a-8eef-961b769f7f10 to disappear
Nov 30 04:35:23.281: INFO: Pod client-containers-958f6784-4644-489a-8eef-961b769f7f10 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Nov 30 04:35:23.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-515" for this suite. 11/30/22 04:35:23.288
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":249,"skipped":4482,"failed":0}
------------------------------
• [4.076 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:35:19.218
    Nov 30 04:35:19.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename containers 11/30/22 04:35:19.218
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:19.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:19.238
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 11/30/22 04:35:19.24
    Nov 30 04:35:19.250: INFO: Waiting up to 5m0s for pod "client-containers-958f6784-4644-489a-8eef-961b769f7f10" in namespace "containers-515" to be "Succeeded or Failed"
    Nov 30 04:35:19.253: INFO: Pod "client-containers-958f6784-4644-489a-8eef-961b769f7f10": Phase="Pending", Reason="", readiness=false. Elapsed: 3.201624ms
    Nov 30 04:35:21.257: INFO: Pod "client-containers-958f6784-4644-489a-8eef-961b769f7f10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006810759s
    Nov 30 04:35:23.257: INFO: Pod "client-containers-958f6784-4644-489a-8eef-961b769f7f10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00721643s
    STEP: Saw pod success 11/30/22 04:35:23.257
    Nov 30 04:35:23.257: INFO: Pod "client-containers-958f6784-4644-489a-8eef-961b769f7f10" satisfied condition "Succeeded or Failed"
    Nov 30 04:35:23.260: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod client-containers-958f6784-4644-489a-8eef-961b769f7f10 container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 04:35:23.265
    Nov 30 04:35:23.278: INFO: Waiting for pod client-containers-958f6784-4644-489a-8eef-961b769f7f10 to disappear
    Nov 30 04:35:23.281: INFO: Pod client-containers-958f6784-4644-489a-8eef-961b769f7f10 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Nov 30 04:35:23.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-515" for this suite. 11/30/22 04:35:23.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:35:23.296
Nov 30 04:35:23.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 04:35:23.297
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:23.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:23.314
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-3c374b32-a0f9-481f-81b4-a077c2863918 11/30/22 04:35:23.34
STEP: Creating a pod to test consume secrets 11/30/22 04:35:23.348
Nov 30 04:35:23.359: INFO: Waiting up to 5m0s for pod "pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351" in namespace "secrets-5993" to be "Succeeded or Failed"
Nov 30 04:35:23.363: INFO: Pod "pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890657ms
Nov 30 04:35:25.366: INFO: Pod "pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351": Phase="Running", Reason="", readiness=false. Elapsed: 2.00776425s
Nov 30 04:35:27.366: INFO: Pod "pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007847888s
STEP: Saw pod success 11/30/22 04:35:27.367
Nov 30 04:35:27.367: INFO: Pod "pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351" satisfied condition "Succeeded or Failed"
Nov 30 04:35:27.369: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351 container secret-volume-test: <nil>
STEP: delete the pod 11/30/22 04:35:27.374
Nov 30 04:35:27.384: INFO: Waiting for pod pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351 to disappear
Nov 30 04:35:27.386: INFO: Pod pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Nov 30 04:35:27.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5993" for this suite. 11/30/22 04:35:27.39
STEP: Destroying namespace "secret-namespace-8832" for this suite. 11/30/22 04:35:27.4
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":250,"skipped":4525,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:35:23.296
    Nov 30 04:35:23.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 04:35:23.297
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:23.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:23.314
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-3c374b32-a0f9-481f-81b4-a077c2863918 11/30/22 04:35:23.34
    STEP: Creating a pod to test consume secrets 11/30/22 04:35:23.348
    Nov 30 04:35:23.359: INFO: Waiting up to 5m0s for pod "pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351" in namespace "secrets-5993" to be "Succeeded or Failed"
    Nov 30 04:35:23.363: INFO: Pod "pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890657ms
    Nov 30 04:35:25.366: INFO: Pod "pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351": Phase="Running", Reason="", readiness=false. Elapsed: 2.00776425s
    Nov 30 04:35:27.366: INFO: Pod "pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007847888s
    STEP: Saw pod success 11/30/22 04:35:27.367
    Nov 30 04:35:27.367: INFO: Pod "pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351" satisfied condition "Succeeded or Failed"
    Nov 30 04:35:27.369: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351 container secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 04:35:27.374
    Nov 30 04:35:27.384: INFO: Waiting for pod pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351 to disappear
    Nov 30 04:35:27.386: INFO: Pod pod-secrets-fc28d24a-3d52-411a-a0d4-503bdc465351 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 04:35:27.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5993" for this suite. 11/30/22 04:35:27.39
    STEP: Destroying namespace "secret-namespace-8832" for this suite. 11/30/22 04:35:27.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:35:27.406
Nov 30 04:35:27.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename dns 11/30/22 04:35:27.407
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:27.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:27.428
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 11/30/22 04:35:27.431
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4393.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local; sleep 1; done
 11/30/22 04:35:27.435
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4393.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local; sleep 1; done
 11/30/22 04:35:27.435
STEP: creating a pod to probe DNS 11/30/22 04:35:27.435
STEP: submitting the pod to kubernetes 11/30/22 04:35:27.436
Nov 30 04:35:27.449: INFO: Waiting up to 15m0s for pod "dns-test-49212005-10b4-4d7a-9e22-2fc2f56d0bfa" in namespace "dns-4393" to be "running"
Nov 30 04:35:27.452: INFO: Pod "dns-test-49212005-10b4-4d7a-9e22-2fc2f56d0bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915204ms
Nov 30 04:35:29.456: INFO: Pod "dns-test-49212005-10b4-4d7a-9e22-2fc2f56d0bfa": Phase="Running", Reason="", readiness=true. Elapsed: 2.006888069s
Nov 30 04:35:29.456: INFO: Pod "dns-test-49212005-10b4-4d7a-9e22-2fc2f56d0bfa" satisfied condition "running"
STEP: retrieving the pod 11/30/22 04:35:29.456
STEP: looking for the results for each expected name from probers 11/30/22 04:35:29.459
Nov 30 04:35:29.466: INFO: DNS probes using dns-test-49212005-10b4-4d7a-9e22-2fc2f56d0bfa succeeded

STEP: deleting the pod 11/30/22 04:35:29.466
STEP: changing the externalName to bar.example.com 11/30/22 04:35:29.476
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4393.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local; sleep 1; done
 11/30/22 04:35:29.483
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4393.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local; sleep 1; done
 11/30/22 04:35:29.483
STEP: creating a second pod to probe DNS 11/30/22 04:35:29.483
STEP: submitting the pod to kubernetes 11/30/22 04:35:29.483
Nov 30 04:35:29.491: INFO: Waiting up to 15m0s for pod "dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1" in namespace "dns-4393" to be "running"
Nov 30 04:35:29.498: INFO: Pod "dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.454646ms
Nov 30 04:35:31.502: INFO: Pod "dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010975904s
Nov 30 04:35:31.502: INFO: Pod "dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1" satisfied condition "running"
STEP: retrieving the pod 11/30/22 04:35:31.502
STEP: looking for the results for each expected name from probers 11/30/22 04:35:31.504
Nov 30 04:35:31.508: INFO: File wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local from pod  dns-4393/dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 30 04:35:31.510: INFO: File jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local from pod  dns-4393/dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 30 04:35:31.510: INFO: Lookups using dns-4393/dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 failed for: [wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local]

Nov 30 04:35:36.518: INFO: File wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local from pod  dns-4393/dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 30 04:35:36.521: INFO: File jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local from pod  dns-4393/dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 30 04:35:36.521: INFO: Lookups using dns-4393/dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 failed for: [wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local]

Nov 30 04:35:41.519: INFO: DNS probes using dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 succeeded

STEP: deleting the pod 11/30/22 04:35:41.519
STEP: changing the service to type=ClusterIP 11/30/22 04:35:41.531
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4393.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local; sleep 1; done
 11/30/22 04:35:41.552
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4393.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local; sleep 1; done
 11/30/22 04:35:41.552
STEP: creating a third pod to probe DNS 11/30/22 04:35:41.552
STEP: submitting the pod to kubernetes 11/30/22 04:35:41.554
Nov 30 04:35:41.583: INFO: Waiting up to 15m0s for pod "dns-test-4b489056-c3bf-4445-82d3-2ad58627b89e" in namespace "dns-4393" to be "running"
Nov 30 04:35:41.585: INFO: Pod "dns-test-4b489056-c3bf-4445-82d3-2ad58627b89e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.782525ms
Nov 30 04:35:43.589: INFO: Pod "dns-test-4b489056-c3bf-4445-82d3-2ad58627b89e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006652309s
Nov 30 04:35:43.589: INFO: Pod "dns-test-4b489056-c3bf-4445-82d3-2ad58627b89e" satisfied condition "running"
STEP: retrieving the pod 11/30/22 04:35:43.589
STEP: looking for the results for each expected name from probers 11/30/22 04:35:43.592
Nov 30 04:35:43.598: INFO: DNS probes using dns-test-4b489056-c3bf-4445-82d3-2ad58627b89e succeeded

STEP: deleting the pod 11/30/22 04:35:43.598
STEP: deleting the test externalName service 11/30/22 04:35:43.62
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Nov 30 04:35:43.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4393" for this suite. 11/30/22 04:35:43.659
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":251,"skipped":4556,"failed":0}
------------------------------
• [SLOW TEST] [16.273 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:35:27.406
    Nov 30 04:35:27.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename dns 11/30/22 04:35:27.407
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:27.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:27.428
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 11/30/22 04:35:27.431
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4393.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local; sleep 1; done
     11/30/22 04:35:27.435
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4393.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local; sleep 1; done
     11/30/22 04:35:27.435
    STEP: creating a pod to probe DNS 11/30/22 04:35:27.435
    STEP: submitting the pod to kubernetes 11/30/22 04:35:27.436
    Nov 30 04:35:27.449: INFO: Waiting up to 15m0s for pod "dns-test-49212005-10b4-4d7a-9e22-2fc2f56d0bfa" in namespace "dns-4393" to be "running"
    Nov 30 04:35:27.452: INFO: Pod "dns-test-49212005-10b4-4d7a-9e22-2fc2f56d0bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915204ms
    Nov 30 04:35:29.456: INFO: Pod "dns-test-49212005-10b4-4d7a-9e22-2fc2f56d0bfa": Phase="Running", Reason="", readiness=true. Elapsed: 2.006888069s
    Nov 30 04:35:29.456: INFO: Pod "dns-test-49212005-10b4-4d7a-9e22-2fc2f56d0bfa" satisfied condition "running"
    STEP: retrieving the pod 11/30/22 04:35:29.456
    STEP: looking for the results for each expected name from probers 11/30/22 04:35:29.459
    Nov 30 04:35:29.466: INFO: DNS probes using dns-test-49212005-10b4-4d7a-9e22-2fc2f56d0bfa succeeded

    STEP: deleting the pod 11/30/22 04:35:29.466
    STEP: changing the externalName to bar.example.com 11/30/22 04:35:29.476
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4393.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local; sleep 1; done
     11/30/22 04:35:29.483
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4393.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local; sleep 1; done
     11/30/22 04:35:29.483
    STEP: creating a second pod to probe DNS 11/30/22 04:35:29.483
    STEP: submitting the pod to kubernetes 11/30/22 04:35:29.483
    Nov 30 04:35:29.491: INFO: Waiting up to 15m0s for pod "dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1" in namespace "dns-4393" to be "running"
    Nov 30 04:35:29.498: INFO: Pod "dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.454646ms
    Nov 30 04:35:31.502: INFO: Pod "dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010975904s
    Nov 30 04:35:31.502: INFO: Pod "dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1" satisfied condition "running"
    STEP: retrieving the pod 11/30/22 04:35:31.502
    STEP: looking for the results for each expected name from probers 11/30/22 04:35:31.504
    Nov 30 04:35:31.508: INFO: File wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local from pod  dns-4393/dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 30 04:35:31.510: INFO: File jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local from pod  dns-4393/dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 30 04:35:31.510: INFO: Lookups using dns-4393/dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 failed for: [wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local]

    Nov 30 04:35:36.518: INFO: File wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local from pod  dns-4393/dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 30 04:35:36.521: INFO: File jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local from pod  dns-4393/dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 30 04:35:36.521: INFO: Lookups using dns-4393/dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 failed for: [wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local]

    Nov 30 04:35:41.519: INFO: DNS probes using dns-test-9b888967-35c0-465e-b43a-d7c25151a6d1 succeeded

    STEP: deleting the pod 11/30/22 04:35:41.519
    STEP: changing the service to type=ClusterIP 11/30/22 04:35:41.531
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4393.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4393.svc.cluster.local; sleep 1; done
     11/30/22 04:35:41.552
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4393.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4393.svc.cluster.local; sleep 1; done
     11/30/22 04:35:41.552
    STEP: creating a third pod to probe DNS 11/30/22 04:35:41.552
    STEP: submitting the pod to kubernetes 11/30/22 04:35:41.554
    Nov 30 04:35:41.583: INFO: Waiting up to 15m0s for pod "dns-test-4b489056-c3bf-4445-82d3-2ad58627b89e" in namespace "dns-4393" to be "running"
    Nov 30 04:35:41.585: INFO: Pod "dns-test-4b489056-c3bf-4445-82d3-2ad58627b89e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.782525ms
    Nov 30 04:35:43.589: INFO: Pod "dns-test-4b489056-c3bf-4445-82d3-2ad58627b89e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006652309s
    Nov 30 04:35:43.589: INFO: Pod "dns-test-4b489056-c3bf-4445-82d3-2ad58627b89e" satisfied condition "running"
    STEP: retrieving the pod 11/30/22 04:35:43.589
    STEP: looking for the results for each expected name from probers 11/30/22 04:35:43.592
    Nov 30 04:35:43.598: INFO: DNS probes using dns-test-4b489056-c3bf-4445-82d3-2ad58627b89e succeeded

    STEP: deleting the pod 11/30/22 04:35:43.598
    STEP: deleting the test externalName service 11/30/22 04:35:43.62
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Nov 30 04:35:43.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4393" for this suite. 11/30/22 04:35:43.659
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:35:43.679
Nov 30 04:35:43.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename dns 11/30/22 04:35:43.68
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:43.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:43.704
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 11/30/22 04:35:43.708
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 11/30/22 04:35:43.708
STEP: creating a pod to probe DNS 11/30/22 04:35:43.708
STEP: submitting the pod to kubernetes 11/30/22 04:35:43.708
Nov 30 04:35:43.722: INFO: Waiting up to 15m0s for pod "dns-test-e2906a68-c8b9-434d-9e87-65c4c537a20c" in namespace "dns-8363" to be "running"
Nov 30 04:35:43.732: INFO: Pod "dns-test-e2906a68-c8b9-434d-9e87-65c4c537a20c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.970851ms
Nov 30 04:35:45.735: INFO: Pod "dns-test-e2906a68-c8b9-434d-9e87-65c4c537a20c": Phase="Running", Reason="", readiness=true. Elapsed: 2.013742707s
Nov 30 04:35:45.735: INFO: Pod "dns-test-e2906a68-c8b9-434d-9e87-65c4c537a20c" satisfied condition "running"
STEP: retrieving the pod 11/30/22 04:35:45.735
STEP: looking for the results for each expected name from probers 11/30/22 04:35:45.738
Nov 30 04:35:45.752: INFO: DNS probes using dns-8363/dns-test-e2906a68-c8b9-434d-9e87-65c4c537a20c succeeded

STEP: deleting the pod 11/30/22 04:35:45.752
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Nov 30 04:35:45.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8363" for this suite. 11/30/22 04:35:45.774
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":252,"skipped":4557,"failed":0}
------------------------------
• [2.103 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:35:43.679
    Nov 30 04:35:43.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename dns 11/30/22 04:35:43.68
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:43.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:43.704
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     11/30/22 04:35:43.708
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     11/30/22 04:35:43.708
    STEP: creating a pod to probe DNS 11/30/22 04:35:43.708
    STEP: submitting the pod to kubernetes 11/30/22 04:35:43.708
    Nov 30 04:35:43.722: INFO: Waiting up to 15m0s for pod "dns-test-e2906a68-c8b9-434d-9e87-65c4c537a20c" in namespace "dns-8363" to be "running"
    Nov 30 04:35:43.732: INFO: Pod "dns-test-e2906a68-c8b9-434d-9e87-65c4c537a20c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.970851ms
    Nov 30 04:35:45.735: INFO: Pod "dns-test-e2906a68-c8b9-434d-9e87-65c4c537a20c": Phase="Running", Reason="", readiness=true. Elapsed: 2.013742707s
    Nov 30 04:35:45.735: INFO: Pod "dns-test-e2906a68-c8b9-434d-9e87-65c4c537a20c" satisfied condition "running"
    STEP: retrieving the pod 11/30/22 04:35:45.735
    STEP: looking for the results for each expected name from probers 11/30/22 04:35:45.738
    Nov 30 04:35:45.752: INFO: DNS probes using dns-8363/dns-test-e2906a68-c8b9-434d-9e87-65c4c537a20c succeeded

    STEP: deleting the pod 11/30/22 04:35:45.752
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Nov 30 04:35:45.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8363" for this suite. 11/30/22 04:35:45.774
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:35:45.782
Nov 30 04:35:45.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 04:35:45.783
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:45.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:45.803
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-3395 11/30/22 04:35:45.805
STEP: creating service affinity-clusterip in namespace services-3395 11/30/22 04:35:45.805
STEP: creating replication controller affinity-clusterip in namespace services-3395 11/30/22 04:35:45.83
I1130 04:35:45.840021      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3395, replica count: 3
I1130 04:35:48.891861      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 30 04:35:48.899: INFO: Creating new exec pod
Nov 30 04:35:48.928: INFO: Waiting up to 5m0s for pod "execpod-affinitykv8x9" in namespace "services-3395" to be "running"
Nov 30 04:35:48.931: INFO: Pod "execpod-affinitykv8x9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67375ms
Nov 30 04:35:50.934: INFO: Pod "execpod-affinitykv8x9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006007123s
Nov 30 04:35:50.934: INFO: Pod "execpod-affinitykv8x9" satisfied condition "running"
Nov 30 04:35:51.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3395 exec execpod-affinitykv8x9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Nov 30 04:35:52.056: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Nov 30 04:35:52.056: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 04:35:52.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3395 exec execpod-affinitykv8x9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.244.83 80'
Nov 30 04:35:52.183: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.244.83 80\nConnection to 10.111.244.83 80 port [tcp/http] succeeded!\n"
Nov 30 04:35:52.183: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 30 04:35:52.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3395 exec execpod-affinitykv8x9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.244.83:80/ ; done'
Nov 30 04:35:52.386: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n"
Nov 30 04:35:52.386: INFO: stdout: "\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz"
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
Nov 30 04:35:52.386: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3395, will wait for the garbage collector to delete the pods 11/30/22 04:35:52.397
Nov 30 04:35:52.458: INFO: Deleting ReplicationController affinity-clusterip took: 6.227753ms
Nov 30 04:35:52.559: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.230439ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 04:35:54.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3395" for this suite. 11/30/22 04:35:54.623
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":253,"skipped":4558,"failed":0}
------------------------------
• [SLOW TEST] [8.851 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:35:45.782
    Nov 30 04:35:45.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 04:35:45.783
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:45.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:45.803
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-3395 11/30/22 04:35:45.805
    STEP: creating service affinity-clusterip in namespace services-3395 11/30/22 04:35:45.805
    STEP: creating replication controller affinity-clusterip in namespace services-3395 11/30/22 04:35:45.83
    I1130 04:35:45.840021      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3395, replica count: 3
    I1130 04:35:48.891861      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 30 04:35:48.899: INFO: Creating new exec pod
    Nov 30 04:35:48.928: INFO: Waiting up to 5m0s for pod "execpod-affinitykv8x9" in namespace "services-3395" to be "running"
    Nov 30 04:35:48.931: INFO: Pod "execpod-affinitykv8x9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67375ms
    Nov 30 04:35:50.934: INFO: Pod "execpod-affinitykv8x9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006007123s
    Nov 30 04:35:50.934: INFO: Pod "execpod-affinitykv8x9" satisfied condition "running"
    Nov 30 04:35:51.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3395 exec execpod-affinitykv8x9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Nov 30 04:35:52.056: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Nov 30 04:35:52.056: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 04:35:52.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3395 exec execpod-affinitykv8x9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.244.83 80'
    Nov 30 04:35:52.183: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.244.83 80\nConnection to 10.111.244.83 80 port [tcp/http] succeeded!\n"
    Nov 30 04:35:52.183: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Nov 30 04:35:52.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-3395 exec execpod-affinitykv8x9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.244.83:80/ ; done'
    Nov 30 04:35:52.386: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.83:80/\n"
    Nov 30 04:35:52.386: INFO: stdout: "\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz\naffinity-clusterip-m6wfz"
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Received response from host: affinity-clusterip-m6wfz
    Nov 30 04:35:52.386: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3395, will wait for the garbage collector to delete the pods 11/30/22 04:35:52.397
    Nov 30 04:35:52.458: INFO: Deleting ReplicationController affinity-clusterip took: 6.227753ms
    Nov 30 04:35:52.559: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.230439ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 04:35:54.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3395" for this suite. 11/30/22 04:35:54.623
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:35:54.634
Nov 30 04:35:54.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:35:54.635
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:54.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:54.661
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-904e688f-460b-45d3-b220-458fd4720375 11/30/22 04:35:54.664
STEP: Creating a pod to test consume secrets 11/30/22 04:35:54.673
Nov 30 04:35:54.724: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064" in namespace "projected-7483" to be "Succeeded or Failed"
Nov 30 04:35:54.729: INFO: Pod "pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064": Phase="Pending", Reason="", readiness=false. Elapsed: 5.048003ms
Nov 30 04:35:56.732: INFO: Pod "pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008123493s
Nov 30 04:35:58.733: INFO: Pod "pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009160386s
STEP: Saw pod success 11/30/22 04:35:58.733
Nov 30 04:35:58.733: INFO: Pod "pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064" satisfied condition "Succeeded or Failed"
Nov 30 04:35:58.740: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/30/22 04:35:58.745
Nov 30 04:35:58.768: INFO: Waiting for pod pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064 to disappear
Nov 30 04:35:58.771: INFO: Pod pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Nov 30 04:35:58.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7483" for this suite. 11/30/22 04:35:58.775
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":254,"skipped":4565,"failed":0}
------------------------------
• [4.149 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:35:54.634
    Nov 30 04:35:54.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:35:54.635
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:54.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:54.661
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-904e688f-460b-45d3-b220-458fd4720375 11/30/22 04:35:54.664
    STEP: Creating a pod to test consume secrets 11/30/22 04:35:54.673
    Nov 30 04:35:54.724: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064" in namespace "projected-7483" to be "Succeeded or Failed"
    Nov 30 04:35:54.729: INFO: Pod "pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064": Phase="Pending", Reason="", readiness=false. Elapsed: 5.048003ms
    Nov 30 04:35:56.732: INFO: Pod "pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008123493s
    Nov 30 04:35:58.733: INFO: Pod "pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009160386s
    STEP: Saw pod success 11/30/22 04:35:58.733
    Nov 30 04:35:58.733: INFO: Pod "pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064" satisfied condition "Succeeded or Failed"
    Nov 30 04:35:58.740: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 04:35:58.745
    Nov 30 04:35:58.768: INFO: Waiting for pod pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064 to disappear
    Nov 30 04:35:58.771: INFO: Pod pod-projected-secrets-216b48e7-b0e9-4302-b040-a1f9823b3064 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Nov 30 04:35:58.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7483" for this suite. 11/30/22 04:35:58.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:35:58.784
Nov 30 04:35:58.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename resourcequota 11/30/22 04:35:58.785
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:58.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:58.803
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 11/30/22 04:35:58.806
STEP: Counting existing ResourceQuota 11/30/22 04:36:04.81
STEP: Creating a ResourceQuota 11/30/22 04:36:09.814
STEP: Ensuring resource quota status is calculated 11/30/22 04:36:09.818
STEP: Creating a Secret 11/30/22 04:36:11.822
STEP: Ensuring resource quota status captures secret creation 11/30/22 04:36:11.832
STEP: Deleting a secret 11/30/22 04:36:13.835
STEP: Ensuring resource quota status released usage 11/30/22 04:36:13.842
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Nov 30 04:36:15.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2917" for this suite. 11/30/22 04:36:15.849
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":255,"skipped":4574,"failed":0}
------------------------------
• [SLOW TEST] [17.073 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:35:58.784
    Nov 30 04:35:58.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename resourcequota 11/30/22 04:35:58.785
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:35:58.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:35:58.803
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 11/30/22 04:35:58.806
    STEP: Counting existing ResourceQuota 11/30/22 04:36:04.81
    STEP: Creating a ResourceQuota 11/30/22 04:36:09.814
    STEP: Ensuring resource quota status is calculated 11/30/22 04:36:09.818
    STEP: Creating a Secret 11/30/22 04:36:11.822
    STEP: Ensuring resource quota status captures secret creation 11/30/22 04:36:11.832
    STEP: Deleting a secret 11/30/22 04:36:13.835
    STEP: Ensuring resource quota status released usage 11/30/22 04:36:13.842
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Nov 30 04:36:15.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2917" for this suite. 11/30/22 04:36:15.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:36:15.857
Nov 30 04:36:15.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename subpath 11/30/22 04:36:15.858
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:36:15.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:36:15.882
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/30/22 04:36:15.884
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-qq2l 11/30/22 04:36:15.895
STEP: Creating a pod to test atomic-volume-subpath 11/30/22 04:36:15.895
Nov 30 04:36:15.922: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qq2l" in namespace "subpath-4029" to be "Succeeded or Failed"
Nov 30 04:36:15.925: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.665074ms
Nov 30 04:36:17.928: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 2.005853544s
Nov 30 04:36:19.928: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 4.006111541s
Nov 30 04:36:21.929: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 6.007075809s
Nov 30 04:36:23.929: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 8.007144143s
Nov 30 04:36:25.929: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 10.006597467s
Nov 30 04:36:27.928: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 12.006097108s
Nov 30 04:36:29.928: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 14.006164121s
Nov 30 04:36:31.929: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 16.006655207s
Nov 30 04:36:33.929: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 18.007075164s
Nov 30 04:36:35.929: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 20.006968112s
Nov 30 04:36:37.928: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=false. Elapsed: 22.006199995s
Nov 30 04:36:39.928: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005817985s
STEP: Saw pod success 11/30/22 04:36:39.928
Nov 30 04:36:39.928: INFO: Pod "pod-subpath-test-projected-qq2l" satisfied condition "Succeeded or Failed"
Nov 30 04:36:39.930: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-subpath-test-projected-qq2l container test-container-subpath-projected-qq2l: <nil>
STEP: delete the pod 11/30/22 04:36:39.935
Nov 30 04:36:39.945: INFO: Waiting for pod pod-subpath-test-projected-qq2l to disappear
Nov 30 04:36:39.947: INFO: Pod pod-subpath-test-projected-qq2l no longer exists
STEP: Deleting pod pod-subpath-test-projected-qq2l 11/30/22 04:36:39.947
Nov 30 04:36:39.947: INFO: Deleting pod "pod-subpath-test-projected-qq2l" in namespace "subpath-4029"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Nov 30 04:36:39.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4029" for this suite. 11/30/22 04:36:39.953
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":256,"skipped":4583,"failed":0}
------------------------------
• [SLOW TEST] [24.101 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:36:15.857
    Nov 30 04:36:15.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename subpath 11/30/22 04:36:15.858
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:36:15.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:36:15.882
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/30/22 04:36:15.884
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-qq2l 11/30/22 04:36:15.895
    STEP: Creating a pod to test atomic-volume-subpath 11/30/22 04:36:15.895
    Nov 30 04:36:15.922: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qq2l" in namespace "subpath-4029" to be "Succeeded or Failed"
    Nov 30 04:36:15.925: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.665074ms
    Nov 30 04:36:17.928: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 2.005853544s
    Nov 30 04:36:19.928: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 4.006111541s
    Nov 30 04:36:21.929: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 6.007075809s
    Nov 30 04:36:23.929: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 8.007144143s
    Nov 30 04:36:25.929: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 10.006597467s
    Nov 30 04:36:27.928: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 12.006097108s
    Nov 30 04:36:29.928: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 14.006164121s
    Nov 30 04:36:31.929: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 16.006655207s
    Nov 30 04:36:33.929: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 18.007075164s
    Nov 30 04:36:35.929: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=true. Elapsed: 20.006968112s
    Nov 30 04:36:37.928: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Running", Reason="", readiness=false. Elapsed: 22.006199995s
    Nov 30 04:36:39.928: INFO: Pod "pod-subpath-test-projected-qq2l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005817985s
    STEP: Saw pod success 11/30/22 04:36:39.928
    Nov 30 04:36:39.928: INFO: Pod "pod-subpath-test-projected-qq2l" satisfied condition "Succeeded or Failed"
    Nov 30 04:36:39.930: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-subpath-test-projected-qq2l container test-container-subpath-projected-qq2l: <nil>
    STEP: delete the pod 11/30/22 04:36:39.935
    Nov 30 04:36:39.945: INFO: Waiting for pod pod-subpath-test-projected-qq2l to disappear
    Nov 30 04:36:39.947: INFO: Pod pod-subpath-test-projected-qq2l no longer exists
    STEP: Deleting pod pod-subpath-test-projected-qq2l 11/30/22 04:36:39.947
    Nov 30 04:36:39.947: INFO: Deleting pod "pod-subpath-test-projected-qq2l" in namespace "subpath-4029"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Nov 30 04:36:39.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4029" for this suite. 11/30/22 04:36:39.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:36:39.959
Nov 30 04:36:39.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:36:39.959
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:36:39.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:36:39.978
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-9f8661f4-72ac-46af-b80d-b373dd84c73f 11/30/22 04:36:39.98
STEP: Creating a pod to test consume secrets 11/30/22 04:36:39.987
Nov 30 04:36:40.019: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6" in namespace "projected-6789" to be "Succeeded or Failed"
Nov 30 04:36:40.022: INFO: Pod "pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.641747ms
Nov 30 04:36:42.026: INFO: Pod "pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006450292s
Nov 30 04:36:44.025: INFO: Pod "pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006401106s
STEP: Saw pod success 11/30/22 04:36:44.026
Nov 30 04:36:44.026: INFO: Pod "pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6" satisfied condition "Succeeded or Failed"
Nov 30 04:36:44.029: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/30/22 04:36:44.033
Nov 30 04:36:44.047: INFO: Waiting for pod pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6 to disappear
Nov 30 04:36:44.050: INFO: Pod pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Nov 30 04:36:44.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6789" for this suite. 11/30/22 04:36:44.054
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":257,"skipped":4627,"failed":0}
------------------------------
• [4.105 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:36:39.959
    Nov 30 04:36:39.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:36:39.959
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:36:39.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:36:39.978
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-9f8661f4-72ac-46af-b80d-b373dd84c73f 11/30/22 04:36:39.98
    STEP: Creating a pod to test consume secrets 11/30/22 04:36:39.987
    Nov 30 04:36:40.019: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6" in namespace "projected-6789" to be "Succeeded or Failed"
    Nov 30 04:36:40.022: INFO: Pod "pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.641747ms
    Nov 30 04:36:42.026: INFO: Pod "pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006450292s
    Nov 30 04:36:44.025: INFO: Pod "pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006401106s
    STEP: Saw pod success 11/30/22 04:36:44.026
    Nov 30 04:36:44.026: INFO: Pod "pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6" satisfied condition "Succeeded or Failed"
    Nov 30 04:36:44.029: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 04:36:44.033
    Nov 30 04:36:44.047: INFO: Waiting for pod pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6 to disappear
    Nov 30 04:36:44.050: INFO: Pod pod-projected-secrets-94181a8b-aa60-4d64-a2f2-bf9ec4a78ef6 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Nov 30 04:36:44.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6789" for this suite. 11/30/22 04:36:44.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:36:44.064
Nov 30 04:36:44.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 04:36:44.065
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:36:44.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:36:44.089
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 04:36:44.105
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:36:44.352
STEP: Deploying the webhook pod 11/30/22 04:36:44.359
STEP: Wait for the deployment to be ready 11/30/22 04:36:44.374
Nov 30 04:36:44.381: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 04:36:46.392
STEP: Verifying the service has paired with the endpoint 11/30/22 04:36:46.404
Nov 30 04:36:47.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 11/30/22 04:36:47.408
STEP: Creating a custom resource definition that should be denied by the webhook 11/30/22 04:36:47.422
Nov 30 04:36:47.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:36:47.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5127" for this suite. 11/30/22 04:36:47.441
STEP: Destroying namespace "webhook-5127-markers" for this suite. 11/30/22 04:36:47.447
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":258,"skipped":4648,"failed":0}
------------------------------
• [3.464 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:36:44.064
    Nov 30 04:36:44.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 04:36:44.065
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:36:44.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:36:44.089
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 04:36:44.105
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:36:44.352
    STEP: Deploying the webhook pod 11/30/22 04:36:44.359
    STEP: Wait for the deployment to be ready 11/30/22 04:36:44.374
    Nov 30 04:36:44.381: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 04:36:46.392
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:36:46.404
    Nov 30 04:36:47.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 11/30/22 04:36:47.408
    STEP: Creating a custom resource definition that should be denied by the webhook 11/30/22 04:36:47.422
    Nov 30 04:36:47.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:36:47.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5127" for this suite. 11/30/22 04:36:47.441
    STEP: Destroying namespace "webhook-5127-markers" for this suite. 11/30/22 04:36:47.447
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:36:47.529
Nov 30 04:36:47.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename replicaset 11/30/22 04:36:47.53
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:36:47.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:36:47.562
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 11/30/22 04:36:47.572
STEP: Verify that the required pods have come up. 11/30/22 04:36:47.579
Nov 30 04:36:47.588: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 30 04:36:52.592: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/30/22 04:36:52.592
STEP: Getting /status 11/30/22 04:36:52.592
Nov 30 04:36:52.596: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 11/30/22 04:36:52.596
Nov 30 04:36:52.604: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 11/30/22 04:36:52.604
Nov 30 04:36:52.606: INFO: Observed &ReplicaSet event: ADDED
Nov 30 04:36:52.606: INFO: Observed &ReplicaSet event: MODIFIED
Nov 30 04:36:52.606: INFO: Observed &ReplicaSet event: MODIFIED
Nov 30 04:36:52.606: INFO: Observed &ReplicaSet event: MODIFIED
Nov 30 04:36:52.606: INFO: Found replicaset test-rs in namespace replicaset-8086 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 30 04:36:52.606: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 11/30/22 04:36:52.606
Nov 30 04:36:52.606: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 30 04:36:52.614: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 11/30/22 04:36:52.614
Nov 30 04:36:52.616: INFO: Observed &ReplicaSet event: ADDED
Nov 30 04:36:52.616: INFO: Observed &ReplicaSet event: MODIFIED
Nov 30 04:36:52.616: INFO: Observed &ReplicaSet event: MODIFIED
Nov 30 04:36:52.616: INFO: Observed &ReplicaSet event: MODIFIED
Nov 30 04:36:52.616: INFO: Observed replicaset test-rs in namespace replicaset-8086 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 30 04:36:52.616: INFO: Observed &ReplicaSet event: MODIFIED
Nov 30 04:36:52.616: INFO: Found replicaset test-rs in namespace replicaset-8086 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Nov 30 04:36:52.616: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Nov 30 04:36:52.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8086" for this suite. 11/30/22 04:36:52.62
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":259,"skipped":4669,"failed":0}
------------------------------
• [SLOW TEST] [5.098 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:36:47.529
    Nov 30 04:36:47.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename replicaset 11/30/22 04:36:47.53
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:36:47.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:36:47.562
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 11/30/22 04:36:47.572
    STEP: Verify that the required pods have come up. 11/30/22 04:36:47.579
    Nov 30 04:36:47.588: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 30 04:36:52.592: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/30/22 04:36:52.592
    STEP: Getting /status 11/30/22 04:36:52.592
    Nov 30 04:36:52.596: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 11/30/22 04:36:52.596
    Nov 30 04:36:52.604: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 11/30/22 04:36:52.604
    Nov 30 04:36:52.606: INFO: Observed &ReplicaSet event: ADDED
    Nov 30 04:36:52.606: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 30 04:36:52.606: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 30 04:36:52.606: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 30 04:36:52.606: INFO: Found replicaset test-rs in namespace replicaset-8086 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 30 04:36:52.606: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 11/30/22 04:36:52.606
    Nov 30 04:36:52.606: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Nov 30 04:36:52.614: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 11/30/22 04:36:52.614
    Nov 30 04:36:52.616: INFO: Observed &ReplicaSet event: ADDED
    Nov 30 04:36:52.616: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 30 04:36:52.616: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 30 04:36:52.616: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 30 04:36:52.616: INFO: Observed replicaset test-rs in namespace replicaset-8086 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 30 04:36:52.616: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 30 04:36:52.616: INFO: Found replicaset test-rs in namespace replicaset-8086 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Nov 30 04:36:52.616: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Nov 30 04:36:52.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8086" for this suite. 11/30/22 04:36:52.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:36:52.628
Nov 30 04:36:52.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename daemonsets 11/30/22 04:36:52.629
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:36:52.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:36:52.648
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 11/30/22 04:36:52.678
STEP: Check that daemon pods launch on every node of the cluster. 11/30/22 04:36:52.683
Nov 30 04:36:52.689: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:36:52.689: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:36:52.689: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:36:52.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:36:52.694: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:36:53.699: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:36:53.699: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:36:53.699: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:36:53.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 30 04:36:53.702: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:36:54.700: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:36:54.700: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:36:54.700: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:36:54.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Nov 30 04:36:54.703: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Getting /status 11/30/22 04:36:54.706
Nov 30 04:36:54.709: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 11/30/22 04:36:54.709
Nov 30 04:36:54.717: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 11/30/22 04:36:54.717
Nov 30 04:36:54.718: INFO: Observed &DaemonSet event: ADDED
Nov 30 04:36:54.718: INFO: Observed &DaemonSet event: MODIFIED
Nov 30 04:36:54.718: INFO: Observed &DaemonSet event: MODIFIED
Nov 30 04:36:54.718: INFO: Observed &DaemonSet event: MODIFIED
Nov 30 04:36:54.719: INFO: Observed &DaemonSet event: MODIFIED
Nov 30 04:36:54.719: INFO: Observed &DaemonSet event: MODIFIED
Nov 30 04:36:54.719: INFO: Found daemon set daemon-set in namespace daemonsets-5296 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 30 04:36:54.719: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 11/30/22 04:36:54.719
STEP: watching for the daemon set status to be patched 11/30/22 04:36:54.728
Nov 30 04:36:54.729: INFO: Observed &DaemonSet event: ADDED
Nov 30 04:36:54.729: INFO: Observed &DaemonSet event: MODIFIED
Nov 30 04:36:54.729: INFO: Observed &DaemonSet event: MODIFIED
Nov 30 04:36:54.729: INFO: Observed &DaemonSet event: MODIFIED
Nov 30 04:36:54.729: INFO: Observed &DaemonSet event: MODIFIED
Nov 30 04:36:54.729: INFO: Observed &DaemonSet event: MODIFIED
Nov 30 04:36:54.729: INFO: Observed daemon set daemon-set in namespace daemonsets-5296 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 30 04:36:54.730: INFO: Observed &DaemonSet event: MODIFIED
Nov 30 04:36:54.730: INFO: Found daemon set daemon-set in namespace daemonsets-5296 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Nov 30 04:36:54.730: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 11/30/22 04:36:54.736
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5296, will wait for the garbage collector to delete the pods 11/30/22 04:36:54.736
Nov 30 04:36:54.795: INFO: Deleting DaemonSet.extensions daemon-set took: 5.686592ms
Nov 30 04:36:54.896: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.908048ms
Nov 30 04:36:57.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:36:57.500: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 30 04:36:57.502: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54170"},"items":null}

Nov 30 04:36:57.504: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54170"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Nov 30 04:36:57.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5296" for this suite. 11/30/22 04:36:57.526
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":260,"skipped":4706,"failed":0}
------------------------------
• [4.903 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:36:52.628
    Nov 30 04:36:52.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename daemonsets 11/30/22 04:36:52.629
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:36:52.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:36:52.648
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 11/30/22 04:36:52.678
    STEP: Check that daemon pods launch on every node of the cluster. 11/30/22 04:36:52.683
    Nov 30 04:36:52.689: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:36:52.689: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:36:52.689: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:36:52.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:36:52.694: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:36:53.699: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:36:53.699: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:36:53.699: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:36:53.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 30 04:36:53.702: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:36:54.700: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:36:54.700: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:36:54.700: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:36:54.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Nov 30 04:36:54.703: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Getting /status 11/30/22 04:36:54.706
    Nov 30 04:36:54.709: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 11/30/22 04:36:54.709
    Nov 30 04:36:54.717: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 11/30/22 04:36:54.717
    Nov 30 04:36:54.718: INFO: Observed &DaemonSet event: ADDED
    Nov 30 04:36:54.718: INFO: Observed &DaemonSet event: MODIFIED
    Nov 30 04:36:54.718: INFO: Observed &DaemonSet event: MODIFIED
    Nov 30 04:36:54.718: INFO: Observed &DaemonSet event: MODIFIED
    Nov 30 04:36:54.719: INFO: Observed &DaemonSet event: MODIFIED
    Nov 30 04:36:54.719: INFO: Observed &DaemonSet event: MODIFIED
    Nov 30 04:36:54.719: INFO: Found daemon set daemon-set in namespace daemonsets-5296 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 30 04:36:54.719: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 11/30/22 04:36:54.719
    STEP: watching for the daemon set status to be patched 11/30/22 04:36:54.728
    Nov 30 04:36:54.729: INFO: Observed &DaemonSet event: ADDED
    Nov 30 04:36:54.729: INFO: Observed &DaemonSet event: MODIFIED
    Nov 30 04:36:54.729: INFO: Observed &DaemonSet event: MODIFIED
    Nov 30 04:36:54.729: INFO: Observed &DaemonSet event: MODIFIED
    Nov 30 04:36:54.729: INFO: Observed &DaemonSet event: MODIFIED
    Nov 30 04:36:54.729: INFO: Observed &DaemonSet event: MODIFIED
    Nov 30 04:36:54.729: INFO: Observed daemon set daemon-set in namespace daemonsets-5296 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 30 04:36:54.730: INFO: Observed &DaemonSet event: MODIFIED
    Nov 30 04:36:54.730: INFO: Found daemon set daemon-set in namespace daemonsets-5296 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Nov 30 04:36:54.730: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 11/30/22 04:36:54.736
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5296, will wait for the garbage collector to delete the pods 11/30/22 04:36:54.736
    Nov 30 04:36:54.795: INFO: Deleting DaemonSet.extensions daemon-set took: 5.686592ms
    Nov 30 04:36:54.896: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.908048ms
    Nov 30 04:36:57.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:36:57.500: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 30 04:36:57.502: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54170"},"items":null}

    Nov 30 04:36:57.504: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54170"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 04:36:57.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5296" for this suite. 11/30/22 04:36:57.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:36:57.532
Nov 30 04:36:57.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename statefulset 11/30/22 04:36:57.533
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:36:57.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:36:57.551
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5525 11/30/22 04:36:57.553
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-5525 11/30/22 04:36:57.565
Nov 30 04:36:57.573: INFO: Found 0 stateful pods, waiting for 1
Nov 30 04:37:07.578: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 11/30/22 04:37:07.583
STEP: Getting /status 11/30/22 04:37:07.589
Nov 30 04:37:07.594: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 11/30/22 04:37:07.594
Nov 30 04:37:07.603: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 11/30/22 04:37:07.603
Nov 30 04:37:07.604: INFO: Observed &StatefulSet event: ADDED
Nov 30 04:37:07.605: INFO: Found Statefulset ss in namespace statefulset-5525 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 30 04:37:07.605: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 11/30/22 04:37:07.605
Nov 30 04:37:07.605: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 30 04:37:07.616: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 11/30/22 04:37:07.616
Nov 30 04:37:07.618: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Nov 30 04:37:07.618: INFO: Deleting all statefulset in ns statefulset-5525
Nov 30 04:37:07.620: INFO: Scaling statefulset ss to 0
Nov 30 04:37:17.639: INFO: Waiting for statefulset status.replicas updated to 0
Nov 30 04:37:17.642: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Nov 30 04:37:17.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5525" for this suite. 11/30/22 04:37:17.661
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":261,"skipped":4716,"failed":0}
------------------------------
• [SLOW TEST] [20.135 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:36:57.532
    Nov 30 04:36:57.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename statefulset 11/30/22 04:36:57.533
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:36:57.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:36:57.551
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5525 11/30/22 04:36:57.553
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-5525 11/30/22 04:36:57.565
    Nov 30 04:36:57.573: INFO: Found 0 stateful pods, waiting for 1
    Nov 30 04:37:07.578: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 11/30/22 04:37:07.583
    STEP: Getting /status 11/30/22 04:37:07.589
    Nov 30 04:37:07.594: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 11/30/22 04:37:07.594
    Nov 30 04:37:07.603: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 11/30/22 04:37:07.603
    Nov 30 04:37:07.604: INFO: Observed &StatefulSet event: ADDED
    Nov 30 04:37:07.605: INFO: Found Statefulset ss in namespace statefulset-5525 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 30 04:37:07.605: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 11/30/22 04:37:07.605
    Nov 30 04:37:07.605: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Nov 30 04:37:07.616: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 11/30/22 04:37:07.616
    Nov 30 04:37:07.618: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Nov 30 04:37:07.618: INFO: Deleting all statefulset in ns statefulset-5525
    Nov 30 04:37:07.620: INFO: Scaling statefulset ss to 0
    Nov 30 04:37:17.639: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 30 04:37:17.642: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Nov 30 04:37:17.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5525" for this suite. 11/30/22 04:37:17.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:37:17.668
Nov 30 04:37:17.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename subpath 11/30/22 04:37:17.668
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:37:17.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:37:17.692
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/30/22 04:37:17.695
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-sfkt 11/30/22 04:37:17.712
STEP: Creating a pod to test atomic-volume-subpath 11/30/22 04:37:17.712
Nov 30 04:37:17.750: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sfkt" in namespace "subpath-3363" to be "Succeeded or Failed"
Nov 30 04:37:17.758: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Pending", Reason="", readiness=false. Elapsed: 7.891826ms
Nov 30 04:37:19.762: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 2.011874294s
Nov 30 04:37:21.761: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 4.011463102s
Nov 30 04:37:23.763: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 6.013035679s
Nov 30 04:37:25.766: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 8.015792544s
Nov 30 04:37:27.767: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 10.01674234s
Nov 30 04:37:29.761: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 12.011381901s
Nov 30 04:37:31.762: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 14.011841182s
Nov 30 04:37:33.762: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 16.012028187s
Nov 30 04:37:35.764: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 18.013633559s
Nov 30 04:37:37.761: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 20.01119719s
Nov 30 04:37:39.762: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=false. Elapsed: 22.011660976s
Nov 30 04:37:41.763: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012966959s
STEP: Saw pod success 11/30/22 04:37:41.763
Nov 30 04:37:41.763: INFO: Pod "pod-subpath-test-secret-sfkt" satisfied condition "Succeeded or Failed"
Nov 30 04:37:41.766: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-subpath-test-secret-sfkt container test-container-subpath-secret-sfkt: <nil>
STEP: delete the pod 11/30/22 04:37:41.77
Nov 30 04:37:41.779: INFO: Waiting for pod pod-subpath-test-secret-sfkt to disappear
Nov 30 04:37:41.782: INFO: Pod pod-subpath-test-secret-sfkt no longer exists
STEP: Deleting pod pod-subpath-test-secret-sfkt 11/30/22 04:37:41.782
Nov 30 04:37:41.782: INFO: Deleting pod "pod-subpath-test-secret-sfkt" in namespace "subpath-3363"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Nov 30 04:37:41.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3363" for this suite. 11/30/22 04:37:41.788
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":262,"skipped":4722,"failed":0}
------------------------------
• [SLOW TEST] [24.127 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:37:17.668
    Nov 30 04:37:17.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename subpath 11/30/22 04:37:17.668
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:37:17.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:37:17.692
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/30/22 04:37:17.695
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-sfkt 11/30/22 04:37:17.712
    STEP: Creating a pod to test atomic-volume-subpath 11/30/22 04:37:17.712
    Nov 30 04:37:17.750: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sfkt" in namespace "subpath-3363" to be "Succeeded or Failed"
    Nov 30 04:37:17.758: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Pending", Reason="", readiness=false. Elapsed: 7.891826ms
    Nov 30 04:37:19.762: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 2.011874294s
    Nov 30 04:37:21.761: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 4.011463102s
    Nov 30 04:37:23.763: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 6.013035679s
    Nov 30 04:37:25.766: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 8.015792544s
    Nov 30 04:37:27.767: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 10.01674234s
    Nov 30 04:37:29.761: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 12.011381901s
    Nov 30 04:37:31.762: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 14.011841182s
    Nov 30 04:37:33.762: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 16.012028187s
    Nov 30 04:37:35.764: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 18.013633559s
    Nov 30 04:37:37.761: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=true. Elapsed: 20.01119719s
    Nov 30 04:37:39.762: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Running", Reason="", readiness=false. Elapsed: 22.011660976s
    Nov 30 04:37:41.763: INFO: Pod "pod-subpath-test-secret-sfkt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012966959s
    STEP: Saw pod success 11/30/22 04:37:41.763
    Nov 30 04:37:41.763: INFO: Pod "pod-subpath-test-secret-sfkt" satisfied condition "Succeeded or Failed"
    Nov 30 04:37:41.766: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-subpath-test-secret-sfkt container test-container-subpath-secret-sfkt: <nil>
    STEP: delete the pod 11/30/22 04:37:41.77
    Nov 30 04:37:41.779: INFO: Waiting for pod pod-subpath-test-secret-sfkt to disappear
    Nov 30 04:37:41.782: INFO: Pod pod-subpath-test-secret-sfkt no longer exists
    STEP: Deleting pod pod-subpath-test-secret-sfkt 11/30/22 04:37:41.782
    Nov 30 04:37:41.782: INFO: Deleting pod "pod-subpath-test-secret-sfkt" in namespace "subpath-3363"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Nov 30 04:37:41.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3363" for this suite. 11/30/22 04:37:41.788
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:37:41.795
Nov 30 04:37:41.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 04:37:41.796
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:37:41.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:37:41.825
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 04:37:41.845
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:37:42.211
STEP: Deploying the webhook pod 11/30/22 04:37:42.218
STEP: Wait for the deployment to be ready 11/30/22 04:37:42.229
Nov 30 04:37:42.236: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 04:37:44.261
STEP: Verifying the service has paired with the endpoint 11/30/22 04:37:44.277
Nov 30 04:37:45.277: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 11/30/22 04:37:45.343
STEP: Creating a configMap that should be mutated 11/30/22 04:37:45.354
STEP: Deleting the collection of validation webhooks 11/30/22 04:37:45.378
STEP: Creating a configMap that should not be mutated 11/30/22 04:37:45.435
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:37:45.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1813" for this suite. 11/30/22 04:37:45.451
STEP: Destroying namespace "webhook-1813-markers" for this suite. 11/30/22 04:37:45.457
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":263,"skipped":4722,"failed":0}
------------------------------
• [3.798 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:37:41.795
    Nov 30 04:37:41.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 04:37:41.796
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:37:41.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:37:41.825
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 04:37:41.845
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:37:42.211
    STEP: Deploying the webhook pod 11/30/22 04:37:42.218
    STEP: Wait for the deployment to be ready 11/30/22 04:37:42.229
    Nov 30 04:37:42.236: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 04:37:44.261
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:37:44.277
    Nov 30 04:37:45.277: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 11/30/22 04:37:45.343
    STEP: Creating a configMap that should be mutated 11/30/22 04:37:45.354
    STEP: Deleting the collection of validation webhooks 11/30/22 04:37:45.378
    STEP: Creating a configMap that should not be mutated 11/30/22 04:37:45.435
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:37:45.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1813" for this suite. 11/30/22 04:37:45.451
    STEP: Destroying namespace "webhook-1813-markers" for this suite. 11/30/22 04:37:45.457
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:37:45.594
Nov 30 04:37:45.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 04:37:45.595
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:37:45.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:37:45.717
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 11/30/22 04:37:45.719
Nov 30 04:37:45.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: rename a version 11/30/22 04:37:53.397
STEP: check the new version name is served 11/30/22 04:37:53.418
STEP: check the old version name is removed 11/30/22 04:37:55.433
STEP: check the other version is not changed 11/30/22 04:37:56.774
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:38:01.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7021" for this suite. 11/30/22 04:38:01.901
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":264,"skipped":4756,"failed":0}
------------------------------
• [SLOW TEST] [16.313 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:37:45.594
    Nov 30 04:37:45.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 04:37:45.595
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:37:45.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:37:45.717
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 11/30/22 04:37:45.719
    Nov 30 04:37:45.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: rename a version 11/30/22 04:37:53.397
    STEP: check the new version name is served 11/30/22 04:37:53.418
    STEP: check the old version name is removed 11/30/22 04:37:55.433
    STEP: check the other version is not changed 11/30/22 04:37:56.774
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:38:01.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7021" for this suite. 11/30/22 04:38:01.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:38:01.908
Nov 30 04:38:01.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 04:38:01.908
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:01.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:01.928
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 11/30/22 04:38:01.93
Nov 30 04:38:01.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:38:04.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:38:15.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9102" for this suite. 11/30/22 04:38:15.016
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":265,"skipped":4781,"failed":0}
------------------------------
• [SLOW TEST] [13.125 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:38:01.908
    Nov 30 04:38:01.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 04:38:01.908
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:01.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:01.928
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 11/30/22 04:38:01.93
    Nov 30 04:38:01.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:38:04.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:38:15.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9102" for this suite. 11/30/22 04:38:15.016
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:38:15.033
Nov 30 04:38:15.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-lifecycle-hook 11/30/22 04:38:15.033
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:15.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:15.07
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 11/30/22 04:38:15.082
Nov 30 04:38:15.123: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9182" to be "running and ready"
Nov 30 04:38:15.126: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.294281ms
Nov 30 04:38:15.127: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:38:17.130: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.00694696s
Nov 30 04:38:17.130: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 30 04:38:17.130: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 11/30/22 04:38:17.133
Nov 30 04:38:17.140: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9182" to be "running and ready"
Nov 30 04:38:17.144: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.45904ms
Nov 30 04:38:17.144: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:38:19.148: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007685404s
Nov 30 04:38:19.148: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Nov 30 04:38:19.148: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 11/30/22 04:38:19.153
Nov 30 04:38:19.160: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 30 04:38:19.164: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 30 04:38:21.164: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 30 04:38:21.167: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 30 04:38:23.165: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 30 04:38:23.168: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 11/30/22 04:38:23.168
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Nov 30 04:38:23.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9182" for this suite. 11/30/22 04:38:23.18
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":266,"skipped":4783,"failed":0}
------------------------------
• [SLOW TEST] [8.152 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:38:15.033
    Nov 30 04:38:15.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/30/22 04:38:15.033
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:15.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:15.07
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 11/30/22 04:38:15.082
    Nov 30 04:38:15.123: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9182" to be "running and ready"
    Nov 30 04:38:15.126: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.294281ms
    Nov 30 04:38:15.127: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:38:17.130: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.00694696s
    Nov 30 04:38:17.130: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 30 04:38:17.130: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 11/30/22 04:38:17.133
    Nov 30 04:38:17.140: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9182" to be "running and ready"
    Nov 30 04:38:17.144: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.45904ms
    Nov 30 04:38:17.144: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:38:19.148: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007685404s
    Nov 30 04:38:19.148: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Nov 30 04:38:19.148: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 11/30/22 04:38:19.153
    Nov 30 04:38:19.160: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Nov 30 04:38:19.164: INFO: Pod pod-with-prestop-exec-hook still exists
    Nov 30 04:38:21.164: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Nov 30 04:38:21.167: INFO: Pod pod-with-prestop-exec-hook still exists
    Nov 30 04:38:23.165: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Nov 30 04:38:23.168: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 11/30/22 04:38:23.168
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Nov 30 04:38:23.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9182" for this suite. 11/30/22 04:38:23.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:38:23.185
Nov 30 04:38:23.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename var-expansion 11/30/22 04:38:23.186
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:23.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:23.202
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 11/30/22 04:38:23.205
Nov 30 04:38:23.240: INFO: Waiting up to 5m0s for pod "var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e" in namespace "var-expansion-2259" to be "Succeeded or Failed"
Nov 30 04:38:23.243: INFO: Pod "var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.456796ms
Nov 30 04:38:25.248: INFO: Pod "var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008689266s
Nov 30 04:38:27.247: INFO: Pod "var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007081539s
STEP: Saw pod success 11/30/22 04:38:27.247
Nov 30 04:38:27.247: INFO: Pod "var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e" satisfied condition "Succeeded or Failed"
Nov 30 04:38:27.250: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e container dapi-container: <nil>
STEP: delete the pod 11/30/22 04:38:27.26
Nov 30 04:38:27.271: INFO: Waiting for pod var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e to disappear
Nov 30 04:38:27.273: INFO: Pod var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Nov 30 04:38:27.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2259" for this suite. 11/30/22 04:38:27.277
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":267,"skipped":4794,"failed":0}
------------------------------
• [4.099 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:38:23.185
    Nov 30 04:38:23.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename var-expansion 11/30/22 04:38:23.186
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:23.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:23.202
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 11/30/22 04:38:23.205
    Nov 30 04:38:23.240: INFO: Waiting up to 5m0s for pod "var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e" in namespace "var-expansion-2259" to be "Succeeded or Failed"
    Nov 30 04:38:23.243: INFO: Pod "var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.456796ms
    Nov 30 04:38:25.248: INFO: Pod "var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008689266s
    Nov 30 04:38:27.247: INFO: Pod "var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007081539s
    STEP: Saw pod success 11/30/22 04:38:27.247
    Nov 30 04:38:27.247: INFO: Pod "var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e" satisfied condition "Succeeded or Failed"
    Nov 30 04:38:27.250: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e container dapi-container: <nil>
    STEP: delete the pod 11/30/22 04:38:27.26
    Nov 30 04:38:27.271: INFO: Waiting for pod var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e to disappear
    Nov 30 04:38:27.273: INFO: Pod var-expansion-f027f822-9c1e-442c-bc17-d5d8b4766b7e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Nov 30 04:38:27.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2259" for this suite. 11/30/22 04:38:27.277
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:38:27.284
Nov 30 04:38:27.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 04:38:27.285
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:27.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:27.301
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 04:38:27.325
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:38:28.14
STEP: Deploying the webhook pod 11/30/22 04:38:28.148
STEP: Wait for the deployment to be ready 11/30/22 04:38:28.164
Nov 30 04:38:28.180: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 04:38:30.19
STEP: Verifying the service has paired with the endpoint 11/30/22 04:38:30.214
Nov 30 04:38:31.214: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 11/30/22 04:38:31.218
STEP: create a configmap that should be updated by the webhook 11/30/22 04:38:31.237
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:38:31.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1714" for this suite. 11/30/22 04:38:31.272
STEP: Destroying namespace "webhook-1714-markers" for this suite. 11/30/22 04:38:31.281
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":268,"skipped":4797,"failed":0}
------------------------------
• [4.063 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:38:27.284
    Nov 30 04:38:27.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 04:38:27.285
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:27.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:27.301
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 04:38:27.325
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:38:28.14
    STEP: Deploying the webhook pod 11/30/22 04:38:28.148
    STEP: Wait for the deployment to be ready 11/30/22 04:38:28.164
    Nov 30 04:38:28.180: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 04:38:30.19
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:38:30.214
    Nov 30 04:38:31.214: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 11/30/22 04:38:31.218
    STEP: create a configmap that should be updated by the webhook 11/30/22 04:38:31.237
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:38:31.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1714" for this suite. 11/30/22 04:38:31.272
    STEP: Destroying namespace "webhook-1714-markers" for this suite. 11/30/22 04:38:31.281
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:38:31.349
Nov 30 04:38:31.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pod-network-test 11/30/22 04:38:31.349
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:31.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:31.386
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-5060 11/30/22 04:38:31.389
STEP: creating a selector 11/30/22 04:38:31.39
STEP: Creating the service pods in kubernetes 11/30/22 04:38:31.39
Nov 30 04:38:31.390: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 30 04:38:31.468: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5060" to be "running and ready"
Nov 30 04:38:31.475: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.60852ms
Nov 30 04:38:31.475: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:38:33.479: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011405641s
Nov 30 04:38:33.479: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:38:35.480: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01253771s
Nov 30 04:38:35.480: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:38:37.479: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011708713s
Nov 30 04:38:37.479: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:38:39.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013190879s
Nov 30 04:38:39.481: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:38:41.479: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011599638s
Nov 30 04:38:41.479: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:38:43.480: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011924169s
Nov 30 04:38:43.480: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:38:45.479: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011331296s
Nov 30 04:38:45.479: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:38:47.480: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012049157s
Nov 30 04:38:47.480: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:38:49.479: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011694976s
Nov 30 04:38:49.479: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:38:51.479: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.0110236s
Nov 30 04:38:51.479: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:38:53.482: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01435139s
Nov 30 04:38:53.482: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 30 04:38:53.482: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 30 04:38:53.485: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5060" to be "running and ready"
Nov 30 04:38:53.487: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.407063ms
Nov 30 04:38:53.487: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 30 04:38:53.487: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 30 04:38:53.489: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5060" to be "running and ready"
Nov 30 04:38:53.495: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.226714ms
Nov 30 04:38:53.495: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 30 04:38:53.495: INFO: Pod "netserver-2" satisfied condition "running and ready"
Nov 30 04:38:53.497: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5060" to be "running and ready"
Nov 30 04:38:53.500: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.562069ms
Nov 30 04:38:53.500: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Nov 30 04:38:53.500: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 11/30/22 04:38:53.502
W1130 04:38:53.537690      20 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Nov 30 04:38:53.537: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5060" to be "running"
Nov 30 04:38:53.547: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.844194ms
Nov 30 04:38:55.561: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023652907s
Nov 30 04:38:55.561: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 30 04:38:55.570: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5060" to be "running"
Nov 30 04:38:55.599: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 29.301747ms
Nov 30 04:38:55.599: INFO: Pod "host-test-container-pod" satisfied condition "running"
Nov 30 04:38:55.615: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Nov 30 04:38:55.615: INFO: Going to poll 192.168.12.237 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Nov 30 04:38:55.620: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.12.237:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5060 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:38:55.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:38:55.621: INFO: ExecWithOptions: Clientset creation
Nov 30 04:38:55.621: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5060/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.12.237%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 30 04:38:55.691: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 30 04:38:55.691: INFO: Going to poll 192.168.228.164 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Nov 30 04:38:55.694: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.228.164:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5060 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:38:55.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:38:55.694: INFO: ExecWithOptions: Clientset creation
Nov 30 04:38:55.694: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5060/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.228.164%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 30 04:38:55.738: INFO: Found all 1 expected endpoints: [netserver-1]
Nov 30 04:38:55.738: INFO: Going to poll 192.168.251.143 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Nov 30 04:38:55.742: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.251.143:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5060 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:38:55.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:38:55.742: INFO: ExecWithOptions: Clientset creation
Nov 30 04:38:55.742: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5060/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.251.143%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 30 04:38:55.820: INFO: Found all 1 expected endpoints: [netserver-2]
Nov 30 04:38:55.820: INFO: Going to poll 192.168.254.61 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Nov 30 04:38:55.823: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.254.61:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5060 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:38:55.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:38:55.824: INFO: ExecWithOptions: Clientset creation
Nov 30 04:38:55.824: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5060/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.254.61%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 30 04:38:55.888: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Nov 30 04:38:55.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5060" for this suite. 11/30/22 04:38:55.892
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":269,"skipped":4827,"failed":0}
------------------------------
• [SLOW TEST] [24.549 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:38:31.349
    Nov 30 04:38:31.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pod-network-test 11/30/22 04:38:31.349
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:31.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:31.386
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-5060 11/30/22 04:38:31.389
    STEP: creating a selector 11/30/22 04:38:31.39
    STEP: Creating the service pods in kubernetes 11/30/22 04:38:31.39
    Nov 30 04:38:31.390: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 30 04:38:31.468: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5060" to be "running and ready"
    Nov 30 04:38:31.475: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.60852ms
    Nov 30 04:38:31.475: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:38:33.479: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011405641s
    Nov 30 04:38:33.479: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:38:35.480: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01253771s
    Nov 30 04:38:35.480: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:38:37.479: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011708713s
    Nov 30 04:38:37.479: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:38:39.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013190879s
    Nov 30 04:38:39.481: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:38:41.479: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011599638s
    Nov 30 04:38:41.479: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:38:43.480: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011924169s
    Nov 30 04:38:43.480: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:38:45.479: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011331296s
    Nov 30 04:38:45.479: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:38:47.480: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012049157s
    Nov 30 04:38:47.480: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:38:49.479: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011694976s
    Nov 30 04:38:49.479: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:38:51.479: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.0110236s
    Nov 30 04:38:51.479: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:38:53.482: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01435139s
    Nov 30 04:38:53.482: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 30 04:38:53.482: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 30 04:38:53.485: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5060" to be "running and ready"
    Nov 30 04:38:53.487: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.407063ms
    Nov 30 04:38:53.487: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 30 04:38:53.487: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 30 04:38:53.489: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5060" to be "running and ready"
    Nov 30 04:38:53.495: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.226714ms
    Nov 30 04:38:53.495: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 30 04:38:53.495: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Nov 30 04:38:53.497: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5060" to be "running and ready"
    Nov 30 04:38:53.500: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.562069ms
    Nov 30 04:38:53.500: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Nov 30 04:38:53.500: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 11/30/22 04:38:53.502
    W1130 04:38:53.537690      20 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Nov 30 04:38:53.537: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5060" to be "running"
    Nov 30 04:38:53.547: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.844194ms
    Nov 30 04:38:55.561: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023652907s
    Nov 30 04:38:55.561: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 30 04:38:55.570: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5060" to be "running"
    Nov 30 04:38:55.599: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 29.301747ms
    Nov 30 04:38:55.599: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Nov 30 04:38:55.615: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Nov 30 04:38:55.615: INFO: Going to poll 192.168.12.237 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Nov 30 04:38:55.620: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.12.237:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5060 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:38:55.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:38:55.621: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:38:55.621: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5060/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.12.237%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 30 04:38:55.691: INFO: Found all 1 expected endpoints: [netserver-0]
    Nov 30 04:38:55.691: INFO: Going to poll 192.168.228.164 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Nov 30 04:38:55.694: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.228.164:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5060 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:38:55.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:38:55.694: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:38:55.694: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5060/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.228.164%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 30 04:38:55.738: INFO: Found all 1 expected endpoints: [netserver-1]
    Nov 30 04:38:55.738: INFO: Going to poll 192.168.251.143 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Nov 30 04:38:55.742: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.251.143:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5060 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:38:55.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:38:55.742: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:38:55.742: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5060/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.251.143%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 30 04:38:55.820: INFO: Found all 1 expected endpoints: [netserver-2]
    Nov 30 04:38:55.820: INFO: Going to poll 192.168.254.61 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Nov 30 04:38:55.823: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.254.61:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5060 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:38:55.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:38:55.824: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:38:55.824: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5060/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.254.61%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 30 04:38:55.888: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Nov 30 04:38:55.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5060" for this suite. 11/30/22 04:38:55.892
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:38:55.898
Nov 30 04:38:55.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 04:38:55.899
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:55.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:55.917
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 04:38:55.936
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:38:56.187
STEP: Deploying the webhook pod 11/30/22 04:38:56.192
STEP: Wait for the deployment to be ready 11/30/22 04:38:56.202
Nov 30 04:38:56.214: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 04:38:58.223
STEP: Verifying the service has paired with the endpoint 11/30/22 04:38:58.237
Nov 30 04:38:59.237: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 11/30/22 04:38:59.24
STEP: create a pod that should be updated by the webhook 11/30/22 04:38:59.252
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:38:59.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6407" for this suite. 11/30/22 04:38:59.284
STEP: Destroying namespace "webhook-6407-markers" for this suite. 11/30/22 04:38:59.293
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":270,"skipped":4830,"failed":0}
------------------------------
• [3.473 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:38:55.898
    Nov 30 04:38:55.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 04:38:55.899
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:55.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:55.917
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 04:38:55.936
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:38:56.187
    STEP: Deploying the webhook pod 11/30/22 04:38:56.192
    STEP: Wait for the deployment to be ready 11/30/22 04:38:56.202
    Nov 30 04:38:56.214: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 04:38:58.223
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:38:58.237
    Nov 30 04:38:59.237: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 11/30/22 04:38:59.24
    STEP: create a pod that should be updated by the webhook 11/30/22 04:38:59.252
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:38:59.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6407" for this suite. 11/30/22 04:38:59.284
    STEP: Destroying namespace "webhook-6407-markers" for this suite. 11/30/22 04:38:59.293
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:38:59.373
Nov 30 04:38:59.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:38:59.374
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:59.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:59.398
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 11/30/22 04:38:59.403
Nov 30 04:38:59.429: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48" in namespace "projected-398" to be "Succeeded or Failed"
Nov 30 04:38:59.434: INFO: Pod "downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.977546ms
Nov 30 04:39:01.441: INFO: Pod "downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011624715s
Nov 30 04:39:03.437: INFO: Pod "downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00778204s
STEP: Saw pod success 11/30/22 04:39:03.437
Nov 30 04:39:03.437: INFO: Pod "downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48" satisfied condition "Succeeded or Failed"
Nov 30 04:39:03.440: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48 container client-container: <nil>
STEP: delete the pod 11/30/22 04:39:03.446
Nov 30 04:39:03.456: INFO: Waiting for pod downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48 to disappear
Nov 30 04:39:03.458: INFO: Pod downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Nov 30 04:39:03.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-398" for this suite. 11/30/22 04:39:03.462
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":271,"skipped":4915,"failed":0}
------------------------------
• [4.093 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:38:59.373
    Nov 30 04:38:59.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:38:59.374
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:38:59.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:38:59.398
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 11/30/22 04:38:59.403
    Nov 30 04:38:59.429: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48" in namespace "projected-398" to be "Succeeded or Failed"
    Nov 30 04:38:59.434: INFO: Pod "downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.977546ms
    Nov 30 04:39:01.441: INFO: Pod "downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011624715s
    Nov 30 04:39:03.437: INFO: Pod "downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00778204s
    STEP: Saw pod success 11/30/22 04:39:03.437
    Nov 30 04:39:03.437: INFO: Pod "downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48" satisfied condition "Succeeded or Failed"
    Nov 30 04:39:03.440: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48 container client-container: <nil>
    STEP: delete the pod 11/30/22 04:39:03.446
    Nov 30 04:39:03.456: INFO: Waiting for pod downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48 to disappear
    Nov 30 04:39:03.458: INFO: Pod downwardapi-volume-3d518ce9-e7c1-408a-98ab-f4afd8de7d48 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Nov 30 04:39:03.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-398" for this suite. 11/30/22 04:39:03.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:39:03.467
Nov 30 04:39:03.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename daemonsets 11/30/22 04:39:03.468
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:03.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:03.48
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Nov 30 04:39:03.510: INFO: Create a RollingUpdate DaemonSet
Nov 30 04:39:03.516: INFO: Check that daemon pods launch on every node of the cluster
Nov 30 04:39:03.524: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:03.524: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:03.524: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:03.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:39:03.531: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:39:04.536: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:04.537: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:04.537: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:04.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:39:04.540: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:39:05.536: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:05.536: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:05.536: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:05.552: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Nov 30 04:39:05.552: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
Nov 30 04:39:05.552: INFO: Update the DaemonSet to trigger a rollout
Nov 30 04:39:05.590: INFO: Updating DaemonSet daemon-set
Nov 30 04:39:08.604: INFO: Roll back the DaemonSet before rollout is complete
Nov 30 04:39:08.612: INFO: Updating DaemonSet daemon-set
Nov 30 04:39:08.612: INFO: Make sure DaemonSet rollback is complete
Nov 30 04:39:08.620: INFO: Wrong image for pod: daemon-set-vjvll. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2, got: foo:non-existent.
Nov 30 04:39:08.620: INFO: Pod daemon-set-vjvll is not available
Nov 30 04:39:08.626: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:08.626: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:08.626: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:09.639: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:09.639: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:09.639: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:10.636: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:10.637: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:10.637: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:11.634: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:11.634: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:11.634: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:12.636: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:12.636: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:12.636: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:13.636: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:13.637: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:13.637: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:14.631: INFO: Pod daemon-set-8qf8x is not available
Nov 30 04:39:14.636: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:14.636: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:14.636: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 11/30/22 04:39:14.643
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-69, will wait for the garbage collector to delete the pods 11/30/22 04:39:14.643
Nov 30 04:39:14.706: INFO: Deleting DaemonSet.extensions daemon-set took: 9.152248ms
Nov 30 04:39:14.807: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.571316ms
Nov 30 04:39:16.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:39:16.410: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 30 04:39:16.413: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"55756"},"items":null}

Nov 30 04:39:16.415: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"55756"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Nov 30 04:39:16.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-69" for this suite. 11/30/22 04:39:16.434
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":272,"skipped":4934,"failed":0}
------------------------------
• [SLOW TEST] [12.972 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:39:03.467
    Nov 30 04:39:03.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename daemonsets 11/30/22 04:39:03.468
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:03.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:03.48
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Nov 30 04:39:03.510: INFO: Create a RollingUpdate DaemonSet
    Nov 30 04:39:03.516: INFO: Check that daemon pods launch on every node of the cluster
    Nov 30 04:39:03.524: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:03.524: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:03.524: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:03.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:39:03.531: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:39:04.536: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:04.537: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:04.537: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:04.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:39:04.540: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:39:05.536: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:05.536: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:05.536: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:05.552: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Nov 30 04:39:05.552: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    Nov 30 04:39:05.552: INFO: Update the DaemonSet to trigger a rollout
    Nov 30 04:39:05.590: INFO: Updating DaemonSet daemon-set
    Nov 30 04:39:08.604: INFO: Roll back the DaemonSet before rollout is complete
    Nov 30 04:39:08.612: INFO: Updating DaemonSet daemon-set
    Nov 30 04:39:08.612: INFO: Make sure DaemonSet rollback is complete
    Nov 30 04:39:08.620: INFO: Wrong image for pod: daemon-set-vjvll. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2, got: foo:non-existent.
    Nov 30 04:39:08.620: INFO: Pod daemon-set-vjvll is not available
    Nov 30 04:39:08.626: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:08.626: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:08.626: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:09.639: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:09.639: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:09.639: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:10.636: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:10.637: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:10.637: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:11.634: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:11.634: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:11.634: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:12.636: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:12.636: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:12.636: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:13.636: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:13.637: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:13.637: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:14.631: INFO: Pod daemon-set-8qf8x is not available
    Nov 30 04:39:14.636: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:14.636: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:14.636: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 11/30/22 04:39:14.643
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-69, will wait for the garbage collector to delete the pods 11/30/22 04:39:14.643
    Nov 30 04:39:14.706: INFO: Deleting DaemonSet.extensions daemon-set took: 9.152248ms
    Nov 30 04:39:14.807: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.571316ms
    Nov 30 04:39:16.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:39:16.410: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 30 04:39:16.413: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"55756"},"items":null}

    Nov 30 04:39:16.415: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"55756"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 04:39:16.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-69" for this suite. 11/30/22 04:39:16.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:39:16.439
Nov 30 04:39:16.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename daemonsets 11/30/22 04:39:16.44
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:16.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:16.454
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 11/30/22 04:39:16.475
STEP: Check that daemon pods launch on every node of the cluster. 11/30/22 04:39:16.48
Nov 30 04:39:16.488: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:16.488: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:16.488: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:16.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:39:16.491: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:39:17.496: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:17.496: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:17.496: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:17.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 30 04:39:17.500: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:39:18.496: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:18.496: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:18.496: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:39:18.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Nov 30 04:39:18.500: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: listing all DeamonSets 11/30/22 04:39:18.502
STEP: DeleteCollection of the DaemonSets 11/30/22 04:39:18.505
STEP: Verify that ReplicaSets have been deleted 11/30/22 04:39:18.513
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Nov 30 04:39:18.537: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"55852"},"items":null}

Nov 30 04:39:18.541: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"55852"},"items":[{"metadata":{"name":"daemon-set-29jvn","generateName":"daemon-set-","namespace":"daemonsets-6537","uid":"157bc235-a9c4-4a93-a396-d28420603474","resourceVersion":"55849","creationTimestamp":"2022-11-30T04:39:16Z","deletionTimestamp":"2022-11-30T04:39:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"855db7788d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.228.150\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.228.150\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"989c9556-de75-40bf-9fa4-7f413a062571","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"989c9556-de75-40bf-9fa4-7f413a062571\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.228.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wmjhf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wmjhf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-ok72912g-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-ok72912g-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:18Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:18Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"}],"hostIP":"10.0.10.26","podIP":"192.168.228.150","podIPs":[{"ip":"192.168.228.150"}],"startTime":"2022-11-30T04:39:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-11-30T04:39:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019","containerID":"containerd://4255a0bf206aa2177b7bbfe80d8972d2af8111c105444b68f7b2fa47c91eaa7c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ddv4b","generateName":"daemon-set-","namespace":"daemonsets-6537","uid":"72e79266-74c4-48e5-9f89-d2cc6967104e","resourceVersion":"55850","creationTimestamp":"2022-11-30T04:39:16Z","deletionTimestamp":"2022-11-30T04:39:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"855db7788d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.12.247\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.12.247\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"989c9556-de75-40bf-9fa4-7f413a062571","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"989c9556-de75-40bf-9fa4-7f413a062571\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-s2kmh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-s2kmh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"}],"hostIP":"10.0.10.8","podIP":"192.168.12.247","podIPs":[{"ip":"192.168.12.247"}],"startTime":"2022-11-30T04:39:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-11-30T04:39:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019","containerID":"containerd://bfec649a12b125a2085b74fbd1f72332fe97e3ddf973b0bb852c98ebc62d208d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-q6hr6","generateName":"daemon-set-","namespace":"daemonsets-6537","uid":"dc2e1050-af44-4946-8144-eaac7e7a1af8","resourceVersion":"55852","creationTimestamp":"2022-11-30T04:39:16Z","deletionTimestamp":"2022-11-30T04:39:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"855db7788d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.251.144\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.251.144\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"989c9556-de75-40bf-9fa4-7f413a062571","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"989c9556-de75-40bf-9fa4-7f413a062571\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.251.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-trmdr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-trmdr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"}],"hostIP":"10.0.10.17","podIP":"192.168.251.144","podIPs":[{"ip":"192.168.251.144"}],"startTime":"2022-11-30T04:39:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-11-30T04:39:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019","containerID":"containerd://2cb585a988e9e0c2d8c22cfcce274a4400c324ebbe04ef3385f57c0000f6ff0c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-x8g4p","generateName":"daemon-set-","namespace":"daemonsets-6537","uid":"706a5512-5471-4da5-85b9-01d92aee1d5c","resourceVersion":"55848","creationTimestamp":"2022-11-30T04:39:16Z","deletionTimestamp":"2022-11-30T04:39:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"855db7788d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.254.63\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.254.63\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"989c9556-de75-40bf-9fa4-7f413a062571","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"989c9556-de75-40bf-9fa4-7f413a062571\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.254.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-k4ldk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-k4ldk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:18Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:18Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"}],"hostIP":"10.0.10.2","podIP":"192.168.254.63","podIPs":[{"ip":"192.168.254.63"}],"startTime":"2022-11-30T04:39:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-11-30T04:39:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019","containerID":"containerd://5547df0e4f0d9ef06e41899eb76963069eb3ac3583eb0266b6547789704ce2fe","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Nov 30 04:39:18.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6537" for this suite. 11/30/22 04:39:18.566
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":273,"skipped":4944,"failed":0}
------------------------------
• [2.137 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:39:16.439
    Nov 30 04:39:16.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename daemonsets 11/30/22 04:39:16.44
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:16.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:16.454
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 11/30/22 04:39:16.475
    STEP: Check that daemon pods launch on every node of the cluster. 11/30/22 04:39:16.48
    Nov 30 04:39:16.488: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:16.488: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:16.488: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:16.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:39:16.491: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:39:17.496: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:17.496: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:17.496: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:17.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 30 04:39:17.500: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:39:18.496: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:18.496: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:18.496: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:39:18.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Nov 30 04:39:18.500: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: listing all DeamonSets 11/30/22 04:39:18.502
    STEP: DeleteCollection of the DaemonSets 11/30/22 04:39:18.505
    STEP: Verify that ReplicaSets have been deleted 11/30/22 04:39:18.513
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Nov 30 04:39:18.537: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"55852"},"items":null}

    Nov 30 04:39:18.541: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"55852"},"items":[{"metadata":{"name":"daemon-set-29jvn","generateName":"daemon-set-","namespace":"daemonsets-6537","uid":"157bc235-a9c4-4a93-a396-d28420603474","resourceVersion":"55849","creationTimestamp":"2022-11-30T04:39:16Z","deletionTimestamp":"2022-11-30T04:39:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"855db7788d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.228.150\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.228.150\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"989c9556-de75-40bf-9fa4-7f413a062571","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"989c9556-de75-40bf-9fa4-7f413a062571\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.228.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wmjhf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wmjhf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-ok72912g-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-ok72912g-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:18Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:18Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"}],"hostIP":"10.0.10.26","podIP":"192.168.228.150","podIPs":[{"ip":"192.168.228.150"}],"startTime":"2022-11-30T04:39:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-11-30T04:39:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019","containerID":"containerd://4255a0bf206aa2177b7bbfe80d8972d2af8111c105444b68f7b2fa47c91eaa7c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ddv4b","generateName":"daemon-set-","namespace":"daemonsets-6537","uid":"72e79266-74c4-48e5-9f89-d2cc6967104e","resourceVersion":"55850","creationTimestamp":"2022-11-30T04:39:16Z","deletionTimestamp":"2022-11-30T04:39:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"855db7788d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.12.247\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.12.247\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"989c9556-de75-40bf-9fa4-7f413a062571","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"989c9556-de75-40bf-9fa4-7f413a062571\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-s2kmh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-s2kmh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"}],"hostIP":"10.0.10.8","podIP":"192.168.12.247","podIPs":[{"ip":"192.168.12.247"}],"startTime":"2022-11-30T04:39:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-11-30T04:39:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019","containerID":"containerd://bfec649a12b125a2085b74fbd1f72332fe97e3ddf973b0bb852c98ebc62d208d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-q6hr6","generateName":"daemon-set-","namespace":"daemonsets-6537","uid":"dc2e1050-af44-4946-8144-eaac7e7a1af8","resourceVersion":"55852","creationTimestamp":"2022-11-30T04:39:16Z","deletionTimestamp":"2022-11-30T04:39:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"855db7788d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.251.144\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.251.144\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"989c9556-de75-40bf-9fa4-7f413a062571","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"989c9556-de75-40bf-9fa4-7f413a062571\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.251.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-trmdr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-trmdr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"}],"hostIP":"10.0.10.17","podIP":"192.168.251.144","podIPs":[{"ip":"192.168.251.144"}],"startTime":"2022-11-30T04:39:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-11-30T04:39:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019","containerID":"containerd://2cb585a988e9e0c2d8c22cfcce274a4400c324ebbe04ef3385f57c0000f6ff0c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-x8g4p","generateName":"daemon-set-","namespace":"daemonsets-6537","uid":"706a5512-5471-4da5-85b9-01d92aee1d5c","resourceVersion":"55848","creationTimestamp":"2022-11-30T04:39:16Z","deletionTimestamp":"2022-11-30T04:39:48Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"855db7788d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.254.63\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.254.63\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"989c9556-de75-40bf-9fa4-7f413a062571","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"989c9556-de75-40bf-9fa4-7f413a062571\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-11-30T04:39:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.254.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-k4ldk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-k4ldk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:18Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:18Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-11-30T04:39:16Z"}],"hostIP":"10.0.10.2","podIP":"192.168.254.63","podIPs":[{"ip":"192.168.254.63"}],"startTime":"2022-11-30T04:39:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-11-30T04:39:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019","containerID":"containerd://5547df0e4f0d9ef06e41899eb76963069eb3ac3583eb0266b6547789704ce2fe","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 04:39:18.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6537" for this suite. 11/30/22 04:39:18.566
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:39:18.577
Nov 30 04:39:18.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename deployment 11/30/22 04:39:18.577
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:18.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:18.598
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Nov 30 04:39:18.600: INFO: Creating deployment "webserver-deployment"
Nov 30 04:39:18.605: INFO: Waiting for observed generation 1
Nov 30 04:39:20.611: INFO: Waiting for all required pods to come up
Nov 30 04:39:20.615: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 11/30/22 04:39:20.615
Nov 30 04:39:20.615: INFO: Waiting up to 5m0s for pod "webserver-deployment-894b96578-hvxjn" in namespace "deployment-6883" to be "running"
Nov 30 04:39:20.615: INFO: Waiting up to 5m0s for pod "webserver-deployment-894b96578-tt59t" in namespace "deployment-6883" to be "running"
Nov 30 04:39:20.618: INFO: Pod "webserver-deployment-894b96578-hvxjn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.514598ms
Nov 30 04:39:20.618: INFO: Pod "webserver-deployment-894b96578-tt59t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.539656ms
Nov 30 04:39:22.622: INFO: Pod "webserver-deployment-894b96578-hvxjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006147975s
Nov 30 04:39:22.622: INFO: Pod "webserver-deployment-894b96578-hvxjn" satisfied condition "running"
Nov 30 04:39:22.622: INFO: Pod "webserver-deployment-894b96578-tt59t": Phase="Running", Reason="", readiness=true. Elapsed: 2.006043318s
Nov 30 04:39:22.622: INFO: Pod "webserver-deployment-894b96578-tt59t" satisfied condition "running"
Nov 30 04:39:22.622: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 30 04:39:22.627: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 30 04:39:22.636: INFO: Updating deployment webserver-deployment
Nov 30 04:39:22.636: INFO: Waiting for observed generation 2
Nov 30 04:39:24.641: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 30 04:39:24.644: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 30 04:39:24.647: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 30 04:39:24.654: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 30 04:39:24.654: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 30 04:39:24.657: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 30 04:39:24.662: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 30 04:39:24.662: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 30 04:39:24.670: INFO: Updating deployment webserver-deployment
Nov 30 04:39:24.670: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 30 04:39:24.676: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 30 04:39:24.682: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 30 04:39:24.727: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6883  586b0aa7-3de9-44e2-b371-89e211a1e217 56250 3 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002168e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2022-11-30 04:39:22 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-11-30 04:39:24 +0000 UTC,LastTransitionTime:2022-11-30 04:39:24 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov 30 04:39:24.736: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-6883  06327b87-1acd-435f-8992-125a086809c5 56248 3 2022-11-30 04:39:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 586b0aa7-3de9-44e2-b371-89e211a1e217 0xc002169297 0xc002169298}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"586b0aa7-3de9-44e2-b371-89e211a1e217\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002169338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 30 04:39:24.736: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 30 04:39:24.736: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-894b96578  deployment-6883  6022ee67-0b77-4722-a1c3-37bceb9e8755 56247 3 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 586b0aa7-3de9-44e2-b371-89e211a1e217 0xc002169397 0xc002169398}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"586b0aa7-3de9-44e2-b371-89e211a1e217\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 894b96578,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002169428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov 30 04:39:24.747: INFO: Pod "webserver-deployment-69b7448995-24qgm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-24qgm webserver-deployment-69b7448995- deployment-6883  97dac094-893a-46cc-a680-3f9fefaa259b 56187 0 2022-11-30 04:39:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.251.146"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.251.146"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005498817 0xc005498818}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jwlpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jwlpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.17,PodIP:,StartTime:2022-11-30 04:39:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.747: INFO: Pod "webserver-deployment-69b7448995-5mzhj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5mzhj webserver-deployment-69b7448995- deployment-6883  168222fe-d9f8-4418-9c07-f31808f58752 56190 0 2022-11-30 04:39:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.254.32"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.254.32"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005498a10 0xc005498a11}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcshq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcshq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.2,PodIP:,StartTime:2022-11-30 04:39:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.747: INFO: Pod "webserver-deployment-69b7448995-b25xs" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-b25xs webserver-deployment-69b7448995- deployment-6883  68129e43-d36c-4f05-9aef-a28e67c19b56 56183 0 2022-11-30 04:39:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.228.142"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.228.142"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005498c00 0xc005498c01}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qmq5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qmq5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-ok72912g-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.26,PodIP:,StartTime:2022-11-30 04:39:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.747: INFO: Pod "webserver-deployment-69b7448995-hd7jp" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hd7jp webserver-deployment-69b7448995- deployment-6883  83a4843d-b2ae-41cd-8ef0-eaa1f2420394 56203 0 2022-11-30 04:39:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.254"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.254"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005498df0 0xc005498df1}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hpv8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hpv8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:,StartTime:2022-11-30 04:39:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.747: INFO: Pod "webserver-deployment-69b7448995-k68jp" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-k68jp webserver-deployment-69b7448995- deployment-6883  5ba42d88-33b5-43ed-ae43-df6a767d4e8c 56259 0 2022-11-30 04:39:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005498ff0 0xc005498ff1}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wr22g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wr22g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-69b7448995-kv7j5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kv7j5 webserver-deployment-69b7448995- deployment-6883  30e291d3-d515-4431-b0ab-6a941ed66b3e 56238 0 2022-11-30 04:39:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.252"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.252"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005499150 0xc005499151}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49spc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49spc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:,StartTime:2022-11-30 04:39:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-69b7448995-qr82g" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qr82g webserver-deployment-69b7448995- deployment-6883  2c47d2f6-2f8b-4155-963c-2ff29eea04ec 56253 0 2022-11-30 04:39:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005499350 0xc005499351}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vzcj2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vzcj2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-69b7448995-s64t5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-s64t5 webserver-deployment-69b7448995- deployment-6883  693fee21-42bf-4120-adc7-340ac58c3315 56263 0 2022-11-30 04:39:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc0054994b0 0xc0054994b1}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vpp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vpp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-ok72912g-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-894b96578-7pvfx" is not available:
&Pod{ObjectMeta:{webserver-deployment-894b96578-7pvfx webserver-deployment-894b96578- deployment-6883  696111b0-04b9-40c8-a150-04994958a07f 56258 0 2022-11-30 04:39:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc005499610 0xc005499611}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhppf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhppf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-894b96578-9spsd" is not available:
&Pod{ObjectMeta:{webserver-deployment-894b96578-9spsd webserver-deployment-894b96578- deployment-6883  a1a5a253-1b90-4ca5-92f6-886a9df25368 56266 0 2022-11-30 04:39:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc005499760 0xc005499761}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sn8pg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sn8pg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-894b96578-cq7lr" is available:
&Pod{ObjectMeta:{webserver-deployment-894b96578-cq7lr webserver-deployment-894b96578- deployment-6883  22bc59e3-fa6c-4b7a-b24e-32fe6cae5264 56013 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.228.155"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.228.155"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc0054998b0 0xc0054998b1}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.228.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gx2gc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gx2gc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-ok72912g-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.26,PodIP:192.168.228.155,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://8f8c757cf2262f9c1bde7c1b96777b805de065a01e56fac9d6fab096a9c4f309,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.228.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-894b96578-fzhq4" is available:
&Pod{ObjectMeta:{webserver-deployment-894b96578-fzhq4 webserver-deployment-894b96578- deployment-6883  6a080c43-b129-41d6-8c39-c20f3d35d47d 56027 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.254.5"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.254.5"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc005499aa0 0xc005499aa1}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.254.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcggf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcggf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.2,PodIP:192.168.254.5,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://973d626f433d8cf9046e1975249a83bc074934e0488cad4c269ec52f755e6500,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.254.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-894b96578-gmgtd" is not available:
&Pod{ObjectMeta:{webserver-deployment-894b96578-gmgtd webserver-deployment-894b96578- deployment-6883  b3439d09-265d-4a1e-8267-2047823c6aa7 56267 0 2022-11-30 04:39:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc005499c90 0xc005499c91}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tmr99,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tmr99,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.749: INFO: Pod "webserver-deployment-894b96578-hpqg9" is available:
&Pod{ObjectMeta:{webserver-deployment-894b96578-hpqg9 webserver-deployment-894b96578- deployment-6883  c98743aa-6c34-4bb2-a727-dcd6d4fecea8 56031 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.251.147"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.251.147"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc005499dc7 0xc005499dc8}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.251.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvppv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvppv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.17,PodIP:192.168.251.147,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://fef58bacc08b1c4912586db0ea8eab8d75317ec3f377331c7321c4ede5ddea0f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.251.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.749: INFO: Pod "webserver-deployment-894b96578-qk84l" is available:
&Pod{ObjectMeta:{webserver-deployment-894b96578-qk84l webserver-deployment-894b96578- deployment-6883  3a9b156e-1fc1-4600-aec3-9e15ed472e43 56040 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.251.157"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.251.157"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc005499fd0 0xc005499fd1}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.251.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jmxtq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jmxtq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.17,PodIP:192.168.251.157,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://de89ddf29505111580b492e76f2d72bc7b8a57ebb05b22c593205735dfcbced7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.251.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.749: INFO: Pod "webserver-deployment-894b96578-qqrz2" is available:
&Pod{ObjectMeta:{webserver-deployment-894b96578-qqrz2 webserver-deployment-894b96578- deployment-6883  7ebc80d8-a58e-44b2-a8de-07568a4e1654 56019 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.228.145"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.228.145"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc003e32250 0xc003e32251}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.228.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzp6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzp6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-ok72912g-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.26,PodIP:192.168.228.145,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://b4a19e0f140165f8176c8757a3f893ae20a7e7113911d5bf041c84bc4ef3c0f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.228.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.749: INFO: Pod "webserver-deployment-894b96578-tt59t" is available:
&Pod{ObjectMeta:{webserver-deployment-894b96578-tt59t webserver-deployment-894b96578- deployment-6883  f2f5fa77-cf3f-4ea3-a93d-c7ce9b5cb698 56057 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.253"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.253"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc003e32440 0xc003e32441}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kstr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kstr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.253,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://838cf3c836ad4fa0d6cb4a9a6e34e9e6b4973ed1aa972a2fb7703fbabd97943f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.749: INFO: Pod "webserver-deployment-894b96578-w2qdl" is available:
&Pod{ObjectMeta:{webserver-deployment-894b96578-w2qdl webserver-deployment-894b96578- deployment-6883  5402261f-89e9-48b9-9e8c-10cec9a6ba92 56015 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.250"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.250"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc003e32630 0xc003e32631}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jddpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jddpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.250,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://ee17b58aa9922330298ce194669dcab5225fed164c960809c4e94df942a15224,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:39:24.749: INFO: Pod "webserver-deployment-894b96578-ztkdx" is available:
&Pod{ObjectMeta:{webserver-deployment-894b96578-ztkdx webserver-deployment-894b96578- deployment-6883  0ad44871-36b3-46aa-9b9c-a565825aa96b 56024 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.254.10"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.254.10"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc003e32820 0xc003e32821}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.254.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mm8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mm8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.2,PodIP:192.168.254.10,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://4a970cfc2fdc330f0400df36131e799da9cc018c3178f9be3fed3d0806389038,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.254.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Nov 30 04:39:24.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6883" for this suite. 11/30/22 04:39:24.756
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":274,"skipped":4946,"failed":0}
------------------------------
• [SLOW TEST] [6.223 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:39:18.577
    Nov 30 04:39:18.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename deployment 11/30/22 04:39:18.577
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:18.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:18.598
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Nov 30 04:39:18.600: INFO: Creating deployment "webserver-deployment"
    Nov 30 04:39:18.605: INFO: Waiting for observed generation 1
    Nov 30 04:39:20.611: INFO: Waiting for all required pods to come up
    Nov 30 04:39:20.615: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 11/30/22 04:39:20.615
    Nov 30 04:39:20.615: INFO: Waiting up to 5m0s for pod "webserver-deployment-894b96578-hvxjn" in namespace "deployment-6883" to be "running"
    Nov 30 04:39:20.615: INFO: Waiting up to 5m0s for pod "webserver-deployment-894b96578-tt59t" in namespace "deployment-6883" to be "running"
    Nov 30 04:39:20.618: INFO: Pod "webserver-deployment-894b96578-hvxjn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.514598ms
    Nov 30 04:39:20.618: INFO: Pod "webserver-deployment-894b96578-tt59t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.539656ms
    Nov 30 04:39:22.622: INFO: Pod "webserver-deployment-894b96578-hvxjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006147975s
    Nov 30 04:39:22.622: INFO: Pod "webserver-deployment-894b96578-hvxjn" satisfied condition "running"
    Nov 30 04:39:22.622: INFO: Pod "webserver-deployment-894b96578-tt59t": Phase="Running", Reason="", readiness=true. Elapsed: 2.006043318s
    Nov 30 04:39:22.622: INFO: Pod "webserver-deployment-894b96578-tt59t" satisfied condition "running"
    Nov 30 04:39:22.622: INFO: Waiting for deployment "webserver-deployment" to complete
    Nov 30 04:39:22.627: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Nov 30 04:39:22.636: INFO: Updating deployment webserver-deployment
    Nov 30 04:39:22.636: INFO: Waiting for observed generation 2
    Nov 30 04:39:24.641: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Nov 30 04:39:24.644: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Nov 30 04:39:24.647: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Nov 30 04:39:24.654: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Nov 30 04:39:24.654: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Nov 30 04:39:24.657: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Nov 30 04:39:24.662: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Nov 30 04:39:24.662: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Nov 30 04:39:24.670: INFO: Updating deployment webserver-deployment
    Nov 30 04:39:24.670: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Nov 30 04:39:24.676: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Nov 30 04:39:24.682: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 30 04:39:24.727: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-6883  586b0aa7-3de9-44e2-b371-89e211a1e217 56250 3 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002168e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2022-11-30 04:39:22 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-11-30 04:39:24 +0000 UTC,LastTransitionTime:2022-11-30 04:39:24 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Nov 30 04:39:24.736: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-6883  06327b87-1acd-435f-8992-125a086809c5 56248 3 2022-11-30 04:39:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 586b0aa7-3de9-44e2-b371-89e211a1e217 0xc002169297 0xc002169298}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"586b0aa7-3de9-44e2-b371-89e211a1e217\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002169338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 30 04:39:24.736: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Nov 30 04:39:24.736: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-894b96578  deployment-6883  6022ee67-0b77-4722-a1c3-37bceb9e8755 56247 3 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 586b0aa7-3de9-44e2-b371-89e211a1e217 0xc002169397 0xc002169398}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"586b0aa7-3de9-44e2-b371-89e211a1e217\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 894b96578,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002169428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Nov 30 04:39:24.747: INFO: Pod "webserver-deployment-69b7448995-24qgm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-24qgm webserver-deployment-69b7448995- deployment-6883  97dac094-893a-46cc-a680-3f9fefaa259b 56187 0 2022-11-30 04:39:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.251.146"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.251.146"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005498817 0xc005498818}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jwlpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jwlpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.17,PodIP:,StartTime:2022-11-30 04:39:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.747: INFO: Pod "webserver-deployment-69b7448995-5mzhj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5mzhj webserver-deployment-69b7448995- deployment-6883  168222fe-d9f8-4418-9c07-f31808f58752 56190 0 2022-11-30 04:39:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.254.32"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.254.32"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005498a10 0xc005498a11}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcshq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcshq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.2,PodIP:,StartTime:2022-11-30 04:39:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.747: INFO: Pod "webserver-deployment-69b7448995-b25xs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-b25xs webserver-deployment-69b7448995- deployment-6883  68129e43-d36c-4f05-9aef-a28e67c19b56 56183 0 2022-11-30 04:39:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.228.142"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.228.142"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005498c00 0xc005498c01}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qmq5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qmq5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-ok72912g-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.26,PodIP:,StartTime:2022-11-30 04:39:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.747: INFO: Pod "webserver-deployment-69b7448995-hd7jp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hd7jp webserver-deployment-69b7448995- deployment-6883  83a4843d-b2ae-41cd-8ef0-eaa1f2420394 56203 0 2022-11-30 04:39:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.254"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.254"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005498df0 0xc005498df1}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hpv8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hpv8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:,StartTime:2022-11-30 04:39:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.747: INFO: Pod "webserver-deployment-69b7448995-k68jp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-k68jp webserver-deployment-69b7448995- deployment-6883  5ba42d88-33b5-43ed-ae43-df6a767d4e8c 56259 0 2022-11-30 04:39:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005498ff0 0xc005498ff1}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wr22g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wr22g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-69b7448995-kv7j5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kv7j5 webserver-deployment-69b7448995- deployment-6883  30e291d3-d515-4431-b0ab-6a941ed66b3e 56238 0 2022-11-30 04:39:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.252"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.252"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005499150 0xc005499151}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:39:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49spc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49spc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:,StartTime:2022-11-30 04:39:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-69b7448995-qr82g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qr82g webserver-deployment-69b7448995- deployment-6883  2c47d2f6-2f8b-4155-963c-2ff29eea04ec 56253 0 2022-11-30 04:39:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc005499350 0xc005499351}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vzcj2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vzcj2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-69b7448995-s64t5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-s64t5 webserver-deployment-69b7448995- deployment-6883  693fee21-42bf-4120-adc7-340ac58c3315 56263 0 2022-11-30 04:39:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 06327b87-1acd-435f-8992-125a086809c5 0xc0054994b0 0xc0054994b1}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06327b87-1acd-435f-8992-125a086809c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vpp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vpp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-ok72912g-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-894b96578-7pvfx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-894b96578-7pvfx webserver-deployment-894b96578- deployment-6883  696111b0-04b9-40c8-a150-04994958a07f 56258 0 2022-11-30 04:39:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc005499610 0xc005499611}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhppf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhppf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-894b96578-9spsd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-894b96578-9spsd webserver-deployment-894b96578- deployment-6883  a1a5a253-1b90-4ca5-92f6-886a9df25368 56266 0 2022-11-30 04:39:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc005499760 0xc005499761}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sn8pg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sn8pg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-894b96578-cq7lr" is available:
    &Pod{ObjectMeta:{webserver-deployment-894b96578-cq7lr webserver-deployment-894b96578- deployment-6883  22bc59e3-fa6c-4b7a-b24e-32fe6cae5264 56013 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.228.155"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.228.155"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc0054998b0 0xc0054998b1}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.228.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gx2gc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gx2gc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-ok72912g-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.26,PodIP:192.168.228.155,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://8f8c757cf2262f9c1bde7c1b96777b805de065a01e56fac9d6fab096a9c4f309,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.228.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-894b96578-fzhq4" is available:
    &Pod{ObjectMeta:{webserver-deployment-894b96578-fzhq4 webserver-deployment-894b96578- deployment-6883  6a080c43-b129-41d6-8c39-c20f3d35d47d 56027 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.254.5"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.254.5"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc005499aa0 0xc005499aa1}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.254.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcggf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcggf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.2,PodIP:192.168.254.5,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://973d626f433d8cf9046e1975249a83bc074934e0488cad4c269ec52f755e6500,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.254.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.748: INFO: Pod "webserver-deployment-894b96578-gmgtd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-894b96578-gmgtd webserver-deployment-894b96578- deployment-6883  b3439d09-265d-4a1e-8267-2047823c6aa7 56267 0 2022-11-30 04:39:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc005499c90 0xc005499c91}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tmr99,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tmr99,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.749: INFO: Pod "webserver-deployment-894b96578-hpqg9" is available:
    &Pod{ObjectMeta:{webserver-deployment-894b96578-hpqg9 webserver-deployment-894b96578- deployment-6883  c98743aa-6c34-4bb2-a727-dcd6d4fecea8 56031 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.251.147"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.251.147"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc005499dc7 0xc005499dc8}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.251.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvppv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvppv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.17,PodIP:192.168.251.147,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://fef58bacc08b1c4912586db0ea8eab8d75317ec3f377331c7321c4ede5ddea0f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.251.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.749: INFO: Pod "webserver-deployment-894b96578-qk84l" is available:
    &Pod{ObjectMeta:{webserver-deployment-894b96578-qk84l webserver-deployment-894b96578- deployment-6883  3a9b156e-1fc1-4600-aec3-9e15ed472e43 56040 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.251.157"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.251.157"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc005499fd0 0xc005499fd1}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.251.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jmxtq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jmxtq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.17,PodIP:192.168.251.157,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://de89ddf29505111580b492e76f2d72bc7b8a57ebb05b22c593205735dfcbced7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.251.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.749: INFO: Pod "webserver-deployment-894b96578-qqrz2" is available:
    &Pod{ObjectMeta:{webserver-deployment-894b96578-qqrz2 webserver-deployment-894b96578- deployment-6883  7ebc80d8-a58e-44b2-a8de-07568a4e1654 56019 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.228.145"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.228.145"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc003e32250 0xc003e32251}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.228.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzp6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzp6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-ok72912g-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.26,PodIP:192.168.228.145,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://b4a19e0f140165f8176c8757a3f893ae20a7e7113911d5bf041c84bc4ef3c0f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.228.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.749: INFO: Pod "webserver-deployment-894b96578-tt59t" is available:
    &Pod{ObjectMeta:{webserver-deployment-894b96578-tt59t webserver-deployment-894b96578- deployment-6883  f2f5fa77-cf3f-4ea3-a93d-c7ce9b5cb698 56057 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.253"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.253"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc003e32440 0xc003e32441}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kstr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kstr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.253,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://838cf3c836ad4fa0d6cb4a9a6e34e9e6b4973ed1aa972a2fb7703fbabd97943f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.749: INFO: Pod "webserver-deployment-894b96578-w2qdl" is available:
    &Pod{ObjectMeta:{webserver-deployment-894b96578-w2qdl webserver-deployment-894b96578- deployment-6883  5402261f-89e9-48b9-9e8c-10cec9a6ba92 56015 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.250"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.250"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc003e32630 0xc003e32631}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jddpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jddpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.250,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://ee17b58aa9922330298ce194669dcab5225fed164c960809c4e94df942a15224,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:39:24.749: INFO: Pod "webserver-deployment-894b96578-ztkdx" is available:
    &Pod{ObjectMeta:{webserver-deployment-894b96578-ztkdx webserver-deployment-894b96578- deployment-6883  0ad44871-36b3-46aa-9b9c-a565825aa96b 56024 0 2022-11-30 04:39:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.254.10"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.254.10"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-894b96578 6022ee67-0b77-4722-a1c3-37bceb9e8755 0xc003e32820 0xc003e32821}] [] [{kube-controller-manager Update v1 2022-11-30 04:39:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6022ee67-0b77-4722-a1c3-37bceb9e8755\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:39:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.254.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mm8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mm8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:39:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.2,PodIP:192.168.254.10,StartTime:2022-11-30 04:39:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:39:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://4a970cfc2fdc330f0400df36131e799da9cc018c3178f9be3fed3d0806389038,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.254.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Nov 30 04:39:24.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6883" for this suite. 11/30/22 04:39:24.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:39:24.804
Nov 30 04:39:24.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pods 11/30/22 04:39:24.806
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:24.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:24.863
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 11/30/22 04:39:24.868
Nov 30 04:39:24.883: INFO: Waiting up to 5m0s for pod "pod-vh9vh" in namespace "pods-490" to be "running"
Nov 30 04:39:24.887: INFO: Pod "pod-vh9vh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.157731ms
Nov 30 04:39:26.891: INFO: Pod "pod-vh9vh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007712057s
Nov 30 04:39:28.891: INFO: Pod "pod-vh9vh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008049397s
Nov 30 04:39:30.891: INFO: Pod "pod-vh9vh": Phase="Running", Reason="", readiness=true. Elapsed: 6.008376386s
Nov 30 04:39:30.891: INFO: Pod "pod-vh9vh" satisfied condition "running"
STEP: patching /status 11/30/22 04:39:30.891
Nov 30 04:39:30.901: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Nov 30 04:39:30.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-490" for this suite. 11/30/22 04:39:30.907
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":275,"skipped":4986,"failed":0}
------------------------------
• [SLOW TEST] [6.114 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:39:24.804
    Nov 30 04:39:24.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pods 11/30/22 04:39:24.806
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:24.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:24.863
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 11/30/22 04:39:24.868
    Nov 30 04:39:24.883: INFO: Waiting up to 5m0s for pod "pod-vh9vh" in namespace "pods-490" to be "running"
    Nov 30 04:39:24.887: INFO: Pod "pod-vh9vh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.157731ms
    Nov 30 04:39:26.891: INFO: Pod "pod-vh9vh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007712057s
    Nov 30 04:39:28.891: INFO: Pod "pod-vh9vh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008049397s
    Nov 30 04:39:30.891: INFO: Pod "pod-vh9vh": Phase="Running", Reason="", readiness=true. Elapsed: 6.008376386s
    Nov 30 04:39:30.891: INFO: Pod "pod-vh9vh" satisfied condition "running"
    STEP: patching /status 11/30/22 04:39:30.891
    Nov 30 04:39:30.901: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Nov 30 04:39:30.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-490" for this suite. 11/30/22 04:39:30.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:39:30.919
Nov 30 04:39:30.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename deployment 11/30/22 04:39:30.919
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:30.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:30.947
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 11/30/22 04:39:30.978
STEP: waiting for Deployment to be created 11/30/22 04:39:30.989
STEP: waiting for all Replicas to be Ready 11/30/22 04:39:30.99
Nov 30 04:39:30.992: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 30 04:39:30.992: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 30 04:39:31.006: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 30 04:39:31.006: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 30 04:39:31.018: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 30 04:39:31.018: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 30 04:39:31.086: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 30 04:39:31.086: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 30 04:39:32.098: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 30 04:39:32.098: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 30 04:39:35.474: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 11/30/22 04:39:35.474
W1130 04:39:35.481340      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Nov 30 04:39:35.483: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 11/30/22 04:39:35.483
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:35.497: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:35.497: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:35.520: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:35.520: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:35.553: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
Nov 30 04:39:35.553: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
Nov 30 04:39:35.611: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
Nov 30 04:39:35.611: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
Nov 30 04:39:38.489: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:38.490: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:38.527: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
STEP: listing Deployments 11/30/22 04:39:38.527
Nov 30 04:39:38.541: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 11/30/22 04:39:38.541
Nov 30 04:39:38.555: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 11/30/22 04:39:38.555
Nov 30 04:39:38.561: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 30 04:39:38.565: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 30 04:39:38.622: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 30 04:39:38.632: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 30 04:39:39.412: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 30 04:39:39.899: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Nov 30 04:39:39.937: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 30 04:39:39.960: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 30 04:39:41.133: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 11/30/22 04:39:41.17
STEP: fetching the DeploymentStatus 11/30/22 04:39:41.177
Nov 30 04:39:41.181: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
Nov 30 04:39:41.181: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
Nov 30 04:39:41.181: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
Nov 30 04:39:41.182: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
Nov 30 04:39:41.182: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:41.182: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 3
Nov 30 04:39:41.182: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:41.182: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
Nov 30 04:39:41.182: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 3
STEP: deleting the Deployment 11/30/22 04:39:41.182
Nov 30 04:39:41.192: INFO: observed event type MODIFIED
Nov 30 04:39:41.192: INFO: observed event type MODIFIED
Nov 30 04:39:41.192: INFO: observed event type MODIFIED
Nov 30 04:39:41.192: INFO: observed event type MODIFIED
Nov 30 04:39:41.192: INFO: observed event type MODIFIED
Nov 30 04:39:41.192: INFO: observed event type MODIFIED
Nov 30 04:39:41.192: INFO: observed event type MODIFIED
Nov 30 04:39:41.192: INFO: observed event type MODIFIED
Nov 30 04:39:41.192: INFO: observed event type MODIFIED
Nov 30 04:39:41.192: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 30 04:39:41.196: INFO: Log out all the ReplicaSets if there is no deployment created
Nov 30 04:39:41.205: INFO: ReplicaSet "test-deployment-7ddf9dd855":
&ReplicaSet{ObjectMeta:{test-deployment-7ddf9dd855  deployment-7435  79ac1cca-8edf-4075-9594-2ebb729be057 56734 3 2022-11-30 04:39:30 +0000 UTC <nil> <nil> map[pod-template-hash:7ddf9dd855 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 2076277e-d6df-4c2d-afdd-f78ecda41ae8 0xc003c30fa7 0xc003c30fa8}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2076277e-d6df-4c2d-afdd-f78ecda41ae8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:39:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7ddf9dd855,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7ddf9dd855 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c31030 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Nov 30 04:39:41.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7435" for this suite. 11/30/22 04:39:41.222
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":276,"skipped":4997,"failed":0}
------------------------------
• [SLOW TEST] [10.317 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:39:30.919
    Nov 30 04:39:30.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename deployment 11/30/22 04:39:30.919
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:30.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:30.947
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 11/30/22 04:39:30.978
    STEP: waiting for Deployment to be created 11/30/22 04:39:30.989
    STEP: waiting for all Replicas to be Ready 11/30/22 04:39:30.99
    Nov 30 04:39:30.992: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 30 04:39:30.992: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 30 04:39:31.006: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 30 04:39:31.006: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 30 04:39:31.018: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 30 04:39:31.018: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 30 04:39:31.086: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 30 04:39:31.086: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 30 04:39:32.098: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Nov 30 04:39:32.098: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Nov 30 04:39:35.474: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 11/30/22 04:39:35.474
    W1130 04:39:35.481340      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Nov 30 04:39:35.483: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 11/30/22 04:39:35.483
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 0
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:35.485: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:35.497: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:35.497: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:35.520: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:35.520: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:35.553: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
    Nov 30 04:39:35.553: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
    Nov 30 04:39:35.611: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
    Nov 30 04:39:35.611: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
    Nov 30 04:39:38.489: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:38.490: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:38.527: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
    STEP: listing Deployments 11/30/22 04:39:38.527
    Nov 30 04:39:38.541: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 11/30/22 04:39:38.541
    Nov 30 04:39:38.555: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 11/30/22 04:39:38.555
    Nov 30 04:39:38.561: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 30 04:39:38.565: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 30 04:39:38.622: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 30 04:39:38.632: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 30 04:39:39.412: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 30 04:39:39.899: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 30 04:39:39.937: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 30 04:39:39.960: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 30 04:39:41.133: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 11/30/22 04:39:41.17
    STEP: fetching the DeploymentStatus 11/30/22 04:39:41.177
    Nov 30 04:39:41.181: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
    Nov 30 04:39:41.181: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
    Nov 30 04:39:41.181: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
    Nov 30 04:39:41.182: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 1
    Nov 30 04:39:41.182: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:41.182: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 3
    Nov 30 04:39:41.182: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:41.182: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 2
    Nov 30 04:39:41.182: INFO: observed Deployment test-deployment in namespace deployment-7435 with ReadyReplicas 3
    STEP: deleting the Deployment 11/30/22 04:39:41.182
    Nov 30 04:39:41.192: INFO: observed event type MODIFIED
    Nov 30 04:39:41.192: INFO: observed event type MODIFIED
    Nov 30 04:39:41.192: INFO: observed event type MODIFIED
    Nov 30 04:39:41.192: INFO: observed event type MODIFIED
    Nov 30 04:39:41.192: INFO: observed event type MODIFIED
    Nov 30 04:39:41.192: INFO: observed event type MODIFIED
    Nov 30 04:39:41.192: INFO: observed event type MODIFIED
    Nov 30 04:39:41.192: INFO: observed event type MODIFIED
    Nov 30 04:39:41.192: INFO: observed event type MODIFIED
    Nov 30 04:39:41.192: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 30 04:39:41.196: INFO: Log out all the ReplicaSets if there is no deployment created
    Nov 30 04:39:41.205: INFO: ReplicaSet "test-deployment-7ddf9dd855":
    &ReplicaSet{ObjectMeta:{test-deployment-7ddf9dd855  deployment-7435  79ac1cca-8edf-4075-9594-2ebb729be057 56734 3 2022-11-30 04:39:30 +0000 UTC <nil> <nil> map[pod-template-hash:7ddf9dd855 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 2076277e-d6df-4c2d-afdd-f78ecda41ae8 0xc003c30fa7 0xc003c30fa8}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2076277e-d6df-4c2d-afdd-f78ecda41ae8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:39:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7ddf9dd855,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7ddf9dd855 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c31030 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Nov 30 04:39:41.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7435" for this suite. 11/30/22 04:39:41.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:39:41.236
Nov 30 04:39:41.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename replicaset 11/30/22 04:39:41.237
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:41.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:41.255
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 11/30/22 04:39:41.257
Nov 30 04:39:41.280: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1168" to be "running and ready"
Nov 30 04:39:41.287: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.336444ms
Nov 30 04:39:41.287: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:39:43.291: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.011098405s
Nov 30 04:39:43.291: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Nov 30 04:39:43.291: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 11/30/22 04:39:43.294
STEP: Then the orphan pod is adopted 11/30/22 04:39:43.298
STEP: When the matched label of one of its pods change 11/30/22 04:39:44.308
Nov 30 04:39:44.311: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 11/30/22 04:39:44.323
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Nov 30 04:39:45.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1168" for this suite. 11/30/22 04:39:45.334
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":277,"skipped":5014,"failed":0}
------------------------------
• [4.104 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:39:41.236
    Nov 30 04:39:41.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename replicaset 11/30/22 04:39:41.237
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:41.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:41.255
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 11/30/22 04:39:41.257
    Nov 30 04:39:41.280: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1168" to be "running and ready"
    Nov 30 04:39:41.287: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.336444ms
    Nov 30 04:39:41.287: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:39:43.291: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.011098405s
    Nov 30 04:39:43.291: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Nov 30 04:39:43.291: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 11/30/22 04:39:43.294
    STEP: Then the orphan pod is adopted 11/30/22 04:39:43.298
    STEP: When the matched label of one of its pods change 11/30/22 04:39:44.308
    Nov 30 04:39:44.311: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 11/30/22 04:39:44.323
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Nov 30 04:39:45.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1168" for this suite. 11/30/22 04:39:45.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:39:45.341
Nov 30 04:39:45.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename watch 11/30/22 04:39:45.342
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:45.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:45.357
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 11/30/22 04:39:45.36
STEP: starting a background goroutine to produce watch events 11/30/22 04:39:45.364
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 11/30/22 04:39:45.364
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Nov 30 04:39:48.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6506" for this suite. 11/30/22 04:39:48.148
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":278,"skipped":5037,"failed":0}
------------------------------
• [2.857 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:39:45.341
    Nov 30 04:39:45.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename watch 11/30/22 04:39:45.342
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:45.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:45.357
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 11/30/22 04:39:45.36
    STEP: starting a background goroutine to produce watch events 11/30/22 04:39:45.364
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 11/30/22 04:39:45.364
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Nov 30 04:39:48.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6506" for this suite. 11/30/22 04:39:48.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:39:48.199
Nov 30 04:39:48.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename gc 11/30/22 04:39:48.2
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:48.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:48.223
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Nov 30 04:39:48.277: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8a787705-227f-4daa-95c5-5adfc186213b", Controller:(*bool)(0xc0017c2f36), BlockOwnerDeletion:(*bool)(0xc0017c2f37)}}
Nov 30 04:39:48.287: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"79c8674b-b45b-42d3-aa5d-7d17d6a0723a", Controller:(*bool)(0xc003cb218e), BlockOwnerDeletion:(*bool)(0xc003cb218f)}}
Nov 30 04:39:48.296: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c1c48bf0-f7fa-4d62-99f2-10bf7d201da1", Controller:(*bool)(0xc003d1a5f6), BlockOwnerDeletion:(*bool)(0xc003d1a5f7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Nov 30 04:39:53.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6185" for this suite. 11/30/22 04:39:53.312
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":279,"skipped":5048,"failed":0}
------------------------------
• [SLOW TEST] [5.119 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:39:48.199
    Nov 30 04:39:48.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename gc 11/30/22 04:39:48.2
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:48.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:48.223
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Nov 30 04:39:48.277: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8a787705-227f-4daa-95c5-5adfc186213b", Controller:(*bool)(0xc0017c2f36), BlockOwnerDeletion:(*bool)(0xc0017c2f37)}}
    Nov 30 04:39:48.287: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"79c8674b-b45b-42d3-aa5d-7d17d6a0723a", Controller:(*bool)(0xc003cb218e), BlockOwnerDeletion:(*bool)(0xc003cb218f)}}
    Nov 30 04:39:48.296: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c1c48bf0-f7fa-4d62-99f2-10bf7d201da1", Controller:(*bool)(0xc003d1a5f6), BlockOwnerDeletion:(*bool)(0xc003d1a5f7)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Nov 30 04:39:53.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6185" for this suite. 11/30/22 04:39:53.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:39:53.319
Nov 30 04:39:53.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 04:39:53.319
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:53.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:53.339
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Nov 30 04:39:53.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/30/22 04:39:55.858
Nov 30 04:39:55.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-8031 --namespace=crd-publish-openapi-8031 create -f -'
Nov 30 04:39:56.392: INFO: stderr: ""
Nov 30 04:39:56.392: INFO: stdout: "e2e-test-crd-publish-openapi-3413-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 30 04:39:56.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-8031 --namespace=crd-publish-openapi-8031 delete e2e-test-crd-publish-openapi-3413-crds test-cr'
Nov 30 04:39:56.501: INFO: stderr: ""
Nov 30 04:39:56.501: INFO: stdout: "e2e-test-crd-publish-openapi-3413-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 30 04:39:56.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-8031 --namespace=crd-publish-openapi-8031 apply -f -'
Nov 30 04:39:56.989: INFO: stderr: ""
Nov 30 04:39:56.989: INFO: stdout: "e2e-test-crd-publish-openapi-3413-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 30 04:39:56.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-8031 --namespace=crd-publish-openapi-8031 delete e2e-test-crd-publish-openapi-3413-crds test-cr'
Nov 30 04:39:57.060: INFO: stderr: ""
Nov 30 04:39:57.060: INFO: stdout: "e2e-test-crd-publish-openapi-3413-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 11/30/22 04:39:57.06
Nov 30 04:39:57.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-8031 explain e2e-test-crd-publish-openapi-3413-crds'
Nov 30 04:39:57.542: INFO: stderr: ""
Nov 30 04:39:57.542: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3413-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:39:59.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8031" for this suite. 11/30/22 04:39:59.951
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":280,"skipped":5057,"failed":0}
------------------------------
• [SLOW TEST] [6.639 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:39:53.319
    Nov 30 04:39:53.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 04:39:53.319
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:53.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:53.339
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Nov 30 04:39:53.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/30/22 04:39:55.858
    Nov 30 04:39:55.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-8031 --namespace=crd-publish-openapi-8031 create -f -'
    Nov 30 04:39:56.392: INFO: stderr: ""
    Nov 30 04:39:56.392: INFO: stdout: "e2e-test-crd-publish-openapi-3413-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Nov 30 04:39:56.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-8031 --namespace=crd-publish-openapi-8031 delete e2e-test-crd-publish-openapi-3413-crds test-cr'
    Nov 30 04:39:56.501: INFO: stderr: ""
    Nov 30 04:39:56.501: INFO: stdout: "e2e-test-crd-publish-openapi-3413-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Nov 30 04:39:56.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-8031 --namespace=crd-publish-openapi-8031 apply -f -'
    Nov 30 04:39:56.989: INFO: stderr: ""
    Nov 30 04:39:56.989: INFO: stdout: "e2e-test-crd-publish-openapi-3413-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Nov 30 04:39:56.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-8031 --namespace=crd-publish-openapi-8031 delete e2e-test-crd-publish-openapi-3413-crds test-cr'
    Nov 30 04:39:57.060: INFO: stderr: ""
    Nov 30 04:39:57.060: INFO: stdout: "e2e-test-crd-publish-openapi-3413-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 11/30/22 04:39:57.06
    Nov 30 04:39:57.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-8031 explain e2e-test-crd-publish-openapi-3413-crds'
    Nov 30 04:39:57.542: INFO: stderr: ""
    Nov 30 04:39:57.542: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3413-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:39:59.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8031" for this suite. 11/30/22 04:39:59.951
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:39:59.958
Nov 30 04:39:59.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 04:39:59.959
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:59.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:59.979
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 11/30/22 04:39:59.981
Nov 30 04:40:00.010: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e" in namespace "downward-api-4478" to be "Succeeded or Failed"
Nov 30 04:40:00.015: INFO: Pod "downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.388684ms
Nov 30 04:40:02.018: INFO: Pod "downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008519093s
Nov 30 04:40:04.020: INFO: Pod "downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009910151s
STEP: Saw pod success 11/30/22 04:40:04.02
Nov 30 04:40:04.020: INFO: Pod "downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e" satisfied condition "Succeeded or Failed"
Nov 30 04:40:04.023: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e container client-container: <nil>
STEP: delete the pod 11/30/22 04:40:04.034
Nov 30 04:40:04.044: INFO: Waiting for pod downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e to disappear
Nov 30 04:40:04.047: INFO: Pod downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Nov 30 04:40:04.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4478" for this suite. 11/30/22 04:40:04.058
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":281,"skipped":5057,"failed":0}
------------------------------
• [4.105 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:39:59.958
    Nov 30 04:39:59.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 04:39:59.959
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:39:59.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:39:59.979
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 11/30/22 04:39:59.981
    Nov 30 04:40:00.010: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e" in namespace "downward-api-4478" to be "Succeeded or Failed"
    Nov 30 04:40:00.015: INFO: Pod "downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.388684ms
    Nov 30 04:40:02.018: INFO: Pod "downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008519093s
    Nov 30 04:40:04.020: INFO: Pod "downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009910151s
    STEP: Saw pod success 11/30/22 04:40:04.02
    Nov 30 04:40:04.020: INFO: Pod "downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e" satisfied condition "Succeeded or Failed"
    Nov 30 04:40:04.023: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e container client-container: <nil>
    STEP: delete the pod 11/30/22 04:40:04.034
    Nov 30 04:40:04.044: INFO: Waiting for pod downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e to disappear
    Nov 30 04:40:04.047: INFO: Pod downwardapi-volume-b600cb36-35d4-457a-8b22-3224df2c920e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Nov 30 04:40:04.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4478" for this suite. 11/30/22 04:40:04.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:40:04.064
Nov 30 04:40:04.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-runtime 11/30/22 04:40:04.065
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:04.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:04.082
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 11/30/22 04:40:04.084
STEP: wait for the container to reach Failed 11/30/22 04:40:04.097
STEP: get the container status 11/30/22 04:40:07.112
STEP: the container should be terminated 11/30/22 04:40:07.115
STEP: the termination message should be set 11/30/22 04:40:07.115
Nov 30 04:40:07.115: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 11/30/22 04:40:07.115
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Nov 30 04:40:07.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8541" for this suite. 11/30/22 04:40:07.135
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":282,"skipped":5065,"failed":0}
------------------------------
• [3.077 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:40:04.064
    Nov 30 04:40:04.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-runtime 11/30/22 04:40:04.065
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:04.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:04.082
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 11/30/22 04:40:04.084
    STEP: wait for the container to reach Failed 11/30/22 04:40:04.097
    STEP: get the container status 11/30/22 04:40:07.112
    STEP: the container should be terminated 11/30/22 04:40:07.115
    STEP: the termination message should be set 11/30/22 04:40:07.115
    Nov 30 04:40:07.115: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 11/30/22 04:40:07.115
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Nov 30 04:40:07.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8541" for this suite. 11/30/22 04:40:07.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:40:07.142
Nov 30 04:40:07.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 04:40:07.142
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:07.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:07.163
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 11/30/22 04:40:07.165
STEP: fetching the ConfigMap 11/30/22 04:40:07.17
STEP: patching the ConfigMap 11/30/22 04:40:07.173
STEP: listing all ConfigMaps in all namespaces with a label selector 11/30/22 04:40:07.182
STEP: deleting the ConfigMap by collection with a label selector 11/30/22 04:40:07.186
STEP: listing all ConfigMaps in test namespace 11/30/22 04:40:07.192
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 04:40:07.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3081" for this suite. 11/30/22 04:40:07.198
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":283,"skipped":5072,"failed":0}
------------------------------
• [0.061 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:40:07.142
    Nov 30 04:40:07.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 04:40:07.142
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:07.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:07.163
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 11/30/22 04:40:07.165
    STEP: fetching the ConfigMap 11/30/22 04:40:07.17
    STEP: patching the ConfigMap 11/30/22 04:40:07.173
    STEP: listing all ConfigMaps in all namespaces with a label selector 11/30/22 04:40:07.182
    STEP: deleting the ConfigMap by collection with a label selector 11/30/22 04:40:07.186
    STEP: listing all ConfigMaps in test namespace 11/30/22 04:40:07.192
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 04:40:07.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3081" for this suite. 11/30/22 04:40:07.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:40:07.204
Nov 30 04:40:07.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename replication-controller 11/30/22 04:40:07.204
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:07.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:07.223
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 11/30/22 04:40:07.23
STEP: waiting for RC to be added 11/30/22 04:40:07.235
STEP: waiting for available Replicas 11/30/22 04:40:07.235
STEP: patching ReplicationController 11/30/22 04:40:08.961
STEP: waiting for RC to be modified 11/30/22 04:40:08.969
STEP: patching ReplicationController status 11/30/22 04:40:08.97
STEP: waiting for RC to be modified 11/30/22 04:40:08.98
STEP: waiting for available Replicas 11/30/22 04:40:08.98
STEP: fetching ReplicationController status 11/30/22 04:40:08.987
STEP: patching ReplicationController scale 11/30/22 04:40:08.99
STEP: waiting for RC to be modified 11/30/22 04:40:09
STEP: waiting for ReplicationController's scale to be the max amount 11/30/22 04:40:09
STEP: fetching ReplicationController; ensuring that it's patched 11/30/22 04:40:10.195
STEP: updating ReplicationController status 11/30/22 04:40:10.198
STEP: waiting for RC to be modified 11/30/22 04:40:10.206
STEP: listing all ReplicationControllers 11/30/22 04:40:10.207
STEP: checking that ReplicationController has expected values 11/30/22 04:40:10.209
STEP: deleting ReplicationControllers by collection 11/30/22 04:40:10.209
STEP: waiting for ReplicationController to have a DELETED watchEvent 11/30/22 04:40:10.22
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Nov 30 04:40:10.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6127" for this suite. 11/30/22 04:40:10.258
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":284,"skipped":5108,"failed":0}
------------------------------
• [3.060 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:40:07.204
    Nov 30 04:40:07.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename replication-controller 11/30/22 04:40:07.204
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:07.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:07.223
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 11/30/22 04:40:07.23
    STEP: waiting for RC to be added 11/30/22 04:40:07.235
    STEP: waiting for available Replicas 11/30/22 04:40:07.235
    STEP: patching ReplicationController 11/30/22 04:40:08.961
    STEP: waiting for RC to be modified 11/30/22 04:40:08.969
    STEP: patching ReplicationController status 11/30/22 04:40:08.97
    STEP: waiting for RC to be modified 11/30/22 04:40:08.98
    STEP: waiting for available Replicas 11/30/22 04:40:08.98
    STEP: fetching ReplicationController status 11/30/22 04:40:08.987
    STEP: patching ReplicationController scale 11/30/22 04:40:08.99
    STEP: waiting for RC to be modified 11/30/22 04:40:09
    STEP: waiting for ReplicationController's scale to be the max amount 11/30/22 04:40:09
    STEP: fetching ReplicationController; ensuring that it's patched 11/30/22 04:40:10.195
    STEP: updating ReplicationController status 11/30/22 04:40:10.198
    STEP: waiting for RC to be modified 11/30/22 04:40:10.206
    STEP: listing all ReplicationControllers 11/30/22 04:40:10.207
    STEP: checking that ReplicationController has expected values 11/30/22 04:40:10.209
    STEP: deleting ReplicationControllers by collection 11/30/22 04:40:10.209
    STEP: waiting for ReplicationController to have a DELETED watchEvent 11/30/22 04:40:10.22
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Nov 30 04:40:10.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6127" for this suite. 11/30/22 04:40:10.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:40:10.265
Nov 30 04:40:10.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename sched-pred 11/30/22 04:40:10.266
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:10.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:10.287
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Nov 30 04:40:10.291: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 30 04:40:10.305: INFO: Waiting for terminating namespaces to be deleted...
Nov 30 04:40:10.308: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins before test
Nov 30 04:40:10.320: INFO: calico-node-9bcg6 from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.320: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 04:40:10.320: INFO: csi-cinder-nodeplugin-hnpf8 from kube-system started at 2022-11-30 02:52:37 +0000 UTC (3 container statuses recorded)
Nov 30 04:40:10.320: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 04:40:10.320: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 04:40:10.320: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 04:40:10.320: INFO: eric-tm-external-connectivity-frontend-speaker-wdz6h from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.320: INFO: 	Container speaker ready: true, restart count 0
Nov 30 04:40:10.320: INFO: kube-multus-ds-amd64-nw2qp from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.320: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 04:40:10.320: INFO: kube-proxy-stdnr from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.320: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 30 04:40:10.320: INFO: kucero-qz2ld from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.320: INFO: 	Container kucero ready: true, restart count 0
Nov 30 04:40:10.320: INFO: node-local-dns-wvqkc from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.320: INFO: 	Container node-cache ready: true, restart count 1
Nov 30 04:40:10.320: INFO: eric-pm-node-exporter-rlfnb from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.320: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 04:40:10.320: INFO: node-cert-exporter-hn5jk from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.320: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 04:40:10.320: INFO: rc-test-sb79h from replication-controller-6127 started at 2022-11-30 04:40:07 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.320: INFO: 	Container rc-test ready: true, restart count 0
Nov 30 04:40:10.320: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-6bvhh from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 04:40:10.320: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 04:40:10.320: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 30 04:40:10.320: INFO: webhook-to-be-mutated from webhook-6407 started at 2022-11-30 04:38:59 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.320: INFO: 	Container example ready: false, restart count 0
Nov 30 04:40:10.320: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins before test
Nov 30 04:40:10.338: INFO: default-http-backend-6f4f64db57-dnh7m from ingress-nginx started at 2022-11-30 02:55:20 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container default-http-backend ready: true, restart count 0
Nov 30 04:40:10.338: INFO: nginx-ingress-controller-844dff9bc5-684fv from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Nov 30 04:40:10.338: INFO: calico-node-thlq4 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 04:40:10.338: INFO: calicoctl-c47c68f5-42nfq from kube-system started at 2022-11-30 03:07:12 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container calicoctl ready: true, restart count 0
Nov 30 04:40:10.338: INFO: csi-cinder-nodeplugin-5hx92 from kube-system started at 2022-11-30 02:52:16 +0000 UTC (3 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 04:40:10.338: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 04:40:10.338: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 04:40:10.338: INFO: eric-app-sys-info-handler-754d6dcc6b-5xw88 from kube-system started at 2022-11-30 03:02:36 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
Nov 30 04:40:10.338: INFO: eric-data-document-database-pg-0 from kube-system started at 2022-11-30 03:03:48 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
Nov 30 04:40:10.338: INFO: eric-tm-external-connectivity-frontend-controller-689dd9fdkths4 from kube-system started at 2022-11-30 03:57:29 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container controller ready: true, restart count 0
Nov 30 04:40:10.338: INFO: eric-tm-external-connectivity-frontend-speaker-vkmgp from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container speaker ready: true, restart count 0
Nov 30 04:40:10.338: INFO: kube-multus-ds-amd64-lhrwf from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 04:40:10.338: INFO: kube-proxy-g68qp from kube-system started at 2022-11-30 02:51:41 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container kube-proxy ready: true, restart count 1
Nov 30 04:40:10.338: INFO: kucero-29zhp from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container kucero ready: true, restart count 0
Nov 30 04:40:10.338: INFO: network-resources-injector-6dfc58d4f-rm8m2 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container webhook-server ready: true, restart count 0
Nov 30 04:40:10.338: INFO: node-local-dns-4fvm6 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container node-cache ready: true, restart count 0
Nov 30 04:40:10.338: INFO: eric-pm-alertmanager-76c454d9f7-d2tvr from monitoring started at 2022-11-30 03:57:29 +0000 UTC (2 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
Nov 30 04:40:10.338: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
Nov 30 04:40:10.338: INFO: eric-pm-kube-state-metrics-6696677569-fnnhj from monitoring started at 2022-11-30 02:58:51 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
Nov 30 04:40:10.338: INFO: eric-pm-node-exporter-dhtts from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 04:40:10.338: INFO: eric-pm-pushgateway-6db47dc9d7-fbx4n from monitoring started at 2022-11-30 02:59:19 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container pushgateway ready: true, restart count 0
Nov 30 04:40:10.338: INFO: eric-victoria-metrics-agent-c9f978858-cmz9r from monitoring started at 2022-11-30 02:57:48 +0000 UTC (2 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
Nov 30 04:40:10.338: INFO: 	Container vmagent-config-reload ready: true, restart count 0
Nov 30 04:40:10.338: INFO: node-cert-exporter-jbl7k from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 04:40:10.338: INFO: rc-test-js5rt from replication-controller-6127 started at 2022-11-30 04:40:09 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container rc-test ready: true, restart count 0
Nov 30 04:40:10.338: INFO: sonobuoy-e2e-job-681d55e1c5a74dc7 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container e2e ready: true, restart count 0
Nov 30 04:40:10.338: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 04:40:10.338: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-nbjs4 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 04:40:10.338: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 04:40:10.338: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 30 04:40:10.338: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins before test
Nov 30 04:40:10.353: INFO: nginx-ingress-controller-844dff9bc5-7std7 from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Nov 30 04:40:10.353: INFO: calico-node-c6ckn from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 04:40:10.353: INFO: csi-cinder-nodeplugin-xvtjm from kube-system started at 2022-11-30 02:52:53 +0000 UTC (3 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 04:40:10.353: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 04:40:10.353: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 04:40:10.353: INFO: eric-lm-combined-server-license-server-client-d9b78bd6b-5r5rz from kube-system started at 2022-11-30 03:04:36 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Nov 30 04:40:10.353: INFO: eric-tm-external-connectivity-frontend-speaker-9k8gj from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container speaker ready: true, restart count 0
Nov 30 04:40:10.353: INFO: kube-multus-ds-amd64-rk7dk from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 04:40:10.353: INFO: kube-proxy-2zzdw from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 30 04:40:10.353: INFO: kucero-nds94 from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container kucero ready: true, restart count 0
Nov 30 04:40:10.353: INFO: node-local-dns-4bsnx from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container node-cache ready: true, restart count 1
Nov 30 04:40:10.353: INFO: eric-pm-node-exporter-xs7lt from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 04:40:10.353: INFO: eric-pm-server-utils-7585bb6b5d-7btvg from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
Nov 30 04:40:10.353: INFO: eric-victoria-metrics-alert-server-54c5c5474c-mhjjn from monitoring started at 2022-11-30 03:38:30 +0000 UTC (2 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
Nov 30 04:40:10.353: INFO: 	Container vmalert-config-reload ready: true, restart count 0
Nov 30 04:40:10.353: INFO: eric-victoria-metrics-cluster-vmselect-55c4b48dfb-lnx7j from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
Nov 30 04:40:10.353: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2022-11-30 02:57:08 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
Nov 30 04:40:10.353: INFO: node-cert-exporter-l77s4 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 04:40:10.353: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-88nb5 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 04:40:10.353: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 04:40:10.353: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 30 04:40:10.353: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins before test
Nov 30 04:40:10.368: INFO: calico-node-klj9m from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.368: INFO: 	Container calico-node ready: true, restart count 0
Nov 30 04:40:10.368: INFO: ccd-license-consumer-5949d66498-mjgr8 from kube-system started at 2022-11-30 03:05:25 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.368: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Nov 30 04:40:10.368: INFO: csi-cinder-nodeplugin-sw4wz from kube-system started at 2022-11-30 02:52:31 +0000 UTC (3 container statuses recorded)
Nov 30 04:40:10.368: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Nov 30 04:40:10.368: INFO: 	Container liveness-probe ready: true, restart count 0
Nov 30 04:40:10.368: INFO: 	Container node-driver-registrar ready: true, restart count 0
Nov 30 04:40:10.368: INFO: eric-lcm-container-registry-registry-7f7856985-4rfb5 from kube-system started at 2022-11-30 02:56:13 +0000 UTC (3 container statuses recorded)
Nov 30 04:40:10.368: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
Nov 30 04:40:10.368: INFO: 	Container registry ready: true, restart count 0
Nov 30 04:40:10.368: INFO: 	Container sidecar ready: true, restart count 0
Nov 30 04:40:10.368: INFO: eric-lm-combined-server-license-consumer-handler-75bfcf7bc2tbcb from kube-system started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.368: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Nov 30 04:40:10.368: INFO: eric-tm-external-connectivity-frontend-speaker-vmfpl from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.368: INFO: 	Container speaker ready: true, restart count 0
Nov 30 04:40:10.368: INFO: kube-multus-ds-amd64-cf4v4 from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.368: INFO: 	Container kube-multus ready: true, restart count 0
Nov 30 04:40:10.369: INFO: kube-proxy-6s7bb from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.369: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 30 04:40:10.369: INFO: kucero-5bwbh from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.369: INFO: 	Container kucero ready: true, restart count 0
Nov 30 04:40:10.369: INFO: metrics-server-7567d6784b-dw2c8 from kube-system started at 2022-11-30 03:01:29 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.369: INFO: 	Container metrics-server ready: true, restart count 0
Nov 30 04:40:10.369: INFO: network-resources-injector-6dfc58d4f-nnqj7 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.369: INFO: 	Container webhook-server ready: true, restart count 0
Nov 30 04:40:10.369: INFO: node-local-dns-4hp9c from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.369: INFO: 	Container node-cache ready: true, restart count 1
Nov 30 04:40:10.369: INFO: eric-pm-node-exporter-lmw6v from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.369: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Nov 30 04:40:10.369: INFO: eric-victoria-metrics-cluster-vminsert-6c76f96c54-jsw5z from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.369: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
Nov 30 04:40:10.369: INFO: isp-logger-5b4d57f796-5ncwr from monitoring started at 2022-11-30 03:06:55 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.369: INFO: 	Container isp-logger ready: true, restart count 0
Nov 30 04:40:10.369: INFO: node-cert-exporter-v97f6 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.369: INFO: 	Container node-cert-exporter ready: true, restart count 0
Nov 30 04:40:10.369: INFO: sonobuoy from sonobuoy started at 2022-11-30 03:19:54 +0000 UTC (1 container statuses recorded)
Nov 30 04:40:10.369: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 30 04:40:10.369: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-z79zw from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
Nov 30 04:40:10.369: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 30 04:40:10.369: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 11/30/22 04:40:10.369
Nov 30 04:40:10.398: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1067" to be "running"
Nov 30 04:40:10.400: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.476922ms
Nov 30 04:40:12.407: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008833069s
Nov 30 04:40:12.407: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 11/30/22 04:40:12.41
STEP: Trying to apply a random label on the found node. 11/30/22 04:40:12.422
STEP: verifying the node has the label kubernetes.io/e2e-aee970cb-e291-4c45-8c38-8a473bd3d091 42 11/30/22 04:40:12.434
STEP: Trying to relaunch the pod, now with labels. 11/30/22 04:40:12.441
Nov 30 04:40:12.448: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1067" to be "not pending"
Nov 30 04:40:12.454: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 5.278334ms
Nov 30 04:40:14.457: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008698201s
Nov 30 04:40:14.457: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-aee970cb-e291-4c45-8c38-8a473bd3d091 off the node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins 11/30/22 04:40:14.46
STEP: verifying the node doesn't have the label kubernetes.io/e2e-aee970cb-e291-4c45-8c38-8a473bd3d091 11/30/22 04:40:14.471
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Nov 30 04:40:14.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1067" for this suite. 11/30/22 04:40:14.479
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":285,"skipped":5158,"failed":0}
------------------------------
• [4.220 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:40:10.265
    Nov 30 04:40:10.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename sched-pred 11/30/22 04:40:10.266
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:10.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:10.287
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Nov 30 04:40:10.291: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 30 04:40:10.305: INFO: Waiting for terminating namespaces to be deleted...
    Nov 30 04:40:10.308: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins before test
    Nov 30 04:40:10.320: INFO: calico-node-9bcg6 from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.320: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: csi-cinder-nodeplugin-hnpf8 from kube-system started at 2022-11-30 02:52:37 +0000 UTC (3 container statuses recorded)
    Nov 30 04:40:10.320: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: eric-tm-external-connectivity-frontend-speaker-wdz6h from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.320: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: kube-multus-ds-amd64-nw2qp from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.320: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: kube-proxy-stdnr from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.320: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: kucero-qz2ld from kube-system started at 2022-11-30 02:52:37 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.320: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: node-local-dns-wvqkc from kube-system started at 2022-11-30 02:51:57 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.320: INFO: 	Container node-cache ready: true, restart count 1
    Nov 30 04:40:10.320: INFO: eric-pm-node-exporter-rlfnb from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.320: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: node-cert-exporter-hn5jk from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.320: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: rc-test-sb79h from replication-controller-6127 started at 2022-11-30 04:40:07 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.320: INFO: 	Container rc-test ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-6bvhh from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 04:40:10.320: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 30 04:40:10.320: INFO: webhook-to-be-mutated from webhook-6407 started at 2022-11-30 04:38:59 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.320: INFO: 	Container example ready: false, restart count 0
    Nov 30 04:40:10.320: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-ok72912g-n92-ci-ibd-23-jenkins before test
    Nov 30 04:40:10.338: INFO: default-http-backend-6f4f64db57-dnh7m from ingress-nginx started at 2022-11-30 02:55:20 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container default-http-backend ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: nginx-ingress-controller-844dff9bc5-684fv from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: calico-node-thlq4 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: calicoctl-c47c68f5-42nfq from kube-system started at 2022-11-30 03:07:12 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container calicoctl ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: csi-cinder-nodeplugin-5hx92 from kube-system started at 2022-11-30 02:52:16 +0000 UTC (3 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: eric-app-sys-info-handler-754d6dcc6b-5xw88 from kube-system started at 2022-11-30 03:02:36 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: eric-data-document-database-pg-0 from kube-system started at 2022-11-30 03:03:48 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: eric-tm-external-connectivity-frontend-controller-689dd9fdkths4 from kube-system started at 2022-11-30 03:57:29 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container controller ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: eric-tm-external-connectivity-frontend-speaker-vkmgp from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: kube-multus-ds-amd64-lhrwf from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: kube-proxy-g68qp from kube-system started at 2022-11-30 02:51:41 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container kube-proxy ready: true, restart count 1
    Nov 30 04:40:10.338: INFO: kucero-29zhp from kube-system started at 2022-11-30 02:52:16 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: network-resources-injector-6dfc58d4f-rm8m2 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container webhook-server ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: node-local-dns-4fvm6 from kube-system started at 2022-11-30 02:51:39 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container node-cache ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: eric-pm-alertmanager-76c454d9f7-d2tvr from monitoring started at 2022-11-30 03:57:29 +0000 UTC (2 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: eric-pm-kube-state-metrics-6696677569-fnnhj from monitoring started at 2022-11-30 02:58:51 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: eric-pm-node-exporter-dhtts from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: eric-pm-pushgateway-6db47dc9d7-fbx4n from monitoring started at 2022-11-30 02:59:19 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container pushgateway ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: eric-victoria-metrics-agent-c9f978858-cmz9r from monitoring started at 2022-11-30 02:57:48 +0000 UTC (2 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: 	Container vmagent-config-reload ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: node-cert-exporter-jbl7k from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: rc-test-js5rt from replication-controller-6127 started at 2022-11-30 04:40:09 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container rc-test ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: sonobuoy-e2e-job-681d55e1c5a74dc7 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container e2e ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-nbjs4 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 04:40:10.338: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 30 04:40:10.338: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins before test
    Nov 30 04:40:10.353: INFO: nginx-ingress-controller-844dff9bc5-7std7 from ingress-nginx started at 2022-11-30 02:55:22 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: calico-node-c6ckn from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: csi-cinder-nodeplugin-xvtjm from kube-system started at 2022-11-30 02:52:53 +0000 UTC (3 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: eric-lm-combined-server-license-server-client-d9b78bd6b-5r5rz from kube-system started at 2022-11-30 03:04:36 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: eric-tm-external-connectivity-frontend-speaker-9k8gj from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: kube-multus-ds-amd64-rk7dk from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: kube-proxy-2zzdw from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: kucero-nds94 from kube-system started at 2022-11-30 02:52:53 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: node-local-dns-4bsnx from kube-system started at 2022-11-30 02:51:48 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container node-cache ready: true, restart count 1
    Nov 30 04:40:10.353: INFO: eric-pm-node-exporter-xs7lt from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: eric-pm-server-utils-7585bb6b5d-7btvg from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: eric-victoria-metrics-alert-server-54c5c5474c-mhjjn from monitoring started at 2022-11-30 03:38:30 +0000 UTC (2 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: 	Container vmalert-config-reload ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: eric-victoria-metrics-cluster-vmselect-55c4b48dfb-lnx7j from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2022-11-30 02:57:08 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: node-cert-exporter-l77s4 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-88nb5 from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 04:40:10.353: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 30 04:40:10.353: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins before test
    Nov 30 04:40:10.368: INFO: calico-node-klj9m from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.368: INFO: 	Container calico-node ready: true, restart count 0
    Nov 30 04:40:10.368: INFO: ccd-license-consumer-5949d66498-mjgr8 from kube-system started at 2022-11-30 03:05:25 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.368: INFO: 	Container ccd-license-consumer ready: true, restart count 0
    Nov 30 04:40:10.368: INFO: csi-cinder-nodeplugin-sw4wz from kube-system started at 2022-11-30 02:52:31 +0000 UTC (3 container statuses recorded)
    Nov 30 04:40:10.368: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Nov 30 04:40:10.368: INFO: 	Container liveness-probe ready: true, restart count 0
    Nov 30 04:40:10.368: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Nov 30 04:40:10.368: INFO: eric-lcm-container-registry-registry-7f7856985-4rfb5 from kube-system started at 2022-11-30 02:56:13 +0000 UTC (3 container statuses recorded)
    Nov 30 04:40:10.368: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
    Nov 30 04:40:10.368: INFO: 	Container registry ready: true, restart count 0
    Nov 30 04:40:10.368: INFO: 	Container sidecar ready: true, restart count 0
    Nov 30 04:40:10.368: INFO: eric-lm-combined-server-license-consumer-handler-75bfcf7bc2tbcb from kube-system started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.368: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
    Nov 30 04:40:10.368: INFO: eric-tm-external-connectivity-frontend-speaker-vmfpl from kube-system started at 2022-11-30 02:54:02 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.368: INFO: 	Container speaker ready: true, restart count 0
    Nov 30 04:40:10.368: INFO: kube-multus-ds-amd64-cf4v4 from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.368: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 30 04:40:10.369: INFO: kube-proxy-6s7bb from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.369: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 30 04:40:10.369: INFO: kucero-5bwbh from kube-system started at 2022-11-30 02:52:31 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.369: INFO: 	Container kucero ready: true, restart count 0
    Nov 30 04:40:10.369: INFO: metrics-server-7567d6784b-dw2c8 from kube-system started at 2022-11-30 03:01:29 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.369: INFO: 	Container metrics-server ready: true, restart count 0
    Nov 30 04:40:10.369: INFO: network-resources-injector-6dfc58d4f-nnqj7 from kube-system started at 2022-11-30 03:06:25 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.369: INFO: 	Container webhook-server ready: true, restart count 0
    Nov 30 04:40:10.369: INFO: node-local-dns-4hp9c from kube-system started at 2022-11-30 02:51:51 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.369: INFO: 	Container node-cache ready: true, restart count 1
    Nov 30 04:40:10.369: INFO: eric-pm-node-exporter-lmw6v from monitoring started at 2022-11-30 02:58:24 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.369: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Nov 30 04:40:10.369: INFO: eric-victoria-metrics-cluster-vminsert-6c76f96c54-jsw5z from monitoring started at 2022-11-30 03:38:30 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.369: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
    Nov 30 04:40:10.369: INFO: isp-logger-5b4d57f796-5ncwr from monitoring started at 2022-11-30 03:06:55 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.369: INFO: 	Container isp-logger ready: true, restart count 0
    Nov 30 04:40:10.369: INFO: node-cert-exporter-v97f6 from monitoring started at 2022-11-30 02:59:43 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.369: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Nov 30 04:40:10.369: INFO: sonobuoy from sonobuoy started at 2022-11-30 03:19:54 +0000 UTC (1 container statuses recorded)
    Nov 30 04:40:10.369: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 30 04:40:10.369: INFO: sonobuoy-systemd-logs-daemon-set-0f6b965f621a44d9-z79zw from sonobuoy started at 2022-11-30 03:19:56 +0000 UTC (2 container statuses recorded)
    Nov 30 04:40:10.369: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 30 04:40:10.369: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 11/30/22 04:40:10.369
    Nov 30 04:40:10.398: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1067" to be "running"
    Nov 30 04:40:10.400: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.476922ms
    Nov 30 04:40:12.407: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008833069s
    Nov 30 04:40:12.407: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 11/30/22 04:40:12.41
    STEP: Trying to apply a random label on the found node. 11/30/22 04:40:12.422
    STEP: verifying the node has the label kubernetes.io/e2e-aee970cb-e291-4c45-8c38-8a473bd3d091 42 11/30/22 04:40:12.434
    STEP: Trying to relaunch the pod, now with labels. 11/30/22 04:40:12.441
    Nov 30 04:40:12.448: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1067" to be "not pending"
    Nov 30 04:40:12.454: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 5.278334ms
    Nov 30 04:40:14.457: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008698201s
    Nov 30 04:40:14.457: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-aee970cb-e291-4c45-8c38-8a473bd3d091 off the node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins 11/30/22 04:40:14.46
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-aee970cb-e291-4c45-8c38-8a473bd3d091 11/30/22 04:40:14.471
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 04:40:14.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1067" for this suite. 11/30/22 04:40:14.479
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:40:14.485
Nov 30 04:40:14.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename proxy 11/30/22 04:40:14.486
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:14.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:14.503
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 11/30/22 04:40:14.522
STEP: creating replication controller proxy-service-4dq8h in namespace proxy-6730 11/30/22 04:40:14.522
I1130 04:40:14.535808      20 runners.go:193] Created replication controller with name: proxy-service-4dq8h, namespace: proxy-6730, replica count: 1
I1130 04:40:15.587457      20 runners.go:193] proxy-service-4dq8h Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1130 04:40:16.588501      20 runners.go:193] proxy-service-4dq8h Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1130 04:40:17.589201      20 runners.go:193] proxy-service-4dq8h Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 30 04:40:17.592: INFO: setup took 3.087244681s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 11/30/22 04:40:17.592
Nov 30 04:40:17.603: INFO: (0) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 10.249311ms)
Nov 30 04:40:17.603: INFO: (0) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 10.552749ms)
Nov 30 04:40:17.603: INFO: (0) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 10.888463ms)
Nov 30 04:40:17.605: INFO: (0) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 12.43021ms)
Nov 30 04:40:17.605: INFO: (0) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 12.592598ms)
Nov 30 04:40:17.606: INFO: (0) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 13.073576ms)
Nov 30 04:40:17.610: INFO: (0) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 17.618071ms)
Nov 30 04:40:17.610: INFO: (0) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 17.699416ms)
Nov 30 04:40:17.611: INFO: (0) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 18.417421ms)
Nov 30 04:40:17.611: INFO: (0) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 18.625832ms)
Nov 30 04:40:17.611: INFO: (0) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 18.746451ms)
Nov 30 04:40:17.611: INFO: (0) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 18.606808ms)
Nov 30 04:40:17.611: INFO: (0) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 18.71755ms)
Nov 30 04:40:17.612: INFO: (0) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 19.106835ms)
Nov 30 04:40:17.612: INFO: (0) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 19.184291ms)
Nov 30 04:40:17.612: INFO: (0) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 19.161438ms)
Nov 30 04:40:17.619: INFO: (1) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.145443ms)
Nov 30 04:40:17.619: INFO: (1) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.118017ms)
Nov 30 04:40:17.619: INFO: (1) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 7.601137ms)
Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 7.797688ms)
Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 8.266921ms)
Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 8.228361ms)
Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 8.274115ms)
Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 8.580376ms)
Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 8.655726ms)
Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 8.524559ms)
Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 8.532223ms)
Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 8.587451ms)
Nov 30 04:40:17.622: INFO: (1) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 9.52839ms)
Nov 30 04:40:17.622: INFO: (1) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 9.893142ms)
Nov 30 04:40:17.622: INFO: (1) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 9.933448ms)
Nov 30 04:40:17.623: INFO: (1) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 10.74546ms)
Nov 30 04:40:17.628: INFO: (2) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 5.479416ms)
Nov 30 04:40:17.628: INFO: (2) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.46701ms)
Nov 30 04:40:17.629: INFO: (2) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 6.105948ms)
Nov 30 04:40:17.629: INFO: (2) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 6.238181ms)
Nov 30 04:40:17.629: INFO: (2) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 6.253466ms)
Nov 30 04:40:17.629: INFO: (2) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 6.419097ms)
Nov 30 04:40:17.629: INFO: (2) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 6.564842ms)
Nov 30 04:40:17.630: INFO: (2) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 6.850296ms)
Nov 30 04:40:17.632: INFO: (2) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 9.321322ms)
Nov 30 04:40:17.633: INFO: (2) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 9.816102ms)
Nov 30 04:40:17.633: INFO: (2) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 9.776523ms)
Nov 30 04:40:17.633: INFO: (2) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 10.04201ms)
Nov 30 04:40:17.633: INFO: (2) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 10.053268ms)
Nov 30 04:40:17.635: INFO: (2) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 12.608494ms)
Nov 30 04:40:17.637: INFO: (2) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 14.311943ms)
Nov 30 04:40:17.640: INFO: (2) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 17.751416ms)
Nov 30 04:40:17.647: INFO: (3) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 6.004384ms)
Nov 30 04:40:17.647: INFO: (3) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 6.16634ms)
Nov 30 04:40:17.647: INFO: (3) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 6.321307ms)
Nov 30 04:40:17.648: INFO: (3) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.521181ms)
Nov 30 04:40:17.648: INFO: (3) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 7.748001ms)
Nov 30 04:40:17.648: INFO: (3) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 7.894903ms)
Nov 30 04:40:17.648: INFO: (3) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.80992ms)
Nov 30 04:40:17.648: INFO: (3) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 7.855296ms)
Nov 30 04:40:17.648: INFO: (3) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 7.785275ms)
Nov 30 04:40:17.649: INFO: (3) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 8.112802ms)
Nov 30 04:40:17.649: INFO: (3) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 8.316082ms)
Nov 30 04:40:17.651: INFO: (3) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 10.273221ms)
Nov 30 04:40:17.651: INFO: (3) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 10.192316ms)
Nov 30 04:40:17.651: INFO: (3) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 10.493576ms)
Nov 30 04:40:17.652: INFO: (3) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 11.495573ms)
Nov 30 04:40:17.652: INFO: (3) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 11.773418ms)
Nov 30 04:40:17.656: INFO: (4) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 3.815265ms)
Nov 30 04:40:17.658: INFO: (4) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.588011ms)
Nov 30 04:40:17.658: INFO: (4) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 5.811591ms)
Nov 30 04:40:17.659: INFO: (4) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 5.903487ms)
Nov 30 04:40:17.659: INFO: (4) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 6.472208ms)
Nov 30 04:40:17.659: INFO: (4) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 6.079622ms)
Nov 30 04:40:17.659: INFO: (4) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 6.506042ms)
Nov 30 04:40:17.659: INFO: (4) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.214933ms)
Nov 30 04:40:17.659: INFO: (4) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 6.580093ms)
Nov 30 04:40:17.660: INFO: (4) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 7.691357ms)
Nov 30 04:40:17.661: INFO: (4) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 7.880764ms)
Nov 30 04:40:17.661: INFO: (4) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 7.816641ms)
Nov 30 04:40:17.661: INFO: (4) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 7.79855ms)
Nov 30 04:40:17.661: INFO: (4) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 8.321246ms)
Nov 30 04:40:17.662: INFO: (4) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 9.02747ms)
Nov 30 04:40:17.662: INFO: (4) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 9.030374ms)
Nov 30 04:40:17.667: INFO: (5) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 4.964179ms)
Nov 30 04:40:17.667: INFO: (5) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 5.131416ms)
Nov 30 04:40:17.669: INFO: (5) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.982256ms)
Nov 30 04:40:17.669: INFO: (5) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 7.077556ms)
Nov 30 04:40:17.670: INFO: (5) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.979707ms)
Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 8.161734ms)
Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 8.102807ms)
Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 8.247506ms)
Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 8.197782ms)
Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 8.641224ms)
Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 8.420227ms)
Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 8.969593ms)
Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 8.391027ms)
Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 8.382637ms)
Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 8.65652ms)
Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 8.829633ms)
Nov 30 04:40:17.676: INFO: (6) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 4.644489ms)
Nov 30 04:40:17.676: INFO: (6) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 4.579348ms)
Nov 30 04:40:17.676: INFO: (6) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 4.724775ms)
Nov 30 04:40:17.676: INFO: (6) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 5.051333ms)
Nov 30 04:40:17.677: INFO: (6) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.605037ms)
Nov 30 04:40:17.678: INFO: (6) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 6.329511ms)
Nov 30 04:40:17.678: INFO: (6) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.140862ms)
Nov 30 04:40:17.678: INFO: (6) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 6.366703ms)
Nov 30 04:40:17.679: INFO: (6) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 6.948052ms)
Nov 30 04:40:17.679: INFO: (6) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 7.044921ms)
Nov 30 04:40:17.680: INFO: (6) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 8.574202ms)
Nov 30 04:40:17.681: INFO: (6) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 9.333561ms)
Nov 30 04:40:17.681: INFO: (6) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 9.411696ms)
Nov 30 04:40:17.681: INFO: (6) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 9.347505ms)
Nov 30 04:40:17.682: INFO: (6) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 10.5971ms)
Nov 30 04:40:17.682: INFO: (6) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 10.94851ms)
Nov 30 04:40:17.686: INFO: (7) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 3.32258ms)
Nov 30 04:40:17.687: INFO: (7) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 3.738673ms)
Nov 30 04:40:17.687: INFO: (7) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 4.134094ms)
Nov 30 04:40:17.688: INFO: (7) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.009916ms)
Nov 30 04:40:17.689: INFO: (7) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 5.369602ms)
Nov 30 04:40:17.689: INFO: (7) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 5.465598ms)
Nov 30 04:40:17.689: INFO: (7) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 5.569656ms)
Nov 30 04:40:17.689: INFO: (7) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 5.863464ms)
Nov 30 04:40:17.689: INFO: (7) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 6.022095ms)
Nov 30 04:40:17.690: INFO: (7) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 7.148681ms)
Nov 30 04:40:17.691: INFO: (7) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 7.392886ms)
Nov 30 04:40:17.691: INFO: (7) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 8.063997ms)
Nov 30 04:40:17.691: INFO: (7) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 7.680842ms)
Nov 30 04:40:17.691: INFO: (7) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 8.460821ms)
Nov 30 04:40:17.691: INFO: (7) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 8.131119ms)
Nov 30 04:40:17.691: INFO: (7) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 8.541917ms)
Nov 30 04:40:17.697: INFO: (8) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 5.010875ms)
Nov 30 04:40:17.697: INFO: (8) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.094873ms)
Nov 30 04:40:17.698: INFO: (8) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 5.865826ms)
Nov 30 04:40:17.698: INFO: (8) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.317515ms)
Nov 30 04:40:17.698: INFO: (8) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 6.593461ms)
Nov 30 04:40:17.698: INFO: (8) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 6.586266ms)
Nov 30 04:40:17.700: INFO: (8) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 8.083343ms)
Nov 30 04:40:17.700: INFO: (8) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 8.280758ms)
Nov 30 04:40:17.700: INFO: (8) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 8.724602ms)
Nov 30 04:40:17.700: INFO: (8) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 8.808542ms)
Nov 30 04:40:17.701: INFO: (8) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 9.035149ms)
Nov 30 04:40:17.701: INFO: (8) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 9.481535ms)
Nov 30 04:40:17.701: INFO: (8) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 9.196733ms)
Nov 30 04:40:17.701: INFO: (8) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 9.640854ms)
Nov 30 04:40:17.701: INFO: (8) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 9.533934ms)
Nov 30 04:40:17.702: INFO: (8) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 9.717138ms)
Nov 30 04:40:17.706: INFO: (9) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 4.532381ms)
Nov 30 04:40:17.709: INFO: (9) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 7.618608ms)
Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 7.373481ms)
Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.566461ms)
Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 7.667596ms)
Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.460439ms)
Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.865486ms)
Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 8.182487ms)
Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 7.732206ms)
Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 8.079001ms)
Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 8.328543ms)
Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 8.58987ms)
Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 8.203028ms)
Nov 30 04:40:17.711: INFO: (9) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 8.366848ms)
Nov 30 04:40:17.712: INFO: (9) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 9.698883ms)
Nov 30 04:40:17.712: INFO: (9) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 10.05238ms)
Nov 30 04:40:17.717: INFO: (10) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.333225ms)
Nov 30 04:40:17.717: INFO: (10) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 5.552429ms)
Nov 30 04:40:17.718: INFO: (10) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 6.112879ms)
Nov 30 04:40:17.719: INFO: (10) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.852337ms)
Nov 30 04:40:17.720: INFO: (10) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 7.817482ms)
Nov 30 04:40:17.720: INFO: (10) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 7.81677ms)
Nov 30 04:40:17.720: INFO: (10) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.797543ms)
Nov 30 04:40:17.720: INFO: (10) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 7.967009ms)
Nov 30 04:40:17.721: INFO: (10) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 8.856108ms)
Nov 30 04:40:17.721: INFO: (10) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 8.914139ms)
Nov 30 04:40:17.721: INFO: (10) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 8.829443ms)
Nov 30 04:40:17.721: INFO: (10) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 8.869746ms)
Nov 30 04:40:17.722: INFO: (10) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 9.286818ms)
Nov 30 04:40:17.722: INFO: (10) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 9.49005ms)
Nov 30 04:40:17.722: INFO: (10) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 9.97488ms)
Nov 30 04:40:17.722: INFO: (10) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 9.866919ms)
Nov 30 04:40:17.727: INFO: (11) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 4.638216ms)
Nov 30 04:40:17.727: INFO: (11) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 5.14397ms)
Nov 30 04:40:17.729: INFO: (11) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.33126ms)
Nov 30 04:40:17.730: INFO: (11) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 6.991692ms)
Nov 30 04:40:17.730: INFO: (11) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 7.515158ms)
Nov 30 04:40:17.730: INFO: (11) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.099989ms)
Nov 30 04:40:17.730: INFO: (11) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.842331ms)
Nov 30 04:40:17.731: INFO: (11) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 8.422495ms)
Nov 30 04:40:17.731: INFO: (11) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 8.338159ms)
Nov 30 04:40:17.731: INFO: (11) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 8.463509ms)
Nov 30 04:40:17.731: INFO: (11) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 8.90278ms)
Nov 30 04:40:17.734: INFO: (11) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 11.3435ms)
Nov 30 04:40:17.734: INFO: (11) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 11.241294ms)
Nov 30 04:40:17.734: INFO: (11) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 11.29468ms)
Nov 30 04:40:17.734: INFO: (11) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 11.900454ms)
Nov 30 04:40:17.734: INFO: (11) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 12.037927ms)
Nov 30 04:40:17.741: INFO: (12) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 6.545035ms)
Nov 30 04:40:17.741: INFO: (12) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 6.436932ms)
Nov 30 04:40:17.741: INFO: (12) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 6.194943ms)
Nov 30 04:40:17.742: INFO: (12) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 7.435428ms)
Nov 30 04:40:17.743: INFO: (12) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.839424ms)
Nov 30 04:40:17.743: INFO: (12) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 8.361805ms)
Nov 30 04:40:17.743: INFO: (12) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 8.199553ms)
Nov 30 04:40:17.743: INFO: (12) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 8.690791ms)
Nov 30 04:40:17.743: INFO: (12) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 8.328646ms)
Nov 30 04:40:17.744: INFO: (12) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 8.793806ms)
Nov 30 04:40:17.744: INFO: (12) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 9.340653ms)
Nov 30 04:40:17.744: INFO: (12) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 9.205052ms)
Nov 30 04:40:17.745: INFO: (12) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 9.758557ms)
Nov 30 04:40:17.745: INFO: (12) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 9.954333ms)
Nov 30 04:40:17.745: INFO: (12) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 9.660689ms)
Nov 30 04:40:17.745: INFO: (12) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 9.980696ms)
Nov 30 04:40:17.749: INFO: (13) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 3.698634ms)
Nov 30 04:40:17.749: INFO: (13) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 3.802307ms)
Nov 30 04:40:17.749: INFO: (13) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 3.779338ms)
Nov 30 04:40:17.750: INFO: (13) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 4.115885ms)
Nov 30 04:40:17.753: INFO: (13) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.109423ms)
Nov 30 04:40:17.753: INFO: (13) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.324342ms)
Nov 30 04:40:17.753: INFO: (13) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 7.672348ms)
Nov 30 04:40:17.755: INFO: (13) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 9.228929ms)
Nov 30 04:40:17.755: INFO: (13) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 9.864943ms)
Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 11.409984ms)
Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 10.903017ms)
Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 10.802775ms)
Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 11.087676ms)
Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 11.709124ms)
Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 11.458625ms)
Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 11.423737ms)
Nov 30 04:40:17.763: INFO: (14) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 5.679275ms)
Nov 30 04:40:17.763: INFO: (14) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 5.481195ms)
Nov 30 04:40:17.765: INFO: (14) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.407669ms)
Nov 30 04:40:17.766: INFO: (14) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 7.854022ms)
Nov 30 04:40:17.766: INFO: (14) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 8.392365ms)
Nov 30 04:40:17.766: INFO: (14) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 8.804913ms)
Nov 30 04:40:17.766: INFO: (14) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 8.936553ms)
Nov 30 04:40:17.767: INFO: (14) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 9.634237ms)
Nov 30 04:40:17.768: INFO: (14) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 10.537127ms)
Nov 30 04:40:17.768: INFO: (14) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 10.512654ms)
Nov 30 04:40:17.768: INFO: (14) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 10.791352ms)
Nov 30 04:40:17.768: INFO: (14) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 10.585822ms)
Nov 30 04:40:17.769: INFO: (14) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 11.078127ms)
Nov 30 04:40:17.769: INFO: (14) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 10.826994ms)
Nov 30 04:40:17.769: INFO: (14) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 10.866977ms)
Nov 30 04:40:17.769: INFO: (14) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 11.393574ms)
Nov 30 04:40:17.782: INFO: (15) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 13.042891ms)
Nov 30 04:40:17.782: INFO: (15) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 13.198808ms)
Nov 30 04:40:17.783: INFO: (15) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 13.621849ms)
Nov 30 04:40:17.783: INFO: (15) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 14.250146ms)
Nov 30 04:40:17.783: INFO: (15) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 14.081142ms)
Nov 30 04:40:17.783: INFO: (15) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 14.200613ms)
Nov 30 04:40:17.784: INFO: (15) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 14.582729ms)
Nov 30 04:40:17.784: INFO: (15) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 14.745639ms)
Nov 30 04:40:17.784: INFO: (15) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 14.909071ms)
Nov 30 04:40:17.785: INFO: (15) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 15.998434ms)
Nov 30 04:40:17.785: INFO: (15) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 15.717222ms)
Nov 30 04:40:17.786: INFO: (15) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 16.444423ms)
Nov 30 04:40:17.786: INFO: (15) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 16.981331ms)
Nov 30 04:40:17.786: INFO: (15) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 17.267239ms)
Nov 30 04:40:17.786: INFO: (15) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 17.158385ms)
Nov 30 04:40:17.788: INFO: (15) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 18.911908ms)
Nov 30 04:40:17.795: INFO: (16) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 6.616819ms)
Nov 30 04:40:17.795: INFO: (16) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 6.367139ms)
Nov 30 04:40:17.796: INFO: (16) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.390949ms)
Nov 30 04:40:17.796: INFO: (16) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 7.829262ms)
Nov 30 04:40:17.796: INFO: (16) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 7.797267ms)
Nov 30 04:40:17.796: INFO: (16) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 8.214793ms)
Nov 30 04:40:17.796: INFO: (16) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 8.266782ms)
Nov 30 04:40:17.797: INFO: (16) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 8.537147ms)
Nov 30 04:40:17.797: INFO: (16) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 8.489445ms)
Nov 30 04:40:17.801: INFO: (16) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 12.5615ms)
Nov 30 04:40:17.801: INFO: (16) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 12.810273ms)
Nov 30 04:40:17.803: INFO: (16) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 14.646865ms)
Nov 30 04:40:17.803: INFO: (16) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 14.484977ms)
Nov 30 04:40:17.803: INFO: (16) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 14.758717ms)
Nov 30 04:40:17.803: INFO: (16) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 14.983945ms)
Nov 30 04:40:17.803: INFO: (16) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 15.009748ms)
Nov 30 04:40:17.809: INFO: (17) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 4.985141ms)
Nov 30 04:40:17.809: INFO: (17) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 4.968631ms)
Nov 30 04:40:17.809: INFO: (17) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 5.646179ms)
Nov 30 04:40:17.809: INFO: (17) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.384543ms)
Nov 30 04:40:17.810: INFO: (17) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 5.939841ms)
Nov 30 04:40:17.810: INFO: (17) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 6.284779ms)
Nov 30 04:40:17.810: INFO: (17) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 6.008925ms)
Nov 30 04:40:17.810: INFO: (17) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 6.567893ms)
Nov 30 04:40:17.812: INFO: (17) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 7.444732ms)
Nov 30 04:40:17.814: INFO: (17) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 10.645452ms)
Nov 30 04:40:17.814: INFO: (17) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 10.368658ms)
Nov 30 04:40:17.815: INFO: (17) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 10.485237ms)
Nov 30 04:40:17.815: INFO: (17) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 11.099059ms)
Nov 30 04:40:17.815: INFO: (17) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 11.016352ms)
Nov 30 04:40:17.817: INFO: (17) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 12.777955ms)
Nov 30 04:40:17.817: INFO: (17) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 13.662521ms)
Nov 30 04:40:17.822: INFO: (18) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 4.439612ms)
Nov 30 04:40:17.822: INFO: (18) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 4.776517ms)
Nov 30 04:40:17.822: INFO: (18) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 4.692091ms)
Nov 30 04:40:17.822: INFO: (18) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 4.872464ms)
Nov 30 04:40:17.823: INFO: (18) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 5.024252ms)
Nov 30 04:40:17.823: INFO: (18) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 5.230864ms)
Nov 30 04:40:17.824: INFO: (18) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 6.752748ms)
Nov 30 04:40:17.825: INFO: (18) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 6.957043ms)
Nov 30 04:40:17.825: INFO: (18) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 7.270268ms)
Nov 30 04:40:17.825: INFO: (18) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.526512ms)
Nov 30 04:40:17.825: INFO: (18) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 7.759403ms)
Nov 30 04:40:17.827: INFO: (18) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 9.090511ms)
Nov 30 04:40:17.827: INFO: (18) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 9.063335ms)
Nov 30 04:40:17.828: INFO: (18) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 10.465218ms)
Nov 30 04:40:17.828: INFO: (18) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 10.448388ms)
Nov 30 04:40:17.828: INFO: (18) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 10.729761ms)
Nov 30 04:40:17.834: INFO: (19) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 5.503344ms)
Nov 30 04:40:17.834: INFO: (19) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 5.685829ms)
Nov 30 04:40:17.834: INFO: (19) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 5.947866ms)
Nov 30 04:40:17.834: INFO: (19) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 6.136243ms)
Nov 30 04:40:17.835: INFO: (19) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 6.802576ms)
Nov 30 04:40:17.835: INFO: (19) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.736922ms)
Nov 30 04:40:17.835: INFO: (19) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 6.653091ms)
Nov 30 04:40:17.835: INFO: (19) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 7.055146ms)
Nov 30 04:40:17.836: INFO: (19) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 7.290586ms)
Nov 30 04:40:17.836: INFO: (19) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 7.25947ms)
Nov 30 04:40:17.838: INFO: (19) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 9.495085ms)
Nov 30 04:40:17.838: INFO: (19) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 9.481216ms)
Nov 30 04:40:17.838: INFO: (19) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 9.751094ms)
Nov 30 04:40:17.838: INFO: (19) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 9.799072ms)
Nov 30 04:40:17.838: INFO: (19) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 9.913829ms)
Nov 30 04:40:17.839: INFO: (19) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 10.267592ms)
STEP: deleting ReplicationController proxy-service-4dq8h in namespace proxy-6730, will wait for the garbage collector to delete the pods 11/30/22 04:40:17.839
Nov 30 04:40:17.897: INFO: Deleting ReplicationController proxy-service-4dq8h took: 5.900922ms
Nov 30 04:40:17.998: INFO: Terminating ReplicationController proxy-service-4dq8h pods took: 100.935707ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Nov 30 04:40:20.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6730" for this suite. 11/30/22 04:40:20.104
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":286,"skipped":5162,"failed":0}
------------------------------
• [SLOW TEST] [5.627 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:40:14.485
    Nov 30 04:40:14.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename proxy 11/30/22 04:40:14.486
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:14.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:14.503
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 11/30/22 04:40:14.522
    STEP: creating replication controller proxy-service-4dq8h in namespace proxy-6730 11/30/22 04:40:14.522
    I1130 04:40:14.535808      20 runners.go:193] Created replication controller with name: proxy-service-4dq8h, namespace: proxy-6730, replica count: 1
    I1130 04:40:15.587457      20 runners.go:193] proxy-service-4dq8h Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1130 04:40:16.588501      20 runners.go:193] proxy-service-4dq8h Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I1130 04:40:17.589201      20 runners.go:193] proxy-service-4dq8h Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 30 04:40:17.592: INFO: setup took 3.087244681s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 11/30/22 04:40:17.592
    Nov 30 04:40:17.603: INFO: (0) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 10.249311ms)
    Nov 30 04:40:17.603: INFO: (0) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 10.552749ms)
    Nov 30 04:40:17.603: INFO: (0) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 10.888463ms)
    Nov 30 04:40:17.605: INFO: (0) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 12.43021ms)
    Nov 30 04:40:17.605: INFO: (0) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 12.592598ms)
    Nov 30 04:40:17.606: INFO: (0) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 13.073576ms)
    Nov 30 04:40:17.610: INFO: (0) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 17.618071ms)
    Nov 30 04:40:17.610: INFO: (0) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 17.699416ms)
    Nov 30 04:40:17.611: INFO: (0) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 18.417421ms)
    Nov 30 04:40:17.611: INFO: (0) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 18.625832ms)
    Nov 30 04:40:17.611: INFO: (0) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 18.746451ms)
    Nov 30 04:40:17.611: INFO: (0) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 18.606808ms)
    Nov 30 04:40:17.611: INFO: (0) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 18.71755ms)
    Nov 30 04:40:17.612: INFO: (0) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 19.106835ms)
    Nov 30 04:40:17.612: INFO: (0) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 19.184291ms)
    Nov 30 04:40:17.612: INFO: (0) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 19.161438ms)
    Nov 30 04:40:17.619: INFO: (1) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.145443ms)
    Nov 30 04:40:17.619: INFO: (1) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.118017ms)
    Nov 30 04:40:17.619: INFO: (1) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 7.601137ms)
    Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 7.797688ms)
    Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 8.266921ms)
    Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 8.228361ms)
    Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 8.274115ms)
    Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 8.580376ms)
    Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 8.655726ms)
    Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 8.524559ms)
    Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 8.532223ms)
    Nov 30 04:40:17.620: INFO: (1) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 8.587451ms)
    Nov 30 04:40:17.622: INFO: (1) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 9.52839ms)
    Nov 30 04:40:17.622: INFO: (1) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 9.893142ms)
    Nov 30 04:40:17.622: INFO: (1) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 9.933448ms)
    Nov 30 04:40:17.623: INFO: (1) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 10.74546ms)
    Nov 30 04:40:17.628: INFO: (2) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 5.479416ms)
    Nov 30 04:40:17.628: INFO: (2) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.46701ms)
    Nov 30 04:40:17.629: INFO: (2) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 6.105948ms)
    Nov 30 04:40:17.629: INFO: (2) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 6.238181ms)
    Nov 30 04:40:17.629: INFO: (2) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 6.253466ms)
    Nov 30 04:40:17.629: INFO: (2) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 6.419097ms)
    Nov 30 04:40:17.629: INFO: (2) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 6.564842ms)
    Nov 30 04:40:17.630: INFO: (2) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 6.850296ms)
    Nov 30 04:40:17.632: INFO: (2) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 9.321322ms)
    Nov 30 04:40:17.633: INFO: (2) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 9.816102ms)
    Nov 30 04:40:17.633: INFO: (2) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 9.776523ms)
    Nov 30 04:40:17.633: INFO: (2) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 10.04201ms)
    Nov 30 04:40:17.633: INFO: (2) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 10.053268ms)
    Nov 30 04:40:17.635: INFO: (2) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 12.608494ms)
    Nov 30 04:40:17.637: INFO: (2) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 14.311943ms)
    Nov 30 04:40:17.640: INFO: (2) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 17.751416ms)
    Nov 30 04:40:17.647: INFO: (3) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 6.004384ms)
    Nov 30 04:40:17.647: INFO: (3) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 6.16634ms)
    Nov 30 04:40:17.647: INFO: (3) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 6.321307ms)
    Nov 30 04:40:17.648: INFO: (3) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.521181ms)
    Nov 30 04:40:17.648: INFO: (3) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 7.748001ms)
    Nov 30 04:40:17.648: INFO: (3) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 7.894903ms)
    Nov 30 04:40:17.648: INFO: (3) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.80992ms)
    Nov 30 04:40:17.648: INFO: (3) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 7.855296ms)
    Nov 30 04:40:17.648: INFO: (3) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 7.785275ms)
    Nov 30 04:40:17.649: INFO: (3) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 8.112802ms)
    Nov 30 04:40:17.649: INFO: (3) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 8.316082ms)
    Nov 30 04:40:17.651: INFO: (3) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 10.273221ms)
    Nov 30 04:40:17.651: INFO: (3) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 10.192316ms)
    Nov 30 04:40:17.651: INFO: (3) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 10.493576ms)
    Nov 30 04:40:17.652: INFO: (3) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 11.495573ms)
    Nov 30 04:40:17.652: INFO: (3) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 11.773418ms)
    Nov 30 04:40:17.656: INFO: (4) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 3.815265ms)
    Nov 30 04:40:17.658: INFO: (4) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.588011ms)
    Nov 30 04:40:17.658: INFO: (4) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 5.811591ms)
    Nov 30 04:40:17.659: INFO: (4) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 5.903487ms)
    Nov 30 04:40:17.659: INFO: (4) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 6.472208ms)
    Nov 30 04:40:17.659: INFO: (4) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 6.079622ms)
    Nov 30 04:40:17.659: INFO: (4) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 6.506042ms)
    Nov 30 04:40:17.659: INFO: (4) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.214933ms)
    Nov 30 04:40:17.659: INFO: (4) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 6.580093ms)
    Nov 30 04:40:17.660: INFO: (4) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 7.691357ms)
    Nov 30 04:40:17.661: INFO: (4) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 7.880764ms)
    Nov 30 04:40:17.661: INFO: (4) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 7.816641ms)
    Nov 30 04:40:17.661: INFO: (4) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 7.79855ms)
    Nov 30 04:40:17.661: INFO: (4) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 8.321246ms)
    Nov 30 04:40:17.662: INFO: (4) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 9.02747ms)
    Nov 30 04:40:17.662: INFO: (4) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 9.030374ms)
    Nov 30 04:40:17.667: INFO: (5) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 4.964179ms)
    Nov 30 04:40:17.667: INFO: (5) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 5.131416ms)
    Nov 30 04:40:17.669: INFO: (5) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.982256ms)
    Nov 30 04:40:17.669: INFO: (5) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 7.077556ms)
    Nov 30 04:40:17.670: INFO: (5) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.979707ms)
    Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 8.161734ms)
    Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 8.102807ms)
    Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 8.247506ms)
    Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 8.197782ms)
    Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 8.641224ms)
    Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 8.420227ms)
    Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 8.969593ms)
    Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 8.391027ms)
    Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 8.382637ms)
    Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 8.65652ms)
    Nov 30 04:40:17.671: INFO: (5) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 8.829633ms)
    Nov 30 04:40:17.676: INFO: (6) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 4.644489ms)
    Nov 30 04:40:17.676: INFO: (6) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 4.579348ms)
    Nov 30 04:40:17.676: INFO: (6) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 4.724775ms)
    Nov 30 04:40:17.676: INFO: (6) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 5.051333ms)
    Nov 30 04:40:17.677: INFO: (6) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.605037ms)
    Nov 30 04:40:17.678: INFO: (6) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 6.329511ms)
    Nov 30 04:40:17.678: INFO: (6) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.140862ms)
    Nov 30 04:40:17.678: INFO: (6) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 6.366703ms)
    Nov 30 04:40:17.679: INFO: (6) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 6.948052ms)
    Nov 30 04:40:17.679: INFO: (6) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 7.044921ms)
    Nov 30 04:40:17.680: INFO: (6) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 8.574202ms)
    Nov 30 04:40:17.681: INFO: (6) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 9.333561ms)
    Nov 30 04:40:17.681: INFO: (6) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 9.411696ms)
    Nov 30 04:40:17.681: INFO: (6) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 9.347505ms)
    Nov 30 04:40:17.682: INFO: (6) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 10.5971ms)
    Nov 30 04:40:17.682: INFO: (6) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 10.94851ms)
    Nov 30 04:40:17.686: INFO: (7) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 3.32258ms)
    Nov 30 04:40:17.687: INFO: (7) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 3.738673ms)
    Nov 30 04:40:17.687: INFO: (7) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 4.134094ms)
    Nov 30 04:40:17.688: INFO: (7) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.009916ms)
    Nov 30 04:40:17.689: INFO: (7) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 5.369602ms)
    Nov 30 04:40:17.689: INFO: (7) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 5.465598ms)
    Nov 30 04:40:17.689: INFO: (7) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 5.569656ms)
    Nov 30 04:40:17.689: INFO: (7) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 5.863464ms)
    Nov 30 04:40:17.689: INFO: (7) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 6.022095ms)
    Nov 30 04:40:17.690: INFO: (7) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 7.148681ms)
    Nov 30 04:40:17.691: INFO: (7) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 7.392886ms)
    Nov 30 04:40:17.691: INFO: (7) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 8.063997ms)
    Nov 30 04:40:17.691: INFO: (7) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 7.680842ms)
    Nov 30 04:40:17.691: INFO: (7) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 8.460821ms)
    Nov 30 04:40:17.691: INFO: (7) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 8.131119ms)
    Nov 30 04:40:17.691: INFO: (7) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 8.541917ms)
    Nov 30 04:40:17.697: INFO: (8) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 5.010875ms)
    Nov 30 04:40:17.697: INFO: (8) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.094873ms)
    Nov 30 04:40:17.698: INFO: (8) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 5.865826ms)
    Nov 30 04:40:17.698: INFO: (8) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.317515ms)
    Nov 30 04:40:17.698: INFO: (8) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 6.593461ms)
    Nov 30 04:40:17.698: INFO: (8) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 6.586266ms)
    Nov 30 04:40:17.700: INFO: (8) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 8.083343ms)
    Nov 30 04:40:17.700: INFO: (8) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 8.280758ms)
    Nov 30 04:40:17.700: INFO: (8) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 8.724602ms)
    Nov 30 04:40:17.700: INFO: (8) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 8.808542ms)
    Nov 30 04:40:17.701: INFO: (8) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 9.035149ms)
    Nov 30 04:40:17.701: INFO: (8) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 9.481535ms)
    Nov 30 04:40:17.701: INFO: (8) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 9.196733ms)
    Nov 30 04:40:17.701: INFO: (8) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 9.640854ms)
    Nov 30 04:40:17.701: INFO: (8) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 9.533934ms)
    Nov 30 04:40:17.702: INFO: (8) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 9.717138ms)
    Nov 30 04:40:17.706: INFO: (9) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 4.532381ms)
    Nov 30 04:40:17.709: INFO: (9) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 7.618608ms)
    Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 7.373481ms)
    Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.566461ms)
    Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 7.667596ms)
    Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.460439ms)
    Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.865486ms)
    Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 8.182487ms)
    Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 7.732206ms)
    Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 8.079001ms)
    Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 8.328543ms)
    Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 8.58987ms)
    Nov 30 04:40:17.710: INFO: (9) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 8.203028ms)
    Nov 30 04:40:17.711: INFO: (9) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 8.366848ms)
    Nov 30 04:40:17.712: INFO: (9) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 9.698883ms)
    Nov 30 04:40:17.712: INFO: (9) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 10.05238ms)
    Nov 30 04:40:17.717: INFO: (10) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.333225ms)
    Nov 30 04:40:17.717: INFO: (10) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 5.552429ms)
    Nov 30 04:40:17.718: INFO: (10) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 6.112879ms)
    Nov 30 04:40:17.719: INFO: (10) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.852337ms)
    Nov 30 04:40:17.720: INFO: (10) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 7.817482ms)
    Nov 30 04:40:17.720: INFO: (10) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 7.81677ms)
    Nov 30 04:40:17.720: INFO: (10) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.797543ms)
    Nov 30 04:40:17.720: INFO: (10) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 7.967009ms)
    Nov 30 04:40:17.721: INFO: (10) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 8.856108ms)
    Nov 30 04:40:17.721: INFO: (10) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 8.914139ms)
    Nov 30 04:40:17.721: INFO: (10) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 8.829443ms)
    Nov 30 04:40:17.721: INFO: (10) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 8.869746ms)
    Nov 30 04:40:17.722: INFO: (10) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 9.286818ms)
    Nov 30 04:40:17.722: INFO: (10) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 9.49005ms)
    Nov 30 04:40:17.722: INFO: (10) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 9.97488ms)
    Nov 30 04:40:17.722: INFO: (10) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 9.866919ms)
    Nov 30 04:40:17.727: INFO: (11) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 4.638216ms)
    Nov 30 04:40:17.727: INFO: (11) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 5.14397ms)
    Nov 30 04:40:17.729: INFO: (11) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.33126ms)
    Nov 30 04:40:17.730: INFO: (11) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 6.991692ms)
    Nov 30 04:40:17.730: INFO: (11) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 7.515158ms)
    Nov 30 04:40:17.730: INFO: (11) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.099989ms)
    Nov 30 04:40:17.730: INFO: (11) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 7.842331ms)
    Nov 30 04:40:17.731: INFO: (11) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 8.422495ms)
    Nov 30 04:40:17.731: INFO: (11) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 8.338159ms)
    Nov 30 04:40:17.731: INFO: (11) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 8.463509ms)
    Nov 30 04:40:17.731: INFO: (11) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 8.90278ms)
    Nov 30 04:40:17.734: INFO: (11) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 11.3435ms)
    Nov 30 04:40:17.734: INFO: (11) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 11.241294ms)
    Nov 30 04:40:17.734: INFO: (11) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 11.29468ms)
    Nov 30 04:40:17.734: INFO: (11) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 11.900454ms)
    Nov 30 04:40:17.734: INFO: (11) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 12.037927ms)
    Nov 30 04:40:17.741: INFO: (12) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 6.545035ms)
    Nov 30 04:40:17.741: INFO: (12) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 6.436932ms)
    Nov 30 04:40:17.741: INFO: (12) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 6.194943ms)
    Nov 30 04:40:17.742: INFO: (12) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 7.435428ms)
    Nov 30 04:40:17.743: INFO: (12) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.839424ms)
    Nov 30 04:40:17.743: INFO: (12) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 8.361805ms)
    Nov 30 04:40:17.743: INFO: (12) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 8.199553ms)
    Nov 30 04:40:17.743: INFO: (12) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 8.690791ms)
    Nov 30 04:40:17.743: INFO: (12) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 8.328646ms)
    Nov 30 04:40:17.744: INFO: (12) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 8.793806ms)
    Nov 30 04:40:17.744: INFO: (12) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 9.340653ms)
    Nov 30 04:40:17.744: INFO: (12) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 9.205052ms)
    Nov 30 04:40:17.745: INFO: (12) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 9.758557ms)
    Nov 30 04:40:17.745: INFO: (12) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 9.954333ms)
    Nov 30 04:40:17.745: INFO: (12) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 9.660689ms)
    Nov 30 04:40:17.745: INFO: (12) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 9.980696ms)
    Nov 30 04:40:17.749: INFO: (13) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 3.698634ms)
    Nov 30 04:40:17.749: INFO: (13) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 3.802307ms)
    Nov 30 04:40:17.749: INFO: (13) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 3.779338ms)
    Nov 30 04:40:17.750: INFO: (13) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 4.115885ms)
    Nov 30 04:40:17.753: INFO: (13) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.109423ms)
    Nov 30 04:40:17.753: INFO: (13) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.324342ms)
    Nov 30 04:40:17.753: INFO: (13) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 7.672348ms)
    Nov 30 04:40:17.755: INFO: (13) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 9.228929ms)
    Nov 30 04:40:17.755: INFO: (13) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 9.864943ms)
    Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 11.409984ms)
    Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 10.903017ms)
    Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 10.802775ms)
    Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 11.087676ms)
    Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 11.709124ms)
    Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 11.458625ms)
    Nov 30 04:40:17.757: INFO: (13) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 11.423737ms)
    Nov 30 04:40:17.763: INFO: (14) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 5.679275ms)
    Nov 30 04:40:17.763: INFO: (14) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 5.481195ms)
    Nov 30 04:40:17.765: INFO: (14) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.407669ms)
    Nov 30 04:40:17.766: INFO: (14) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 7.854022ms)
    Nov 30 04:40:17.766: INFO: (14) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 8.392365ms)
    Nov 30 04:40:17.766: INFO: (14) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 8.804913ms)
    Nov 30 04:40:17.766: INFO: (14) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 8.936553ms)
    Nov 30 04:40:17.767: INFO: (14) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 9.634237ms)
    Nov 30 04:40:17.768: INFO: (14) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 10.537127ms)
    Nov 30 04:40:17.768: INFO: (14) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 10.512654ms)
    Nov 30 04:40:17.768: INFO: (14) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 10.791352ms)
    Nov 30 04:40:17.768: INFO: (14) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 10.585822ms)
    Nov 30 04:40:17.769: INFO: (14) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 11.078127ms)
    Nov 30 04:40:17.769: INFO: (14) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 10.826994ms)
    Nov 30 04:40:17.769: INFO: (14) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 10.866977ms)
    Nov 30 04:40:17.769: INFO: (14) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 11.393574ms)
    Nov 30 04:40:17.782: INFO: (15) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 13.042891ms)
    Nov 30 04:40:17.782: INFO: (15) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 13.198808ms)
    Nov 30 04:40:17.783: INFO: (15) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 13.621849ms)
    Nov 30 04:40:17.783: INFO: (15) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 14.250146ms)
    Nov 30 04:40:17.783: INFO: (15) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 14.081142ms)
    Nov 30 04:40:17.783: INFO: (15) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 14.200613ms)
    Nov 30 04:40:17.784: INFO: (15) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 14.582729ms)
    Nov 30 04:40:17.784: INFO: (15) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 14.745639ms)
    Nov 30 04:40:17.784: INFO: (15) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 14.909071ms)
    Nov 30 04:40:17.785: INFO: (15) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 15.998434ms)
    Nov 30 04:40:17.785: INFO: (15) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 15.717222ms)
    Nov 30 04:40:17.786: INFO: (15) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 16.444423ms)
    Nov 30 04:40:17.786: INFO: (15) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 16.981331ms)
    Nov 30 04:40:17.786: INFO: (15) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 17.267239ms)
    Nov 30 04:40:17.786: INFO: (15) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 17.158385ms)
    Nov 30 04:40:17.788: INFO: (15) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 18.911908ms)
    Nov 30 04:40:17.795: INFO: (16) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 6.616819ms)
    Nov 30 04:40:17.795: INFO: (16) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 6.367139ms)
    Nov 30 04:40:17.796: INFO: (16) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.390949ms)
    Nov 30 04:40:17.796: INFO: (16) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 7.829262ms)
    Nov 30 04:40:17.796: INFO: (16) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 7.797267ms)
    Nov 30 04:40:17.796: INFO: (16) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 8.214793ms)
    Nov 30 04:40:17.796: INFO: (16) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 8.266782ms)
    Nov 30 04:40:17.797: INFO: (16) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 8.537147ms)
    Nov 30 04:40:17.797: INFO: (16) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 8.489445ms)
    Nov 30 04:40:17.801: INFO: (16) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 12.5615ms)
    Nov 30 04:40:17.801: INFO: (16) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 12.810273ms)
    Nov 30 04:40:17.803: INFO: (16) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 14.646865ms)
    Nov 30 04:40:17.803: INFO: (16) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 14.484977ms)
    Nov 30 04:40:17.803: INFO: (16) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 14.758717ms)
    Nov 30 04:40:17.803: INFO: (16) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 14.983945ms)
    Nov 30 04:40:17.803: INFO: (16) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 15.009748ms)
    Nov 30 04:40:17.809: INFO: (17) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 4.985141ms)
    Nov 30 04:40:17.809: INFO: (17) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 4.968631ms)
    Nov 30 04:40:17.809: INFO: (17) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 5.646179ms)
    Nov 30 04:40:17.809: INFO: (17) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 5.384543ms)
    Nov 30 04:40:17.810: INFO: (17) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 5.939841ms)
    Nov 30 04:40:17.810: INFO: (17) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 6.284779ms)
    Nov 30 04:40:17.810: INFO: (17) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 6.008925ms)
    Nov 30 04:40:17.810: INFO: (17) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 6.567893ms)
    Nov 30 04:40:17.812: INFO: (17) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 7.444732ms)
    Nov 30 04:40:17.814: INFO: (17) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 10.645452ms)
    Nov 30 04:40:17.814: INFO: (17) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 10.368658ms)
    Nov 30 04:40:17.815: INFO: (17) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 10.485237ms)
    Nov 30 04:40:17.815: INFO: (17) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 11.099059ms)
    Nov 30 04:40:17.815: INFO: (17) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 11.016352ms)
    Nov 30 04:40:17.817: INFO: (17) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 12.777955ms)
    Nov 30 04:40:17.817: INFO: (17) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 13.662521ms)
    Nov 30 04:40:17.822: INFO: (18) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 4.439612ms)
    Nov 30 04:40:17.822: INFO: (18) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 4.776517ms)
    Nov 30 04:40:17.822: INFO: (18) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 4.692091ms)
    Nov 30 04:40:17.822: INFO: (18) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 4.872464ms)
    Nov 30 04:40:17.823: INFO: (18) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 5.024252ms)
    Nov 30 04:40:17.823: INFO: (18) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 5.230864ms)
    Nov 30 04:40:17.824: INFO: (18) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 6.752748ms)
    Nov 30 04:40:17.825: INFO: (18) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 6.957043ms)
    Nov 30 04:40:17.825: INFO: (18) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 7.270268ms)
    Nov 30 04:40:17.825: INFO: (18) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 7.526512ms)
    Nov 30 04:40:17.825: INFO: (18) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 7.759403ms)
    Nov 30 04:40:17.827: INFO: (18) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 9.090511ms)
    Nov 30 04:40:17.827: INFO: (18) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 9.063335ms)
    Nov 30 04:40:17.828: INFO: (18) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 10.465218ms)
    Nov 30 04:40:17.828: INFO: (18) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 10.448388ms)
    Nov 30 04:40:17.828: INFO: (18) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 10.729761ms)
    Nov 30 04:40:17.834: INFO: (19) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">... (200; 5.503344ms)
    Nov 30 04:40:17.834: INFO: (19) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 5.685829ms)
    Nov 30 04:40:17.834: INFO: (19) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:1080/proxy/rewriteme">test<... (200; 5.947866ms)
    Nov 30 04:40:17.834: INFO: (19) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:462/proxy/: tls qux (200; 6.136243ms)
    Nov 30 04:40:17.835: INFO: (19) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:162/proxy/: bar (200; 6.802576ms)
    Nov 30 04:40:17.835: INFO: (19) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 6.736922ms)
    Nov 30 04:40:17.835: INFO: (19) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname1/proxy/: foo (200; 6.653091ms)
    Nov 30 04:40:17.835: INFO: (19) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:443/proxy/tlsrewritem... (200; 7.055146ms)
    Nov 30 04:40:17.836: INFO: (19) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname1/proxy/: foo (200; 7.290586ms)
    Nov 30 04:40:17.836: INFO: (19) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname1/proxy/: tls baz (200; 7.25947ms)
    Nov 30 04:40:17.838: INFO: (19) /api/v1/namespaces/proxy-6730/services/http:proxy-service-4dq8h:portname2/proxy/: bar (200; 9.495085ms)
    Nov 30 04:40:17.838: INFO: (19) /api/v1/namespaces/proxy-6730/pods/http:proxy-service-4dq8h-rgzfd:160/proxy/: foo (200; 9.481216ms)
    Nov 30 04:40:17.838: INFO: (19) /api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/: <a href="/api/v1/namespaces/proxy-6730/pods/proxy-service-4dq8h-rgzfd/proxy/rewriteme">test</a> (200; 9.751094ms)
    Nov 30 04:40:17.838: INFO: (19) /api/v1/namespaces/proxy-6730/pods/https:proxy-service-4dq8h-rgzfd:460/proxy/: tls baz (200; 9.799072ms)
    Nov 30 04:40:17.838: INFO: (19) /api/v1/namespaces/proxy-6730/services/https:proxy-service-4dq8h:tlsportname2/proxy/: tls qux (200; 9.913829ms)
    Nov 30 04:40:17.839: INFO: (19) /api/v1/namespaces/proxy-6730/services/proxy-service-4dq8h:portname2/proxy/: bar (200; 10.267592ms)
    STEP: deleting ReplicationController proxy-service-4dq8h in namespace proxy-6730, will wait for the garbage collector to delete the pods 11/30/22 04:40:17.839
    Nov 30 04:40:17.897: INFO: Deleting ReplicationController proxy-service-4dq8h took: 5.900922ms
    Nov 30 04:40:17.998: INFO: Terminating ReplicationController proxy-service-4dq8h pods took: 100.935707ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Nov 30 04:40:20.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6730" for this suite. 11/30/22 04:40:20.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:40:20.113
Nov 30 04:40:20.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename containers 11/30/22 04:40:20.114
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:20.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:20.147
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 11/30/22 04:40:20.149
Nov 30 04:40:20.182: INFO: Waiting up to 5m0s for pod "client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b" in namespace "containers-6772" to be "Succeeded or Failed"
Nov 30 04:40:20.194: INFO: Pod "client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.990522ms
Nov 30 04:40:22.197: INFO: Pod "client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014535681s
Nov 30 04:40:24.197: INFO: Pod "client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015268529s
STEP: Saw pod success 11/30/22 04:40:24.197
Nov 30 04:40:24.198: INFO: Pod "client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b" satisfied condition "Succeeded or Failed"
Nov 30 04:40:24.201: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b container agnhost-container: <nil>
STEP: delete the pod 11/30/22 04:40:24.208
Nov 30 04:40:24.223: INFO: Waiting for pod client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b to disappear
Nov 30 04:40:24.225: INFO: Pod client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Nov 30 04:40:24.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6772" for this suite. 11/30/22 04:40:24.23
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":287,"skipped":5179,"failed":0}
------------------------------
• [4.123 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:40:20.113
    Nov 30 04:40:20.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename containers 11/30/22 04:40:20.114
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:20.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:20.147
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 11/30/22 04:40:20.149
    Nov 30 04:40:20.182: INFO: Waiting up to 5m0s for pod "client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b" in namespace "containers-6772" to be "Succeeded or Failed"
    Nov 30 04:40:20.194: INFO: Pod "client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.990522ms
    Nov 30 04:40:22.197: INFO: Pod "client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014535681s
    Nov 30 04:40:24.197: INFO: Pod "client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015268529s
    STEP: Saw pod success 11/30/22 04:40:24.197
    Nov 30 04:40:24.198: INFO: Pod "client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b" satisfied condition "Succeeded or Failed"
    Nov 30 04:40:24.201: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 04:40:24.208
    Nov 30 04:40:24.223: INFO: Waiting for pod client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b to disappear
    Nov 30 04:40:24.225: INFO: Pod client-containers-e8a2b65e-085b-4d67-9a43-5a69056df13b no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Nov 30 04:40:24.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-6772" for this suite. 11/30/22 04:40:24.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:40:24.237
Nov 30 04:40:24.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:40:24.238
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:24.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:24.257
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-bf863be3-684c-46f0-a972-5954a9c16e8b 11/30/22 04:40:24.259
STEP: Creating a pod to test consume configMaps 11/30/22 04:40:24.264
Nov 30 04:40:24.276: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec" in namespace "projected-1085" to be "Succeeded or Failed"
Nov 30 04:40:24.278: INFO: Pod "pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.462867ms
Nov 30 04:40:26.282: INFO: Pod "pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec": Phase="Running", Reason="", readiness=false. Elapsed: 2.006041674s
Nov 30 04:40:28.282: INFO: Pod "pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006022567s
STEP: Saw pod success 11/30/22 04:40:28.282
Nov 30 04:40:28.282: INFO: Pod "pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec" satisfied condition "Succeeded or Failed"
Nov 30 04:40:28.284: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec container agnhost-container: <nil>
STEP: delete the pod 11/30/22 04:40:28.289
Nov 30 04:40:28.303: INFO: Waiting for pod pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec to disappear
Nov 30 04:40:28.305: INFO: Pod pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Nov 30 04:40:28.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1085" for this suite. 11/30/22 04:40:28.31
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":288,"skipped":5195,"failed":0}
------------------------------
• [4.079 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:40:24.237
    Nov 30 04:40:24.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:40:24.238
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:24.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:24.257
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-bf863be3-684c-46f0-a972-5954a9c16e8b 11/30/22 04:40:24.259
    STEP: Creating a pod to test consume configMaps 11/30/22 04:40:24.264
    Nov 30 04:40:24.276: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec" in namespace "projected-1085" to be "Succeeded or Failed"
    Nov 30 04:40:24.278: INFO: Pod "pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.462867ms
    Nov 30 04:40:26.282: INFO: Pod "pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec": Phase="Running", Reason="", readiness=false. Elapsed: 2.006041674s
    Nov 30 04:40:28.282: INFO: Pod "pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006022567s
    STEP: Saw pod success 11/30/22 04:40:28.282
    Nov 30 04:40:28.282: INFO: Pod "pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec" satisfied condition "Succeeded or Failed"
    Nov 30 04:40:28.284: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 04:40:28.289
    Nov 30 04:40:28.303: INFO: Waiting for pod pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec to disappear
    Nov 30 04:40:28.305: INFO: Pod pod-projected-configmaps-1fb76a90-71b4-476c-8c14-6117efbaeaec no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Nov 30 04:40:28.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1085" for this suite. 11/30/22 04:40:28.31
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:40:28.316
Nov 30 04:40:28.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 04:40:28.317
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:28.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:28.335
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 04:40:28.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9673" for this suite. 11/30/22 04:40:28.379
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":289,"skipped":5198,"failed":0}
------------------------------
• [0.068 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:40:28.316
    Nov 30 04:40:28.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 04:40:28.317
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:28.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:28.335
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 04:40:28.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9673" for this suite. 11/30/22 04:40:28.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:40:28.385
Nov 30 04:40:28.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename job 11/30/22 04:40:28.386
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:28.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:28.407
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 11/30/22 04:40:28.409
STEP: Ensuring job reaches completions 11/30/22 04:40:28.421
STEP: Ensuring pods with index for job exist 11/30/22 04:40:38.426
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Nov 30 04:40:38.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-379" for this suite. 11/30/22 04:40:38.434
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":290,"skipped":5222,"failed":0}
------------------------------
• [SLOW TEST] [10.056 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:40:28.385
    Nov 30 04:40:28.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename job 11/30/22 04:40:28.386
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:28.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:28.407
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 11/30/22 04:40:28.409
    STEP: Ensuring job reaches completions 11/30/22 04:40:28.421
    STEP: Ensuring pods with index for job exist 11/30/22 04:40:38.426
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Nov 30 04:40:38.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-379" for this suite. 11/30/22 04:40:38.434
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:40:38.441
Nov 30 04:40:38.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pod-network-test 11/30/22 04:40:38.442
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:38.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:38.46
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7182 11/30/22 04:40:38.462
STEP: creating a selector 11/30/22 04:40:38.462
STEP: Creating the service pods in kubernetes 11/30/22 04:40:38.462
Nov 30 04:40:38.462: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 30 04:40:38.542: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7182" to be "running and ready"
Nov 30 04:40:38.549: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.148459ms
Nov 30 04:40:38.549: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:40:40.554: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01181425s
Nov 30 04:40:40.554: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:40:42.553: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011271827s
Nov 30 04:40:42.553: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:40:44.554: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01259292s
Nov 30 04:40:44.554: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:40:46.556: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013820158s
Nov 30 04:40:46.556: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:40:48.559: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017289131s
Nov 30 04:40:48.559: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:40:50.553: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010894845s
Nov 30 04:40:50.553: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:40:52.553: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011035595s
Nov 30 04:40:52.553: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:40:54.554: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012536775s
Nov 30 04:40:54.554: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:40:56.554: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012093847s
Nov 30 04:40:56.554: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:40:58.554: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011981926s
Nov 30 04:40:58.554: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 30 04:41:00.553: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010877578s
Nov 30 04:41:00.553: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 30 04:41:00.553: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 30 04:41:00.555: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7182" to be "running and ready"
Nov 30 04:41:00.559: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.108967ms
Nov 30 04:41:00.559: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 30 04:41:00.559: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 30 04:41:00.561: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7182" to be "running and ready"
Nov 30 04:41:00.564: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.786358ms
Nov 30 04:41:00.564: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 30 04:41:00.564: INFO: Pod "netserver-2" satisfied condition "running and ready"
Nov 30 04:41:00.567: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7182" to be "running and ready"
Nov 30 04:41:00.570: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.661664ms
Nov 30 04:41:00.570: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Nov 30 04:41:00.570: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 11/30/22 04:41:00.573
Nov 30 04:41:00.607: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7182" to be "running"
Nov 30 04:41:00.610: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.191906ms
Nov 30 04:41:02.615: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007710299s
Nov 30 04:41:02.615: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 30 04:41:02.617: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Nov 30 04:41:02.617: INFO: Breadth first check of 192.168.12.218 on host 10.0.10.8...
Nov 30 04:41:02.620: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.227:9080/dial?request=hostname&protocol=http&host=192.168.12.218&port=8083&tries=1'] Namespace:pod-network-test-7182 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:41:02.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:41:02.621: INFO: ExecWithOptions: Clientset creation
Nov 30 04:41:02.621: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7182/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.12.218%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 30 04:41:02.689: INFO: Waiting for responses: map[]
Nov 30 04:41:02.689: INFO: reached 192.168.12.218 after 0/1 tries
Nov 30 04:41:02.689: INFO: Breadth first check of 192.168.228.156 on host 10.0.10.26...
Nov 30 04:41:02.692: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.227:9080/dial?request=hostname&protocol=http&host=192.168.228.156&port=8083&tries=1'] Namespace:pod-network-test-7182 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:41:02.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:41:02.692: INFO: ExecWithOptions: Clientset creation
Nov 30 04:41:02.692: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7182/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.228.156%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 30 04:41:02.766: INFO: Waiting for responses: map[]
Nov 30 04:41:02.766: INFO: reached 192.168.228.156 after 0/1 tries
Nov 30 04:41:02.766: INFO: Breadth first check of 192.168.251.152 on host 10.0.10.17...
Nov 30 04:41:02.769: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.227:9080/dial?request=hostname&protocol=http&host=192.168.251.152&port=8083&tries=1'] Namespace:pod-network-test-7182 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:41:02.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:41:02.770: INFO: ExecWithOptions: Clientset creation
Nov 30 04:41:02.770: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7182/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.251.152%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 30 04:41:02.853: INFO: Waiting for responses: map[]
Nov 30 04:41:02.853: INFO: reached 192.168.251.152 after 0/1 tries
Nov 30 04:41:02.853: INFO: Breadth first check of 192.168.254.16 on host 10.0.10.2...
Nov 30 04:41:02.856: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.227:9080/dial?request=hostname&protocol=http&host=192.168.254.16&port=8083&tries=1'] Namespace:pod-network-test-7182 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:41:02.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:41:02.857: INFO: ExecWithOptions: Clientset creation
Nov 30 04:41:02.857: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7182/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.254.16%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 30 04:41:02.939: INFO: Waiting for responses: map[]
Nov 30 04:41:02.939: INFO: reached 192.168.254.16 after 0/1 tries
Nov 30 04:41:02.939: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Nov 30 04:41:02.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7182" for this suite. 11/30/22 04:41:02.944
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":291,"skipped":5225,"failed":0}
------------------------------
• [SLOW TEST] [24.509 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:40:38.441
    Nov 30 04:40:38.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pod-network-test 11/30/22 04:40:38.442
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:40:38.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:40:38.46
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7182 11/30/22 04:40:38.462
    STEP: creating a selector 11/30/22 04:40:38.462
    STEP: Creating the service pods in kubernetes 11/30/22 04:40:38.462
    Nov 30 04:40:38.462: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 30 04:40:38.542: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7182" to be "running and ready"
    Nov 30 04:40:38.549: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.148459ms
    Nov 30 04:40:38.549: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:40:40.554: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01181425s
    Nov 30 04:40:40.554: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:40:42.553: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011271827s
    Nov 30 04:40:42.553: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:40:44.554: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01259292s
    Nov 30 04:40:44.554: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:40:46.556: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013820158s
    Nov 30 04:40:46.556: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:40:48.559: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017289131s
    Nov 30 04:40:48.559: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:40:50.553: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010894845s
    Nov 30 04:40:50.553: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:40:52.553: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011035595s
    Nov 30 04:40:52.553: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:40:54.554: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012536775s
    Nov 30 04:40:54.554: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:40:56.554: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012093847s
    Nov 30 04:40:56.554: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:40:58.554: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011981926s
    Nov 30 04:40:58.554: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 30 04:41:00.553: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010877578s
    Nov 30 04:41:00.553: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 30 04:41:00.553: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 30 04:41:00.555: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7182" to be "running and ready"
    Nov 30 04:41:00.559: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.108967ms
    Nov 30 04:41:00.559: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 30 04:41:00.559: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 30 04:41:00.561: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7182" to be "running and ready"
    Nov 30 04:41:00.564: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.786358ms
    Nov 30 04:41:00.564: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 30 04:41:00.564: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Nov 30 04:41:00.567: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7182" to be "running and ready"
    Nov 30 04:41:00.570: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.661664ms
    Nov 30 04:41:00.570: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Nov 30 04:41:00.570: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 11/30/22 04:41:00.573
    Nov 30 04:41:00.607: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7182" to be "running"
    Nov 30 04:41:00.610: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.191906ms
    Nov 30 04:41:02.615: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007710299s
    Nov 30 04:41:02.615: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 30 04:41:02.617: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Nov 30 04:41:02.617: INFO: Breadth first check of 192.168.12.218 on host 10.0.10.8...
    Nov 30 04:41:02.620: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.227:9080/dial?request=hostname&protocol=http&host=192.168.12.218&port=8083&tries=1'] Namespace:pod-network-test-7182 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:41:02.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:41:02.621: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:41:02.621: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7182/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.12.218%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 30 04:41:02.689: INFO: Waiting for responses: map[]
    Nov 30 04:41:02.689: INFO: reached 192.168.12.218 after 0/1 tries
    Nov 30 04:41:02.689: INFO: Breadth first check of 192.168.228.156 on host 10.0.10.26...
    Nov 30 04:41:02.692: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.227:9080/dial?request=hostname&protocol=http&host=192.168.228.156&port=8083&tries=1'] Namespace:pod-network-test-7182 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:41:02.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:41:02.692: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:41:02.692: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7182/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.228.156%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 30 04:41:02.766: INFO: Waiting for responses: map[]
    Nov 30 04:41:02.766: INFO: reached 192.168.228.156 after 0/1 tries
    Nov 30 04:41:02.766: INFO: Breadth first check of 192.168.251.152 on host 10.0.10.17...
    Nov 30 04:41:02.769: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.227:9080/dial?request=hostname&protocol=http&host=192.168.251.152&port=8083&tries=1'] Namespace:pod-network-test-7182 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:41:02.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:41:02.770: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:41:02.770: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7182/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.251.152%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 30 04:41:02.853: INFO: Waiting for responses: map[]
    Nov 30 04:41:02.853: INFO: reached 192.168.251.152 after 0/1 tries
    Nov 30 04:41:02.853: INFO: Breadth first check of 192.168.254.16 on host 10.0.10.2...
    Nov 30 04:41:02.856: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.12.227:9080/dial?request=hostname&protocol=http&host=192.168.254.16&port=8083&tries=1'] Namespace:pod-network-test-7182 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:41:02.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:41:02.857: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:41:02.857: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7182/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.12.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.254.16%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 30 04:41:02.939: INFO: Waiting for responses: map[]
    Nov 30 04:41:02.939: INFO: reached 192.168.254.16 after 0/1 tries
    Nov 30 04:41:02.939: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Nov 30 04:41:02.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-7182" for this suite. 11/30/22 04:41:02.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:41:02.95
Nov 30 04:41:02.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 04:41:02.951
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:02.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:02.973
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 11/30/22 04:41:02.976
Nov 30 04:41:02.992: INFO: Waiting up to 5m0s for pod "pod-572d3b51-17a3-417f-8879-42f068e23273" in namespace "emptydir-8045" to be "Succeeded or Failed"
Nov 30 04:41:02.995: INFO: Pod "pod-572d3b51-17a3-417f-8879-42f068e23273": Phase="Pending", Reason="", readiness=false. Elapsed: 2.800863ms
Nov 30 04:41:04.999: INFO: Pod "pod-572d3b51-17a3-417f-8879-42f068e23273": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007173739s
Nov 30 04:41:06.998: INFO: Pod "pod-572d3b51-17a3-417f-8879-42f068e23273": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006259596s
STEP: Saw pod success 11/30/22 04:41:06.998
Nov 30 04:41:06.998: INFO: Pod "pod-572d3b51-17a3-417f-8879-42f068e23273" satisfied condition "Succeeded or Failed"
Nov 30 04:41:07.001: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-572d3b51-17a3-417f-8879-42f068e23273 container test-container: <nil>
STEP: delete the pod 11/30/22 04:41:07.005
Nov 30 04:41:07.014: INFO: Waiting for pod pod-572d3b51-17a3-417f-8879-42f068e23273 to disappear
Nov 30 04:41:07.017: INFO: Pod pod-572d3b51-17a3-417f-8879-42f068e23273 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 04:41:07.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8045" for this suite. 11/30/22 04:41:07.021
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":292,"skipped":5230,"failed":0}
------------------------------
• [4.075 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:41:02.95
    Nov 30 04:41:02.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 04:41:02.951
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:02.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:02.973
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 11/30/22 04:41:02.976
    Nov 30 04:41:02.992: INFO: Waiting up to 5m0s for pod "pod-572d3b51-17a3-417f-8879-42f068e23273" in namespace "emptydir-8045" to be "Succeeded or Failed"
    Nov 30 04:41:02.995: INFO: Pod "pod-572d3b51-17a3-417f-8879-42f068e23273": Phase="Pending", Reason="", readiness=false. Elapsed: 2.800863ms
    Nov 30 04:41:04.999: INFO: Pod "pod-572d3b51-17a3-417f-8879-42f068e23273": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007173739s
    Nov 30 04:41:06.998: INFO: Pod "pod-572d3b51-17a3-417f-8879-42f068e23273": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006259596s
    STEP: Saw pod success 11/30/22 04:41:06.998
    Nov 30 04:41:06.998: INFO: Pod "pod-572d3b51-17a3-417f-8879-42f068e23273" satisfied condition "Succeeded or Failed"
    Nov 30 04:41:07.001: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-572d3b51-17a3-417f-8879-42f068e23273 container test-container: <nil>
    STEP: delete the pod 11/30/22 04:41:07.005
    Nov 30 04:41:07.014: INFO: Waiting for pod pod-572d3b51-17a3-417f-8879-42f068e23273 to disappear
    Nov 30 04:41:07.017: INFO: Pod pod-572d3b51-17a3-417f-8879-42f068e23273 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 04:41:07.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8045" for this suite. 11/30/22 04:41:07.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:41:07.026
Nov 30 04:41:07.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename dns 11/30/22 04:41:07.026
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:07.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:07.049
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1564.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1564.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 11/30/22 04:41:07.051
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1564.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1564.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 11/30/22 04:41:07.051
STEP: creating a pod to probe /etc/hosts 11/30/22 04:41:07.051
STEP: submitting the pod to kubernetes 11/30/22 04:41:07.051
Nov 30 04:41:07.067: INFO: Waiting up to 15m0s for pod "dns-test-eb2539f9-571d-4f69-b983-76ca49026032" in namespace "dns-1564" to be "running"
Nov 30 04:41:07.071: INFO: Pod "dns-test-eb2539f9-571d-4f69-b983-76ca49026032": Phase="Pending", Reason="", readiness=false. Elapsed: 3.518188ms
Nov 30 04:41:09.074: INFO: Pod "dns-test-eb2539f9-571d-4f69-b983-76ca49026032": Phase="Running", Reason="", readiness=true. Elapsed: 2.007082096s
Nov 30 04:41:09.074: INFO: Pod "dns-test-eb2539f9-571d-4f69-b983-76ca49026032" satisfied condition "running"
STEP: retrieving the pod 11/30/22 04:41:09.074
STEP: looking for the results for each expected name from probers 11/30/22 04:41:09.078
Nov 30 04:41:09.090: INFO: DNS probes using dns-1564/dns-test-eb2539f9-571d-4f69-b983-76ca49026032 succeeded

STEP: deleting the pod 11/30/22 04:41:09.09
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Nov 30 04:41:09.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1564" for this suite. 11/30/22 04:41:09.104
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":293,"skipped":5238,"failed":0}
------------------------------
• [2.083 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:41:07.026
    Nov 30 04:41:07.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename dns 11/30/22 04:41:07.026
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:07.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:07.049
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1564.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1564.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     11/30/22 04:41:07.051
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1564.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1564.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     11/30/22 04:41:07.051
    STEP: creating a pod to probe /etc/hosts 11/30/22 04:41:07.051
    STEP: submitting the pod to kubernetes 11/30/22 04:41:07.051
    Nov 30 04:41:07.067: INFO: Waiting up to 15m0s for pod "dns-test-eb2539f9-571d-4f69-b983-76ca49026032" in namespace "dns-1564" to be "running"
    Nov 30 04:41:07.071: INFO: Pod "dns-test-eb2539f9-571d-4f69-b983-76ca49026032": Phase="Pending", Reason="", readiness=false. Elapsed: 3.518188ms
    Nov 30 04:41:09.074: INFO: Pod "dns-test-eb2539f9-571d-4f69-b983-76ca49026032": Phase="Running", Reason="", readiness=true. Elapsed: 2.007082096s
    Nov 30 04:41:09.074: INFO: Pod "dns-test-eb2539f9-571d-4f69-b983-76ca49026032" satisfied condition "running"
    STEP: retrieving the pod 11/30/22 04:41:09.074
    STEP: looking for the results for each expected name from probers 11/30/22 04:41:09.078
    Nov 30 04:41:09.090: INFO: DNS probes using dns-1564/dns-test-eb2539f9-571d-4f69-b983-76ca49026032 succeeded

    STEP: deleting the pod 11/30/22 04:41:09.09
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Nov 30 04:41:09.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1564" for this suite. 11/30/22 04:41:09.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:41:09.109
Nov 30 04:41:09.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 04:41:09.11
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:09.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:09.128
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 11/30/22 04:41:09.131
Nov 30 04:41:09.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728" in namespace "downward-api-4043" to be "Succeeded or Failed"
Nov 30 04:41:09.148: INFO: Pod "downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728": Phase="Pending", Reason="", readiness=false. Elapsed: 3.17315ms
Nov 30 04:41:11.152: INFO: Pod "downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007598297s
Nov 30 04:41:13.153: INFO: Pod "downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008520679s
STEP: Saw pod success 11/30/22 04:41:13.153
Nov 30 04:41:13.153: INFO: Pod "downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728" satisfied condition "Succeeded or Failed"
Nov 30 04:41:13.156: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728 container client-container: <nil>
STEP: delete the pod 11/30/22 04:41:13.161
Nov 30 04:41:13.173: INFO: Waiting for pod downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728 to disappear
Nov 30 04:41:13.176: INFO: Pod downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Nov 30 04:41:13.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4043" for this suite. 11/30/22 04:41:13.179
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":294,"skipped":5245,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:41:09.109
    Nov 30 04:41:09.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 04:41:09.11
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:09.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:09.128
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 11/30/22 04:41:09.131
    Nov 30 04:41:09.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728" in namespace "downward-api-4043" to be "Succeeded or Failed"
    Nov 30 04:41:09.148: INFO: Pod "downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728": Phase="Pending", Reason="", readiness=false. Elapsed: 3.17315ms
    Nov 30 04:41:11.152: INFO: Pod "downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007598297s
    Nov 30 04:41:13.153: INFO: Pod "downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008520679s
    STEP: Saw pod success 11/30/22 04:41:13.153
    Nov 30 04:41:13.153: INFO: Pod "downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728" satisfied condition "Succeeded or Failed"
    Nov 30 04:41:13.156: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728 container client-container: <nil>
    STEP: delete the pod 11/30/22 04:41:13.161
    Nov 30 04:41:13.173: INFO: Waiting for pod downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728 to disappear
    Nov 30 04:41:13.176: INFO: Pod downwardapi-volume-c589cd65-df9b-403e-b070-f5731de71728 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Nov 30 04:41:13.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4043" for this suite. 11/30/22 04:41:13.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:41:13.188
Nov 30 04:41:13.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename tables 11/30/22 04:41:13.189
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:13.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:13.208
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Nov 30 04:41:13.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1015" for this suite. 11/30/22 04:41:13.221
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":295,"skipped":5253,"failed":0}
------------------------------
• [0.038 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:41:13.188
    Nov 30 04:41:13.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename tables 11/30/22 04:41:13.189
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:13.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:13.208
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Nov 30 04:41:13.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-1015" for this suite. 11/30/22 04:41:13.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:41:13.227
Nov 30 04:41:13.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename watch 11/30/22 04:41:13.228
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:13.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:13.245
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 11/30/22 04:41:13.248
STEP: creating a new configmap 11/30/22 04:41:13.248
STEP: modifying the configmap once 11/30/22 04:41:13.252
STEP: changing the label value of the configmap 11/30/22 04:41:13.261
STEP: Expecting to observe a delete notification for the watched object 11/30/22 04:41:13.267
Nov 30 04:41:13.268: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7256  0695480e-0888-4db6-acd8-8807b09ba98a 58427 0 2022-11-30 04:41:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-11-30 04:41:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:41:13.268: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7256  0695480e-0888-4db6-acd8-8807b09ba98a 58429 0 2022-11-30 04:41:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-11-30 04:41:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:41:13.268: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7256  0695480e-0888-4db6-acd8-8807b09ba98a 58431 0 2022-11-30 04:41:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-11-30 04:41:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 11/30/22 04:41:13.268
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 11/30/22 04:41:13.275
STEP: changing the label value of the configmap back 11/30/22 04:41:23.276
STEP: modifying the configmap a third time 11/30/22 04:41:23.284
STEP: deleting the configmap 11/30/22 04:41:23.291
STEP: Expecting to observe an add notification for the watched object when the label value was restored 11/30/22 04:41:23.296
Nov 30 04:41:23.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7256  0695480e-0888-4db6-acd8-8807b09ba98a 58518 0 2022-11-30 04:41:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-11-30 04:41:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:41:23.297: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7256  0695480e-0888-4db6-acd8-8807b09ba98a 58519 0 2022-11-30 04:41:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-11-30 04:41:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:41:23.297: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7256  0695480e-0888-4db6-acd8-8807b09ba98a 58520 0 2022-11-30 04:41:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-11-30 04:41:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Nov 30 04:41:23.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7256" for this suite. 11/30/22 04:41:23.301
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":296,"skipped":5284,"failed":0}
------------------------------
• [SLOW TEST] [10.080 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:41:13.227
    Nov 30 04:41:13.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename watch 11/30/22 04:41:13.228
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:13.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:13.245
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 11/30/22 04:41:13.248
    STEP: creating a new configmap 11/30/22 04:41:13.248
    STEP: modifying the configmap once 11/30/22 04:41:13.252
    STEP: changing the label value of the configmap 11/30/22 04:41:13.261
    STEP: Expecting to observe a delete notification for the watched object 11/30/22 04:41:13.267
    Nov 30 04:41:13.268: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7256  0695480e-0888-4db6-acd8-8807b09ba98a 58427 0 2022-11-30 04:41:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-11-30 04:41:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:41:13.268: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7256  0695480e-0888-4db6-acd8-8807b09ba98a 58429 0 2022-11-30 04:41:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-11-30 04:41:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:41:13.268: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7256  0695480e-0888-4db6-acd8-8807b09ba98a 58431 0 2022-11-30 04:41:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-11-30 04:41:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 11/30/22 04:41:13.268
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 11/30/22 04:41:13.275
    STEP: changing the label value of the configmap back 11/30/22 04:41:23.276
    STEP: modifying the configmap a third time 11/30/22 04:41:23.284
    STEP: deleting the configmap 11/30/22 04:41:23.291
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 11/30/22 04:41:23.296
    Nov 30 04:41:23.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7256  0695480e-0888-4db6-acd8-8807b09ba98a 58518 0 2022-11-30 04:41:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-11-30 04:41:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:41:23.297: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7256  0695480e-0888-4db6-acd8-8807b09ba98a 58519 0 2022-11-30 04:41:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-11-30 04:41:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:41:23.297: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7256  0695480e-0888-4db6-acd8-8807b09ba98a 58520 0 2022-11-30 04:41:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-11-30 04:41:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Nov 30 04:41:23.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7256" for this suite. 11/30/22 04:41:23.301
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:41:23.307
Nov 30 04:41:23.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 04:41:23.308
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:23.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:23.332
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-7727/configmap-test-1f7de888-1527-4a6e-9055-23d309c4d0c7 11/30/22 04:41:23.335
STEP: Creating a pod to test consume configMaps 11/30/22 04:41:23.34
Nov 30 04:41:23.373: INFO: Waiting up to 5m0s for pod "pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10" in namespace "configmap-7727" to be "Succeeded or Failed"
Nov 30 04:41:23.375: INFO: Pod "pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67562ms
Nov 30 04:41:25.380: INFO: Pod "pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007497192s
Nov 30 04:41:27.380: INFO: Pod "pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007639619s
STEP: Saw pod success 11/30/22 04:41:27.38
Nov 30 04:41:27.380: INFO: Pod "pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10" satisfied condition "Succeeded or Failed"
Nov 30 04:41:27.383: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10 container env-test: <nil>
STEP: delete the pod 11/30/22 04:41:27.389
Nov 30 04:41:27.400: INFO: Waiting for pod pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10 to disappear
Nov 30 04:41:27.402: INFO: Pod pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 04:41:27.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7727" for this suite. 11/30/22 04:41:27.407
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":297,"skipped":5287,"failed":0}
------------------------------
• [4.105 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:41:23.307
    Nov 30 04:41:23.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 04:41:23.308
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:23.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:23.332
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-7727/configmap-test-1f7de888-1527-4a6e-9055-23d309c4d0c7 11/30/22 04:41:23.335
    STEP: Creating a pod to test consume configMaps 11/30/22 04:41:23.34
    Nov 30 04:41:23.373: INFO: Waiting up to 5m0s for pod "pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10" in namespace "configmap-7727" to be "Succeeded or Failed"
    Nov 30 04:41:23.375: INFO: Pod "pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67562ms
    Nov 30 04:41:25.380: INFO: Pod "pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007497192s
    Nov 30 04:41:27.380: INFO: Pod "pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007639619s
    STEP: Saw pod success 11/30/22 04:41:27.38
    Nov 30 04:41:27.380: INFO: Pod "pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10" satisfied condition "Succeeded or Failed"
    Nov 30 04:41:27.383: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10 container env-test: <nil>
    STEP: delete the pod 11/30/22 04:41:27.389
    Nov 30 04:41:27.400: INFO: Waiting for pod pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10 to disappear
    Nov 30 04:41:27.402: INFO: Pod pod-configmaps-0a52c2da-dbfc-43b9-b3b9-f59539465c10 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 04:41:27.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7727" for this suite. 11/30/22 04:41:27.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:41:27.417
Nov 30 04:41:27.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 04:41:27.418
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:27.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:27.438
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 11/30/22 04:41:27.44
Nov 30 04:41:27.440: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Nov 30 04:41:27.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 create -f -'
Nov 30 04:41:27.992: INFO: stderr: ""
Nov 30 04:41:27.992: INFO: stdout: "service/agnhost-replica created\n"
Nov 30 04:41:27.992: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Nov 30 04:41:27.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 create -f -'
Nov 30 04:41:28.460: INFO: stderr: ""
Nov 30 04:41:28.460: INFO: stdout: "service/agnhost-primary created\n"
Nov 30 04:41:28.460: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 30 04:41:28.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 create -f -'
Nov 30 04:41:29.015: INFO: stderr: ""
Nov 30 04:41:29.015: INFO: stdout: "service/frontend created\n"
Nov 30 04:41:29.016: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov 30 04:41:29.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 create -f -'
Nov 30 04:41:29.184: INFO: stderr: ""
Nov 30 04:41:29.184: INFO: stdout: "deployment.apps/frontend created\n"
Nov 30 04:41:29.184: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 30 04:41:29.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 create -f -'
Nov 30 04:41:29.357: INFO: stderr: ""
Nov 30 04:41:29.357: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Nov 30 04:41:29.358: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 30 04:41:29.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 create -f -'
Nov 30 04:41:29.522: INFO: stderr: ""
Nov 30 04:41:29.522: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 11/30/22 04:41:29.522
Nov 30 04:41:29.522: INFO: Waiting for all frontend pods to be Running.
Nov 30 04:41:34.573: INFO: Waiting for frontend to serve content.
Nov 30 04:41:34.583: INFO: Trying to add a new entry to the guestbook.
Nov 30 04:41:34.591: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 11/30/22 04:41:34.598
Nov 30 04:41:34.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 delete --grace-period=0 --force -f -'
Nov 30 04:41:34.687: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 30 04:41:34.687: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 11/30/22 04:41:34.687
Nov 30 04:41:34.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 delete --grace-period=0 --force -f -'
Nov 30 04:41:34.786: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 30 04:41:34.786: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 11/30/22 04:41:34.786
Nov 30 04:41:34.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 delete --grace-period=0 --force -f -'
Nov 30 04:41:34.880: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 30 04:41:34.881: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 11/30/22 04:41:34.881
Nov 30 04:41:34.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 delete --grace-period=0 --force -f -'
Nov 30 04:41:34.949: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 30 04:41:34.949: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 11/30/22 04:41:34.949
Nov 30 04:41:34.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 delete --grace-period=0 --force -f -'
Nov 30 04:41:35.012: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 30 04:41:35.012: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 11/30/22 04:41:35.013
Nov 30 04:41:35.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 delete --grace-period=0 --force -f -'
Nov 30 04:41:35.098: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 30 04:41:35.098: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 04:41:35.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8927" for this suite. 11/30/22 04:41:35.106
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":298,"skipped":5339,"failed":0}
------------------------------
• [SLOW TEST] [7.694 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:41:27.417
    Nov 30 04:41:27.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 04:41:27.418
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:27.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:27.438
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 11/30/22 04:41:27.44
    Nov 30 04:41:27.440: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Nov 30 04:41:27.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 create -f -'
    Nov 30 04:41:27.992: INFO: stderr: ""
    Nov 30 04:41:27.992: INFO: stdout: "service/agnhost-replica created\n"
    Nov 30 04:41:27.992: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Nov 30 04:41:27.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 create -f -'
    Nov 30 04:41:28.460: INFO: stderr: ""
    Nov 30 04:41:28.460: INFO: stdout: "service/agnhost-primary created\n"
    Nov 30 04:41:28.460: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Nov 30 04:41:28.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 create -f -'
    Nov 30 04:41:29.015: INFO: stderr: ""
    Nov 30 04:41:29.015: INFO: stdout: "service/frontend created\n"
    Nov 30 04:41:29.016: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Nov 30 04:41:29.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 create -f -'
    Nov 30 04:41:29.184: INFO: stderr: ""
    Nov 30 04:41:29.184: INFO: stdout: "deployment.apps/frontend created\n"
    Nov 30 04:41:29.184: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Nov 30 04:41:29.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 create -f -'
    Nov 30 04:41:29.357: INFO: stderr: ""
    Nov 30 04:41:29.357: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Nov 30 04:41:29.358: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Nov 30 04:41:29.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 create -f -'
    Nov 30 04:41:29.522: INFO: stderr: ""
    Nov 30 04:41:29.522: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 11/30/22 04:41:29.522
    Nov 30 04:41:29.522: INFO: Waiting for all frontend pods to be Running.
    Nov 30 04:41:34.573: INFO: Waiting for frontend to serve content.
    Nov 30 04:41:34.583: INFO: Trying to add a new entry to the guestbook.
    Nov 30 04:41:34.591: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 11/30/22 04:41:34.598
    Nov 30 04:41:34.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 delete --grace-period=0 --force -f -'
    Nov 30 04:41:34.687: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 30 04:41:34.687: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 11/30/22 04:41:34.687
    Nov 30 04:41:34.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 delete --grace-period=0 --force -f -'
    Nov 30 04:41:34.786: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 30 04:41:34.786: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 11/30/22 04:41:34.786
    Nov 30 04:41:34.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 delete --grace-period=0 --force -f -'
    Nov 30 04:41:34.880: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 30 04:41:34.881: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 11/30/22 04:41:34.881
    Nov 30 04:41:34.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 delete --grace-period=0 --force -f -'
    Nov 30 04:41:34.949: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 30 04:41:34.949: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 11/30/22 04:41:34.949
    Nov 30 04:41:34.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 delete --grace-period=0 --force -f -'
    Nov 30 04:41:35.012: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 30 04:41:35.012: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 11/30/22 04:41:35.013
    Nov 30 04:41:35.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-8927 delete --grace-period=0 --force -f -'
    Nov 30 04:41:35.098: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 30 04:41:35.098: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 04:41:35.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8927" for this suite. 11/30/22 04:41:35.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:41:35.113
Nov 30 04:41:35.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename proxy 11/30/22 04:41:35.114
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:35.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:35.138
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Nov 30 04:41:35.141: INFO: Creating pod...
Nov 30 04:41:35.168: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9534" to be "running"
Nov 30 04:41:35.172: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.160895ms
Nov 30 04:41:37.175: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00644946s
Nov 30 04:41:39.176: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.007854547s
Nov 30 04:41:39.176: INFO: Pod "agnhost" satisfied condition "running"
Nov 30 04:41:39.176: INFO: Creating service...
Nov 30 04:41:39.188: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/DELETE
Nov 30 04:41:39.200: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 30 04:41:39.200: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/GET
Nov 30 04:41:39.214: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 30 04:41:39.214: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/HEAD
Nov 30 04:41:39.220: INFO: http.Client request:HEAD | StatusCode:200
Nov 30 04:41:39.220: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/OPTIONS
Nov 30 04:41:39.223: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 30 04:41:39.223: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/PATCH
Nov 30 04:41:39.227: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 30 04:41:39.227: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/POST
Nov 30 04:41:39.238: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 30 04:41:39.238: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/PUT
Nov 30 04:41:39.242: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 30 04:41:39.242: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/DELETE
Nov 30 04:41:39.248: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 30 04:41:39.248: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/GET
Nov 30 04:41:39.253: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 30 04:41:39.253: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/HEAD
Nov 30 04:41:39.260: INFO: http.Client request:HEAD | StatusCode:200
Nov 30 04:41:39.260: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/OPTIONS
Nov 30 04:41:39.265: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 30 04:41:39.265: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/PATCH
Nov 30 04:41:39.273: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 30 04:41:39.273: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/POST
Nov 30 04:41:39.277: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 30 04:41:39.277: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/PUT
Nov 30 04:41:39.283: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Nov 30 04:41:39.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9534" for this suite. 11/30/22 04:41:39.293
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":299,"skipped":5386,"failed":0}
------------------------------
• [4.186 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:41:35.113
    Nov 30 04:41:35.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename proxy 11/30/22 04:41:35.114
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:35.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:35.138
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Nov 30 04:41:35.141: INFO: Creating pod...
    Nov 30 04:41:35.168: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9534" to be "running"
    Nov 30 04:41:35.172: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.160895ms
    Nov 30 04:41:37.175: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00644946s
    Nov 30 04:41:39.176: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.007854547s
    Nov 30 04:41:39.176: INFO: Pod "agnhost" satisfied condition "running"
    Nov 30 04:41:39.176: INFO: Creating service...
    Nov 30 04:41:39.188: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/DELETE
    Nov 30 04:41:39.200: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 30 04:41:39.200: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/GET
    Nov 30 04:41:39.214: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Nov 30 04:41:39.214: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/HEAD
    Nov 30 04:41:39.220: INFO: http.Client request:HEAD | StatusCode:200
    Nov 30 04:41:39.220: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/OPTIONS
    Nov 30 04:41:39.223: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 30 04:41:39.223: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/PATCH
    Nov 30 04:41:39.227: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 30 04:41:39.227: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/POST
    Nov 30 04:41:39.238: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 30 04:41:39.238: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/pods/agnhost/proxy/some/path/with/PUT
    Nov 30 04:41:39.242: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Nov 30 04:41:39.242: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/DELETE
    Nov 30 04:41:39.248: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 30 04:41:39.248: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/GET
    Nov 30 04:41:39.253: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Nov 30 04:41:39.253: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/HEAD
    Nov 30 04:41:39.260: INFO: http.Client request:HEAD | StatusCode:200
    Nov 30 04:41:39.260: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/OPTIONS
    Nov 30 04:41:39.265: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 30 04:41:39.265: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/PATCH
    Nov 30 04:41:39.273: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 30 04:41:39.273: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/POST
    Nov 30 04:41:39.277: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 30 04:41:39.277: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9534/services/test-service/proxy/some/path/with/PUT
    Nov 30 04:41:39.283: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Nov 30 04:41:39.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-9534" for this suite. 11/30/22 04:41:39.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:41:39.3
Nov 30 04:41:39.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename namespaces 11/30/22 04:41:39.301
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:39.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:39.336
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 11/30/22 04:41:39.339
STEP: patching the Namespace 11/30/22 04:41:39.36
STEP: get the Namespace and ensuring it has the label 11/30/22 04:41:39.368
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Nov 30 04:41:39.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-199" for this suite. 11/30/22 04:41:39.381
STEP: Destroying namespace "nspatchtest-0e36591e-f6f8-4cb4-96f5-da3a0ba01aa0-601" for this suite. 11/30/22 04:41:39.386
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":300,"skipped":5423,"failed":0}
------------------------------
• [0.095 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:41:39.3
    Nov 30 04:41:39.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename namespaces 11/30/22 04:41:39.301
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:39.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:39.336
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 11/30/22 04:41:39.339
    STEP: patching the Namespace 11/30/22 04:41:39.36
    STEP: get the Namespace and ensuring it has the label 11/30/22 04:41:39.368
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 04:41:39.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-199" for this suite. 11/30/22 04:41:39.381
    STEP: Destroying namespace "nspatchtest-0e36591e-f6f8-4cb4-96f5-da3a0ba01aa0-601" for this suite. 11/30/22 04:41:39.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:41:39.395
Nov 30 04:41:39.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pods 11/30/22 04:41:39.396
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:39.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:39.42
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 11/30/22 04:41:39.422
STEP: submitting the pod to kubernetes 11/30/22 04:41:39.422
Nov 30 04:41:39.435: INFO: Waiting up to 5m0s for pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe" in namespace "pods-1712" to be "running and ready"
Nov 30 04:41:39.438: INFO: Pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369965ms
Nov 30 04:41:39.438: INFO: The phase of Pod pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:41:41.441: INFO: Pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.006620101s
Nov 30 04:41:41.442: INFO: The phase of Pod pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe is Running (Ready = true)
Nov 30 04:41:41.442: INFO: Pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 11/30/22 04:41:41.444
STEP: updating the pod 11/30/22 04:41:41.447
Nov 30 04:41:41.960: INFO: Successfully updated pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe"
Nov 30 04:41:41.960: INFO: Waiting up to 5m0s for pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe" in namespace "pods-1712" to be "running"
Nov 30 04:41:41.965: INFO: Pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe": Phase="Running", Reason="", readiness=true. Elapsed: 5.051732ms
Nov 30 04:41:41.965: INFO: Pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 11/30/22 04:41:41.965
Nov 30 04:41:41.968: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Nov 30 04:41:41.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1712" for this suite. 11/30/22 04:41:41.975
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":301,"skipped":5437,"failed":0}
------------------------------
• [2.587 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:41:39.395
    Nov 30 04:41:39.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pods 11/30/22 04:41:39.396
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:39.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:39.42
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 11/30/22 04:41:39.422
    STEP: submitting the pod to kubernetes 11/30/22 04:41:39.422
    Nov 30 04:41:39.435: INFO: Waiting up to 5m0s for pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe" in namespace "pods-1712" to be "running and ready"
    Nov 30 04:41:39.438: INFO: Pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369965ms
    Nov 30 04:41:39.438: INFO: The phase of Pod pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:41:41.441: INFO: Pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.006620101s
    Nov 30 04:41:41.442: INFO: The phase of Pod pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe is Running (Ready = true)
    Nov 30 04:41:41.442: INFO: Pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 11/30/22 04:41:41.444
    STEP: updating the pod 11/30/22 04:41:41.447
    Nov 30 04:41:41.960: INFO: Successfully updated pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe"
    Nov 30 04:41:41.960: INFO: Waiting up to 5m0s for pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe" in namespace "pods-1712" to be "running"
    Nov 30 04:41:41.965: INFO: Pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe": Phase="Running", Reason="", readiness=true. Elapsed: 5.051732ms
    Nov 30 04:41:41.965: INFO: Pod "pod-update-1b20cfb6-de8c-4d2e-9367-c8e97c7733fe" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 11/30/22 04:41:41.965
    Nov 30 04:41:41.968: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Nov 30 04:41:41.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1712" for this suite. 11/30/22 04:41:41.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:41:41.983
Nov 30 04:41:41.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename namespaces 11/30/22 04:41:41.984
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:42.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:42.013
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 11/30/22 04:41:42.016
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:42.084
STEP: Creating a service in the namespace 11/30/22 04:41:42.086
STEP: Deleting the namespace 11/30/22 04:41:42.114
STEP: Waiting for the namespace to be removed. 11/30/22 04:41:42.124
STEP: Recreating the namespace 11/30/22 04:41:48.128
STEP: Verifying there is no service in the namespace 11/30/22 04:41:48.176
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Nov 30 04:41:48.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7632" for this suite. 11/30/22 04:41:48.19
STEP: Destroying namespace "nsdeletetest-8049" for this suite. 11/30/22 04:41:48.199
Nov 30 04:41:48.202: INFO: Namespace nsdeletetest-8049 was already deleted
STEP: Destroying namespace "nsdeletetest-7096" for this suite. 11/30/22 04:41:48.202
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":302,"skipped":5454,"failed":0}
------------------------------
• [SLOW TEST] [6.225 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:41:41.983
    Nov 30 04:41:41.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename namespaces 11/30/22 04:41:41.984
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:42.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:42.013
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 11/30/22 04:41:42.016
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:42.084
    STEP: Creating a service in the namespace 11/30/22 04:41:42.086
    STEP: Deleting the namespace 11/30/22 04:41:42.114
    STEP: Waiting for the namespace to be removed. 11/30/22 04:41:42.124
    STEP: Recreating the namespace 11/30/22 04:41:48.128
    STEP: Verifying there is no service in the namespace 11/30/22 04:41:48.176
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 04:41:48.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-7632" for this suite. 11/30/22 04:41:48.19
    STEP: Destroying namespace "nsdeletetest-8049" for this suite. 11/30/22 04:41:48.199
    Nov 30 04:41:48.202: INFO: Namespace nsdeletetest-8049 was already deleted
    STEP: Destroying namespace "nsdeletetest-7096" for this suite. 11/30/22 04:41:48.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:41:48.208
Nov 30 04:41:48.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename gc 11/30/22 04:41:48.209
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:48.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:48.231
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 11/30/22 04:41:48.237
STEP: delete the rc 11/30/22 04:41:53.255
STEP: wait for the rc to be deleted 11/30/22 04:41:53.263
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 11/30/22 04:41:58.267
STEP: Gathering metrics 11/30/22 04:42:28.278
Nov 30 04:42:28.359: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
Nov 30 04:42:28.380: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 20.645303ms
Nov 30 04:42:28.380: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
Nov 30 04:42:28.380: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
Nov 30 04:42:28.432: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 30 04:42:28.432: INFO: Deleting pod "simpletest.rc-2f7r4" in namespace "gc-3468"
Nov 30 04:42:28.446: INFO: Deleting pod "simpletest.rc-2hdl2" in namespace "gc-3468"
Nov 30 04:42:28.456: INFO: Deleting pod "simpletest.rc-2qhfq" in namespace "gc-3468"
Nov 30 04:42:28.469: INFO: Deleting pod "simpletest.rc-2s2v6" in namespace "gc-3468"
Nov 30 04:42:28.479: INFO: Deleting pod "simpletest.rc-42xwj" in namespace "gc-3468"
Nov 30 04:42:28.491: INFO: Deleting pod "simpletest.rc-4c77c" in namespace "gc-3468"
Nov 30 04:42:28.505: INFO: Deleting pod "simpletest.rc-4nbsx" in namespace "gc-3468"
Nov 30 04:42:28.521: INFO: Deleting pod "simpletest.rc-4tfzw" in namespace "gc-3468"
Nov 30 04:42:28.536: INFO: Deleting pod "simpletest.rc-4xl9f" in namespace "gc-3468"
Nov 30 04:42:28.546: INFO: Deleting pod "simpletest.rc-57vtz" in namespace "gc-3468"
Nov 30 04:42:28.558: INFO: Deleting pod "simpletest.rc-5x5sv" in namespace "gc-3468"
Nov 30 04:42:28.574: INFO: Deleting pod "simpletest.rc-64nvx" in namespace "gc-3468"
Nov 30 04:42:28.587: INFO: Deleting pod "simpletest.rc-6kmqd" in namespace "gc-3468"
Nov 30 04:42:28.601: INFO: Deleting pod "simpletest.rc-6lr4q" in namespace "gc-3468"
Nov 30 04:42:28.617: INFO: Deleting pod "simpletest.rc-6wk2m" in namespace "gc-3468"
Nov 30 04:42:28.632: INFO: Deleting pod "simpletest.rc-7cccq" in namespace "gc-3468"
Nov 30 04:42:28.643: INFO: Deleting pod "simpletest.rc-7mhwr" in namespace "gc-3468"
Nov 30 04:42:28.657: INFO: Deleting pod "simpletest.rc-7rqnz" in namespace "gc-3468"
Nov 30 04:42:28.676: INFO: Deleting pod "simpletest.rc-7s2bt" in namespace "gc-3468"
Nov 30 04:42:28.694: INFO: Deleting pod "simpletest.rc-7tb7q" in namespace "gc-3468"
Nov 30 04:42:28.707: INFO: Deleting pod "simpletest.rc-8926n" in namespace "gc-3468"
Nov 30 04:42:28.720: INFO: Deleting pod "simpletest.rc-8bjdm" in namespace "gc-3468"
Nov 30 04:42:28.734: INFO: Deleting pod "simpletest.rc-8d8sd" in namespace "gc-3468"
Nov 30 04:42:28.751: INFO: Deleting pod "simpletest.rc-8ltw5" in namespace "gc-3468"
Nov 30 04:42:28.767: INFO: Deleting pod "simpletest.rc-8n962" in namespace "gc-3468"
Nov 30 04:42:28.786: INFO: Deleting pod "simpletest.rc-8nj8x" in namespace "gc-3468"
Nov 30 04:42:28.806: INFO: Deleting pod "simpletest.rc-8nnjf" in namespace "gc-3468"
Nov 30 04:42:28.840: INFO: Deleting pod "simpletest.rc-8ns6n" in namespace "gc-3468"
Nov 30 04:42:28.867: INFO: Deleting pod "simpletest.rc-98d2d" in namespace "gc-3468"
Nov 30 04:42:28.887: INFO: Deleting pod "simpletest.rc-9lscp" in namespace "gc-3468"
Nov 30 04:42:28.917: INFO: Deleting pod "simpletest.rc-b5fxh" in namespace "gc-3468"
Nov 30 04:42:28.944: INFO: Deleting pod "simpletest.rc-blnkg" in namespace "gc-3468"
Nov 30 04:42:28.970: INFO: Deleting pod "simpletest.rc-c96px" in namespace "gc-3468"
Nov 30 04:42:28.985: INFO: Deleting pod "simpletest.rc-cn7q9" in namespace "gc-3468"
Nov 30 04:42:29.015: INFO: Deleting pod "simpletest.rc-cv8sn" in namespace "gc-3468"
Nov 30 04:42:29.035: INFO: Deleting pod "simpletest.rc-d8bq7" in namespace "gc-3468"
Nov 30 04:42:29.069: INFO: Deleting pod "simpletest.rc-dt6h8" in namespace "gc-3468"
Nov 30 04:42:29.095: INFO: Deleting pod "simpletest.rc-fqff9" in namespace "gc-3468"
Nov 30 04:42:29.122: INFO: Deleting pod "simpletest.rc-g9cb7" in namespace "gc-3468"
Nov 30 04:42:29.146: INFO: Deleting pod "simpletest.rc-gc555" in namespace "gc-3468"
Nov 30 04:42:29.166: INFO: Deleting pod "simpletest.rc-gjhsg" in namespace "gc-3468"
Nov 30 04:42:29.185: INFO: Deleting pod "simpletest.rc-gljcq" in namespace "gc-3468"
Nov 30 04:42:29.220: INFO: Deleting pod "simpletest.rc-hbb89" in namespace "gc-3468"
Nov 30 04:42:29.238: INFO: Deleting pod "simpletest.rc-hpds5" in namespace "gc-3468"
Nov 30 04:42:29.270: INFO: Deleting pod "simpletest.rc-hw8dd" in namespace "gc-3468"
Nov 30 04:42:29.301: INFO: Deleting pod "simpletest.rc-jhtfk" in namespace "gc-3468"
Nov 30 04:42:29.336: INFO: Deleting pod "simpletest.rc-jppfq" in namespace "gc-3468"
Nov 30 04:42:29.355: INFO: Deleting pod "simpletest.rc-jzx6z" in namespace "gc-3468"
Nov 30 04:42:29.386: INFO: Deleting pod "simpletest.rc-k6szj" in namespace "gc-3468"
Nov 30 04:42:29.407: INFO: Deleting pod "simpletest.rc-kggkh" in namespace "gc-3468"
Nov 30 04:42:29.452: INFO: Deleting pod "simpletest.rc-kn9tf" in namespace "gc-3468"
Nov 30 04:42:29.465: INFO: Deleting pod "simpletest.rc-kp9bf" in namespace "gc-3468"
Nov 30 04:42:29.498: INFO: Deleting pod "simpletest.rc-ljpmm" in namespace "gc-3468"
Nov 30 04:42:29.517: INFO: Deleting pod "simpletest.rc-ls8g8" in namespace "gc-3468"
Nov 30 04:42:29.547: INFO: Deleting pod "simpletest.rc-m22vx" in namespace "gc-3468"
Nov 30 04:42:29.567: INFO: Deleting pod "simpletest.rc-m2s4s" in namespace "gc-3468"
Nov 30 04:42:29.585: INFO: Deleting pod "simpletest.rc-mlzmb" in namespace "gc-3468"
Nov 30 04:42:29.638: INFO: Deleting pod "simpletest.rc-mvkdq" in namespace "gc-3468"
Nov 30 04:42:29.653: INFO: Deleting pod "simpletest.rc-n2r6x" in namespace "gc-3468"
Nov 30 04:42:29.667: INFO: Deleting pod "simpletest.rc-nd9p5" in namespace "gc-3468"
Nov 30 04:42:29.682: INFO: Deleting pod "simpletest.rc-ngwmx" in namespace "gc-3468"
Nov 30 04:42:29.698: INFO: Deleting pod "simpletest.rc-nq64v" in namespace "gc-3468"
Nov 30 04:42:29.746: INFO: Deleting pod "simpletest.rc-ns8ml" in namespace "gc-3468"
Nov 30 04:42:29.778: INFO: Deleting pod "simpletest.rc-nvqqc" in namespace "gc-3468"
Nov 30 04:42:29.794: INFO: Deleting pod "simpletest.rc-p9vmp" in namespace "gc-3468"
Nov 30 04:42:29.808: INFO: Deleting pod "simpletest.rc-pnd7q" in namespace "gc-3468"
Nov 30 04:42:29.867: INFO: Deleting pod "simpletest.rc-ppzd9" in namespace "gc-3468"
Nov 30 04:42:29.901: INFO: Deleting pod "simpletest.rc-px48p" in namespace "gc-3468"
Nov 30 04:42:29.928: INFO: Deleting pod "simpletest.rc-q2792" in namespace "gc-3468"
Nov 30 04:42:29.944: INFO: Deleting pod "simpletest.rc-q569z" in namespace "gc-3468"
Nov 30 04:42:29.960: INFO: Deleting pod "simpletest.rc-qh7sm" in namespace "gc-3468"
Nov 30 04:42:29.978: INFO: Deleting pod "simpletest.rc-r22j7" in namespace "gc-3468"
Nov 30 04:42:30.007: INFO: Deleting pod "simpletest.rc-r6gqd" in namespace "gc-3468"
Nov 30 04:42:30.022: INFO: Deleting pod "simpletest.rc-r9zvf" in namespace "gc-3468"
Nov 30 04:42:30.036: INFO: Deleting pod "simpletest.rc-rg7vd" in namespace "gc-3468"
Nov 30 04:42:30.061: INFO: Deleting pod "simpletest.rc-rgmg8" in namespace "gc-3468"
Nov 30 04:42:30.085: INFO: Deleting pod "simpletest.rc-rlxl4" in namespace "gc-3468"
Nov 30 04:42:30.102: INFO: Deleting pod "simpletest.rc-rzzfw" in namespace "gc-3468"
Nov 30 04:42:30.116: INFO: Deleting pod "simpletest.rc-s6s48" in namespace "gc-3468"
Nov 30 04:42:30.131: INFO: Deleting pod "simpletest.rc-sfhhh" in namespace "gc-3468"
Nov 30 04:42:30.160: INFO: Deleting pod "simpletest.rc-sh689" in namespace "gc-3468"
Nov 30 04:42:30.179: INFO: Deleting pod "simpletest.rc-sps8l" in namespace "gc-3468"
Nov 30 04:42:30.220: INFO: Deleting pod "simpletest.rc-st5j5" in namespace "gc-3468"
Nov 30 04:42:30.241: INFO: Deleting pod "simpletest.rc-tgxx6" in namespace "gc-3468"
Nov 30 04:42:30.260: INFO: Deleting pod "simpletest.rc-v7mvv" in namespace "gc-3468"
Nov 30 04:42:30.286: INFO: Deleting pod "simpletest.rc-v8vzh" in namespace "gc-3468"
Nov 30 04:42:30.313: INFO: Deleting pod "simpletest.rc-vct49" in namespace "gc-3468"
Nov 30 04:42:30.333: INFO: Deleting pod "simpletest.rc-vk82g" in namespace "gc-3468"
Nov 30 04:42:30.445: INFO: Deleting pod "simpletest.rc-vr9bj" in namespace "gc-3468"
Nov 30 04:42:30.468: INFO: Deleting pod "simpletest.rc-w6fl7" in namespace "gc-3468"
Nov 30 04:42:30.491: INFO: Deleting pod "simpletest.rc-w7x7k" in namespace "gc-3468"
Nov 30 04:42:30.542: INFO: Deleting pod "simpletest.rc-whh5n" in namespace "gc-3468"
Nov 30 04:42:30.582: INFO: Deleting pod "simpletest.rc-wr6h4" in namespace "gc-3468"
Nov 30 04:42:30.636: INFO: Deleting pod "simpletest.rc-x25w6" in namespace "gc-3468"
Nov 30 04:42:30.687: INFO: Deleting pod "simpletest.rc-x489f" in namespace "gc-3468"
Nov 30 04:42:30.737: INFO: Deleting pod "simpletest.rc-x526p" in namespace "gc-3468"
Nov 30 04:42:30.783: INFO: Deleting pod "simpletest.rc-xntbl" in namespace "gc-3468"
Nov 30 04:42:30.846: INFO: Deleting pod "simpletest.rc-xw5c9" in namespace "gc-3468"
Nov 30 04:42:30.880: INFO: Deleting pod "simpletest.rc-zm6mq" in namespace "gc-3468"
Nov 30 04:42:30.942: INFO: Deleting pod "simpletest.rc-zqlzj" in namespace "gc-3468"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Nov 30 04:42:30.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3468" for this suite. 11/30/22 04:42:31.047
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":303,"skipped":5463,"failed":0}
------------------------------
• [SLOW TEST] [42.882 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:41:48.208
    Nov 30 04:41:48.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename gc 11/30/22 04:41:48.209
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:41:48.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:41:48.231
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 11/30/22 04:41:48.237
    STEP: delete the rc 11/30/22 04:41:53.255
    STEP: wait for the rc to be deleted 11/30/22 04:41:53.263
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 11/30/22 04:41:58.267
    STEP: Gathering metrics 11/30/22 04:42:28.278
    Nov 30 04:42:28.359: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
    Nov 30 04:42:28.380: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 20.645303ms
    Nov 30 04:42:28.380: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
    Nov 30 04:42:28.380: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
    Nov 30 04:42:28.432: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Nov 30 04:42:28.432: INFO: Deleting pod "simpletest.rc-2f7r4" in namespace "gc-3468"
    Nov 30 04:42:28.446: INFO: Deleting pod "simpletest.rc-2hdl2" in namespace "gc-3468"
    Nov 30 04:42:28.456: INFO: Deleting pod "simpletest.rc-2qhfq" in namespace "gc-3468"
    Nov 30 04:42:28.469: INFO: Deleting pod "simpletest.rc-2s2v6" in namespace "gc-3468"
    Nov 30 04:42:28.479: INFO: Deleting pod "simpletest.rc-42xwj" in namespace "gc-3468"
    Nov 30 04:42:28.491: INFO: Deleting pod "simpletest.rc-4c77c" in namespace "gc-3468"
    Nov 30 04:42:28.505: INFO: Deleting pod "simpletest.rc-4nbsx" in namespace "gc-3468"
    Nov 30 04:42:28.521: INFO: Deleting pod "simpletest.rc-4tfzw" in namespace "gc-3468"
    Nov 30 04:42:28.536: INFO: Deleting pod "simpletest.rc-4xl9f" in namespace "gc-3468"
    Nov 30 04:42:28.546: INFO: Deleting pod "simpletest.rc-57vtz" in namespace "gc-3468"
    Nov 30 04:42:28.558: INFO: Deleting pod "simpletest.rc-5x5sv" in namespace "gc-3468"
    Nov 30 04:42:28.574: INFO: Deleting pod "simpletest.rc-64nvx" in namespace "gc-3468"
    Nov 30 04:42:28.587: INFO: Deleting pod "simpletest.rc-6kmqd" in namespace "gc-3468"
    Nov 30 04:42:28.601: INFO: Deleting pod "simpletest.rc-6lr4q" in namespace "gc-3468"
    Nov 30 04:42:28.617: INFO: Deleting pod "simpletest.rc-6wk2m" in namespace "gc-3468"
    Nov 30 04:42:28.632: INFO: Deleting pod "simpletest.rc-7cccq" in namespace "gc-3468"
    Nov 30 04:42:28.643: INFO: Deleting pod "simpletest.rc-7mhwr" in namespace "gc-3468"
    Nov 30 04:42:28.657: INFO: Deleting pod "simpletest.rc-7rqnz" in namespace "gc-3468"
    Nov 30 04:42:28.676: INFO: Deleting pod "simpletest.rc-7s2bt" in namespace "gc-3468"
    Nov 30 04:42:28.694: INFO: Deleting pod "simpletest.rc-7tb7q" in namespace "gc-3468"
    Nov 30 04:42:28.707: INFO: Deleting pod "simpletest.rc-8926n" in namespace "gc-3468"
    Nov 30 04:42:28.720: INFO: Deleting pod "simpletest.rc-8bjdm" in namespace "gc-3468"
    Nov 30 04:42:28.734: INFO: Deleting pod "simpletest.rc-8d8sd" in namespace "gc-3468"
    Nov 30 04:42:28.751: INFO: Deleting pod "simpletest.rc-8ltw5" in namespace "gc-3468"
    Nov 30 04:42:28.767: INFO: Deleting pod "simpletest.rc-8n962" in namespace "gc-3468"
    Nov 30 04:42:28.786: INFO: Deleting pod "simpletest.rc-8nj8x" in namespace "gc-3468"
    Nov 30 04:42:28.806: INFO: Deleting pod "simpletest.rc-8nnjf" in namespace "gc-3468"
    Nov 30 04:42:28.840: INFO: Deleting pod "simpletest.rc-8ns6n" in namespace "gc-3468"
    Nov 30 04:42:28.867: INFO: Deleting pod "simpletest.rc-98d2d" in namespace "gc-3468"
    Nov 30 04:42:28.887: INFO: Deleting pod "simpletest.rc-9lscp" in namespace "gc-3468"
    Nov 30 04:42:28.917: INFO: Deleting pod "simpletest.rc-b5fxh" in namespace "gc-3468"
    Nov 30 04:42:28.944: INFO: Deleting pod "simpletest.rc-blnkg" in namespace "gc-3468"
    Nov 30 04:42:28.970: INFO: Deleting pod "simpletest.rc-c96px" in namespace "gc-3468"
    Nov 30 04:42:28.985: INFO: Deleting pod "simpletest.rc-cn7q9" in namespace "gc-3468"
    Nov 30 04:42:29.015: INFO: Deleting pod "simpletest.rc-cv8sn" in namespace "gc-3468"
    Nov 30 04:42:29.035: INFO: Deleting pod "simpletest.rc-d8bq7" in namespace "gc-3468"
    Nov 30 04:42:29.069: INFO: Deleting pod "simpletest.rc-dt6h8" in namespace "gc-3468"
    Nov 30 04:42:29.095: INFO: Deleting pod "simpletest.rc-fqff9" in namespace "gc-3468"
    Nov 30 04:42:29.122: INFO: Deleting pod "simpletest.rc-g9cb7" in namespace "gc-3468"
    Nov 30 04:42:29.146: INFO: Deleting pod "simpletest.rc-gc555" in namespace "gc-3468"
    Nov 30 04:42:29.166: INFO: Deleting pod "simpletest.rc-gjhsg" in namespace "gc-3468"
    Nov 30 04:42:29.185: INFO: Deleting pod "simpletest.rc-gljcq" in namespace "gc-3468"
    Nov 30 04:42:29.220: INFO: Deleting pod "simpletest.rc-hbb89" in namespace "gc-3468"
    Nov 30 04:42:29.238: INFO: Deleting pod "simpletest.rc-hpds5" in namespace "gc-3468"
    Nov 30 04:42:29.270: INFO: Deleting pod "simpletest.rc-hw8dd" in namespace "gc-3468"
    Nov 30 04:42:29.301: INFO: Deleting pod "simpletest.rc-jhtfk" in namespace "gc-3468"
    Nov 30 04:42:29.336: INFO: Deleting pod "simpletest.rc-jppfq" in namespace "gc-3468"
    Nov 30 04:42:29.355: INFO: Deleting pod "simpletest.rc-jzx6z" in namespace "gc-3468"
    Nov 30 04:42:29.386: INFO: Deleting pod "simpletest.rc-k6szj" in namespace "gc-3468"
    Nov 30 04:42:29.407: INFO: Deleting pod "simpletest.rc-kggkh" in namespace "gc-3468"
    Nov 30 04:42:29.452: INFO: Deleting pod "simpletest.rc-kn9tf" in namespace "gc-3468"
    Nov 30 04:42:29.465: INFO: Deleting pod "simpletest.rc-kp9bf" in namespace "gc-3468"
    Nov 30 04:42:29.498: INFO: Deleting pod "simpletest.rc-ljpmm" in namespace "gc-3468"
    Nov 30 04:42:29.517: INFO: Deleting pod "simpletest.rc-ls8g8" in namespace "gc-3468"
    Nov 30 04:42:29.547: INFO: Deleting pod "simpletest.rc-m22vx" in namespace "gc-3468"
    Nov 30 04:42:29.567: INFO: Deleting pod "simpletest.rc-m2s4s" in namespace "gc-3468"
    Nov 30 04:42:29.585: INFO: Deleting pod "simpletest.rc-mlzmb" in namespace "gc-3468"
    Nov 30 04:42:29.638: INFO: Deleting pod "simpletest.rc-mvkdq" in namespace "gc-3468"
    Nov 30 04:42:29.653: INFO: Deleting pod "simpletest.rc-n2r6x" in namespace "gc-3468"
    Nov 30 04:42:29.667: INFO: Deleting pod "simpletest.rc-nd9p5" in namespace "gc-3468"
    Nov 30 04:42:29.682: INFO: Deleting pod "simpletest.rc-ngwmx" in namespace "gc-3468"
    Nov 30 04:42:29.698: INFO: Deleting pod "simpletest.rc-nq64v" in namespace "gc-3468"
    Nov 30 04:42:29.746: INFO: Deleting pod "simpletest.rc-ns8ml" in namespace "gc-3468"
    Nov 30 04:42:29.778: INFO: Deleting pod "simpletest.rc-nvqqc" in namespace "gc-3468"
    Nov 30 04:42:29.794: INFO: Deleting pod "simpletest.rc-p9vmp" in namespace "gc-3468"
    Nov 30 04:42:29.808: INFO: Deleting pod "simpletest.rc-pnd7q" in namespace "gc-3468"
    Nov 30 04:42:29.867: INFO: Deleting pod "simpletest.rc-ppzd9" in namespace "gc-3468"
    Nov 30 04:42:29.901: INFO: Deleting pod "simpletest.rc-px48p" in namespace "gc-3468"
    Nov 30 04:42:29.928: INFO: Deleting pod "simpletest.rc-q2792" in namespace "gc-3468"
    Nov 30 04:42:29.944: INFO: Deleting pod "simpletest.rc-q569z" in namespace "gc-3468"
    Nov 30 04:42:29.960: INFO: Deleting pod "simpletest.rc-qh7sm" in namespace "gc-3468"
    Nov 30 04:42:29.978: INFO: Deleting pod "simpletest.rc-r22j7" in namespace "gc-3468"
    Nov 30 04:42:30.007: INFO: Deleting pod "simpletest.rc-r6gqd" in namespace "gc-3468"
    Nov 30 04:42:30.022: INFO: Deleting pod "simpletest.rc-r9zvf" in namespace "gc-3468"
    Nov 30 04:42:30.036: INFO: Deleting pod "simpletest.rc-rg7vd" in namespace "gc-3468"
    Nov 30 04:42:30.061: INFO: Deleting pod "simpletest.rc-rgmg8" in namespace "gc-3468"
    Nov 30 04:42:30.085: INFO: Deleting pod "simpletest.rc-rlxl4" in namespace "gc-3468"
    Nov 30 04:42:30.102: INFO: Deleting pod "simpletest.rc-rzzfw" in namespace "gc-3468"
    Nov 30 04:42:30.116: INFO: Deleting pod "simpletest.rc-s6s48" in namespace "gc-3468"
    Nov 30 04:42:30.131: INFO: Deleting pod "simpletest.rc-sfhhh" in namespace "gc-3468"
    Nov 30 04:42:30.160: INFO: Deleting pod "simpletest.rc-sh689" in namespace "gc-3468"
    Nov 30 04:42:30.179: INFO: Deleting pod "simpletest.rc-sps8l" in namespace "gc-3468"
    Nov 30 04:42:30.220: INFO: Deleting pod "simpletest.rc-st5j5" in namespace "gc-3468"
    Nov 30 04:42:30.241: INFO: Deleting pod "simpletest.rc-tgxx6" in namespace "gc-3468"
    Nov 30 04:42:30.260: INFO: Deleting pod "simpletest.rc-v7mvv" in namespace "gc-3468"
    Nov 30 04:42:30.286: INFO: Deleting pod "simpletest.rc-v8vzh" in namespace "gc-3468"
    Nov 30 04:42:30.313: INFO: Deleting pod "simpletest.rc-vct49" in namespace "gc-3468"
    Nov 30 04:42:30.333: INFO: Deleting pod "simpletest.rc-vk82g" in namespace "gc-3468"
    Nov 30 04:42:30.445: INFO: Deleting pod "simpletest.rc-vr9bj" in namespace "gc-3468"
    Nov 30 04:42:30.468: INFO: Deleting pod "simpletest.rc-w6fl7" in namespace "gc-3468"
    Nov 30 04:42:30.491: INFO: Deleting pod "simpletest.rc-w7x7k" in namespace "gc-3468"
    Nov 30 04:42:30.542: INFO: Deleting pod "simpletest.rc-whh5n" in namespace "gc-3468"
    Nov 30 04:42:30.582: INFO: Deleting pod "simpletest.rc-wr6h4" in namespace "gc-3468"
    Nov 30 04:42:30.636: INFO: Deleting pod "simpletest.rc-x25w6" in namespace "gc-3468"
    Nov 30 04:42:30.687: INFO: Deleting pod "simpletest.rc-x489f" in namespace "gc-3468"
    Nov 30 04:42:30.737: INFO: Deleting pod "simpletest.rc-x526p" in namespace "gc-3468"
    Nov 30 04:42:30.783: INFO: Deleting pod "simpletest.rc-xntbl" in namespace "gc-3468"
    Nov 30 04:42:30.846: INFO: Deleting pod "simpletest.rc-xw5c9" in namespace "gc-3468"
    Nov 30 04:42:30.880: INFO: Deleting pod "simpletest.rc-zm6mq" in namespace "gc-3468"
    Nov 30 04:42:30.942: INFO: Deleting pod "simpletest.rc-zqlzj" in namespace "gc-3468"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Nov 30 04:42:30.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3468" for this suite. 11/30/22 04:42:31.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:42:31.091
Nov 30 04:42:31.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename sched-preemption 11/30/22 04:42:31.092
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:42:31.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:42:31.141
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Nov 30 04:42:31.182: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 30 04:43:31.260: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:43:31.264
Nov 30 04:43:31.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename sched-preemption-path 11/30/22 04:43:31.265
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:43:31.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:43:31.292
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 11/30/22 04:43:31.294
STEP: Trying to launch a pod without a label to get a node which can launch it. 11/30/22 04:43:31.294
Nov 30 04:43:31.332: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1464" to be "running"
Nov 30 04:43:31.337: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.830233ms
Nov 30 04:43:33.341: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008606954s
Nov 30 04:43:33.341: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 11/30/22 04:43:33.348
Nov 30 04:43:33.357: INFO: found a healthy node: worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Nov 30 04:43:49.454: INFO: pods created so far: [1 1 1]
Nov 30 04:43:49.454: INFO: length of pods created so far: 3
Nov 30 04:43:51.486: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Nov 30 04:43:58.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1464" for this suite. 11/30/22 04:43:58.492
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Nov 30 04:43:58.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5675" for this suite. 11/30/22 04:43:58.53
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":304,"skipped":5477,"failed":0}
------------------------------
• [SLOW TEST] [87.508 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:42:31.091
    Nov 30 04:42:31.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename sched-preemption 11/30/22 04:42:31.092
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:42:31.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:42:31.141
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Nov 30 04:42:31.182: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 30 04:43:31.260: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:43:31.264
    Nov 30 04:43:31.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename sched-preemption-path 11/30/22 04:43:31.265
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:43:31.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:43:31.292
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 11/30/22 04:43:31.294
    STEP: Trying to launch a pod without a label to get a node which can launch it. 11/30/22 04:43:31.294
    Nov 30 04:43:31.332: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1464" to be "running"
    Nov 30 04:43:31.337: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.830233ms
    Nov 30 04:43:33.341: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008606954s
    Nov 30 04:43:33.341: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 11/30/22 04:43:33.348
    Nov 30 04:43:33.357: INFO: found a healthy node: worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Nov 30 04:43:49.454: INFO: pods created so far: [1 1 1]
    Nov 30 04:43:49.454: INFO: length of pods created so far: 3
    Nov 30 04:43:51.486: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Nov 30 04:43:58.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-1464" for this suite. 11/30/22 04:43:58.492
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 04:43:58.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5675" for this suite. 11/30/22 04:43:58.53
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:43:58.599
Nov 30 04:43:58.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 04:43:58.6
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:43:58.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:43:58.619
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-373e1903-0bbe-44c0-a863-54f82da80d89 11/30/22 04:43:58.623
STEP: Creating a pod to test consume configMaps 11/30/22 04:43:58.627
Nov 30 04:43:58.656: INFO: Waiting up to 5m0s for pod "pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8" in namespace "configmap-5856" to be "Succeeded or Failed"
Nov 30 04:43:58.658: INFO: Pod "pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.560028ms
Nov 30 04:44:00.663: INFO: Pod "pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006716772s
Nov 30 04:44:02.662: INFO: Pod "pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006443616s
STEP: Saw pod success 11/30/22 04:44:02.662
Nov 30 04:44:02.662: INFO: Pod "pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8" satisfied condition "Succeeded or Failed"
Nov 30 04:44:02.665: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8 container agnhost-container: <nil>
STEP: delete the pod 11/30/22 04:44:02.678
Nov 30 04:44:02.689: INFO: Waiting for pod pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8 to disappear
Nov 30 04:44:02.691: INFO: Pod pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 04:44:02.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5856" for this suite. 11/30/22 04:44:02.699
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":305,"skipped":5489,"failed":0}
------------------------------
• [4.105 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:43:58.599
    Nov 30 04:43:58.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 04:43:58.6
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:43:58.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:43:58.619
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-373e1903-0bbe-44c0-a863-54f82da80d89 11/30/22 04:43:58.623
    STEP: Creating a pod to test consume configMaps 11/30/22 04:43:58.627
    Nov 30 04:43:58.656: INFO: Waiting up to 5m0s for pod "pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8" in namespace "configmap-5856" to be "Succeeded or Failed"
    Nov 30 04:43:58.658: INFO: Pod "pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.560028ms
    Nov 30 04:44:00.663: INFO: Pod "pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006716772s
    Nov 30 04:44:02.662: INFO: Pod "pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006443616s
    STEP: Saw pod success 11/30/22 04:44:02.662
    Nov 30 04:44:02.662: INFO: Pod "pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8" satisfied condition "Succeeded or Failed"
    Nov 30 04:44:02.665: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8 container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 04:44:02.678
    Nov 30 04:44:02.689: INFO: Waiting for pod pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8 to disappear
    Nov 30 04:44:02.691: INFO: Pod pod-configmaps-9134e19d-09e7-4be2-91bc-9c3635c766b8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 04:44:02.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5856" for this suite. 11/30/22 04:44:02.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:44:02.704
Nov 30 04:44:02.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 04:44:02.705
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:44:02.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:44:02.727
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 11/30/22 04:44:02.729
Nov 30 04:44:02.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: mark a version not serverd 11/30/22 04:44:10.349
STEP: check the unserved version gets removed 11/30/22 04:44:10.365
STEP: check the other version is not changed 11/30/22 04:44:12.358
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:44:17.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-349" for this suite. 11/30/22 04:44:17.524
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":306,"skipped":5496,"failed":0}
------------------------------
• [SLOW TEST] [14.825 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:44:02.704
    Nov 30 04:44:02.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 04:44:02.705
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:44:02.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:44:02.727
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 11/30/22 04:44:02.729
    Nov 30 04:44:02.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: mark a version not serverd 11/30/22 04:44:10.349
    STEP: check the unserved version gets removed 11/30/22 04:44:10.365
    STEP: check the other version is not changed 11/30/22 04:44:12.358
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:44:17.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-349" for this suite. 11/30/22 04:44:17.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:44:17.53
Nov 30 04:44:17.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 04:44:17.53
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:44:17.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:44:17.548
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Nov 30 04:44:17.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/30/22 04:44:20.037
Nov 30 04:44:20.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-2021 --namespace=crd-publish-openapi-2021 create -f -'
Nov 30 04:44:20.677: INFO: stderr: ""
Nov 30 04:44:20.677: INFO: stdout: "e2e-test-crd-publish-openapi-23-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 30 04:44:20.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-2021 --namespace=crd-publish-openapi-2021 delete e2e-test-crd-publish-openapi-23-crds test-cr'
Nov 30 04:44:20.767: INFO: stderr: ""
Nov 30 04:44:20.767: INFO: stdout: "e2e-test-crd-publish-openapi-23-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 30 04:44:20.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-2021 --namespace=crd-publish-openapi-2021 apply -f -'
Nov 30 04:44:21.240: INFO: stderr: ""
Nov 30 04:44:21.240: INFO: stdout: "e2e-test-crd-publish-openapi-23-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 30 04:44:21.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-2021 --namespace=crd-publish-openapi-2021 delete e2e-test-crd-publish-openapi-23-crds test-cr'
Nov 30 04:44:21.310: INFO: stderr: ""
Nov 30 04:44:21.310: INFO: stdout: "e2e-test-crd-publish-openapi-23-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 11/30/22 04:44:21.31
Nov 30 04:44:21.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-2021 explain e2e-test-crd-publish-openapi-23-crds'
Nov 30 04:44:21.770: INFO: stderr: ""
Nov 30 04:44:21.770: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-23-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:44:24.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2021" for this suite. 11/30/22 04:44:24.272
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":307,"skipped":5503,"failed":0}
------------------------------
• [SLOW TEST] [6.749 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:44:17.53
    Nov 30 04:44:17.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename crd-publish-openapi 11/30/22 04:44:17.53
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:44:17.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:44:17.548
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Nov 30 04:44:17.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/30/22 04:44:20.037
    Nov 30 04:44:20.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-2021 --namespace=crd-publish-openapi-2021 create -f -'
    Nov 30 04:44:20.677: INFO: stderr: ""
    Nov 30 04:44:20.677: INFO: stdout: "e2e-test-crd-publish-openapi-23-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Nov 30 04:44:20.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-2021 --namespace=crd-publish-openapi-2021 delete e2e-test-crd-publish-openapi-23-crds test-cr'
    Nov 30 04:44:20.767: INFO: stderr: ""
    Nov 30 04:44:20.767: INFO: stdout: "e2e-test-crd-publish-openapi-23-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Nov 30 04:44:20.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-2021 --namespace=crd-publish-openapi-2021 apply -f -'
    Nov 30 04:44:21.240: INFO: stderr: ""
    Nov 30 04:44:21.240: INFO: stdout: "e2e-test-crd-publish-openapi-23-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Nov 30 04:44:21.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-2021 --namespace=crd-publish-openapi-2021 delete e2e-test-crd-publish-openapi-23-crds test-cr'
    Nov 30 04:44:21.310: INFO: stderr: ""
    Nov 30 04:44:21.310: INFO: stdout: "e2e-test-crd-publish-openapi-23-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 11/30/22 04:44:21.31
    Nov 30 04:44:21.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=crd-publish-openapi-2021 explain e2e-test-crd-publish-openapi-23-crds'
    Nov 30 04:44:21.770: INFO: stderr: ""
    Nov 30 04:44:21.770: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-23-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:44:24.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2021" for this suite. 11/30/22 04:44:24.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:44:24.279
Nov 30 04:44:24.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename replicaset 11/30/22 04:44:24.28
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:44:24.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:44:24.299
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Nov 30 04:44:24.302: INFO: Creating ReplicaSet my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c
Nov 30 04:44:24.310: INFO: Pod name my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c: Found 0 pods out of 1
Nov 30 04:44:29.315: INFO: Pod name my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c: Found 1 pods out of 1
Nov 30 04:44:29.315: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c" is running
Nov 30 04:44:29.315: INFO: Waiting up to 5m0s for pod "my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c-2xb9l" in namespace "replicaset-3348" to be "running"
Nov 30 04:44:29.317: INFO: Pod "my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c-2xb9l": Phase="Running", Reason="", readiness=true. Elapsed: 2.002846ms
Nov 30 04:44:29.317: INFO: Pod "my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c-2xb9l" satisfied condition "running"
Nov 30 04:44:29.317: INFO: Pod "my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c-2xb9l" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 04:44:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 04:44:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 04:44:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 04:44:24 +0000 UTC Reason: Message:}])
Nov 30 04:44:29.317: INFO: Trying to dial the pod
Nov 30 04:44:34.327: INFO: Controller my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c: Got expected result from replica 1 [my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c-2xb9l]: "my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c-2xb9l", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Nov 30 04:44:34.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3348" for this suite. 11/30/22 04:44:34.331
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":308,"skipped":5509,"failed":0}
------------------------------
• [SLOW TEST] [10.061 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:44:24.279
    Nov 30 04:44:24.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename replicaset 11/30/22 04:44:24.28
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:44:24.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:44:24.299
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Nov 30 04:44:24.302: INFO: Creating ReplicaSet my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c
    Nov 30 04:44:24.310: INFO: Pod name my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c: Found 0 pods out of 1
    Nov 30 04:44:29.315: INFO: Pod name my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c: Found 1 pods out of 1
    Nov 30 04:44:29.315: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c" is running
    Nov 30 04:44:29.315: INFO: Waiting up to 5m0s for pod "my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c-2xb9l" in namespace "replicaset-3348" to be "running"
    Nov 30 04:44:29.317: INFO: Pod "my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c-2xb9l": Phase="Running", Reason="", readiness=true. Elapsed: 2.002846ms
    Nov 30 04:44:29.317: INFO: Pod "my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c-2xb9l" satisfied condition "running"
    Nov 30 04:44:29.317: INFO: Pod "my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c-2xb9l" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 04:44:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 04:44:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 04:44:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-11-30 04:44:24 +0000 UTC Reason: Message:}])
    Nov 30 04:44:29.317: INFO: Trying to dial the pod
    Nov 30 04:44:34.327: INFO: Controller my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c: Got expected result from replica 1 [my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c-2xb9l]: "my-hostname-basic-89395d75-2434-4ae2-a317-cfdf592ce92c-2xb9l", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Nov 30 04:44:34.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3348" for this suite. 11/30/22 04:44:34.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:44:34.343
Nov 30 04:44:34.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-probe 11/30/22 04:44:34.344
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:44:34.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:44:34.369
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-bb624674-92cf-44fa-8018-180e43356291 in namespace container-probe-650 11/30/22 04:44:34.372
Nov 30 04:44:34.407: INFO: Waiting up to 5m0s for pod "busybox-bb624674-92cf-44fa-8018-180e43356291" in namespace "container-probe-650" to be "not pending"
Nov 30 04:44:34.413: INFO: Pod "busybox-bb624674-92cf-44fa-8018-180e43356291": Phase="Pending", Reason="", readiness=false. Elapsed: 5.624113ms
Nov 30 04:44:36.417: INFO: Pod "busybox-bb624674-92cf-44fa-8018-180e43356291": Phase="Running", Reason="", readiness=true. Elapsed: 2.009917406s
Nov 30 04:44:36.417: INFO: Pod "busybox-bb624674-92cf-44fa-8018-180e43356291" satisfied condition "not pending"
Nov 30 04:44:36.417: INFO: Started pod busybox-bb624674-92cf-44fa-8018-180e43356291 in namespace container-probe-650
STEP: checking the pod's current state and verifying that restartCount is present 11/30/22 04:44:36.417
Nov 30 04:44:36.420: INFO: Initial restart count of pod busybox-bb624674-92cf-44fa-8018-180e43356291 is 0
STEP: deleting the pod 11/30/22 04:48:36.918
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Nov 30 04:48:36.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-650" for this suite. 11/30/22 04:48:36.935
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":309,"skipped":5606,"failed":0}
------------------------------
• [SLOW TEST] [242.599 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:44:34.343
    Nov 30 04:44:34.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-probe 11/30/22 04:44:34.344
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:44:34.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:44:34.369
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-bb624674-92cf-44fa-8018-180e43356291 in namespace container-probe-650 11/30/22 04:44:34.372
    Nov 30 04:44:34.407: INFO: Waiting up to 5m0s for pod "busybox-bb624674-92cf-44fa-8018-180e43356291" in namespace "container-probe-650" to be "not pending"
    Nov 30 04:44:34.413: INFO: Pod "busybox-bb624674-92cf-44fa-8018-180e43356291": Phase="Pending", Reason="", readiness=false. Elapsed: 5.624113ms
    Nov 30 04:44:36.417: INFO: Pod "busybox-bb624674-92cf-44fa-8018-180e43356291": Phase="Running", Reason="", readiness=true. Elapsed: 2.009917406s
    Nov 30 04:44:36.417: INFO: Pod "busybox-bb624674-92cf-44fa-8018-180e43356291" satisfied condition "not pending"
    Nov 30 04:44:36.417: INFO: Started pod busybox-bb624674-92cf-44fa-8018-180e43356291 in namespace container-probe-650
    STEP: checking the pod's current state and verifying that restartCount is present 11/30/22 04:44:36.417
    Nov 30 04:44:36.420: INFO: Initial restart count of pod busybox-bb624674-92cf-44fa-8018-180e43356291 is 0
    STEP: deleting the pod 11/30/22 04:48:36.918
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Nov 30 04:48:36.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-650" for this suite. 11/30/22 04:48:36.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:48:36.943
Nov 30 04:48:36.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename gc 11/30/22 04:48:36.943
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:48:36.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:48:36.968
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 11/30/22 04:48:36.978
STEP: create the rc2 11/30/22 04:48:36.986
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 11/30/22 04:48:41.999
STEP: delete the rc simpletest-rc-to-be-deleted 11/30/22 04:48:42.655
STEP: wait for the rc to be deleted 11/30/22 04:48:42.664
Nov 30 04:48:47.676: INFO: 68 pods remaining
Nov 30 04:48:47.676: INFO: 68 pods has nil DeletionTimestamp
Nov 30 04:48:47.676: INFO: 
STEP: Gathering metrics 11/30/22 04:48:52.675
Nov 30 04:48:52.698: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
Nov 30 04:48:52.701: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 2.627693ms
Nov 30 04:48:52.701: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
Nov 30 04:48:52.701: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
Nov 30 04:48:52.744: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 30 04:48:52.744: INFO: Deleting pod "simpletest-rc-to-be-deleted-227dp" in namespace "gc-9592"
Nov 30 04:48:52.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-24kc6" in namespace "gc-9592"
Nov 30 04:48:52.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-27f4l" in namespace "gc-9592"
Nov 30 04:48:52.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hgf7" in namespace "gc-9592"
Nov 30 04:48:52.798: INFO: Deleting pod "simpletest-rc-to-be-deleted-2w9cs" in namespace "gc-9592"
Nov 30 04:48:52.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-46bjh" in namespace "gc-9592"
Nov 30 04:48:52.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-4spq9" in namespace "gc-9592"
Nov 30 04:48:52.844: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vj4p" in namespace "gc-9592"
Nov 30 04:48:52.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-52rxx" in namespace "gc-9592"
Nov 30 04:48:52.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jfkf" in namespace "gc-9592"
Nov 30 04:48:52.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rqf8" in namespace "gc-9592"
Nov 30 04:48:52.899: INFO: Deleting pod "simpletest-rc-to-be-deleted-5snbw" in namespace "gc-9592"
Nov 30 04:48:52.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wnx6" in namespace "gc-9592"
Nov 30 04:48:52.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cnpg" in namespace "gc-9592"
Nov 30 04:48:52.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dg42" in namespace "gc-9592"
Nov 30 04:48:52.966: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qgnq" in namespace "gc-9592"
Nov 30 04:48:52.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-756b5" in namespace "gc-9592"
Nov 30 04:48:52.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-75sj6" in namespace "gc-9592"
Nov 30 04:48:53.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-76dvj" in namespace "gc-9592"
Nov 30 04:48:53.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-79mfk" in namespace "gc-9592"
Nov 30 04:48:53.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bttp" in namespace "gc-9592"
Nov 30 04:48:53.065: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kgvx" in namespace "gc-9592"
Nov 30 04:48:53.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nt9x" in namespace "gc-9592"
Nov 30 04:48:53.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rh2x" in namespace "gc-9592"
Nov 30 04:48:53.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-7s9cm" in namespace "gc-9592"
Nov 30 04:48:53.117: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vjjb" in namespace "gc-9592"
Nov 30 04:48:53.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bvl4" in namespace "gc-9592"
Nov 30 04:48:53.144: INFO: Deleting pod "simpletest-rc-to-be-deleted-8hz4g" in namespace "gc-9592"
Nov 30 04:48:53.165: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mqkj" in namespace "gc-9592"
Nov 30 04:48:53.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-98mhl" in namespace "gc-9592"
Nov 30 04:48:53.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-9chnl" in namespace "gc-9592"
Nov 30 04:48:53.210: INFO: Deleting pod "simpletest-rc-to-be-deleted-9flgs" in namespace "gc-9592"
Nov 30 04:48:53.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gmqz" in namespace "gc-9592"
Nov 30 04:48:53.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ngjs" in namespace "gc-9592"
Nov 30 04:48:53.303: INFO: Deleting pod "simpletest-rc-to-be-deleted-9p6cx" in namespace "gc-9592"
Nov 30 04:48:53.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qqt4" in namespace "gc-9592"
Nov 30 04:48:53.332: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t9l7" in namespace "gc-9592"
Nov 30 04:48:53.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgcck" in namespace "gc-9592"
Nov 30 04:48:53.388: INFO: Deleting pod "simpletest-rc-to-be-deleted-blr9f" in namespace "gc-9592"
Nov 30 04:48:53.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnw8b" in namespace "gc-9592"
Nov 30 04:48:53.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-brknz" in namespace "gc-9592"
Nov 30 04:48:53.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-brmqw" in namespace "gc-9592"
Nov 30 04:48:53.539: INFO: Deleting pod "simpletest-rc-to-be-deleted-brxmf" in namespace "gc-9592"
Nov 30 04:48:53.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-clp52" in namespace "gc-9592"
Nov 30 04:48:53.595: INFO: Deleting pod "simpletest-rc-to-be-deleted-czdf6" in namespace "gc-9592"
Nov 30 04:48:53.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4dz2" in namespace "gc-9592"
Nov 30 04:48:53.650: INFO: Deleting pod "simpletest-rc-to-be-deleted-d78jc" in namespace "gc-9592"
Nov 30 04:48:53.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfttm" in namespace "gc-9592"
Nov 30 04:48:53.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7qhl" in namespace "gc-9592"
Nov 30 04:48:53.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl97l" in namespace "gc-9592"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Nov 30 04:48:53.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9592" for this suite. 11/30/22 04:48:53.752
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":310,"skipped":5621,"failed":0}
------------------------------
• [SLOW TEST] [16.819 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:48:36.943
    Nov 30 04:48:36.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename gc 11/30/22 04:48:36.943
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:48:36.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:48:36.968
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 11/30/22 04:48:36.978
    STEP: create the rc2 11/30/22 04:48:36.986
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 11/30/22 04:48:41.999
    STEP: delete the rc simpletest-rc-to-be-deleted 11/30/22 04:48:42.655
    STEP: wait for the rc to be deleted 11/30/22 04:48:42.664
    Nov 30 04:48:47.676: INFO: 68 pods remaining
    Nov 30 04:48:47.676: INFO: 68 pods has nil DeletionTimestamp
    Nov 30 04:48:47.676: INFO: 
    STEP: Gathering metrics 11/30/22 04:48:52.675
    Nov 30 04:48:52.698: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
    Nov 30 04:48:52.701: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 2.627693ms
    Nov 30 04:48:52.701: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
    Nov 30 04:48:52.701: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
    Nov 30 04:48:52.744: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Nov 30 04:48:52.744: INFO: Deleting pod "simpletest-rc-to-be-deleted-227dp" in namespace "gc-9592"
    Nov 30 04:48:52.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-24kc6" in namespace "gc-9592"
    Nov 30 04:48:52.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-27f4l" in namespace "gc-9592"
    Nov 30 04:48:52.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hgf7" in namespace "gc-9592"
    Nov 30 04:48:52.798: INFO: Deleting pod "simpletest-rc-to-be-deleted-2w9cs" in namespace "gc-9592"
    Nov 30 04:48:52.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-46bjh" in namespace "gc-9592"
    Nov 30 04:48:52.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-4spq9" in namespace "gc-9592"
    Nov 30 04:48:52.844: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vj4p" in namespace "gc-9592"
    Nov 30 04:48:52.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-52rxx" in namespace "gc-9592"
    Nov 30 04:48:52.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jfkf" in namespace "gc-9592"
    Nov 30 04:48:52.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rqf8" in namespace "gc-9592"
    Nov 30 04:48:52.899: INFO: Deleting pod "simpletest-rc-to-be-deleted-5snbw" in namespace "gc-9592"
    Nov 30 04:48:52.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wnx6" in namespace "gc-9592"
    Nov 30 04:48:52.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cnpg" in namespace "gc-9592"
    Nov 30 04:48:52.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dg42" in namespace "gc-9592"
    Nov 30 04:48:52.966: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qgnq" in namespace "gc-9592"
    Nov 30 04:48:52.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-756b5" in namespace "gc-9592"
    Nov 30 04:48:52.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-75sj6" in namespace "gc-9592"
    Nov 30 04:48:53.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-76dvj" in namespace "gc-9592"
    Nov 30 04:48:53.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-79mfk" in namespace "gc-9592"
    Nov 30 04:48:53.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bttp" in namespace "gc-9592"
    Nov 30 04:48:53.065: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kgvx" in namespace "gc-9592"
    Nov 30 04:48:53.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nt9x" in namespace "gc-9592"
    Nov 30 04:48:53.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rh2x" in namespace "gc-9592"
    Nov 30 04:48:53.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-7s9cm" in namespace "gc-9592"
    Nov 30 04:48:53.117: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vjjb" in namespace "gc-9592"
    Nov 30 04:48:53.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bvl4" in namespace "gc-9592"
    Nov 30 04:48:53.144: INFO: Deleting pod "simpletest-rc-to-be-deleted-8hz4g" in namespace "gc-9592"
    Nov 30 04:48:53.165: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mqkj" in namespace "gc-9592"
    Nov 30 04:48:53.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-98mhl" in namespace "gc-9592"
    Nov 30 04:48:53.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-9chnl" in namespace "gc-9592"
    Nov 30 04:48:53.210: INFO: Deleting pod "simpletest-rc-to-be-deleted-9flgs" in namespace "gc-9592"
    Nov 30 04:48:53.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gmqz" in namespace "gc-9592"
    Nov 30 04:48:53.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ngjs" in namespace "gc-9592"
    Nov 30 04:48:53.303: INFO: Deleting pod "simpletest-rc-to-be-deleted-9p6cx" in namespace "gc-9592"
    Nov 30 04:48:53.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qqt4" in namespace "gc-9592"
    Nov 30 04:48:53.332: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t9l7" in namespace "gc-9592"
    Nov 30 04:48:53.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgcck" in namespace "gc-9592"
    Nov 30 04:48:53.388: INFO: Deleting pod "simpletest-rc-to-be-deleted-blr9f" in namespace "gc-9592"
    Nov 30 04:48:53.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnw8b" in namespace "gc-9592"
    Nov 30 04:48:53.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-brknz" in namespace "gc-9592"
    Nov 30 04:48:53.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-brmqw" in namespace "gc-9592"
    Nov 30 04:48:53.539: INFO: Deleting pod "simpletest-rc-to-be-deleted-brxmf" in namespace "gc-9592"
    Nov 30 04:48:53.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-clp52" in namespace "gc-9592"
    Nov 30 04:48:53.595: INFO: Deleting pod "simpletest-rc-to-be-deleted-czdf6" in namespace "gc-9592"
    Nov 30 04:48:53.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4dz2" in namespace "gc-9592"
    Nov 30 04:48:53.650: INFO: Deleting pod "simpletest-rc-to-be-deleted-d78jc" in namespace "gc-9592"
    Nov 30 04:48:53.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfttm" in namespace "gc-9592"
    Nov 30 04:48:53.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7qhl" in namespace "gc-9592"
    Nov 30 04:48:53.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl97l" in namespace "gc-9592"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Nov 30 04:48:53.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9592" for this suite. 11/30/22 04:48:53.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:48:53.762
Nov 30 04:48:53.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 04:48:53.762
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:48:53.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:48:53.828
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 11/30/22 04:48:53.84
Nov 30 04:48:53.885: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5" in namespace "downward-api-4618" to be "Succeeded or Failed"
Nov 30 04:48:53.894: INFO: Pod "downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.351303ms
Nov 30 04:48:55.898: INFO: Pod "downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012991482s
Nov 30 04:48:57.898: INFO: Pod "downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013442984s
STEP: Saw pod success 11/30/22 04:48:57.898
Nov 30 04:48:57.898: INFO: Pod "downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5" satisfied condition "Succeeded or Failed"
Nov 30 04:48:57.901: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5 container client-container: <nil>
STEP: delete the pod 11/30/22 04:48:58.13
Nov 30 04:48:58.146: INFO: Waiting for pod downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5 to disappear
Nov 30 04:48:58.148: INFO: Pod downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Nov 30 04:48:58.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4618" for this suite. 11/30/22 04:48:58.157
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":311,"skipped":5628,"failed":0}
------------------------------
• [4.400 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:48:53.762
    Nov 30 04:48:53.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 04:48:53.762
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:48:53.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:48:53.828
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 11/30/22 04:48:53.84
    Nov 30 04:48:53.885: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5" in namespace "downward-api-4618" to be "Succeeded or Failed"
    Nov 30 04:48:53.894: INFO: Pod "downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.351303ms
    Nov 30 04:48:55.898: INFO: Pod "downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012991482s
    Nov 30 04:48:57.898: INFO: Pod "downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013442984s
    STEP: Saw pod success 11/30/22 04:48:57.898
    Nov 30 04:48:57.898: INFO: Pod "downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5" satisfied condition "Succeeded or Failed"
    Nov 30 04:48:57.901: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5 container client-container: <nil>
    STEP: delete the pod 11/30/22 04:48:58.13
    Nov 30 04:48:58.146: INFO: Waiting for pod downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5 to disappear
    Nov 30 04:48:58.148: INFO: Pod downwardapi-volume-7d36b8fc-d517-4d87-854e-b766fdfb94e5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Nov 30 04:48:58.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4618" for this suite. 11/30/22 04:48:58.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:48:58.163
Nov 30 04:48:58.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename gc 11/30/22 04:48:58.163
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:48:58.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:48:58.184
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 11/30/22 04:48:58.186
STEP: Wait for the Deployment to create new ReplicaSet 11/30/22 04:48:58.198
STEP: delete the deployment 11/30/22 04:48:58.708
STEP: wait for all rs to be garbage collected 11/30/22 04:48:58.714
STEP: expected 0 pods, got 2 pods 11/30/22 04:48:58.738
STEP: Gathering metrics 11/30/22 04:48:59.245
Nov 30 04:48:59.271: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
Nov 30 04:48:59.273: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 2.349953ms
Nov 30 04:48:59.273: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
Nov 30 04:48:59.273: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
Nov 30 04:48:59.317: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Nov 30 04:48:59.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9589" for this suite. 11/30/22 04:48:59.322
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":312,"skipped":5647,"failed":0}
------------------------------
• [1.166 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:48:58.163
    Nov 30 04:48:58.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename gc 11/30/22 04:48:58.163
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:48:58.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:48:58.184
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 11/30/22 04:48:58.186
    STEP: Wait for the Deployment to create new ReplicaSet 11/30/22 04:48:58.198
    STEP: delete the deployment 11/30/22 04:48:58.708
    STEP: wait for all rs to be garbage collected 11/30/22 04:48:58.714
    STEP: expected 0 pods, got 2 pods 11/30/22 04:48:58.738
    STEP: Gathering metrics 11/30/22 04:48:59.245
    Nov 30 04:48:59.271: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
    Nov 30 04:48:59.273: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 2.349953ms
    Nov 30 04:48:59.273: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
    Nov 30 04:48:59.273: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
    Nov 30 04:48:59.317: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Nov 30 04:48:59.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9589" for this suite. 11/30/22 04:48:59.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:48:59.329
Nov 30 04:48:59.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename daemonsets 11/30/22 04:48:59.33
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:48:59.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:48:59.351
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Nov 30 04:48:59.390: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 11/30/22 04:48:59.395
Nov 30 04:48:59.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:48:59.397: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 11/30/22 04:48:59.397
Nov 30 04:48:59.439: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:48:59.439: INFO: Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:49:00.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:49:00.443: INFO: Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:49:01.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 30 04:49:01.442: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 11/30/22 04:49:01.446
Nov 30 04:49:01.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 30 04:49:01.461: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Nov 30 04:49:02.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:49:02.464: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 11/30/22 04:49:02.464
Nov 30 04:49:02.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:49:02.475: INFO: Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:49:03.477: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:49:03.477: INFO: Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:49:04.483: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:49:04.483: INFO: Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:49:05.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 30 04:49:05.480: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 11/30/22 04:49:05.486
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1638, will wait for the garbage collector to delete the pods 11/30/22 04:49:05.486
Nov 30 04:49:05.556: INFO: Deleting DaemonSet.extensions daemon-set took: 18.203054ms
Nov 30 04:49:05.757: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.91424ms
Nov 30 04:49:07.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:49:07.861: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 30 04:49:07.863: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"66833"},"items":null}

Nov 30 04:49:07.865: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"66833"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Nov 30 04:49:07.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1638" for this suite. 11/30/22 04:49:07.902
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":313,"skipped":5653,"failed":0}
------------------------------
• [SLOW TEST] [8.583 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:48:59.329
    Nov 30 04:48:59.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename daemonsets 11/30/22 04:48:59.33
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:48:59.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:48:59.351
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Nov 30 04:48:59.390: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 11/30/22 04:48:59.395
    Nov 30 04:48:59.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:48:59.397: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 11/30/22 04:48:59.397
    Nov 30 04:48:59.439: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:48:59.439: INFO: Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:49:00.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:49:00.443: INFO: Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:49:01.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 30 04:49:01.442: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 11/30/22 04:49:01.446
    Nov 30 04:49:01.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 30 04:49:01.461: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Nov 30 04:49:02.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:49:02.464: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 11/30/22 04:49:02.464
    Nov 30 04:49:02.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:49:02.475: INFO: Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:49:03.477: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:49:03.477: INFO: Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:49:04.483: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:49:04.483: INFO: Node worker-pool1-sou9f2ae-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:49:05.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 30 04:49:05.480: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 11/30/22 04:49:05.486
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1638, will wait for the garbage collector to delete the pods 11/30/22 04:49:05.486
    Nov 30 04:49:05.556: INFO: Deleting DaemonSet.extensions daemon-set took: 18.203054ms
    Nov 30 04:49:05.757: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.91424ms
    Nov 30 04:49:07.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:49:07.861: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 30 04:49:07.863: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"66833"},"items":null}

    Nov 30 04:49:07.865: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"66833"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 04:49:07.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1638" for this suite. 11/30/22 04:49:07.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:49:07.913
Nov 30 04:49:07.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 04:49:07.914
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:49:07.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:49:07.943
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 11/30/22 04:49:07.945
Nov 30 04:49:07.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 create -f -'
Nov 30 04:49:08.558: INFO: stderr: ""
Nov 30 04:49:08.558: INFO: stdout: "pod/pause created\n"
Nov 30 04:49:08.558: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 30 04:49:08.558: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9897" to be "running and ready"
Nov 30 04:49:08.561: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297724ms
Nov 30 04:49:08.561: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins' to be 'Running' but was 'Pending'
Nov 30 04:49:10.565: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007136489s
Nov 30 04:49:10.565: INFO: Pod "pause" satisfied condition "running and ready"
Nov 30 04:49:10.565: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 11/30/22 04:49:10.565
Nov 30 04:49:10.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 label pods pause testing-label=testing-label-value'
Nov 30 04:49:10.632: INFO: stderr: ""
Nov 30 04:49:10.632: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 11/30/22 04:49:10.632
Nov 30 04:49:10.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 get pod pause -L testing-label'
Nov 30 04:49:10.692: INFO: stderr: ""
Nov 30 04:49:10.692: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 11/30/22 04:49:10.692
Nov 30 04:49:10.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 label pods pause testing-label-'
Nov 30 04:49:10.755: INFO: stderr: ""
Nov 30 04:49:10.755: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 11/30/22 04:49:10.755
Nov 30 04:49:10.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 get pod pause -L testing-label'
Nov 30 04:49:10.820: INFO: stderr: ""
Nov 30 04:49:10.820: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 11/30/22 04:49:10.82
Nov 30 04:49:10.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 delete --grace-period=0 --force -f -'
Nov 30 04:49:10.897: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 30 04:49:10.897: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 30 04:49:10.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 get rc,svc -l name=pause --no-headers'
Nov 30 04:49:10.957: INFO: stderr: "No resources found in kubectl-9897 namespace.\n"
Nov 30 04:49:10.957: INFO: stdout: ""
Nov 30 04:49:10.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 30 04:49:11.014: INFO: stderr: ""
Nov 30 04:49:11.014: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 04:49:11.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9897" for this suite. 11/30/22 04:49:11.018
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":314,"skipped":5692,"failed":0}
------------------------------
• [3.110 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:49:07.913
    Nov 30 04:49:07.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 04:49:07.914
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:49:07.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:49:07.943
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 11/30/22 04:49:07.945
    Nov 30 04:49:07.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 create -f -'
    Nov 30 04:49:08.558: INFO: stderr: ""
    Nov 30 04:49:08.558: INFO: stdout: "pod/pause created\n"
    Nov 30 04:49:08.558: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Nov 30 04:49:08.558: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9897" to be "running and ready"
    Nov 30 04:49:08.561: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297724ms
    Nov 30 04:49:08.561: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins' to be 'Running' but was 'Pending'
    Nov 30 04:49:10.565: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007136489s
    Nov 30 04:49:10.565: INFO: Pod "pause" satisfied condition "running and ready"
    Nov 30 04:49:10.565: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 11/30/22 04:49:10.565
    Nov 30 04:49:10.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 label pods pause testing-label=testing-label-value'
    Nov 30 04:49:10.632: INFO: stderr: ""
    Nov 30 04:49:10.632: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 11/30/22 04:49:10.632
    Nov 30 04:49:10.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 get pod pause -L testing-label'
    Nov 30 04:49:10.692: INFO: stderr: ""
    Nov 30 04:49:10.692: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 11/30/22 04:49:10.692
    Nov 30 04:49:10.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 label pods pause testing-label-'
    Nov 30 04:49:10.755: INFO: stderr: ""
    Nov 30 04:49:10.755: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 11/30/22 04:49:10.755
    Nov 30 04:49:10.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 get pod pause -L testing-label'
    Nov 30 04:49:10.820: INFO: stderr: ""
    Nov 30 04:49:10.820: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 11/30/22 04:49:10.82
    Nov 30 04:49:10.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 delete --grace-period=0 --force -f -'
    Nov 30 04:49:10.897: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 30 04:49:10.897: INFO: stdout: "pod \"pause\" force deleted\n"
    Nov 30 04:49:10.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 get rc,svc -l name=pause --no-headers'
    Nov 30 04:49:10.957: INFO: stderr: "No resources found in kubectl-9897 namespace.\n"
    Nov 30 04:49:10.957: INFO: stdout: ""
    Nov 30 04:49:10.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-9897 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Nov 30 04:49:11.014: INFO: stderr: ""
    Nov 30 04:49:11.014: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 04:49:11.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9897" for this suite. 11/30/22 04:49:11.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:49:11.024
Nov 30 04:49:11.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename job 11/30/22 04:49:11.025
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:49:11.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:49:11.051
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 11/30/22 04:49:11.061
STEP: Patching the Job 11/30/22 04:49:11.065
STEP: Watching for Job to be patched 11/30/22 04:49:11.086
Nov 30 04:49:11.087: INFO: Event ADDED observed for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking:]
Nov 30 04:49:11.087: INFO: Event MODIFIED found for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 11/30/22 04:49:11.087
STEP: Watching for Job to be updated 11/30/22 04:49:11.099
Nov 30 04:49:11.100: INFO: Event MODIFIED found for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 30 04:49:11.100: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 11/30/22 04:49:11.1
Nov 30 04:49:11.102: INFO: Job: e2e-6wztd as labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd]
STEP: Waiting for job to complete 11/30/22 04:49:11.103
STEP: Delete a job collection with a labelselector 11/30/22 04:49:21.107
STEP: Watching for Job to be deleted 11/30/22 04:49:21.117
Nov 30 04:49:21.119: INFO: Event MODIFIED observed for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 30 04:49:21.119: INFO: Event MODIFIED observed for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 30 04:49:21.119: INFO: Event MODIFIED observed for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 30 04:49:21.119: INFO: Event MODIFIED observed for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 30 04:49:21.119: INFO: Event MODIFIED observed for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 30 04:49:21.119: INFO: Event DELETED found for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 11/30/22 04:49:21.119
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Nov 30 04:49:21.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6013" for this suite. 11/30/22 04:49:21.127
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":315,"skipped":5712,"failed":0}
------------------------------
• [SLOW TEST] [10.108 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:49:11.024
    Nov 30 04:49:11.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename job 11/30/22 04:49:11.025
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:49:11.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:49:11.051
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 11/30/22 04:49:11.061
    STEP: Patching the Job 11/30/22 04:49:11.065
    STEP: Watching for Job to be patched 11/30/22 04:49:11.086
    Nov 30 04:49:11.087: INFO: Event ADDED observed for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking:]
    Nov 30 04:49:11.087: INFO: Event MODIFIED found for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 11/30/22 04:49:11.087
    STEP: Watching for Job to be updated 11/30/22 04:49:11.099
    Nov 30 04:49:11.100: INFO: Event MODIFIED found for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 30 04:49:11.100: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 11/30/22 04:49:11.1
    Nov 30 04:49:11.102: INFO: Job: e2e-6wztd as labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd]
    STEP: Waiting for job to complete 11/30/22 04:49:11.103
    STEP: Delete a job collection with a labelselector 11/30/22 04:49:21.107
    STEP: Watching for Job to be deleted 11/30/22 04:49:21.117
    Nov 30 04:49:21.119: INFO: Event MODIFIED observed for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 30 04:49:21.119: INFO: Event MODIFIED observed for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 30 04:49:21.119: INFO: Event MODIFIED observed for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 30 04:49:21.119: INFO: Event MODIFIED observed for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 30 04:49:21.119: INFO: Event MODIFIED observed for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 30 04:49:21.119: INFO: Event DELETED found for Job e2e-6wztd in namespace job-6013 with labels: map[e2e-6wztd:patched e2e-job-label:e2e-6wztd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 11/30/22 04:49:21.119
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Nov 30 04:49:21.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6013" for this suite. 11/30/22 04:49:21.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:49:21.133
Nov 30 04:49:21.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename ephemeral-containers-test 11/30/22 04:49:21.134
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:49:21.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:49:21.172
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 11/30/22 04:49:21.174
Nov 30 04:49:21.211: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1033" to be "running and ready"
Nov 30 04:49:21.215: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.118959ms
Nov 30 04:49:21.215: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:49:23.219: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007495534s
Nov 30 04:49:23.219: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Nov 30 04:49:23.219: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 11/30/22 04:49:23.221
Nov 30 04:49:23.234: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1033" to be "container debugger running"
Nov 30 04:49:23.237: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.962475ms
Nov 30 04:49:25.244: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009940771s
Nov 30 04:49:27.240: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006718947s
Nov 30 04:49:27.240: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 11/30/22 04:49:27.24
Nov 30 04:49:27.241: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1033 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 30 04:49:27.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
Nov 30 04:49:27.241: INFO: ExecWithOptions: Clientset creation
Nov 30 04:49:27.241: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-1033/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Nov 30 04:49:27.305: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Nov 30 04:49:27.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-1033" for this suite. 11/30/22 04:49:27.314
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":316,"skipped":5731,"failed":0}
------------------------------
• [SLOW TEST] [6.185 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:49:21.133
    Nov 30 04:49:21.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename ephemeral-containers-test 11/30/22 04:49:21.134
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:49:21.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:49:21.172
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 11/30/22 04:49:21.174
    Nov 30 04:49:21.211: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1033" to be "running and ready"
    Nov 30 04:49:21.215: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.118959ms
    Nov 30 04:49:21.215: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:49:23.219: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007495534s
    Nov 30 04:49:23.219: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Nov 30 04:49:23.219: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 11/30/22 04:49:23.221
    Nov 30 04:49:23.234: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1033" to be "container debugger running"
    Nov 30 04:49:23.237: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.962475ms
    Nov 30 04:49:25.244: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009940771s
    Nov 30 04:49:27.240: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006718947s
    Nov 30 04:49:27.240: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 11/30/22 04:49:27.24
    Nov 30 04:49:27.241: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1033 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 30 04:49:27.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    Nov 30 04:49:27.241: INFO: ExecWithOptions: Clientset creation
    Nov 30 04:49:27.241: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-1033/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Nov 30 04:49:27.305: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Nov 30 04:49:27.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-1033" for this suite. 11/30/22 04:49:27.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:49:27.319
Nov 30 04:49:27.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 04:49:27.32
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:49:27.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:49:27.338
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 11/30/22 04:49:27.341
Nov 30 04:49:27.371: INFO: Waiting up to 5m0s for pod "pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8" in namespace "emptydir-1853" to be "Succeeded or Failed"
Nov 30 04:49:27.377: INFO: Pod "pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.606558ms
Nov 30 04:49:29.381: INFO: Pod "pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009585422s
Nov 30 04:49:31.381: INFO: Pod "pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009581621s
STEP: Saw pod success 11/30/22 04:49:31.381
Nov 30 04:49:31.381: INFO: Pod "pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8" satisfied condition "Succeeded or Failed"
Nov 30 04:49:31.383: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8 container test-container: <nil>
STEP: delete the pod 11/30/22 04:49:31.388
Nov 30 04:49:31.398: INFO: Waiting for pod pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8 to disappear
Nov 30 04:49:31.401: INFO: Pod pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 04:49:31.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1853" for this suite. 11/30/22 04:49:31.404
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":317,"skipped":5738,"failed":0}
------------------------------
• [4.092 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:49:27.319
    Nov 30 04:49:27.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 04:49:27.32
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:49:27.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:49:27.338
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 11/30/22 04:49:27.341
    Nov 30 04:49:27.371: INFO: Waiting up to 5m0s for pod "pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8" in namespace "emptydir-1853" to be "Succeeded or Failed"
    Nov 30 04:49:27.377: INFO: Pod "pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.606558ms
    Nov 30 04:49:29.381: INFO: Pod "pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009585422s
    Nov 30 04:49:31.381: INFO: Pod "pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009581621s
    STEP: Saw pod success 11/30/22 04:49:31.381
    Nov 30 04:49:31.381: INFO: Pod "pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8" satisfied condition "Succeeded or Failed"
    Nov 30 04:49:31.383: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8 container test-container: <nil>
    STEP: delete the pod 11/30/22 04:49:31.388
    Nov 30 04:49:31.398: INFO: Waiting for pod pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8 to disappear
    Nov 30 04:49:31.401: INFO: Pod pod-d6fce07e-12cc-4314-be61-2b4e8e54eec8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 04:49:31.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1853" for this suite. 11/30/22 04:49:31.404
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:49:31.411
Nov 30 04:49:31.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename services 11/30/22 04:49:31.411
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:49:31.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:49:31.434
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7538 11/30/22 04:49:31.437
STEP: changing the ExternalName service to type=ClusterIP 11/30/22 04:49:31.442
STEP: creating replication controller externalname-service in namespace services-7538 11/30/22 04:49:31.469
I1130 04:49:31.474963      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7538, replica count: 2
I1130 04:49:34.526297      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 30 04:49:34.526: INFO: Creating new exec pod
Nov 30 04:49:34.557: INFO: Waiting up to 5m0s for pod "execpodbsrfx" in namespace "services-7538" to be "running"
Nov 30 04:49:34.563: INFO: Pod "execpodbsrfx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.926757ms
Nov 30 04:49:36.567: INFO: Pod "execpodbsrfx": Phase="Running", Reason="", readiness=true. Elapsed: 2.009849389s
Nov 30 04:49:36.567: INFO: Pod "execpodbsrfx" satisfied condition "running"
Nov 30 04:49:37.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7538 exec execpodbsrfx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Nov 30 04:49:37.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 30 04:49:37.707: INFO: stdout: "externalname-service-gj9cc"
Nov 30 04:49:37.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7538 exec execpodbsrfx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.25.199 80'
Nov 30 04:49:37.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.25.199 80\nConnection to 10.106.25.199 80 port [tcp/http] succeeded!\n"
Nov 30 04:49:37.856: INFO: stdout: ""
Nov 30 04:49:38.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7538 exec execpodbsrfx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.25.199 80'
Nov 30 04:49:38.995: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.25.199 80\nConnection to 10.106.25.199 80 port [tcp/http] succeeded!\n"
Nov 30 04:49:38.995: INFO: stdout: "externalname-service-gj9cc"
Nov 30 04:49:38.995: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Nov 30 04:49:39.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7538" for this suite. 11/30/22 04:49:39.027
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":318,"skipped":5738,"failed":0}
------------------------------
• [SLOW TEST] [7.621 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:49:31.411
    Nov 30 04:49:31.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename services 11/30/22 04:49:31.411
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:49:31.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:49:31.434
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7538 11/30/22 04:49:31.437
    STEP: changing the ExternalName service to type=ClusterIP 11/30/22 04:49:31.442
    STEP: creating replication controller externalname-service in namespace services-7538 11/30/22 04:49:31.469
    I1130 04:49:31.474963      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7538, replica count: 2
    I1130 04:49:34.526297      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 30 04:49:34.526: INFO: Creating new exec pod
    Nov 30 04:49:34.557: INFO: Waiting up to 5m0s for pod "execpodbsrfx" in namespace "services-7538" to be "running"
    Nov 30 04:49:34.563: INFO: Pod "execpodbsrfx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.926757ms
    Nov 30 04:49:36.567: INFO: Pod "execpodbsrfx": Phase="Running", Reason="", readiness=true. Elapsed: 2.009849389s
    Nov 30 04:49:36.567: INFO: Pod "execpodbsrfx" satisfied condition "running"
    Nov 30 04:49:37.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7538 exec execpodbsrfx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Nov 30 04:49:37.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Nov 30 04:49:37.707: INFO: stdout: "externalname-service-gj9cc"
    Nov 30 04:49:37.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7538 exec execpodbsrfx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.25.199 80'
    Nov 30 04:49:37.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.25.199 80\nConnection to 10.106.25.199 80 port [tcp/http] succeeded!\n"
    Nov 30 04:49:37.856: INFO: stdout: ""
    Nov 30 04:49:38.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=services-7538 exec execpodbsrfx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.25.199 80'
    Nov 30 04:49:38.995: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.25.199 80\nConnection to 10.106.25.199 80 port [tcp/http] succeeded!\n"
    Nov 30 04:49:38.995: INFO: stdout: "externalname-service-gj9cc"
    Nov 30 04:49:38.995: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Nov 30 04:49:39.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7538" for this suite. 11/30/22 04:49:39.027
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:49:39.033
Nov 30 04:49:39.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-runtime 11/30/22 04:49:39.034
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:49:39.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:49:39.07
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 11/30/22 04:49:39.096
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 11/30/22 04:49:54.177
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 11/30/22 04:49:54.179
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 11/30/22 04:49:54.184
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 11/30/22 04:49:54.184
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 11/30/22 04:49:54.231
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 11/30/22 04:49:57.253
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 11/30/22 04:49:59.263
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 11/30/22 04:49:59.268
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 11/30/22 04:49:59.268
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 11/30/22 04:49:59.319
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 11/30/22 04:50:00.33
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 11/30/22 04:50:03.345
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 11/30/22 04:50:03.35
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 11/30/22 04:50:03.35
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Nov 30 04:50:03.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3550" for this suite. 11/30/22 04:50:03.378
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":319,"skipped":5753,"failed":0}
------------------------------
• [SLOW TEST] [24.353 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:49:39.033
    Nov 30 04:49:39.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-runtime 11/30/22 04:49:39.034
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:49:39.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:49:39.07
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 11/30/22 04:49:39.096
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 11/30/22 04:49:54.177
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 11/30/22 04:49:54.179
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 11/30/22 04:49:54.184
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 11/30/22 04:49:54.184
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 11/30/22 04:49:54.231
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 11/30/22 04:49:57.253
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 11/30/22 04:49:59.263
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 11/30/22 04:49:59.268
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 11/30/22 04:49:59.268
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 11/30/22 04:49:59.319
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 11/30/22 04:50:00.33
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 11/30/22 04:50:03.345
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 11/30/22 04:50:03.35
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 11/30/22 04:50:03.35
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Nov 30 04:50:03.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3550" for this suite. 11/30/22 04:50:03.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:50:03.387
Nov 30 04:50:03.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 04:50:03.388
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:03.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:03.405
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 04:50:03.426
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:50:03.707
STEP: Deploying the webhook pod 11/30/22 04:50:03.719
STEP: Wait for the deployment to be ready 11/30/22 04:50:03.732
Nov 30 04:50:03.739: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 04:50:05.749
STEP: Verifying the service has paired with the endpoint 11/30/22 04:50:05.762
Nov 30 04:50:06.763: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/30/22 04:50:06.767
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/30/22 04:50:06.782
STEP: Creating a dummy validating-webhook-configuration object 11/30/22 04:50:06.796
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 11/30/22 04:50:06.804
STEP: Creating a dummy mutating-webhook-configuration object 11/30/22 04:50:06.809
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 11/30/22 04:50:06.818
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:50:06.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8168" for this suite. 11/30/22 04:50:06.854
STEP: Destroying namespace "webhook-8168-markers" for this suite. 11/30/22 04:50:06.881
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":320,"skipped":5782,"failed":0}
------------------------------
• [3.562 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:50:03.387
    Nov 30 04:50:03.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 04:50:03.388
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:03.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:03.405
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 04:50:03.426
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:50:03.707
    STEP: Deploying the webhook pod 11/30/22 04:50:03.719
    STEP: Wait for the deployment to be ready 11/30/22 04:50:03.732
    Nov 30 04:50:03.739: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 04:50:05.749
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:50:05.762
    Nov 30 04:50:06.763: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/30/22 04:50:06.767
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/30/22 04:50:06.782
    STEP: Creating a dummy validating-webhook-configuration object 11/30/22 04:50:06.796
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 11/30/22 04:50:06.804
    STEP: Creating a dummy mutating-webhook-configuration object 11/30/22 04:50:06.809
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 11/30/22 04:50:06.818
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:50:06.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8168" for this suite. 11/30/22 04:50:06.854
    STEP: Destroying namespace "webhook-8168-markers" for this suite. 11/30/22 04:50:06.881
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:50:06.949
Nov 30 04:50:06.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename var-expansion 11/30/22 04:50:06.949
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:06.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:06.981
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 11/30/22 04:50:06.984
Nov 30 04:50:07.024: INFO: Waiting up to 5m0s for pod "var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e" in namespace "var-expansion-7698" to be "Succeeded or Failed"
Nov 30 04:50:07.027: INFO: Pod "var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.80381ms
Nov 30 04:50:09.030: INFO: Pod "var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005897576s
Nov 30 04:50:11.031: INFO: Pod "var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006695483s
STEP: Saw pod success 11/30/22 04:50:11.031
Nov 30 04:50:11.031: INFO: Pod "var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e" satisfied condition "Succeeded or Failed"
Nov 30 04:50:11.033: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e container dapi-container: <nil>
STEP: delete the pod 11/30/22 04:50:11.038
Nov 30 04:50:11.051: INFO: Waiting for pod var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e to disappear
Nov 30 04:50:11.053: INFO: Pod var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Nov 30 04:50:11.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7698" for this suite. 11/30/22 04:50:11.057
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":321,"skipped":5792,"failed":0}
------------------------------
• [4.115 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:50:06.949
    Nov 30 04:50:06.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename var-expansion 11/30/22 04:50:06.949
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:06.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:06.981
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 11/30/22 04:50:06.984
    Nov 30 04:50:07.024: INFO: Waiting up to 5m0s for pod "var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e" in namespace "var-expansion-7698" to be "Succeeded or Failed"
    Nov 30 04:50:07.027: INFO: Pod "var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.80381ms
    Nov 30 04:50:09.030: INFO: Pod "var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005897576s
    Nov 30 04:50:11.031: INFO: Pod "var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006695483s
    STEP: Saw pod success 11/30/22 04:50:11.031
    Nov 30 04:50:11.031: INFO: Pod "var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e" satisfied condition "Succeeded or Failed"
    Nov 30 04:50:11.033: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e container dapi-container: <nil>
    STEP: delete the pod 11/30/22 04:50:11.038
    Nov 30 04:50:11.051: INFO: Waiting for pod var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e to disappear
    Nov 30 04:50:11.053: INFO: Pod var-expansion-43422cd9-4e7d-47ca-bffd-a73ba881ff5e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Nov 30 04:50:11.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7698" for this suite. 11/30/22 04:50:11.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:50:11.065
Nov 30 04:50:11.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename init-container 11/30/22 04:50:11.066
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:11.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:11.088
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 11/30/22 04:50:11.091
Nov 30 04:50:11.091: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Nov 30 04:50:13.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5314" for this suite. 11/30/22 04:50:13.935
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":322,"skipped":5823,"failed":0}
------------------------------
• [2.876 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:50:11.065
    Nov 30 04:50:11.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename init-container 11/30/22 04:50:11.066
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:11.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:11.088
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 11/30/22 04:50:11.091
    Nov 30 04:50:11.091: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Nov 30 04:50:13.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5314" for this suite. 11/30/22 04:50:13.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:50:13.942
Nov 30 04:50:13.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename configmap 11/30/22 04:50:13.943
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:13.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:13.962
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-fbcf22c2-1f24-41c5-b920-42019496db7a 11/30/22 04:50:13.969
STEP: Creating the pod 11/30/22 04:50:13.977
Nov 30 04:50:13.988: INFO: Waiting up to 5m0s for pod "pod-configmaps-74933506-889b-4697-831b-64bae6c9de6e" in namespace "configmap-8855" to be "running and ready"
Nov 30 04:50:13.990: INFO: Pod "pod-configmaps-74933506-889b-4697-831b-64bae6c9de6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.216951ms
Nov 30 04:50:13.990: INFO: The phase of Pod pod-configmaps-74933506-889b-4697-831b-64bae6c9de6e is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:50:15.994: INFO: Pod "pod-configmaps-74933506-889b-4697-831b-64bae6c9de6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005654479s
Nov 30 04:50:15.994: INFO: The phase of Pod pod-configmaps-74933506-889b-4697-831b-64bae6c9de6e is Running (Ready = true)
Nov 30 04:50:15.994: INFO: Pod "pod-configmaps-74933506-889b-4697-831b-64bae6c9de6e" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-fbcf22c2-1f24-41c5-b920-42019496db7a 11/30/22 04:50:16.001
STEP: waiting to observe update in volume 11/30/22 04:50:16.005
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Nov 30 04:50:18.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8855" for this suite. 11/30/22 04:50:18.021
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":323,"skipped":5863,"failed":0}
------------------------------
• [4.085 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:50:13.942
    Nov 30 04:50:13.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename configmap 11/30/22 04:50:13.943
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:13.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:13.962
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-fbcf22c2-1f24-41c5-b920-42019496db7a 11/30/22 04:50:13.969
    STEP: Creating the pod 11/30/22 04:50:13.977
    Nov 30 04:50:13.988: INFO: Waiting up to 5m0s for pod "pod-configmaps-74933506-889b-4697-831b-64bae6c9de6e" in namespace "configmap-8855" to be "running and ready"
    Nov 30 04:50:13.990: INFO: Pod "pod-configmaps-74933506-889b-4697-831b-64bae6c9de6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.216951ms
    Nov 30 04:50:13.990: INFO: The phase of Pod pod-configmaps-74933506-889b-4697-831b-64bae6c9de6e is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:50:15.994: INFO: Pod "pod-configmaps-74933506-889b-4697-831b-64bae6c9de6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005654479s
    Nov 30 04:50:15.994: INFO: The phase of Pod pod-configmaps-74933506-889b-4697-831b-64bae6c9de6e is Running (Ready = true)
    Nov 30 04:50:15.994: INFO: Pod "pod-configmaps-74933506-889b-4697-831b-64bae6c9de6e" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-fbcf22c2-1f24-41c5-b920-42019496db7a 11/30/22 04:50:16.001
    STEP: waiting to observe update in volume 11/30/22 04:50:16.005
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Nov 30 04:50:18.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8855" for this suite. 11/30/22 04:50:18.021
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:50:18.027
Nov 30 04:50:18.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename replicaset 11/30/22 04:50:18.028
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:18.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:18.057
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 11/30/22 04:50:18.059
STEP: Verify that the required pods have come up 11/30/22 04:50:18.069
Nov 30 04:50:18.073: INFO: Pod name sample-pod: Found 0 pods out of 3
Nov 30 04:50:23.078: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 11/30/22 04:50:23.078
Nov 30 04:50:23.080: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 11/30/22 04:50:23.08
STEP: DeleteCollection of the ReplicaSets 11/30/22 04:50:23.091
STEP: After DeleteCollection verify that ReplicaSets have been deleted 11/30/22 04:50:23.1
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Nov 30 04:50:23.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2639" for this suite. 11/30/22 04:50:23.107
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":324,"skipped":5866,"failed":0}
------------------------------
• [SLOW TEST] [5.087 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:50:18.027
    Nov 30 04:50:18.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename replicaset 11/30/22 04:50:18.028
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:18.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:18.057
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 11/30/22 04:50:18.059
    STEP: Verify that the required pods have come up 11/30/22 04:50:18.069
    Nov 30 04:50:18.073: INFO: Pod name sample-pod: Found 0 pods out of 3
    Nov 30 04:50:23.078: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 11/30/22 04:50:23.078
    Nov 30 04:50:23.080: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 11/30/22 04:50:23.08
    STEP: DeleteCollection of the ReplicaSets 11/30/22 04:50:23.091
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 11/30/22 04:50:23.1
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Nov 30 04:50:23.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2639" for this suite. 11/30/22 04:50:23.107
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:50:23.114
Nov 30 04:50:23.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-probe 11/30/22 04:50:23.115
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:23.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:23.158
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Nov 30 04:50:23.191: INFO: Waiting up to 5m0s for pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c" in namespace "container-probe-5248" to be "running and ready"
Nov 30 04:50:23.194: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.524521ms
Nov 30 04:50:23.194: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:50:25.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 2.006271937s
Nov 30 04:50:25.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
Nov 30 04:50:27.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 4.006271247s
Nov 30 04:50:27.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
Nov 30 04:50:29.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 6.00621589s
Nov 30 04:50:29.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
Nov 30 04:50:31.197: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 8.005793357s
Nov 30 04:50:31.197: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
Nov 30 04:50:33.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 10.006867141s
Nov 30 04:50:33.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
Nov 30 04:50:35.202: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 12.010740729s
Nov 30 04:50:35.202: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
Nov 30 04:50:37.197: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 14.00573317s
Nov 30 04:50:37.197: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
Nov 30 04:50:39.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 16.006252887s
Nov 30 04:50:39.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
Nov 30 04:50:41.197: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 18.005821884s
Nov 30 04:50:41.197: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
Nov 30 04:50:43.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 20.006976917s
Nov 30 04:50:43.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
Nov 30 04:50:45.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=true. Elapsed: 22.006883617s
Nov 30 04:50:45.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = true)
Nov 30 04:50:45.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c" satisfied condition "running and ready"
Nov 30 04:50:45.207: INFO: Container started at 2022-11-30 04:50:23 +0000 UTC, pod became ready at 2022-11-30 04:50:43 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Nov 30 04:50:45.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5248" for this suite. 11/30/22 04:50:45.215
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":325,"skipped":5868,"failed":0}
------------------------------
• [SLOW TEST] [22.105 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:50:23.114
    Nov 30 04:50:23.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-probe 11/30/22 04:50:23.115
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:23.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:23.158
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Nov 30 04:50:23.191: INFO: Waiting up to 5m0s for pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c" in namespace "container-probe-5248" to be "running and ready"
    Nov 30 04:50:23.194: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.524521ms
    Nov 30 04:50:23.194: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:50:25.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 2.006271937s
    Nov 30 04:50:25.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
    Nov 30 04:50:27.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 4.006271247s
    Nov 30 04:50:27.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
    Nov 30 04:50:29.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 6.00621589s
    Nov 30 04:50:29.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
    Nov 30 04:50:31.197: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 8.005793357s
    Nov 30 04:50:31.197: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
    Nov 30 04:50:33.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 10.006867141s
    Nov 30 04:50:33.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
    Nov 30 04:50:35.202: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 12.010740729s
    Nov 30 04:50:35.202: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
    Nov 30 04:50:37.197: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 14.00573317s
    Nov 30 04:50:37.197: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
    Nov 30 04:50:39.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 16.006252887s
    Nov 30 04:50:39.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
    Nov 30 04:50:41.197: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 18.005821884s
    Nov 30 04:50:41.197: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
    Nov 30 04:50:43.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=false. Elapsed: 20.006976917s
    Nov 30 04:50:43.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = false)
    Nov 30 04:50:45.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c": Phase="Running", Reason="", readiness=true. Elapsed: 22.006883617s
    Nov 30 04:50:45.198: INFO: The phase of Pod test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c is Running (Ready = true)
    Nov 30 04:50:45.198: INFO: Pod "test-webserver-446d5d6a-8045-406e-87ec-d67a3a1abd5c" satisfied condition "running and ready"
    Nov 30 04:50:45.207: INFO: Container started at 2022-11-30 04:50:23 +0000 UTC, pod became ready at 2022-11-30 04:50:43 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Nov 30 04:50:45.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5248" for this suite. 11/30/22 04:50:45.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:50:45.22
Nov 30 04:50:45.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename dns 11/30/22 04:50:45.221
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:45.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:45.242
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 11/30/22 04:50:45.244
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 46.139.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.139.46_udp@PTR;check="$$(dig +tcp +noall +answer +search 46.139.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.139.46_tcp@PTR;sleep 1; done
 11/30/22 04:50:45.285
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 46.139.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.139.46_udp@PTR;check="$$(dig +tcp +noall +answer +search 46.139.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.139.46_tcp@PTR;sleep 1; done
 11/30/22 04:50:45.285
STEP: creating a pod to probe DNS 11/30/22 04:50:45.285
STEP: submitting the pod to kubernetes 11/30/22 04:50:45.285
Nov 30 04:50:45.327: INFO: Waiting up to 15m0s for pod "dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3" in namespace "dns-9458" to be "running"
Nov 30 04:50:45.334: INFO: Pod "dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.307593ms
Nov 30 04:50:47.338: INFO: Pod "dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010423444s
Nov 30 04:50:47.338: INFO: Pod "dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3" satisfied condition "running"
STEP: retrieving the pod 11/30/22 04:50:47.338
STEP: looking for the results for each expected name from probers 11/30/22 04:50:47.341
Nov 30 04:50:47.345: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:47.347: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:47.350: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:47.353: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:47.367: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:47.369: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:47.372: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:47.374: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:47.394: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

Nov 30 04:50:52.398: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:52.401: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:52.404: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:52.406: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:52.419: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:52.423: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:52.425: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:52.428: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:52.440: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

Nov 30 04:50:57.397: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:57.401: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:57.403: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:57.406: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:57.419: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:57.422: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:57.424: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:57.426: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:50:57.436: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

Nov 30 04:51:02.398: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:02.401: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:02.403: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:02.406: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:02.419: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:02.422: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:02.426: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:02.428: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:02.438: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

Nov 30 04:51:07.401: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:07.404: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:07.407: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:07.411: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:07.427: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:07.431: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:07.434: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:07.437: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:07.447: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

Nov 30 04:51:12.398: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:12.403: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:12.406: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:12.408: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:12.421: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:12.424: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:12.426: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:12.428: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:12.438: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

Nov 30 04:51:17.399: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:17.403: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:17.406: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:17.409: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:17.426: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:17.429: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:17.431: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:17.434: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
Nov 30 04:51:17.444: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

Nov 30 04:51:22.461: INFO: DNS probes using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 succeeded

STEP: deleting the pod 11/30/22 04:51:22.461
STEP: deleting the test service 11/30/22 04:51:22.487
STEP: deleting the test headless service 11/30/22 04:51:22.515
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Nov 30 04:51:22.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9458" for this suite. 11/30/22 04:51:22.55
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":326,"skipped":5874,"failed":0}
------------------------------
• [SLOW TEST] [37.339 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:50:45.22
    Nov 30 04:50:45.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename dns 11/30/22 04:50:45.221
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:50:45.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:50:45.242
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 11/30/22 04:50:45.244
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 46.139.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.139.46_udp@PTR;check="$$(dig +tcp +noall +answer +search 46.139.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.139.46_tcp@PTR;sleep 1; done
     11/30/22 04:50:45.285
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9458.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 46.139.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.139.46_udp@PTR;check="$$(dig +tcp +noall +answer +search 46.139.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.139.46_tcp@PTR;sleep 1; done
     11/30/22 04:50:45.285
    STEP: creating a pod to probe DNS 11/30/22 04:50:45.285
    STEP: submitting the pod to kubernetes 11/30/22 04:50:45.285
    Nov 30 04:50:45.327: INFO: Waiting up to 15m0s for pod "dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3" in namespace "dns-9458" to be "running"
    Nov 30 04:50:45.334: INFO: Pod "dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.307593ms
    Nov 30 04:50:47.338: INFO: Pod "dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010423444s
    Nov 30 04:50:47.338: INFO: Pod "dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3" satisfied condition "running"
    STEP: retrieving the pod 11/30/22 04:50:47.338
    STEP: looking for the results for each expected name from probers 11/30/22 04:50:47.341
    Nov 30 04:50:47.345: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:47.347: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:47.350: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:47.353: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:47.367: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:47.369: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:47.372: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:47.374: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:47.394: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

    Nov 30 04:50:52.398: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:52.401: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:52.404: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:52.406: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:52.419: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:52.423: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:52.425: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:52.428: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:52.440: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

    Nov 30 04:50:57.397: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:57.401: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:57.403: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:57.406: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:57.419: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:57.422: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:57.424: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:57.426: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:50:57.436: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

    Nov 30 04:51:02.398: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:02.401: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:02.403: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:02.406: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:02.419: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:02.422: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:02.426: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:02.428: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:02.438: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

    Nov 30 04:51:07.401: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:07.404: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:07.407: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:07.411: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:07.427: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:07.431: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:07.434: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:07.437: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:07.447: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

    Nov 30 04:51:12.398: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:12.403: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:12.406: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:12.408: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:12.421: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:12.424: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:12.426: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:12.428: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:12.438: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

    Nov 30 04:51:17.399: INFO: Unable to read wheezy_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:17.403: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:17.406: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:17.409: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:17.426: INFO: Unable to read jessie_udp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:17.429: INFO: Unable to read jessie_tcp@dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:17.431: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:17.434: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local from pod dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3: the server could not find the requested resource (get pods dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3)
    Nov 30 04:51:17.444: INFO: Lookups using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 failed for: [wheezy_udp@dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service.dns-9458.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_udp@dns-test-service.dns-9458.svc.cluster.local jessie_tcp@dns-test-service.dns-9458.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9458.svc.cluster.local]

    Nov 30 04:51:22.461: INFO: DNS probes using dns-9458/dns-test-bf6c4627-26af-45b8-8f71-0ebf10491da3 succeeded

    STEP: deleting the pod 11/30/22 04:51:22.461
    STEP: deleting the test service 11/30/22 04:51:22.487
    STEP: deleting the test headless service 11/30/22 04:51:22.515
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Nov 30 04:51:22.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9458" for this suite. 11/30/22 04:51:22.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:51:22.559
Nov 30 04:51:22.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename deployment 11/30/22 04:51:22.56
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:51:22.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:51:22.638
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Nov 30 04:51:22.641: INFO: Creating simple deployment test-new-deployment
Nov 30 04:51:22.670: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 11/30/22 04:51:24.68
STEP: updating a scale subresource 11/30/22 04:51:24.684
STEP: verifying the deployment Spec.Replicas was modified 11/30/22 04:51:24.693
STEP: Patch a scale subresource 11/30/22 04:51:24.696
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 30 04:51:24.718: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-4324  7ce5b315-5fb1-4590-9ac2-5c1484076286 68412 3 2022-11-30 04:51:22 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-11-30 04:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:51:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003992878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-11-30 04:51:24 +0000 UTC,LastTransitionTime:2022-11-30 04:51:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-894b96578" has successfully progressed.,LastUpdateTime:2022-11-30 04:51:24 +0000 UTC,LastTransitionTime:2022-11-30 04:51:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 30 04:51:24.728: INFO: New ReplicaSet "test-new-deployment-894b96578" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-894b96578  deployment-4324  77b4ebeb-9876-4bc9-824d-c17959ca8f51 68416 2 2022-11-30 04:51:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 7ce5b315-5fb1-4590-9ac2-5c1484076286 0xc003992cd7 0xc003992cd8}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:51:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ce5b315-5fb1-4590-9ac2-5c1484076286\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:51:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 894b96578,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003992d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 30 04:51:24.739: INFO: Pod "test-new-deployment-894b96578-lwzcr" is available:
&Pod{ObjectMeta:{test-new-deployment-894b96578-lwzcr test-new-deployment-894b96578- deployment-4324  a3b2cc6a-004e-4f59-aed8-4a4e69a7889d 68404 0 2022-11-30 04:51:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.230"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.230"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-new-deployment-894b96578 77b4ebeb-9876-4bc9-824d-c17959ca8f51 0xc002d9cca7 0xc002d9cca8}] [] [{kube-controller-manager Update v1 2022-11-30 04:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77b4ebeb-9876-4bc9-824d-c17959ca8f51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:51:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:51:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6dx9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6dx9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:51:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:51:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:51:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:51:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.230,StartTime:2022-11-30 04:51:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:51:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://2918ee4ed0f0f7709e87d5383253c8f5874045a963f769ea24061bd5f050af8d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 30 04:51:24.739: INFO: Pod "test-new-deployment-894b96578-t6tfk" is not available:
&Pod{ObjectMeta:{test-new-deployment-894b96578-t6tfk test-new-deployment-894b96578- deployment-4324  6385b3cb-9f8d-42b8-ba6f-bdd04a9f555e 68414 0 2022-11-30 04:51:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[] [{apps/v1 ReplicaSet test-new-deployment-894b96578 77b4ebeb-9876-4bc9-824d-c17959ca8f51 0xc002d9d280 0xc002d9d281}] [] [{kube-controller-manager Update v1 2022-11-30 04:51:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77b4ebeb-9876-4bc9-824d-c17959ca8f51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k9242,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9242,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-ok72912g-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:51:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Nov 30 04:51:24.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4324" for this suite. 11/30/22 04:51:24.748
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":327,"skipped":5886,"failed":0}
------------------------------
• [2.196 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:51:22.559
    Nov 30 04:51:22.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename deployment 11/30/22 04:51:22.56
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:51:22.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:51:22.638
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Nov 30 04:51:22.641: INFO: Creating simple deployment test-new-deployment
    Nov 30 04:51:22.670: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 11/30/22 04:51:24.68
    STEP: updating a scale subresource 11/30/22 04:51:24.684
    STEP: verifying the deployment Spec.Replicas was modified 11/30/22 04:51:24.693
    STEP: Patch a scale subresource 11/30/22 04:51:24.696
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 30 04:51:24.718: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-4324  7ce5b315-5fb1-4590-9ac2-5c1484076286 68412 3 2022-11-30 04:51:22 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-11-30 04:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:51:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003992878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-11-30 04:51:24 +0000 UTC,LastTransitionTime:2022-11-30 04:51:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-894b96578" has successfully progressed.,LastUpdateTime:2022-11-30 04:51:24 +0000 UTC,LastTransitionTime:2022-11-30 04:51:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 30 04:51:24.728: INFO: New ReplicaSet "test-new-deployment-894b96578" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-894b96578  deployment-4324  77b4ebeb-9876-4bc9-824d-c17959ca8f51 68416 2 2022-11-30 04:51:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 7ce5b315-5fb1-4590-9ac2-5c1484076286 0xc003992cd7 0xc003992cd8}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:51:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ce5b315-5fb1-4590-9ac2-5c1484076286\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:51:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 894b96578,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003992d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 30 04:51:24.739: INFO: Pod "test-new-deployment-894b96578-lwzcr" is available:
    &Pod{ObjectMeta:{test-new-deployment-894b96578-lwzcr test-new-deployment-894b96578- deployment-4324  a3b2cc6a-004e-4f59-aed8-4a4e69a7889d 68404 0 2022-11-30 04:51:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.230"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.230"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-new-deployment-894b96578 77b4ebeb-9876-4bc9-824d-c17959ca8f51 0xc002d9cca7 0xc002d9cca8}] [] [{kube-controller-manager Update v1 2022-11-30 04:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77b4ebeb-9876-4bc9-824d-c17959ca8f51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2022-11-30 04:51:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-11-30 04:51:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6dx9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6dx9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:51:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:51:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:51:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:51:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.230,StartTime:2022-11-30 04:51:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:51:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:758aa7ccf1af0569310f77155e9408e51191291b60d3a49bfee31efc07105019,ContainerID:containerd://2918ee4ed0f0f7709e87d5383253c8f5874045a963f769ea24061bd5f050af8d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 30 04:51:24.739: INFO: Pod "test-new-deployment-894b96578-t6tfk" is not available:
    &Pod{ObjectMeta:{test-new-deployment-894b96578-t6tfk test-new-deployment-894b96578- deployment-4324  6385b3cb-9f8d-42b8-ba6f-bdd04a9f555e 68414 0 2022-11-30 04:51:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:894b96578] map[] [{apps/v1 ReplicaSet test-new-deployment-894b96578 77b4ebeb-9876-4bc9-824d-c17959ca8f51 0xc002d9d280 0xc002d9d281}] [] [{kube-controller-manager Update v1 2022-11-30 04:51:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77b4ebeb-9876-4bc9-824d-c17959ca8f51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k9242,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9242,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-ok72912g-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:51:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Nov 30 04:51:24.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4324" for this suite. 11/30/22 04:51:24.748
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:51:24.756
Nov 30 04:51:24.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename aggregator 11/30/22 04:51:24.756
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:51:24.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:51:24.799
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Nov 30 04:51:24.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 11/30/22 04:51:24.801
Nov 30 04:51:25.297: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 30 04:51:27.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:51:29.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:51:31.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:51:33.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:51:35.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:51:37.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:51:39.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:51:41.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:51:43.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:51:45.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:51:47.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 30 04:51:49.475: INFO: Waited 116.500234ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 11/30/22 04:51:49.523
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 11/30/22 04:51:49.526
STEP: List APIServices 11/30/22 04:51:49.531
Nov 30 04:51:49.538: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Nov 30 04:51:49.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-794" for this suite. 11/30/22 04:51:49.818
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":328,"skipped":5886,"failed":0}
------------------------------
• [SLOW TEST] [25.115 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:51:24.756
    Nov 30 04:51:24.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename aggregator 11/30/22 04:51:24.756
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:51:24.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:51:24.799
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Nov 30 04:51:24.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 11/30/22 04:51:24.801
    Nov 30 04:51:25.297: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Nov 30 04:51:27.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:51:29.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:51:31.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:51:33.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:51:35.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:51:37.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:51:39.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:51:41.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:51:43.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:51:45.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:51:47.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 51, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6c644c65f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 30 04:51:49.475: INFO: Waited 116.500234ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 11/30/22 04:51:49.523
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 11/30/22 04:51:49.526
    STEP: List APIServices 11/30/22 04:51:49.531
    Nov 30 04:51:49.538: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Nov 30 04:51:49.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-794" for this suite. 11/30/22 04:51:49.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:51:49.872
Nov 30 04:51:49.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 04:51:49.873
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:51:49.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:51:49.899
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 04:51:49.914
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:51:50.479
STEP: Deploying the webhook pod 11/30/22 04:51:50.483
STEP: Wait for the deployment to be ready 11/30/22 04:51:50.496
Nov 30 04:51:50.503: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/30/22 04:51:52.511
STEP: Verifying the service has paired with the endpoint 11/30/22 04:51:52.526
Nov 30 04:51:53.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 11/30/22 04:51:53.53
STEP: create a pod 11/30/22 04:51:53.542
Nov 30 04:51:53.568: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7437" to be "running"
Nov 30 04:51:53.571: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.583593ms
Nov 30 04:51:55.597: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029034384s
Nov 30 04:51:55.597: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 11/30/22 04:51:55.597
Nov 30 04:51:55.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=webhook-7437 attach --namespace=webhook-7437 to-be-attached-pod -i -c=container1'
Nov 30 04:51:55.674: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:51:55.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7437" for this suite. 11/30/22 04:51:55.686
STEP: Destroying namespace "webhook-7437-markers" for this suite. 11/30/22 04:51:55.696
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":329,"skipped":5944,"failed":0}
------------------------------
• [SLOW TEST] [5.874 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:51:49.872
    Nov 30 04:51:49.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 04:51:49.873
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:51:49.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:51:49.899
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 04:51:49.914
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:51:50.479
    STEP: Deploying the webhook pod 11/30/22 04:51:50.483
    STEP: Wait for the deployment to be ready 11/30/22 04:51:50.496
    Nov 30 04:51:50.503: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/30/22 04:51:52.511
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:51:52.526
    Nov 30 04:51:53.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 11/30/22 04:51:53.53
    STEP: create a pod 11/30/22 04:51:53.542
    Nov 30 04:51:53.568: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7437" to be "running"
    Nov 30 04:51:53.571: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.583593ms
    Nov 30 04:51:55.597: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029034384s
    Nov 30 04:51:55.597: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 11/30/22 04:51:55.597
    Nov 30 04:51:55.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=webhook-7437 attach --namespace=webhook-7437 to-be-attached-pod -i -c=container1'
    Nov 30 04:51:55.674: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:51:55.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7437" for this suite. 11/30/22 04:51:55.686
    STEP: Destroying namespace "webhook-7437-markers" for this suite. 11/30/22 04:51:55.696
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:51:55.749
Nov 30 04:51:55.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 04:51:55.75
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:51:55.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:51:55.786
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 11/30/22 04:51:55.789
Nov 30 04:51:55.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 run logs-generator --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 30 04:51:55.865: INFO: stderr: ""
Nov 30 04:51:55.866: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 11/30/22 04:51:55.866
Nov 30 04:51:55.866: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 30 04:51:55.866: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3973" to be "running and ready, or succeeded"
Nov 30 04:51:55.874: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.903909ms
Nov 30 04:51:55.874: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins' to be 'Running' but was 'Pending'
Nov 30 04:51:57.880: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.014397275s
Nov 30 04:51:57.880: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 30 04:51:57.880: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 11/30/22 04:51:57.88
Nov 30 04:51:57.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 logs logs-generator logs-generator'
Nov 30 04:51:57.947: INFO: stderr: ""
Nov 30 04:51:57.947: INFO: stdout: "I1130 04:51:56.604292       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/qbjh 318\nI1130 04:51:56.804417       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/vcp5 318\nI1130 04:51:57.005023       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/w8x 436\nI1130 04:51:57.205408       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/qx7 564\nI1130 04:51:57.405295       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/dmgz 432\nI1130 04:51:57.605180       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/fvq 322\nI1130 04:51:57.804442       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2gfh 595\n"
STEP: limiting log lines 11/30/22 04:51:57.947
Nov 30 04:51:57.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 logs logs-generator logs-generator --tail=1'
Nov 30 04:51:58.017: INFO: stderr: ""
Nov 30 04:51:58.017: INFO: stdout: "I1130 04:51:58.004704       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/4mt 523\n"
Nov 30 04:51:58.017: INFO: got output "I1130 04:51:58.004704       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/4mt 523\n"
STEP: limiting log bytes 11/30/22 04:51:58.017
Nov 30 04:51:58.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 logs logs-generator logs-generator --limit-bytes=1'
Nov 30 04:51:58.082: INFO: stderr: ""
Nov 30 04:51:58.082: INFO: stdout: "I"
Nov 30 04:51:58.082: INFO: got output "I"
STEP: exposing timestamps 11/30/22 04:51:58.082
Nov 30 04:51:58.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 logs logs-generator logs-generator --tail=1 --timestamps'
Nov 30 04:51:58.147: INFO: stderr: ""
Nov 30 04:51:58.147: INFO: stdout: "2022-11-30T06:51:58.004866468+02:00 I1130 04:51:58.004704       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/4mt 523\n"
Nov 30 04:51:58.147: INFO: got output "2022-11-30T06:51:58.004866468+02:00 I1130 04:51:58.004704       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/4mt 523\n"
STEP: restricting to a time range 11/30/22 04:51:58.147
Nov 30 04:52:00.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 logs logs-generator logs-generator --since=1s'
Nov 30 04:52:00.714: INFO: stderr: ""
Nov 30 04:52:00.714: INFO: stdout: "I1130 04:51:59.804538       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/vlq 323\nI1130 04:52:00.004929       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/4kq 332\nI1130 04:52:00.205309       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/jpsh 214\nI1130 04:52:00.404652       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/4dlp 261\nI1130 04:52:00.605003       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/shf 569\n"
Nov 30 04:52:00.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 logs logs-generator logs-generator --since=24h'
Nov 30 04:52:00.778: INFO: stderr: ""
Nov 30 04:52:00.778: INFO: stdout: "I1130 04:51:56.604292       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/qbjh 318\nI1130 04:51:56.804417       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/vcp5 318\nI1130 04:51:57.005023       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/w8x 436\nI1130 04:51:57.205408       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/qx7 564\nI1130 04:51:57.405295       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/dmgz 432\nI1130 04:51:57.605180       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/fvq 322\nI1130 04:51:57.804442       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2gfh 595\nI1130 04:51:58.004704       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/4mt 523\nI1130 04:51:58.205003       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/95v 248\nI1130 04:51:58.405307       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/tfh 340\nI1130 04:51:58.604706       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/vwj 332\nI1130 04:51:58.805022       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/ll5h 375\nI1130 04:51:59.004342       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/6w5 478\nI1130 04:51:59.204725       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/4hg 315\nI1130 04:51:59.404889       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/z44j 266\nI1130 04:51:59.605252       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/7569 205\nI1130 04:51:59.804538       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/vlq 323\nI1130 04:52:00.004929       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/4kq 332\nI1130 04:52:00.205309       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/jpsh 214\nI1130 04:52:00.404652       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/4dlp 261\nI1130 04:52:00.605003       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/shf 569\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Nov 30 04:52:00.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 delete pod logs-generator'
Nov 30 04:52:01.519: INFO: stderr: ""
Nov 30 04:52:01.519: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 04:52:01.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3973" for this suite. 11/30/22 04:52:01.526
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":330,"skipped":6021,"failed":0}
------------------------------
• [SLOW TEST] [5.787 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:51:55.749
    Nov 30 04:51:55.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 04:51:55.75
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:51:55.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:51:55.786
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 11/30/22 04:51:55.789
    Nov 30 04:51:55.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 run logs-generator --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Nov 30 04:51:55.865: INFO: stderr: ""
    Nov 30 04:51:55.866: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 11/30/22 04:51:55.866
    Nov 30 04:51:55.866: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Nov 30 04:51:55.866: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3973" to be "running and ready, or succeeded"
    Nov 30 04:51:55.874: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.903909ms
    Nov 30 04:51:55.874: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins' to be 'Running' but was 'Pending'
    Nov 30 04:51:57.880: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.014397275s
    Nov 30 04:51:57.880: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Nov 30 04:51:57.880: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 11/30/22 04:51:57.88
    Nov 30 04:51:57.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 logs logs-generator logs-generator'
    Nov 30 04:51:57.947: INFO: stderr: ""
    Nov 30 04:51:57.947: INFO: stdout: "I1130 04:51:56.604292       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/qbjh 318\nI1130 04:51:56.804417       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/vcp5 318\nI1130 04:51:57.005023       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/w8x 436\nI1130 04:51:57.205408       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/qx7 564\nI1130 04:51:57.405295       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/dmgz 432\nI1130 04:51:57.605180       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/fvq 322\nI1130 04:51:57.804442       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2gfh 595\n"
    STEP: limiting log lines 11/30/22 04:51:57.947
    Nov 30 04:51:57.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 logs logs-generator logs-generator --tail=1'
    Nov 30 04:51:58.017: INFO: stderr: ""
    Nov 30 04:51:58.017: INFO: stdout: "I1130 04:51:58.004704       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/4mt 523\n"
    Nov 30 04:51:58.017: INFO: got output "I1130 04:51:58.004704       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/4mt 523\n"
    STEP: limiting log bytes 11/30/22 04:51:58.017
    Nov 30 04:51:58.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 logs logs-generator logs-generator --limit-bytes=1'
    Nov 30 04:51:58.082: INFO: stderr: ""
    Nov 30 04:51:58.082: INFO: stdout: "I"
    Nov 30 04:51:58.082: INFO: got output "I"
    STEP: exposing timestamps 11/30/22 04:51:58.082
    Nov 30 04:51:58.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 logs logs-generator logs-generator --tail=1 --timestamps'
    Nov 30 04:51:58.147: INFO: stderr: ""
    Nov 30 04:51:58.147: INFO: stdout: "2022-11-30T06:51:58.004866468+02:00 I1130 04:51:58.004704       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/4mt 523\n"
    Nov 30 04:51:58.147: INFO: got output "2022-11-30T06:51:58.004866468+02:00 I1130 04:51:58.004704       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/4mt 523\n"
    STEP: restricting to a time range 11/30/22 04:51:58.147
    Nov 30 04:52:00.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 logs logs-generator logs-generator --since=1s'
    Nov 30 04:52:00.714: INFO: stderr: ""
    Nov 30 04:52:00.714: INFO: stdout: "I1130 04:51:59.804538       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/vlq 323\nI1130 04:52:00.004929       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/4kq 332\nI1130 04:52:00.205309       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/jpsh 214\nI1130 04:52:00.404652       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/4dlp 261\nI1130 04:52:00.605003       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/shf 569\n"
    Nov 30 04:52:00.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 logs logs-generator logs-generator --since=24h'
    Nov 30 04:52:00.778: INFO: stderr: ""
    Nov 30 04:52:00.778: INFO: stdout: "I1130 04:51:56.604292       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/qbjh 318\nI1130 04:51:56.804417       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/vcp5 318\nI1130 04:51:57.005023       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/w8x 436\nI1130 04:51:57.205408       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/qx7 564\nI1130 04:51:57.405295       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/dmgz 432\nI1130 04:51:57.605180       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/fvq 322\nI1130 04:51:57.804442       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2gfh 595\nI1130 04:51:58.004704       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/4mt 523\nI1130 04:51:58.205003       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/95v 248\nI1130 04:51:58.405307       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/tfh 340\nI1130 04:51:58.604706       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/vwj 332\nI1130 04:51:58.805022       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/ll5h 375\nI1130 04:51:59.004342       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/6w5 478\nI1130 04:51:59.204725       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/4hg 315\nI1130 04:51:59.404889       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/z44j 266\nI1130 04:51:59.605252       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/7569 205\nI1130 04:51:59.804538       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/vlq 323\nI1130 04:52:00.004929       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/4kq 332\nI1130 04:52:00.205309       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/jpsh 214\nI1130 04:52:00.404652       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/4dlp 261\nI1130 04:52:00.605003       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/shf 569\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Nov 30 04:52:00.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-3973 delete pod logs-generator'
    Nov 30 04:52:01.519: INFO: stderr: ""
    Nov 30 04:52:01.519: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 04:52:01.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3973" for this suite. 11/30/22 04:52:01.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:52:01.537
Nov 30 04:52:01.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 04:52:01.538
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:52:01.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:52:01.569
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-5e1670ba-8461-4ffc-a05e-8aa5d7fc2850 11/30/22 04:52:01.571
STEP: Creating a pod to test consume secrets 11/30/22 04:52:01.58
Nov 30 04:52:01.613: INFO: Waiting up to 5m0s for pod "pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604" in namespace "secrets-4547" to be "Succeeded or Failed"
Nov 30 04:52:01.617: INFO: Pod "pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604": Phase="Pending", Reason="", readiness=false. Elapsed: 4.124391ms
Nov 30 04:52:03.621: INFO: Pod "pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008389346s
Nov 30 04:52:05.625: INFO: Pod "pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012297134s
STEP: Saw pod success 11/30/22 04:52:05.625
Nov 30 04:52:05.625: INFO: Pod "pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604" satisfied condition "Succeeded or Failed"
Nov 30 04:52:05.632: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604 container secret-volume-test: <nil>
STEP: delete the pod 11/30/22 04:52:05.64
Nov 30 04:52:05.657: INFO: Waiting for pod pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604 to disappear
Nov 30 04:52:05.659: INFO: Pod pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Nov 30 04:52:05.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4547" for this suite. 11/30/22 04:52:05.684
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":331,"skipped":6031,"failed":0}
------------------------------
• [4.162 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:52:01.537
    Nov 30 04:52:01.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 04:52:01.538
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:52:01.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:52:01.569
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-5e1670ba-8461-4ffc-a05e-8aa5d7fc2850 11/30/22 04:52:01.571
    STEP: Creating a pod to test consume secrets 11/30/22 04:52:01.58
    Nov 30 04:52:01.613: INFO: Waiting up to 5m0s for pod "pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604" in namespace "secrets-4547" to be "Succeeded or Failed"
    Nov 30 04:52:01.617: INFO: Pod "pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604": Phase="Pending", Reason="", readiness=false. Elapsed: 4.124391ms
    Nov 30 04:52:03.621: INFO: Pod "pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008389346s
    Nov 30 04:52:05.625: INFO: Pod "pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012297134s
    STEP: Saw pod success 11/30/22 04:52:05.625
    Nov 30 04:52:05.625: INFO: Pod "pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604" satisfied condition "Succeeded or Failed"
    Nov 30 04:52:05.632: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604 container secret-volume-test: <nil>
    STEP: delete the pod 11/30/22 04:52:05.64
    Nov 30 04:52:05.657: INFO: Waiting for pod pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604 to disappear
    Nov 30 04:52:05.659: INFO: Pod pod-secrets-346f4596-b522-4e53-80de-6b8d5551d604 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 04:52:05.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4547" for this suite. 11/30/22 04:52:05.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:52:05.699
Nov 30 04:52:05.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename watch 11/30/22 04:52:05.7
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:52:05.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:52:05.72
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 11/30/22 04:52:05.722
STEP: creating a watch on configmaps with label B 11/30/22 04:52:05.723
STEP: creating a watch on configmaps with label A or B 11/30/22 04:52:05.725
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 11/30/22 04:52:05.726
Nov 30 04:52:05.738: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68952 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:52:05.738: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68952 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 11/30/22 04:52:05.738
Nov 30 04:52:05.748: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68954 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:52:05.748: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68954 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 11/30/22 04:52:05.748
Nov 30 04:52:05.756: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68955 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:52:05.756: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68955 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 11/30/22 04:52:05.756
Nov 30 04:52:05.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68956 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:52:05.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68956 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 11/30/22 04:52:05.761
Nov 30 04:52:05.765: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-850  5464e239-03eb-4ee4-aba5-dfe8e59412dc 68957 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:52:05.765: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-850  5464e239-03eb-4ee4-aba5-dfe8e59412dc 68957 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 11/30/22 04:52:15.766
Nov 30 04:52:15.773: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-850  5464e239-03eb-4ee4-aba5-dfe8e59412dc 69033 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:52:15.773: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-850  5464e239-03eb-4ee4-aba5-dfe8e59412dc 69033 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Nov 30 04:52:25.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-850" for this suite. 11/30/22 04:52:25.778
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":332,"skipped":6044,"failed":0}
------------------------------
• [SLOW TEST] [20.088 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:52:05.699
    Nov 30 04:52:05.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename watch 11/30/22 04:52:05.7
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:52:05.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:52:05.72
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 11/30/22 04:52:05.722
    STEP: creating a watch on configmaps with label B 11/30/22 04:52:05.723
    STEP: creating a watch on configmaps with label A or B 11/30/22 04:52:05.725
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 11/30/22 04:52:05.726
    Nov 30 04:52:05.738: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68952 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:52:05.738: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68952 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 11/30/22 04:52:05.738
    Nov 30 04:52:05.748: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68954 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:52:05.748: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68954 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 11/30/22 04:52:05.748
    Nov 30 04:52:05.756: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68955 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:52:05.756: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68955 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 11/30/22 04:52:05.756
    Nov 30 04:52:05.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68956 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:52:05.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-850  7510dc3e-cb8c-4570-9391-f1c203a5e45d 68956 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 11/30/22 04:52:05.761
    Nov 30 04:52:05.765: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-850  5464e239-03eb-4ee4-aba5-dfe8e59412dc 68957 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:52:05.765: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-850  5464e239-03eb-4ee4-aba5-dfe8e59412dc 68957 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 11/30/22 04:52:15.766
    Nov 30 04:52:15.773: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-850  5464e239-03eb-4ee4-aba5-dfe8e59412dc 69033 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:52:15.773: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-850  5464e239-03eb-4ee4-aba5-dfe8e59412dc 69033 0 2022-11-30 04:52:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-11-30 04:52:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Nov 30 04:52:25.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-850" for this suite. 11/30/22 04:52:25.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:52:25.788
Nov 30 04:52:25.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename var-expansion 11/30/22 04:52:25.788
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:52:25.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:52:25.814
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 11/30/22 04:52:25.816
Nov 30 04:52:25.845: INFO: Waiting up to 5m0s for pod "var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f" in namespace "var-expansion-3976" to be "Succeeded or Failed"
Nov 30 04:52:25.848: INFO: Pod "var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.188862ms
Nov 30 04:52:27.852: INFO: Pod "var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007765174s
Nov 30 04:52:29.851: INFO: Pod "var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006486946s
STEP: Saw pod success 11/30/22 04:52:29.851
Nov 30 04:52:29.851: INFO: Pod "var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f" satisfied condition "Succeeded or Failed"
Nov 30 04:52:29.855: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f container dapi-container: <nil>
STEP: delete the pod 11/30/22 04:52:29.865
Nov 30 04:52:29.877: INFO: Waiting for pod var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f to disappear
Nov 30 04:52:29.881: INFO: Pod var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Nov 30 04:52:29.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3976" for this suite. 11/30/22 04:52:29.885
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":333,"skipped":6068,"failed":0}
------------------------------
• [4.103 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:52:25.788
    Nov 30 04:52:25.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename var-expansion 11/30/22 04:52:25.788
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:52:25.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:52:25.814
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 11/30/22 04:52:25.816
    Nov 30 04:52:25.845: INFO: Waiting up to 5m0s for pod "var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f" in namespace "var-expansion-3976" to be "Succeeded or Failed"
    Nov 30 04:52:25.848: INFO: Pod "var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.188862ms
    Nov 30 04:52:27.852: INFO: Pod "var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007765174s
    Nov 30 04:52:29.851: INFO: Pod "var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006486946s
    STEP: Saw pod success 11/30/22 04:52:29.851
    Nov 30 04:52:29.851: INFO: Pod "var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f" satisfied condition "Succeeded or Failed"
    Nov 30 04:52:29.855: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f container dapi-container: <nil>
    STEP: delete the pod 11/30/22 04:52:29.865
    Nov 30 04:52:29.877: INFO: Waiting for pod var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f to disappear
    Nov 30 04:52:29.881: INFO: Pod var-expansion-d4b86180-607d-401b-b5c1-4ed0762bd43f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Nov 30 04:52:29.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3976" for this suite. 11/30/22 04:52:29.885
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:52:29.89
Nov 30 04:52:29.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename security-context 11/30/22 04:52:29.891
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:52:29.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:52:29.91
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/30/22 04:52:29.912
Nov 30 04:52:29.925: INFO: Waiting up to 5m0s for pod "security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d" in namespace "security-context-5390" to be "Succeeded or Failed"
Nov 30 04:52:29.928: INFO: Pod "security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.415007ms
Nov 30 04:52:31.933: INFO: Pod "security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007419806s
Nov 30 04:52:33.932: INFO: Pod "security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006272709s
STEP: Saw pod success 11/30/22 04:52:33.932
Nov 30 04:52:33.932: INFO: Pod "security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d" satisfied condition "Succeeded or Failed"
Nov 30 04:52:33.934: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d container test-container: <nil>
STEP: delete the pod 11/30/22 04:52:33.939
Nov 30 04:52:33.950: INFO: Waiting for pod security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d to disappear
Nov 30 04:52:33.953: INFO: Pod security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Nov 30 04:52:33.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5390" for this suite. 11/30/22 04:52:33.957
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":334,"skipped":6069,"failed":0}
------------------------------
• [4.072 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:52:29.89
    Nov 30 04:52:29.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename security-context 11/30/22 04:52:29.891
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:52:29.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:52:29.91
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/30/22 04:52:29.912
    Nov 30 04:52:29.925: INFO: Waiting up to 5m0s for pod "security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d" in namespace "security-context-5390" to be "Succeeded or Failed"
    Nov 30 04:52:29.928: INFO: Pod "security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.415007ms
    Nov 30 04:52:31.933: INFO: Pod "security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007419806s
    Nov 30 04:52:33.932: INFO: Pod "security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006272709s
    STEP: Saw pod success 11/30/22 04:52:33.932
    Nov 30 04:52:33.932: INFO: Pod "security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d" satisfied condition "Succeeded or Failed"
    Nov 30 04:52:33.934: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d container test-container: <nil>
    STEP: delete the pod 11/30/22 04:52:33.939
    Nov 30 04:52:33.950: INFO: Waiting for pod security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d to disappear
    Nov 30 04:52:33.953: INFO: Pod security-context-fd24fc09-45b7-4ceb-aa60-cda1b984f11d no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Nov 30 04:52:33.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-5390" for this suite. 11/30/22 04:52:33.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:52:33.963
Nov 30 04:52:33.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-probe 11/30/22 04:52:33.964
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:52:33.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:52:33.99
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3 in namespace container-probe-8089 11/30/22 04:52:33.992
Nov 30 04:52:34.003: INFO: Waiting up to 5m0s for pod "busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3" in namespace "container-probe-8089" to be "not pending"
Nov 30 04:52:34.005: INFO: Pod "busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499994ms
Nov 30 04:52:36.009: INFO: Pod "busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006532856s
Nov 30 04:52:36.009: INFO: Pod "busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3" satisfied condition "not pending"
Nov 30 04:52:36.009: INFO: Started pod busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3 in namespace container-probe-8089
STEP: checking the pod's current state and verifying that restartCount is present 11/30/22 04:52:36.009
Nov 30 04:52:36.013: INFO: Initial restart count of pod busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3 is 0
Nov 30 04:53:26.147: INFO: Restart count of pod container-probe-8089/busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3 is now 1 (50.133925504s elapsed)
STEP: deleting the pod 11/30/22 04:53:26.147
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Nov 30 04:53:26.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8089" for this suite. 11/30/22 04:53:26.165
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":335,"skipped":6110,"failed":0}
------------------------------
• [SLOW TEST] [52.206 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:52:33.963
    Nov 30 04:52:33.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-probe 11/30/22 04:52:33.964
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:52:33.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:52:33.99
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3 in namespace container-probe-8089 11/30/22 04:52:33.992
    Nov 30 04:52:34.003: INFO: Waiting up to 5m0s for pod "busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3" in namespace "container-probe-8089" to be "not pending"
    Nov 30 04:52:34.005: INFO: Pod "busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499994ms
    Nov 30 04:52:36.009: INFO: Pod "busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006532856s
    Nov 30 04:52:36.009: INFO: Pod "busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3" satisfied condition "not pending"
    Nov 30 04:52:36.009: INFO: Started pod busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3 in namespace container-probe-8089
    STEP: checking the pod's current state and verifying that restartCount is present 11/30/22 04:52:36.009
    Nov 30 04:52:36.013: INFO: Initial restart count of pod busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3 is 0
    Nov 30 04:53:26.147: INFO: Restart count of pod container-probe-8089/busybox-ee60803f-13eb-4bf1-90d9-0fe8a00ad2b3 is now 1 (50.133925504s elapsed)
    STEP: deleting the pod 11/30/22 04:53:26.147
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Nov 30 04:53:26.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8089" for this suite. 11/30/22 04:53:26.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:53:26.17
Nov 30 04:53:26.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename crd-webhook 11/30/22 04:53:26.171
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:53:26.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:53:26.189
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 11/30/22 04:53:26.192
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/30/22 04:53:26.603
STEP: Deploying the custom resource conversion webhook pod 11/30/22 04:53:26.609
STEP: Wait for the deployment to be ready 11/30/22 04:53:26.619
Nov 30 04:53:26.625: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 30 04:53:28.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 53, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 53, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 53, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 53, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-585d68999\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/30/22 04:53:30.637
STEP: Verifying the service has paired with the endpoint 11/30/22 04:53:30.649
Nov 30 04:53:31.649: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Nov 30 04:53:31.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Creating a v1 custom resource 11/30/22 04:53:34.228
STEP: v2 custom resource should be converted 11/30/22 04:53:34.232
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:53:34.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7278" for this suite. 11/30/22 04:53:34.752
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":336,"skipped":6130,"failed":0}
------------------------------
• [SLOW TEST] [8.635 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:53:26.17
    Nov 30 04:53:26.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename crd-webhook 11/30/22 04:53:26.171
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:53:26.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:53:26.189
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 11/30/22 04:53:26.192
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/30/22 04:53:26.603
    STEP: Deploying the custom resource conversion webhook pod 11/30/22 04:53:26.609
    STEP: Wait for the deployment to be ready 11/30/22 04:53:26.619
    Nov 30 04:53:26.625: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Nov 30 04:53:28.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.November, 30, 4, 53, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 53, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.November, 30, 4, 53, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.November, 30, 4, 53, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-585d68999\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/30/22 04:53:30.637
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:53:30.649
    Nov 30 04:53:31.649: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Nov 30 04:53:31.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Creating a v1 custom resource 11/30/22 04:53:34.228
    STEP: v2 custom resource should be converted 11/30/22 04:53:34.232
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:53:34.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-7278" for this suite. 11/30/22 04:53:34.752
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:53:34.805
Nov 30 04:53:34.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename runtimeclass 11/30/22 04:53:34.806
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:53:34.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:53:34.834
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Nov 30 04:53:34.877: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1930 to be scheduled
Nov 30 04:53:34.880: INFO: 1 pods are not scheduled: [runtimeclass-1930/test-runtimeclass-runtimeclass-1930-preconfigured-handler-jkpxr(a2216459-ba69-41e3-bc08-7aa42c6e1d8b)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Nov 30 04:53:36.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1930" for this suite. 11/30/22 04:53:36.892
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":337,"skipped":6134,"failed":0}
------------------------------
• [2.092 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:53:34.805
    Nov 30 04:53:34.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename runtimeclass 11/30/22 04:53:34.806
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:53:34.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:53:34.834
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Nov 30 04:53:34.877: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1930 to be scheduled
    Nov 30 04:53:34.880: INFO: 1 pods are not scheduled: [runtimeclass-1930/test-runtimeclass-runtimeclass-1930-preconfigured-handler-jkpxr(a2216459-ba69-41e3-bc08-7aa42c6e1d8b)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Nov 30 04:53:36.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1930" for this suite. 11/30/22 04:53:36.892
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:53:36.897
Nov 30 04:53:36.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:53:36.898
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:53:36.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:53:36.918
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-ec2450ae-b504-4b19-b70e-52b0bfd95608 11/30/22 04:53:36.919
STEP: Creating a pod to test consume configMaps 11/30/22 04:53:36.923
Nov 30 04:53:36.935: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636" in namespace "projected-1973" to be "Succeeded or Failed"
Nov 30 04:53:36.938: INFO: Pod "pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636": Phase="Pending", Reason="", readiness=false. Elapsed: 3.047291ms
Nov 30 04:53:38.942: INFO: Pod "pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007686025s
Nov 30 04:53:40.942: INFO: Pod "pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00767159s
STEP: Saw pod success 11/30/22 04:53:40.942
Nov 30 04:53:40.943: INFO: Pod "pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636" satisfied condition "Succeeded or Failed"
Nov 30 04:53:40.945: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636 container agnhost-container: <nil>
STEP: delete the pod 11/30/22 04:53:40.949
Nov 30 04:53:40.963: INFO: Waiting for pod pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636 to disappear
Nov 30 04:53:40.966: INFO: Pod pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Nov 30 04:53:40.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1973" for this suite. 11/30/22 04:53:40.969
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":338,"skipped":6138,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:53:36.897
    Nov 30 04:53:36.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:53:36.898
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:53:36.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:53:36.918
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-ec2450ae-b504-4b19-b70e-52b0bfd95608 11/30/22 04:53:36.919
    STEP: Creating a pod to test consume configMaps 11/30/22 04:53:36.923
    Nov 30 04:53:36.935: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636" in namespace "projected-1973" to be "Succeeded or Failed"
    Nov 30 04:53:36.938: INFO: Pod "pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636": Phase="Pending", Reason="", readiness=false. Elapsed: 3.047291ms
    Nov 30 04:53:38.942: INFO: Pod "pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007686025s
    Nov 30 04:53:40.942: INFO: Pod "pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00767159s
    STEP: Saw pod success 11/30/22 04:53:40.942
    Nov 30 04:53:40.943: INFO: Pod "pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636" satisfied condition "Succeeded or Failed"
    Nov 30 04:53:40.945: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636 container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 04:53:40.949
    Nov 30 04:53:40.963: INFO: Waiting for pod pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636 to disappear
    Nov 30 04:53:40.966: INFO: Pod pod-projected-configmaps-02c56666-1dd7-4fa8-b97c-b31beb3c5636 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Nov 30 04:53:40.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1973" for this suite. 11/30/22 04:53:40.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:53:40.978
Nov 30 04:53:40.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename container-runtime 11/30/22 04:53:40.979
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:53:40.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:53:40.996
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 11/30/22 04:53:40.998
STEP: wait for the container to reach Succeeded 11/30/22 04:53:41.005
STEP: get the container status 11/30/22 04:53:45.046
STEP: the container should be terminated 11/30/22 04:53:45.052
STEP: the termination message should be set 11/30/22 04:53:45.052
Nov 30 04:53:45.052: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 11/30/22 04:53:45.052
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Nov 30 04:53:45.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-203" for this suite. 11/30/22 04:53:45.091
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":339,"skipped":6247,"failed":0}
------------------------------
• [4.118 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:53:40.978
    Nov 30 04:53:40.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename container-runtime 11/30/22 04:53:40.979
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:53:40.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:53:40.996
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 11/30/22 04:53:40.998
    STEP: wait for the container to reach Succeeded 11/30/22 04:53:41.005
    STEP: get the container status 11/30/22 04:53:45.046
    STEP: the container should be terminated 11/30/22 04:53:45.052
    STEP: the termination message should be set 11/30/22 04:53:45.052
    Nov 30 04:53:45.052: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 11/30/22 04:53:45.052
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Nov 30 04:53:45.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-203" for this suite. 11/30/22 04:53:45.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:53:45.097
Nov 30 04:53:45.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename webhook 11/30/22 04:53:45.098
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:53:45.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:53:45.113
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 11/30/22 04:53:45.128
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:53:45.615
STEP: Deploying the webhook pod 11/30/22 04:53:45.644
STEP: Wait for the deployment to be ready 11/30/22 04:53:45.669
Nov 30 04:53:45.674: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 11/30/22 04:53:47.686
STEP: Verifying the service has paired with the endpoint 11/30/22 04:53:47.717
Nov 30 04:53:48.717: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Nov 30 04:53:48.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4245-crds.webhook.example.com via the AdmissionRegistration API 11/30/22 04:53:49.231
Nov 30 04:53:49.245: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version 11/30/22 04:53:49.353
STEP: Patching Custom Resource Definition to set v2 as storage 11/30/22 04:53:51.404
STEP: Patching the custom resource while v2 is storage version 11/30/22 04:53:51.42
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Nov 30 04:53:51.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8737" for this suite. 11/30/22 04:53:51.984
STEP: Destroying namespace "webhook-8737-markers" for this suite. 11/30/22 04:53:51.989
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":340,"skipped":6287,"failed":0}
------------------------------
• [SLOW TEST] [7.081 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:53:45.097
    Nov 30 04:53:45.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename webhook 11/30/22 04:53:45.098
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:53:45.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:53:45.113
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 11/30/22 04:53:45.128
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/30/22 04:53:45.615
    STEP: Deploying the webhook pod 11/30/22 04:53:45.644
    STEP: Wait for the deployment to be ready 11/30/22 04:53:45.669
    Nov 30 04:53:45.674: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 11/30/22 04:53:47.686
    STEP: Verifying the service has paired with the endpoint 11/30/22 04:53:47.717
    Nov 30 04:53:48.717: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Nov 30 04:53:48.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4245-crds.webhook.example.com via the AdmissionRegistration API 11/30/22 04:53:49.231
    Nov 30 04:53:49.245: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a custom resource while v1 is storage version 11/30/22 04:53:49.353
    STEP: Patching Custom Resource Definition to set v2 as storage 11/30/22 04:53:51.404
    STEP: Patching the custom resource while v2 is storage version 11/30/22 04:53:51.42
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Nov 30 04:53:51.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8737" for this suite. 11/30/22 04:53:51.984
    STEP: Destroying namespace "webhook-8737-markers" for this suite. 11/30/22 04:53:51.989
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:53:52.18
Nov 30 04:53:52.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename resourcequota 11/30/22 04:53:52.18
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:53:52.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:53:52.26
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 11/30/22 04:53:52.262
STEP: Creating a ResourceQuota 11/30/22 04:53:57.266
STEP: Ensuring resource quota status is calculated 11/30/22 04:53:57.273
STEP: Creating a ReplicationController 11/30/22 04:53:59.277
STEP: Ensuring resource quota status captures replication controller creation 11/30/22 04:53:59.286
STEP: Deleting a ReplicationController 11/30/22 04:54:01.289
STEP: Ensuring resource quota status released usage 11/30/22 04:54:01.295
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Nov 30 04:54:03.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9626" for this suite. 11/30/22 04:54:03.302
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":341,"skipped":6323,"failed":0}
------------------------------
• [SLOW TEST] [11.128 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:53:52.18
    Nov 30 04:53:52.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename resourcequota 11/30/22 04:53:52.18
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:53:52.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:53:52.26
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 11/30/22 04:53:52.262
    STEP: Creating a ResourceQuota 11/30/22 04:53:57.266
    STEP: Ensuring resource quota status is calculated 11/30/22 04:53:57.273
    STEP: Creating a ReplicationController 11/30/22 04:53:59.277
    STEP: Ensuring resource quota status captures replication controller creation 11/30/22 04:53:59.286
    STEP: Deleting a ReplicationController 11/30/22 04:54:01.289
    STEP: Ensuring resource quota status released usage 11/30/22 04:54:01.295
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Nov 30 04:54:03.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9626" for this suite. 11/30/22 04:54:03.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:03.308
Nov 30 04:54:03.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename secrets 11/30/22 04:54:03.309
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:03.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:03.339
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Nov 30 04:54:03.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2371" for this suite. 11/30/22 04:54:03.378
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":342,"skipped":6363,"failed":0}
------------------------------
• [0.076 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:03.308
    Nov 30 04:54:03.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename secrets 11/30/22 04:54:03.309
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:03.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:03.339
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Nov 30 04:54:03.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2371" for this suite. 11/30/22 04:54:03.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:03.385
Nov 30 04:54:03.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename containers 11/30/22 04:54:03.386
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:03.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:03.406
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Nov 30 04:54:03.436: INFO: Waiting up to 5m0s for pod "client-containers-14fa0d0b-3b61-4167-886e-4daaf939ab32" in namespace "containers-449" to be "running"
Nov 30 04:54:03.438: INFO: Pod "client-containers-14fa0d0b-3b61-4167-886e-4daaf939ab32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084015ms
Nov 30 04:54:05.442: INFO: Pod "client-containers-14fa0d0b-3b61-4167-886e-4daaf939ab32": Phase="Running", Reason="", readiness=true. Elapsed: 2.005363637s
Nov 30 04:54:05.442: INFO: Pod "client-containers-14fa0d0b-3b61-4167-886e-4daaf939ab32" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Nov 30 04:54:05.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-449" for this suite. 11/30/22 04:54:05.452
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":343,"skipped":6396,"failed":0}
------------------------------
• [2.072 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:03.385
    Nov 30 04:54:03.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename containers 11/30/22 04:54:03.386
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:03.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:03.406
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Nov 30 04:54:03.436: INFO: Waiting up to 5m0s for pod "client-containers-14fa0d0b-3b61-4167-886e-4daaf939ab32" in namespace "containers-449" to be "running"
    Nov 30 04:54:03.438: INFO: Pod "client-containers-14fa0d0b-3b61-4167-886e-4daaf939ab32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084015ms
    Nov 30 04:54:05.442: INFO: Pod "client-containers-14fa0d0b-3b61-4167-886e-4daaf939ab32": Phase="Running", Reason="", readiness=true. Elapsed: 2.005363637s
    Nov 30 04:54:05.442: INFO: Pod "client-containers-14fa0d0b-3b61-4167-886e-4daaf939ab32" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Nov 30 04:54:05.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-449" for this suite. 11/30/22 04:54:05.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:05.458
Nov 30 04:54:05.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename resourcequota 11/30/22 04:54:05.459
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:05.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:05.479
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 11/30/22 04:54:05.481
STEP: Getting a ResourceQuota 11/30/22 04:54:05.489
STEP: Listing all ResourceQuotas with LabelSelector 11/30/22 04:54:05.494
STEP: Patching the ResourceQuota 11/30/22 04:54:05.498
STEP: Deleting a Collection of ResourceQuotas 11/30/22 04:54:05.51
STEP: Verifying the deleted ResourceQuota 11/30/22 04:54:05.521
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Nov 30 04:54:05.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1600" for this suite. 11/30/22 04:54:05.526
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":344,"skipped":6408,"failed":0}
------------------------------
• [0.073 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:05.458
    Nov 30 04:54:05.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename resourcequota 11/30/22 04:54:05.459
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:05.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:05.479
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 11/30/22 04:54:05.481
    STEP: Getting a ResourceQuota 11/30/22 04:54:05.489
    STEP: Listing all ResourceQuotas with LabelSelector 11/30/22 04:54:05.494
    STEP: Patching the ResourceQuota 11/30/22 04:54:05.498
    STEP: Deleting a Collection of ResourceQuotas 11/30/22 04:54:05.51
    STEP: Verifying the deleted ResourceQuota 11/30/22 04:54:05.521
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Nov 30 04:54:05.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1600" for this suite. 11/30/22 04:54:05.526
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:05.531
Nov 30 04:54:05.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubectl 11/30/22 04:54:05.532
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:05.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:05.575
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 11/30/22 04:54:05.578
Nov 30 04:54:05.579: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-7778 proxy --unix-socket=/tmp/kubectl-proxy-unix1111947702/test'
STEP: retrieving proxy /api/ output 11/30/22 04:54:05.624
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Nov 30 04:54:05.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7778" for this suite. 11/30/22 04:54:05.63
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":345,"skipped":6409,"failed":0}
------------------------------
• [0.117 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:05.531
    Nov 30 04:54:05.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubectl 11/30/22 04:54:05.532
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:05.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:05.575
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 11/30/22 04:54:05.578
    Nov 30 04:54:05.579: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1327404838 --namespace=kubectl-7778 proxy --unix-socket=/tmp/kubectl-proxy-unix1111947702/test'
    STEP: retrieving proxy /api/ output 11/30/22 04:54:05.624
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Nov 30 04:54:05.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7778" for this suite. 11/30/22 04:54:05.63
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:05.648
Nov 30 04:54:05.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename runtimeclass 11/30/22 04:54:05.649
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:05.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:05.668
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-5048-delete-me 11/30/22 04:54:05.677
STEP: Waiting for the RuntimeClass to disappear 11/30/22 04:54:05.698
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Nov 30 04:54:05.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5048" for this suite. 11/30/22 04:54:05.712
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":346,"skipped":6409,"failed":0}
------------------------------
• [0.070 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:05.648
    Nov 30 04:54:05.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename runtimeclass 11/30/22 04:54:05.649
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:05.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:05.668
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-5048-delete-me 11/30/22 04:54:05.677
    STEP: Waiting for the RuntimeClass to disappear 11/30/22 04:54:05.698
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Nov 30 04:54:05.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5048" for this suite. 11/30/22 04:54:05.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:05.719
Nov 30 04:54:05.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 04:54:05.72
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:05.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:05.736
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 11/30/22 04:54:05.739
Nov 30 04:54:05.753: INFO: Waiting up to 5m0s for pod "pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07" in namespace "emptydir-9530" to be "Succeeded or Failed"
Nov 30 04:54:05.757: INFO: Pod "pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.907318ms
Nov 30 04:54:07.774: INFO: Pod "pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07": Phase="Running", Reason="", readiness=false. Elapsed: 2.020681855s
Nov 30 04:54:09.761: INFO: Pod "pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00813787s
STEP: Saw pod success 11/30/22 04:54:09.761
Nov 30 04:54:09.761: INFO: Pod "pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07" satisfied condition "Succeeded or Failed"
Nov 30 04:54:09.764: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07 container test-container: <nil>
STEP: delete the pod 11/30/22 04:54:09.77
Nov 30 04:54:09.785: INFO: Waiting for pod pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07 to disappear
Nov 30 04:54:09.788: INFO: Pod pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 04:54:09.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9530" for this suite. 11/30/22 04:54:09.791
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":347,"skipped":6433,"failed":0}
------------------------------
• [4.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:05.719
    Nov 30 04:54:05.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 04:54:05.72
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:05.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:05.736
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 11/30/22 04:54:05.739
    Nov 30 04:54:05.753: INFO: Waiting up to 5m0s for pod "pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07" in namespace "emptydir-9530" to be "Succeeded or Failed"
    Nov 30 04:54:05.757: INFO: Pod "pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.907318ms
    Nov 30 04:54:07.774: INFO: Pod "pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07": Phase="Running", Reason="", readiness=false. Elapsed: 2.020681855s
    Nov 30 04:54:09.761: INFO: Pod "pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00813787s
    STEP: Saw pod success 11/30/22 04:54:09.761
    Nov 30 04:54:09.761: INFO: Pod "pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07" satisfied condition "Succeeded or Failed"
    Nov 30 04:54:09.764: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07 container test-container: <nil>
    STEP: delete the pod 11/30/22 04:54:09.77
    Nov 30 04:54:09.785: INFO: Waiting for pod pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07 to disappear
    Nov 30 04:54:09.788: INFO: Pod pod-ae62a290-9e1f-4a3b-9231-e790bbd43c07 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 04:54:09.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9530" for this suite. 11/30/22 04:54:09.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:09.797
Nov 30 04:54:09.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename daemonsets 11/30/22 04:54:09.798
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:09.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:09.816
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Nov 30 04:54:09.839: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 11/30/22 04:54:09.845
Nov 30 04:54:09.850: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:09.850: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:09.850: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:09.860: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:54:09.860: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:54:10.870: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:10.870: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:10.870: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:10.873: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 30 04:54:10.873: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:54:11.865: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:11.865: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:11.865: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:11.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Nov 30 04:54:11.868: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Update daemon pods image. 11/30/22 04:54:11.878
STEP: Check that daemon pods images are updated. 11/30/22 04:54:11.887
Nov 30 04:54:11.891: INFO: Wrong image for pod: daemon-set-9d7qg. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:11.891: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:11.891: INFO: Wrong image for pod: daemon-set-r4wg8. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:11.891: INFO: Wrong image for pod: daemon-set-ssjcc. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:11.900: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:11.901: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:11.901: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:12.905: INFO: Wrong image for pod: daemon-set-9d7qg. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:12.905: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:12.905: INFO: Wrong image for pod: daemon-set-r4wg8. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:12.908: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:12.908: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:12.908: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:13.905: INFO: Wrong image for pod: daemon-set-9d7qg. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:13.905: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:13.905: INFO: Wrong image for pod: daemon-set-r4wg8. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:13.909: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:13.909: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:13.909: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:14.904: INFO: Pod daemon-set-5w4ml is not available
Nov 30 04:54:14.904: INFO: Wrong image for pod: daemon-set-9d7qg. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:14.904: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:14.904: INFO: Wrong image for pod: daemon-set-r4wg8. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:14.907: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:14.907: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:14.907: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:15.906: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:15.906: INFO: Wrong image for pod: daemon-set-r4wg8. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:15.915: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:15.915: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:15.915: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:16.905: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:16.905: INFO: Pod daemon-set-kxvqf is not available
Nov 30 04:54:16.905: INFO: Wrong image for pod: daemon-set-r4wg8. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:16.909: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:16.909: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:16.909: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:17.905: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:17.911: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:17.911: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:17.911: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:18.905: INFO: Pod daemon-set-6skbb is not available
Nov 30 04:54:18.905: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:18.909: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:18.909: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:18.909: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:19.905: INFO: Pod daemon-set-6skbb is not available
Nov 30 04:54:19.905: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
Nov 30 04:54:19.908: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:19.908: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:19.908: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:20.904: INFO: Pod daemon-set-494bz is not available
Nov 30 04:54:20.907: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:20.907: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:20.907: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 11/30/22 04:54:20.907
Nov 30 04:54:20.911: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:20.911: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:20.911: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:20.914: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 30 04:54:20.914: INFO: Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Nov 30 04:54:21.919: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:21.919: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:21.919: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 30 04:54:21.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Nov 30 04:54:21.922: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 11/30/22 04:54:21.934
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1816, will wait for the garbage collector to delete the pods 11/30/22 04:54:21.934
Nov 30 04:54:21.992: INFO: Deleting DaemonSet.extensions daemon-set took: 5.553023ms
Nov 30 04:54:22.093: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.592058ms
Nov 30 04:54:24.797: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 30 04:54:24.797: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 30 04:54:24.799: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70409"},"items":null}

Nov 30 04:54:24.802: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70409"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Nov 30 04:54:24.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1816" for this suite. 11/30/22 04:54:24.819
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":348,"skipped":6454,"failed":0}
------------------------------
• [SLOW TEST] [15.027 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:09.797
    Nov 30 04:54:09.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename daemonsets 11/30/22 04:54:09.798
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:09.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:09.816
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Nov 30 04:54:09.839: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 11/30/22 04:54:09.845
    Nov 30 04:54:09.850: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:09.850: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:09.850: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:09.860: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:54:09.860: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:54:10.870: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:10.870: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:10.870: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:10.873: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 30 04:54:10.873: INFO: Node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:54:11.865: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:11.865: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:11.865: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:11.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Nov 30 04:54:11.868: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Update daemon pods image. 11/30/22 04:54:11.878
    STEP: Check that daemon pods images are updated. 11/30/22 04:54:11.887
    Nov 30 04:54:11.891: INFO: Wrong image for pod: daemon-set-9d7qg. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:11.891: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:11.891: INFO: Wrong image for pod: daemon-set-r4wg8. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:11.891: INFO: Wrong image for pod: daemon-set-ssjcc. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:11.900: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:11.901: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:11.901: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:12.905: INFO: Wrong image for pod: daemon-set-9d7qg. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:12.905: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:12.905: INFO: Wrong image for pod: daemon-set-r4wg8. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:12.908: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:12.908: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:12.908: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:13.905: INFO: Wrong image for pod: daemon-set-9d7qg. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:13.905: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:13.905: INFO: Wrong image for pod: daemon-set-r4wg8. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:13.909: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:13.909: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:13.909: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:14.904: INFO: Pod daemon-set-5w4ml is not available
    Nov 30 04:54:14.904: INFO: Wrong image for pod: daemon-set-9d7qg. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:14.904: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:14.904: INFO: Wrong image for pod: daemon-set-r4wg8. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:14.907: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:14.907: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:14.907: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:15.906: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:15.906: INFO: Wrong image for pod: daemon-set-r4wg8. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:15.915: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:15.915: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:15.915: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:16.905: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:16.905: INFO: Pod daemon-set-kxvqf is not available
    Nov 30 04:54:16.905: INFO: Wrong image for pod: daemon-set-r4wg8. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:16.909: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:16.909: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:16.909: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:17.905: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:17.911: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:17.911: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:17.911: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:18.905: INFO: Pod daemon-set-6skbb is not available
    Nov 30 04:54:18.905: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:18.909: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:18.909: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:18.909: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:19.905: INFO: Pod daemon-set-6skbb is not available
    Nov 30 04:54:19.905: INFO: Wrong image for pod: daemon-set-b92bt. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2.
    Nov 30 04:54:19.908: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:19.908: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:19.908: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:20.904: INFO: Pod daemon-set-494bz is not available
    Nov 30 04:54:20.907: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:20.907: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:20.907: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 11/30/22 04:54:20.907
    Nov 30 04:54:20.911: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:20.911: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:20.911: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:20.914: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 30 04:54:20.914: INFO: Node worker-pool1-w7rh3pp2-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Nov 30 04:54:21.919: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:21.919: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:21.919: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 30 04:54:21.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Nov 30 04:54:21.922: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 11/30/22 04:54:21.934
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1816, will wait for the garbage collector to delete the pods 11/30/22 04:54:21.934
    Nov 30 04:54:21.992: INFO: Deleting DaemonSet.extensions daemon-set took: 5.553023ms
    Nov 30 04:54:22.093: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.592058ms
    Nov 30 04:54:24.797: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 30 04:54:24.797: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 30 04:54:24.799: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70409"},"items":null}

    Nov 30 04:54:24.802: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70409"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Nov 30 04:54:24.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1816" for this suite. 11/30/22 04:54:24.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:24.825
Nov 30 04:54:24.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:54:24.825
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:24.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:24.894
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-ea0c8ffc-aa43-4afe-a471-701a28b1405f 11/30/22 04:54:24.903
STEP: Creating configMap with name cm-test-opt-upd-5c1bf141-a3a0-4609-a4b5-8b848fe435bf 11/30/22 04:54:24.91
STEP: Creating the pod 11/30/22 04:54:24.914
Nov 30 04:54:24.943: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7debdbd6-0eff-44ad-8e0b-f0d471501731" in namespace "projected-758" to be "running and ready"
Nov 30 04:54:24.946: INFO: Pod "pod-projected-configmaps-7debdbd6-0eff-44ad-8e0b-f0d471501731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.452988ms
Nov 30 04:54:24.946: INFO: The phase of Pod pod-projected-configmaps-7debdbd6-0eff-44ad-8e0b-f0d471501731 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:54:26.949: INFO: Pod "pod-projected-configmaps-7debdbd6-0eff-44ad-8e0b-f0d471501731": Phase="Running", Reason="", readiness=true. Elapsed: 2.005720426s
Nov 30 04:54:26.949: INFO: The phase of Pod pod-projected-configmaps-7debdbd6-0eff-44ad-8e0b-f0d471501731 is Running (Ready = true)
Nov 30 04:54:26.949: INFO: Pod "pod-projected-configmaps-7debdbd6-0eff-44ad-8e0b-f0d471501731" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-ea0c8ffc-aa43-4afe-a471-701a28b1405f 11/30/22 04:54:26.968
STEP: Updating configmap cm-test-opt-upd-5c1bf141-a3a0-4609-a4b5-8b848fe435bf 11/30/22 04:54:26.979
STEP: Creating configMap with name cm-test-opt-create-cfd02867-c1b3-4209-949e-71fedfa6d6f7 11/30/22 04:54:26.987
STEP: waiting to observe update in volume 11/30/22 04:54:26.992
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Nov 30 04:54:29.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-758" for this suite. 11/30/22 04:54:29.016
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":349,"skipped":6464,"failed":0}
------------------------------
• [4.197 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:24.825
    Nov 30 04:54:24.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:54:24.825
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:24.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:24.894
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-ea0c8ffc-aa43-4afe-a471-701a28b1405f 11/30/22 04:54:24.903
    STEP: Creating configMap with name cm-test-opt-upd-5c1bf141-a3a0-4609-a4b5-8b848fe435bf 11/30/22 04:54:24.91
    STEP: Creating the pod 11/30/22 04:54:24.914
    Nov 30 04:54:24.943: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7debdbd6-0eff-44ad-8e0b-f0d471501731" in namespace "projected-758" to be "running and ready"
    Nov 30 04:54:24.946: INFO: Pod "pod-projected-configmaps-7debdbd6-0eff-44ad-8e0b-f0d471501731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.452988ms
    Nov 30 04:54:24.946: INFO: The phase of Pod pod-projected-configmaps-7debdbd6-0eff-44ad-8e0b-f0d471501731 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:54:26.949: INFO: Pod "pod-projected-configmaps-7debdbd6-0eff-44ad-8e0b-f0d471501731": Phase="Running", Reason="", readiness=true. Elapsed: 2.005720426s
    Nov 30 04:54:26.949: INFO: The phase of Pod pod-projected-configmaps-7debdbd6-0eff-44ad-8e0b-f0d471501731 is Running (Ready = true)
    Nov 30 04:54:26.949: INFO: Pod "pod-projected-configmaps-7debdbd6-0eff-44ad-8e0b-f0d471501731" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-ea0c8ffc-aa43-4afe-a471-701a28b1405f 11/30/22 04:54:26.968
    STEP: Updating configmap cm-test-opt-upd-5c1bf141-a3a0-4609-a4b5-8b848fe435bf 11/30/22 04:54:26.979
    STEP: Creating configMap with name cm-test-opt-create-cfd02867-c1b3-4209-949e-71fedfa6d6f7 11/30/22 04:54:26.987
    STEP: waiting to observe update in volume 11/30/22 04:54:26.992
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Nov 30 04:54:29.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-758" for this suite. 11/30/22 04:54:29.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:29.022
Nov 30 04:54:29.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:54:29.023
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:29.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:29.042
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-70c37e7e-c049-49c0-98ca-4d47da9e16f2 11/30/22 04:54:29.044
STEP: Creating a pod to test consume configMaps 11/30/22 04:54:29.051
Nov 30 04:54:29.066: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a" in namespace "projected-3885" to be "Succeeded or Failed"
Nov 30 04:54:29.072: INFO: Pod "pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.697272ms
Nov 30 04:54:31.076: INFO: Pod "pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010051934s
Nov 30 04:54:33.076: INFO: Pod "pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010530157s
STEP: Saw pod success 11/30/22 04:54:33.076
Nov 30 04:54:33.076: INFO: Pod "pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a" satisfied condition "Succeeded or Failed"
Nov 30 04:54:33.079: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a container agnhost-container: <nil>
STEP: delete the pod 11/30/22 04:54:33.084
Nov 30 04:54:33.097: INFO: Waiting for pod pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a to disappear
Nov 30 04:54:33.100: INFO: Pod pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Nov 30 04:54:33.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3885" for this suite. 11/30/22 04:54:33.103
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":350,"skipped":6470,"failed":0}
------------------------------
• [4.087 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:29.022
    Nov 30 04:54:29.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:54:29.023
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:29.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:29.042
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-70c37e7e-c049-49c0-98ca-4d47da9e16f2 11/30/22 04:54:29.044
    STEP: Creating a pod to test consume configMaps 11/30/22 04:54:29.051
    Nov 30 04:54:29.066: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a" in namespace "projected-3885" to be "Succeeded or Failed"
    Nov 30 04:54:29.072: INFO: Pod "pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.697272ms
    Nov 30 04:54:31.076: INFO: Pod "pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010051934s
    Nov 30 04:54:33.076: INFO: Pod "pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010530157s
    STEP: Saw pod success 11/30/22 04:54:33.076
    Nov 30 04:54:33.076: INFO: Pod "pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a" satisfied condition "Succeeded or Failed"
    Nov 30 04:54:33.079: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a container agnhost-container: <nil>
    STEP: delete the pod 11/30/22 04:54:33.084
    Nov 30 04:54:33.097: INFO: Waiting for pod pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a to disappear
    Nov 30 04:54:33.100: INFO: Pod pod-projected-configmaps-1a3ff7b8-d443-42e5-aefa-1102bd2c220a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Nov 30 04:54:33.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3885" for this suite. 11/30/22 04:54:33.103
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:33.109
Nov 30 04:54:33.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 04:54:33.11
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:33.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:33.128
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 11/30/22 04:54:33.129
Nov 30 04:54:33.143: INFO: Waiting up to 5m0s for pod "pod-6bd3573b-573a-477d-bf2e-b44692949849" in namespace "emptydir-4778" to be "Succeeded or Failed"
Nov 30 04:54:33.147: INFO: Pod "pod-6bd3573b-573a-477d-bf2e-b44692949849": Phase="Pending", Reason="", readiness=false. Elapsed: 3.05523ms
Nov 30 04:54:35.150: INFO: Pod "pod-6bd3573b-573a-477d-bf2e-b44692949849": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006053527s
Nov 30 04:54:37.150: INFO: Pod "pod-6bd3573b-573a-477d-bf2e-b44692949849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006466819s
STEP: Saw pod success 11/30/22 04:54:37.15
Nov 30 04:54:37.150: INFO: Pod "pod-6bd3573b-573a-477d-bf2e-b44692949849" satisfied condition "Succeeded or Failed"
Nov 30 04:54:37.153: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-6bd3573b-573a-477d-bf2e-b44692949849 container test-container: <nil>
STEP: delete the pod 11/30/22 04:54:37.158
Nov 30 04:54:37.172: INFO: Waiting for pod pod-6bd3573b-573a-477d-bf2e-b44692949849 to disappear
Nov 30 04:54:37.176: INFO: Pod pod-6bd3573b-573a-477d-bf2e-b44692949849 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 04:54:37.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4778" for this suite. 11/30/22 04:54:37.179
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":351,"skipped":6472,"failed":0}
------------------------------
• [4.081 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:33.109
    Nov 30 04:54:33.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 04:54:33.11
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:33.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:33.128
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 11/30/22 04:54:33.129
    Nov 30 04:54:33.143: INFO: Waiting up to 5m0s for pod "pod-6bd3573b-573a-477d-bf2e-b44692949849" in namespace "emptydir-4778" to be "Succeeded or Failed"
    Nov 30 04:54:33.147: INFO: Pod "pod-6bd3573b-573a-477d-bf2e-b44692949849": Phase="Pending", Reason="", readiness=false. Elapsed: 3.05523ms
    Nov 30 04:54:35.150: INFO: Pod "pod-6bd3573b-573a-477d-bf2e-b44692949849": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006053527s
    Nov 30 04:54:37.150: INFO: Pod "pod-6bd3573b-573a-477d-bf2e-b44692949849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006466819s
    STEP: Saw pod success 11/30/22 04:54:37.15
    Nov 30 04:54:37.150: INFO: Pod "pod-6bd3573b-573a-477d-bf2e-b44692949849" satisfied condition "Succeeded or Failed"
    Nov 30 04:54:37.153: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-6bd3573b-573a-477d-bf2e-b44692949849 container test-container: <nil>
    STEP: delete the pod 11/30/22 04:54:37.158
    Nov 30 04:54:37.172: INFO: Waiting for pod pod-6bd3573b-573a-477d-bf2e-b44692949849 to disappear
    Nov 30 04:54:37.176: INFO: Pod pod-6bd3573b-573a-477d-bf2e-b44692949849 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 04:54:37.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4778" for this suite. 11/30/22 04:54:37.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:37.19
Nov 30 04:54:37.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename svcaccounts 11/30/22 04:54:37.191
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:37.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:37.208
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Nov 30 04:54:37.244: INFO: created pod pod-service-account-defaultsa
Nov 30 04:54:37.244: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 30 04:54:37.259: INFO: created pod pod-service-account-mountsa
Nov 30 04:54:37.259: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 30 04:54:37.274: INFO: created pod pod-service-account-nomountsa
Nov 30 04:54:37.274: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 30 04:54:37.290: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 30 04:54:37.290: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 30 04:54:37.302: INFO: created pod pod-service-account-mountsa-mountspec
Nov 30 04:54:37.302: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 30 04:54:37.315: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 30 04:54:37.315: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 30 04:54:37.331: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 30 04:54:37.331: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 30 04:54:37.341: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 30 04:54:37.341: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 30 04:54:37.359: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 30 04:54:37.359: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Nov 30 04:54:37.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5604" for this suite. 11/30/22 04:54:37.365
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":352,"skipped":6484,"failed":0}
------------------------------
• [0.186 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:37.19
    Nov 30 04:54:37.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename svcaccounts 11/30/22 04:54:37.191
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:37.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:37.208
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Nov 30 04:54:37.244: INFO: created pod pod-service-account-defaultsa
    Nov 30 04:54:37.244: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Nov 30 04:54:37.259: INFO: created pod pod-service-account-mountsa
    Nov 30 04:54:37.259: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Nov 30 04:54:37.274: INFO: created pod pod-service-account-nomountsa
    Nov 30 04:54:37.274: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Nov 30 04:54:37.290: INFO: created pod pod-service-account-defaultsa-mountspec
    Nov 30 04:54:37.290: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Nov 30 04:54:37.302: INFO: created pod pod-service-account-mountsa-mountspec
    Nov 30 04:54:37.302: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Nov 30 04:54:37.315: INFO: created pod pod-service-account-nomountsa-mountspec
    Nov 30 04:54:37.315: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Nov 30 04:54:37.331: INFO: created pod pod-service-account-defaultsa-nomountspec
    Nov 30 04:54:37.331: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Nov 30 04:54:37.341: INFO: created pod pod-service-account-mountsa-nomountspec
    Nov 30 04:54:37.341: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Nov 30 04:54:37.359: INFO: created pod pod-service-account-nomountsa-nomountspec
    Nov 30 04:54:37.359: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Nov 30 04:54:37.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5604" for this suite. 11/30/22 04:54:37.365
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:37.376
Nov 30 04:54:37.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename projected 11/30/22 04:54:37.377
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:37.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:37.398
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 11/30/22 04:54:37.4
Nov 30 04:54:37.414: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85" in namespace "projected-9215" to be "Succeeded or Failed"
Nov 30 04:54:37.423: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85": Phase="Pending", Reason="", readiness=false. Elapsed: 8.812772ms
Nov 30 04:54:39.429: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014479418s
Nov 30 04:54:41.428: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013006233s
Nov 30 04:54:43.427: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012864654s
Nov 30 04:54:45.426: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011913229s
Nov 30 04:54:47.427: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.012416246s
STEP: Saw pod success 11/30/22 04:54:47.427
Nov 30 04:54:47.427: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85" satisfied condition "Succeeded or Failed"
Nov 30 04:54:47.430: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85 container client-container: <nil>
STEP: delete the pod 11/30/22 04:54:47.434
Nov 30 04:54:47.452: INFO: Waiting for pod downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85 to disappear
Nov 30 04:54:47.459: INFO: Pod downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Nov 30 04:54:47.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9215" for this suite. 11/30/22 04:54:47.464
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":353,"skipped":6487,"failed":0}
------------------------------
• [SLOW TEST] [10.094 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:37.376
    Nov 30 04:54:37.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename projected 11/30/22 04:54:37.377
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:37.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:37.398
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 11/30/22 04:54:37.4
    Nov 30 04:54:37.414: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85" in namespace "projected-9215" to be "Succeeded or Failed"
    Nov 30 04:54:37.423: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85": Phase="Pending", Reason="", readiness=false. Elapsed: 8.812772ms
    Nov 30 04:54:39.429: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014479418s
    Nov 30 04:54:41.428: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013006233s
    Nov 30 04:54:43.427: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012864654s
    Nov 30 04:54:45.426: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011913229s
    Nov 30 04:54:47.427: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.012416246s
    STEP: Saw pod success 11/30/22 04:54:47.427
    Nov 30 04:54:47.427: INFO: Pod "downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85" satisfied condition "Succeeded or Failed"
    Nov 30 04:54:47.430: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85 container client-container: <nil>
    STEP: delete the pod 11/30/22 04:54:47.434
    Nov 30 04:54:47.452: INFO: Waiting for pod downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85 to disappear
    Nov 30 04:54:47.459: INFO: Pod downwardapi-volume-c095362e-a948-4dfe-8788-3f2993f8fb85 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Nov 30 04:54:47.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9215" for this suite. 11/30/22 04:54:47.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:47.473
Nov 30 04:54:47.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 04:54:47.473
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:47.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:47.501
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 11/30/22 04:54:47.504
Nov 30 04:54:47.535: INFO: Waiting up to 5m0s for pod "downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e" in namespace "downward-api-3295" to be "Succeeded or Failed"
Nov 30 04:54:47.538: INFO: Pod "downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662408ms
Nov 30 04:54:49.543: INFO: Pod "downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008021359s
Nov 30 04:54:51.542: INFO: Pod "downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006925127s
Nov 30 04:54:53.542: INFO: Pod "downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006911955s
STEP: Saw pod success 11/30/22 04:54:53.542
Nov 30 04:54:53.542: INFO: Pod "downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e" satisfied condition "Succeeded or Failed"
Nov 30 04:54:53.545: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e container dapi-container: <nil>
STEP: delete the pod 11/30/22 04:54:53.55
Nov 30 04:54:53.571: INFO: Waiting for pod downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e to disappear
Nov 30 04:54:53.573: INFO: Pod downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Nov 30 04:54:53.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3295" for this suite. 11/30/22 04:54:53.578
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":354,"skipped":6554,"failed":0}
------------------------------
• [SLOW TEST] [6.114 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:47.473
    Nov 30 04:54:47.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 04:54:47.473
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:47.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:47.501
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 11/30/22 04:54:47.504
    Nov 30 04:54:47.535: INFO: Waiting up to 5m0s for pod "downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e" in namespace "downward-api-3295" to be "Succeeded or Failed"
    Nov 30 04:54:47.538: INFO: Pod "downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662408ms
    Nov 30 04:54:49.543: INFO: Pod "downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008021359s
    Nov 30 04:54:51.542: INFO: Pod "downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006925127s
    Nov 30 04:54:53.542: INFO: Pod "downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006911955s
    STEP: Saw pod success 11/30/22 04:54:53.542
    Nov 30 04:54:53.542: INFO: Pod "downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e" satisfied condition "Succeeded or Failed"
    Nov 30 04:54:53.545: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e container dapi-container: <nil>
    STEP: delete the pod 11/30/22 04:54:53.55
    Nov 30 04:54:53.571: INFO: Waiting for pod downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e to disappear
    Nov 30 04:54:53.573: INFO: Pod downward-api-a69d8e7c-b08f-4854-9016-220d3d2ab11e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Nov 30 04:54:53.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3295" for this suite. 11/30/22 04:54:53.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:54:53.588
Nov 30 04:54:53.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename deployment 11/30/22 04:54:53.589
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:53.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:53.611
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Nov 30 04:54:53.613: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 30 04:54:53.628: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 30 04:54:58.632: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/30/22 04:54:58.632
Nov 30 04:54:58.632: INFO: Creating deployment "test-rolling-update-deployment"
Nov 30 04:54:58.640: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 30 04:54:58.647: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 30 04:55:00.655: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 30 04:55:00.657: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 30 04:55:00.668: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3940  46802493-add1-4fab-9d11-186759d6bbcc 71034 1 2022-11-30 04:54:58 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-11-30 04:54:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:54:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003aa3168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-11-30 04:54:58 +0000 UTC,LastTransitionTime:2022-11-30 04:54:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7f6d6f8956" has successfully progressed.,LastUpdateTime:2022-11-30 04:54:59 +0000 UTC,LastTransitionTime:2022-11-30 04:54:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 30 04:55:00.671: INFO: New ReplicaSet "test-rolling-update-deployment-7f6d6f8956" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7f6d6f8956  deployment-3940  7460d4d6-bfc2-4f0f-b446-3f96c9e09869 71024 1 2022-11-30 04:54:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7f6d6f8956] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 46802493-add1-4fab-9d11-186759d6bbcc 0xc003aa3697 0xc003aa3698}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:54:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46802493-add1-4fab-9d11-186759d6bbcc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:54:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7f6d6f8956,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7f6d6f8956] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003aa3748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 30 04:55:00.671: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 30 04:55:00.671: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3940  27947d9e-5ae6-4d41-8122-f063e2f0b39b 71033 2 2022-11-30 04:54:53 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 46802493-add1-4fab-9d11-186759d6bbcc 0xc003aa3567 0xc003aa3568}] [] [{e2e.test Update apps/v1 2022-11-30 04:54:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:54:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46802493-add1-4fab-9d11-186759d6bbcc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:54:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003aa3628 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 30 04:55:00.674: INFO: Pod "test-rolling-update-deployment-7f6d6f8956-v8zbg" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7f6d6f8956-v8zbg test-rolling-update-deployment-7f6d6f8956- deployment-3940  83bfff77-1e3b-44d7-a57d-d83bc340e15d 71023 0 2022-11-30 04:54:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7f6d6f8956] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.195"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.12.195"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rolling-update-deployment-7f6d6f8956 7460d4d6-bfc2-4f0f-b446-3f96c9e09869 0xc003aa3bc7 0xc003aa3bc8}] [] [{kube-controller-manager Update v1 2022-11-30 04:54:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7460d4d6-bfc2-4f0f-b446-3f96c9e09869\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:54:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:54:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8gx2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8gx2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:54:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:54:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.195,StartTime:2022-11-30 04:54:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:54:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:1c9665737fa6e8ea5fa5031c5053c4bc36943d087d5d1d3926df50505b6236a8,ContainerID:containerd://43faac43e5fa0e1511bd0fe44d36929259bf40cb4de544cc2ada58d226201d18,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.195,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Nov 30 04:55:00.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3940" for this suite. 11/30/22 04:55:00.678
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":355,"skipped":6583,"failed":0}
------------------------------
• [SLOW TEST] [7.100 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:54:53.588
    Nov 30 04:54:53.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename deployment 11/30/22 04:54:53.589
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:54:53.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:54:53.611
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Nov 30 04:54:53.613: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Nov 30 04:54:53.628: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 30 04:54:58.632: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/30/22 04:54:58.632
    Nov 30 04:54:58.632: INFO: Creating deployment "test-rolling-update-deployment"
    Nov 30 04:54:58.640: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Nov 30 04:54:58.647: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Nov 30 04:55:00.655: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Nov 30 04:55:00.657: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 30 04:55:00.668: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3940  46802493-add1-4fab-9d11-186759d6bbcc 71034 1 2022-11-30 04:54:58 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-11-30 04:54:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:54:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003aa3168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-11-30 04:54:58 +0000 UTC,LastTransitionTime:2022-11-30 04:54:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7f6d6f8956" has successfully progressed.,LastUpdateTime:2022-11-30 04:54:59 +0000 UTC,LastTransitionTime:2022-11-30 04:54:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 30 04:55:00.671: INFO: New ReplicaSet "test-rolling-update-deployment-7f6d6f8956" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7f6d6f8956  deployment-3940  7460d4d6-bfc2-4f0f-b446-3f96c9e09869 71024 1 2022-11-30 04:54:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7f6d6f8956] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 46802493-add1-4fab-9d11-186759d6bbcc 0xc003aa3697 0xc003aa3698}] [] [{kube-controller-manager Update apps/v1 2022-11-30 04:54:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46802493-add1-4fab-9d11-186759d6bbcc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:54:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7f6d6f8956,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7f6d6f8956] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003aa3748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 30 04:55:00.671: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Nov 30 04:55:00.671: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3940  27947d9e-5ae6-4d41-8122-f063e2f0b39b 71033 2 2022-11-30 04:54:53 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 46802493-add1-4fab-9d11-186759d6bbcc 0xc003aa3567 0xc003aa3568}] [] [{e2e.test Update apps/v1 2022-11-30 04:54:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:54:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46802493-add1-4fab-9d11-186759d6bbcc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-11-30 04:54:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003aa3628 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 30 04:55:00.674: INFO: Pod "test-rolling-update-deployment-7f6d6f8956-v8zbg" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7f6d6f8956-v8zbg test-rolling-update-deployment-7f6d6f8956- deployment-3940  83bfff77-1e3b-44d7-a57d-d83bc340e15d 71023 0 2022-11-30 04:54:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7f6d6f8956] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.195"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.12.195"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-rolling-update-deployment-7f6d6f8956 7460d4d6-bfc2-4f0f-b446-3f96c9e09869 0xc003aa3bc7 0xc003aa3bc8}] [] [{kube-controller-manager Update v1 2022-11-30 04:54:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7460d4d6-bfc2-4f0f-b446-3f96c9e09869\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-11-30 04:54:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.12.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2022-11-30 04:54:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8gx2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8gx2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:54:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-11-30 04:54:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.8,PodIP:192.168.12.195,StartTime:2022-11-30 04:54:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-11-30 04:54:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.40,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:1c9665737fa6e8ea5fa5031c5053c4bc36943d087d5d1d3926df50505b6236a8,ContainerID:containerd://43faac43e5fa0e1511bd0fe44d36929259bf40cb4de544cc2ada58d226201d18,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.12.195,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Nov 30 04:55:00.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3940" for this suite. 11/30/22 04:55:00.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:55:00.689
Nov 30 04:55:00.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir 11/30/22 04:55:00.69
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:00.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:00.71
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 11/30/22 04:55:00.714
Nov 30 04:55:00.746: INFO: Waiting up to 5m0s for pod "pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567" in namespace "emptydir-2193" to be "Succeeded or Failed"
Nov 30 04:55:00.754: INFO: Pod "pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567": Phase="Pending", Reason="", readiness=false. Elapsed: 8.234455ms
Nov 30 04:55:02.768: INFO: Pod "pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02173843s
Nov 30 04:55:04.758: INFO: Pod "pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012251233s
STEP: Saw pod success 11/30/22 04:55:04.758
Nov 30 04:55:04.758: INFO: Pod "pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567" satisfied condition "Succeeded or Failed"
Nov 30 04:55:04.762: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567 container test-container: <nil>
STEP: delete the pod 11/30/22 04:55:04.77
Nov 30 04:55:04.785: INFO: Waiting for pod pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567 to disappear
Nov 30 04:55:04.787: INFO: Pod pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Nov 30 04:55:04.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2193" for this suite. 11/30/22 04:55:04.791
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":356,"skipped":6598,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:55:00.689
    Nov 30 04:55:00.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir 11/30/22 04:55:00.69
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:00.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:00.71
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 11/30/22 04:55:00.714
    Nov 30 04:55:00.746: INFO: Waiting up to 5m0s for pod "pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567" in namespace "emptydir-2193" to be "Succeeded or Failed"
    Nov 30 04:55:00.754: INFO: Pod "pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567": Phase="Pending", Reason="", readiness=false. Elapsed: 8.234455ms
    Nov 30 04:55:02.768: INFO: Pod "pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02173843s
    Nov 30 04:55:04.758: INFO: Pod "pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012251233s
    STEP: Saw pod success 11/30/22 04:55:04.758
    Nov 30 04:55:04.758: INFO: Pod "pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567" satisfied condition "Succeeded or Failed"
    Nov 30 04:55:04.762: INFO: Trying to get logs from node worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins pod pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567 container test-container: <nil>
    STEP: delete the pod 11/30/22 04:55:04.77
    Nov 30 04:55:04.785: INFO: Waiting for pod pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567 to disappear
    Nov 30 04:55:04.787: INFO: Pod pod-c92f3bc8-1fa4-47a4-a59a-1a39a40e9567 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Nov 30 04:55:04.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2193" for this suite. 11/30/22 04:55:04.791
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:55:04.798
Nov 30 04:55:04.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename csistoragecapacity 11/30/22 04:55:04.799
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:04.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:04.819
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 11/30/22 04:55:04.821
STEP: getting /apis/storage.k8s.io 11/30/22 04:55:04.823
STEP: getting /apis/storage.k8s.io/v1 11/30/22 04:55:04.824
STEP: creating 11/30/22 04:55:04.825
STEP: watching 11/30/22 04:55:04.842
Nov 30 04:55:04.843: INFO: starting watch
STEP: getting 11/30/22 04:55:04.848
STEP: listing in namespace 11/30/22 04:55:04.854
STEP: listing across namespaces 11/30/22 04:55:04.856
STEP: patching 11/30/22 04:55:04.859
STEP: updating 11/30/22 04:55:04.872
Nov 30 04:55:04.876: INFO: waiting for watch events with expected annotations in namespace
Nov 30 04:55:04.876: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 11/30/22 04:55:04.876
STEP: deleting a collection 11/30/22 04:55:04.887
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Nov 30 04:55:04.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-1179" for this suite. 11/30/22 04:55:04.903
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":357,"skipped":6602,"failed":0}
------------------------------
• [0.110 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:55:04.798
    Nov 30 04:55:04.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename csistoragecapacity 11/30/22 04:55:04.799
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:04.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:04.819
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 11/30/22 04:55:04.821
    STEP: getting /apis/storage.k8s.io 11/30/22 04:55:04.823
    STEP: getting /apis/storage.k8s.io/v1 11/30/22 04:55:04.824
    STEP: creating 11/30/22 04:55:04.825
    STEP: watching 11/30/22 04:55:04.842
    Nov 30 04:55:04.843: INFO: starting watch
    STEP: getting 11/30/22 04:55:04.848
    STEP: listing in namespace 11/30/22 04:55:04.854
    STEP: listing across namespaces 11/30/22 04:55:04.856
    STEP: patching 11/30/22 04:55:04.859
    STEP: updating 11/30/22 04:55:04.872
    Nov 30 04:55:04.876: INFO: waiting for watch events with expected annotations in namespace
    Nov 30 04:55:04.876: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 11/30/22 04:55:04.876
    STEP: deleting a collection 11/30/22 04:55:04.887
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Nov 30 04:55:04.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-1179" for this suite. 11/30/22 04:55:04.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:55:04.909
Nov 30 04:55:04.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename watch 11/30/22 04:55:04.909
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:04.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:04.928
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 11/30/22 04:55:04.931
STEP: modifying the configmap once 11/30/22 04:55:04.942
STEP: modifying the configmap a second time 11/30/22 04:55:04.95
STEP: deleting the configmap 11/30/22 04:55:04.958
STEP: creating a watch on configmaps from the resource version returned by the first update 11/30/22 04:55:04.963
STEP: Expecting to observe notifications for all changes to the configmap after the first update 11/30/22 04:55:04.964
Nov 30 04:55:04.964: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8787  bb7506cd-269f-4533-b723-9875bcf294a3 71112 0 2022-11-30 04:55:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-11-30 04:55:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 30 04:55:04.965: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8787  bb7506cd-269f-4533-b723-9875bcf294a3 71113 0 2022-11-30 04:55:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-11-30 04:55:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Nov 30 04:55:04.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8787" for this suite. 11/30/22 04:55:04.968
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":358,"skipped":6639,"failed":0}
------------------------------
• [0.067 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:55:04.909
    Nov 30 04:55:04.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename watch 11/30/22 04:55:04.909
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:04.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:04.928
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 11/30/22 04:55:04.931
    STEP: modifying the configmap once 11/30/22 04:55:04.942
    STEP: modifying the configmap a second time 11/30/22 04:55:04.95
    STEP: deleting the configmap 11/30/22 04:55:04.958
    STEP: creating a watch on configmaps from the resource version returned by the first update 11/30/22 04:55:04.963
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 11/30/22 04:55:04.964
    Nov 30 04:55:04.964: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8787  bb7506cd-269f-4533-b723-9875bcf294a3 71112 0 2022-11-30 04:55:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-11-30 04:55:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 30 04:55:04.965: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8787  bb7506cd-269f-4533-b723-9875bcf294a3 71113 0 2022-11-30 04:55:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-11-30 04:55:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Nov 30 04:55:04.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8787" for this suite. 11/30/22 04:55:04.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:55:04.976
Nov 30 04:55:04.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename emptydir-wrapper 11/30/22 04:55:04.977
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:04.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:05
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Nov 30 04:55:05.037: INFO: Waiting up to 5m0s for pod "pod-secrets-1d860355-b05f-4156-80cb-e8fdef0df0ba" in namespace "emptydir-wrapper-6048" to be "running and ready"
Nov 30 04:55:05.039: INFO: Pod "pod-secrets-1d860355-b05f-4156-80cb-e8fdef0df0ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.411199ms
Nov 30 04:55:05.039: INFO: The phase of Pod pod-secrets-1d860355-b05f-4156-80cb-e8fdef0df0ba is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:55:07.043: INFO: Pod "pod-secrets-1d860355-b05f-4156-80cb-e8fdef0df0ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.006616804s
Nov 30 04:55:07.044: INFO: The phase of Pod pod-secrets-1d860355-b05f-4156-80cb-e8fdef0df0ba is Running (Ready = true)
Nov 30 04:55:07.044: INFO: Pod "pod-secrets-1d860355-b05f-4156-80cb-e8fdef0df0ba" satisfied condition "running and ready"
STEP: Cleaning up the secret 11/30/22 04:55:07.046
STEP: Cleaning up the configmap 11/30/22 04:55:07.051
STEP: Cleaning up the pod 11/30/22 04:55:07.055
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Nov 30 04:55:07.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6048" for this suite. 11/30/22 04:55:07.072
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":359,"skipped":6664,"failed":0}
------------------------------
• [2.102 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:55:04.976
    Nov 30 04:55:04.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename emptydir-wrapper 11/30/22 04:55:04.977
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:04.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:05
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Nov 30 04:55:05.037: INFO: Waiting up to 5m0s for pod "pod-secrets-1d860355-b05f-4156-80cb-e8fdef0df0ba" in namespace "emptydir-wrapper-6048" to be "running and ready"
    Nov 30 04:55:05.039: INFO: Pod "pod-secrets-1d860355-b05f-4156-80cb-e8fdef0df0ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.411199ms
    Nov 30 04:55:05.039: INFO: The phase of Pod pod-secrets-1d860355-b05f-4156-80cb-e8fdef0df0ba is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:55:07.043: INFO: Pod "pod-secrets-1d860355-b05f-4156-80cb-e8fdef0df0ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.006616804s
    Nov 30 04:55:07.044: INFO: The phase of Pod pod-secrets-1d860355-b05f-4156-80cb-e8fdef0df0ba is Running (Ready = true)
    Nov 30 04:55:07.044: INFO: Pod "pod-secrets-1d860355-b05f-4156-80cb-e8fdef0df0ba" satisfied condition "running and ready"
    STEP: Cleaning up the secret 11/30/22 04:55:07.046
    STEP: Cleaning up the configmap 11/30/22 04:55:07.051
    STEP: Cleaning up the pod 11/30/22 04:55:07.055
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Nov 30 04:55:07.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-6048" for this suite. 11/30/22 04:55:07.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:55:07.078
Nov 30 04:55:07.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename downward-api 11/30/22 04:55:07.079
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:07.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:07.095
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 11/30/22 04:55:07.097
Nov 30 04:55:07.106: INFO: Waiting up to 5m0s for pod "labelsupdatef322175a-2263-4110-95d4-c363234fbae7" in namespace "downward-api-2317" to be "running and ready"
Nov 30 04:55:07.117: INFO: Pod "labelsupdatef322175a-2263-4110-95d4-c363234fbae7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.502813ms
Nov 30 04:55:07.117: INFO: The phase of Pod labelsupdatef322175a-2263-4110-95d4-c363234fbae7 is Pending, waiting for it to be Running (with Ready = true)
Nov 30 04:55:09.121: INFO: Pod "labelsupdatef322175a-2263-4110-95d4-c363234fbae7": Phase="Running", Reason="", readiness=true. Elapsed: 2.014328328s
Nov 30 04:55:09.121: INFO: The phase of Pod labelsupdatef322175a-2263-4110-95d4-c363234fbae7 is Running (Ready = true)
Nov 30 04:55:09.121: INFO: Pod "labelsupdatef322175a-2263-4110-95d4-c363234fbae7" satisfied condition "running and ready"
Nov 30 04:55:09.642: INFO: Successfully updated pod "labelsupdatef322175a-2263-4110-95d4-c363234fbae7"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Nov 30 04:55:11.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2317" for this suite. 11/30/22 04:55:11.66
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":360,"skipped":6673,"failed":0}
------------------------------
• [4.587 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:55:07.078
    Nov 30 04:55:07.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename downward-api 11/30/22 04:55:07.079
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:07.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:07.095
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 11/30/22 04:55:07.097
    Nov 30 04:55:07.106: INFO: Waiting up to 5m0s for pod "labelsupdatef322175a-2263-4110-95d4-c363234fbae7" in namespace "downward-api-2317" to be "running and ready"
    Nov 30 04:55:07.117: INFO: Pod "labelsupdatef322175a-2263-4110-95d4-c363234fbae7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.502813ms
    Nov 30 04:55:07.117: INFO: The phase of Pod labelsupdatef322175a-2263-4110-95d4-c363234fbae7 is Pending, waiting for it to be Running (with Ready = true)
    Nov 30 04:55:09.121: INFO: Pod "labelsupdatef322175a-2263-4110-95d4-c363234fbae7": Phase="Running", Reason="", readiness=true. Elapsed: 2.014328328s
    Nov 30 04:55:09.121: INFO: The phase of Pod labelsupdatef322175a-2263-4110-95d4-c363234fbae7 is Running (Ready = true)
    Nov 30 04:55:09.121: INFO: Pod "labelsupdatef322175a-2263-4110-95d4-c363234fbae7" satisfied condition "running and ready"
    Nov 30 04:55:09.642: INFO: Successfully updated pod "labelsupdatef322175a-2263-4110-95d4-c363234fbae7"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Nov 30 04:55:11.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2317" for this suite. 11/30/22 04:55:11.66
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:55:11.665
Nov 30 04:55:11.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename pods 11/30/22 04:55:11.666
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:11.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:11.686
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 11/30/22 04:55:11.688
Nov 30 04:55:11.700: INFO: created test-pod-1
Nov 30 04:55:11.713: INFO: created test-pod-2
Nov 30 04:55:11.727: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 11/30/22 04:55:11.727
Nov 30 04:55:11.727: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1024' to be running and ready
Nov 30 04:55:11.743: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 30 04:55:11.743: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 30 04:55:11.743: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 30 04:55:11.743: INFO: 0 / 3 pods in namespace 'pods-1024' are running and ready (0 seconds elapsed)
Nov 30 04:55:11.743: INFO: expected 0 pod replicas in namespace 'pods-1024', 0 are Running and Ready.
Nov 30 04:55:11.743: INFO: POD         NODE                                         PHASE    GRACE  CONDITIONS
Nov 30 04:55:11.743: INFO: test-pod-1  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC  }]
Nov 30 04:55:11.743: INFO: test-pod-2  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC  }]
Nov 30 04:55:11.743: INFO: test-pod-3  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC  }]
Nov 30 04:55:11.743: INFO: 
Nov 30 04:55:13.751: INFO: 3 / 3 pods in namespace 'pods-1024' are running and ready (2 seconds elapsed)
Nov 30 04:55:13.751: INFO: expected 0 pod replicas in namespace 'pods-1024', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 11/30/22 04:55:13.77
Nov 30 04:55:13.773: INFO: Pod quantity 3 is different from expected quantity 0
Nov 30 04:55:14.778: INFO: Pod quantity 3 is different from expected quantity 0
Nov 30 04:55:15.781: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Nov 30 04:55:16.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1024" for this suite. 11/30/22 04:55:16.782
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":361,"skipped":6673,"failed":0}
------------------------------
• [SLOW TEST] [5.121 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:55:11.665
    Nov 30 04:55:11.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename pods 11/30/22 04:55:11.666
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:11.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:11.686
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 11/30/22 04:55:11.688
    Nov 30 04:55:11.700: INFO: created test-pod-1
    Nov 30 04:55:11.713: INFO: created test-pod-2
    Nov 30 04:55:11.727: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 11/30/22 04:55:11.727
    Nov 30 04:55:11.727: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1024' to be running and ready
    Nov 30 04:55:11.743: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 30 04:55:11.743: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 30 04:55:11.743: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 30 04:55:11.743: INFO: 0 / 3 pods in namespace 'pods-1024' are running and ready (0 seconds elapsed)
    Nov 30 04:55:11.743: INFO: expected 0 pod replicas in namespace 'pods-1024', 0 are Running and Ready.
    Nov 30 04:55:11.743: INFO: POD         NODE                                         PHASE    GRACE  CONDITIONS
    Nov 30 04:55:11.743: INFO: test-pod-1  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC  }]
    Nov 30 04:55:11.743: INFO: test-pod-2  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC  }]
    Nov 30 04:55:11.743: INFO: test-pod-3  worker-pool1-0lmrr2dw-n92-ci-ibd-23-jenkins  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-11-30 04:55:11 +0000 UTC  }]
    Nov 30 04:55:11.743: INFO: 
    Nov 30 04:55:13.751: INFO: 3 / 3 pods in namespace 'pods-1024' are running and ready (2 seconds elapsed)
    Nov 30 04:55:13.751: INFO: expected 0 pod replicas in namespace 'pods-1024', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 11/30/22 04:55:13.77
    Nov 30 04:55:13.773: INFO: Pod quantity 3 is different from expected quantity 0
    Nov 30 04:55:14.778: INFO: Pod quantity 3 is different from expected quantity 0
    Nov 30 04:55:15.781: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Nov 30 04:55:16.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1024" for this suite. 11/30/22 04:55:16.782
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 11/30/22 04:55:16.787
Nov 30 04:55:16.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
STEP: Building a namespace api object, basename kubelet-test 11/30/22 04:55:16.788
STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:16.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:16.811
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Nov 30 04:55:16.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6767" for this suite. 11/30/22 04:55:16.866
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":362,"skipped":6675,"failed":0}
------------------------------
• [0.098 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 11/30/22 04:55:16.787
    Nov 30 04:55:16.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1327404838
    STEP: Building a namespace api object, basename kubelet-test 11/30/22 04:55:16.788
    STEP: Waiting for a default service account to be provisioned in namespace 11/30/22 04:55:16.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/30/22 04:55:16.811
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Nov 30 04:55:16.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6767" for this suite. 11/30/22 04:55:16.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6705,"failed":0}
Nov 30 04:55:16.886: INFO: Running AfterSuite actions on all nodes
Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Nov 30 04:55:16.886: INFO: Running AfterSuite actions on node 1
Nov 30 04:55:16.886: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Nov 30 04:55:16.886: INFO: Running AfterSuite actions on all nodes
    Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Nov 30 04:55:16.886: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Nov 30 04:55:16.886: INFO: Running AfterSuite actions on node 1
    Nov 30 04:55:16.886: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.057 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7067 Specs in 5709.792 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6705 Skipped
PASS

Ginkgo ran 1 suite in 1h35m10.091867833s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

