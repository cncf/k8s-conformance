I0206 17:19:42.259632      20 e2e.go:116] Starting e2e run "22cc322c-ea4b-415b-8190-e6eb23dafba3" on Ginkgo node 1
Feb  6 17:19:42.289: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1675703982 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Feb  6 17:19:42.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
E0206 17:19:42.469817      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0206 17:19:42.469817      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Feb  6 17:19:42.471: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb  6 17:19:42.501: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb  6 17:19:42.575: INFO: 44 / 44 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb  6 17:19:42.576: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Feb  6 17:19:42.576: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb  6 17:19:42.584: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
Feb  6 17:19:42.585: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb  6 17:19:42.585: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'vsphere-cloud-controller-manager' (0 seconds elapsed)
Feb  6 17:19:42.585: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'vsphere-csi-node' (0 seconds elapsed)
Feb  6 17:19:42.585: INFO: e2e test version: v1.25.5
Feb  6 17:19:42.587: INFO: kube-apiserver version: v1.25.5-eks-c248520
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Feb  6 17:19:42.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 17:19:42.592: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.129 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Feb  6 17:19:42.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    E0206 17:19:42.469817      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Feb  6 17:19:42.471: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Feb  6 17:19:42.501: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Feb  6 17:19:42.575: INFO: 44 / 44 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Feb  6 17:19:42.576: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
    Feb  6 17:19:42.576: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Feb  6 17:19:42.584: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
    Feb  6 17:19:42.585: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Feb  6 17:19:42.585: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'vsphere-cloud-controller-manager' (0 seconds elapsed)
    Feb  6 17:19:42.585: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'vsphere-csi-node' (0 seconds elapsed)
    Feb  6 17:19:42.585: INFO: e2e test version: v1.25.5
    Feb  6 17:19:42.587: INFO: kube-apiserver version: v1.25.5-eks-c248520
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Feb  6 17:19:42.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 17:19:42.592: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:19:42.632
Feb  6 17:19:42.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 17:19:42.633
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:19:42.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:19:42.669
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/06/23 17:19:42.673
Feb  6 17:19:42.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-5054 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb  6 17:19:42.875: INFO: stderr: ""
Feb  6 17:19:42.875: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 02/06/23 17:19:42.875
Feb  6 17:19:42.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-5054 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Feb  6 17:19:45.299: INFO: stderr: ""
Feb  6 17:19:45.299: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/06/23 17:19:45.299
Feb  6 17:19:45.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-5054 delete pods e2e-test-httpd-pod'
Feb  6 17:20:05.489: INFO: stderr: ""
Feb  6 17:20:05.489: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 17:20:05.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5054" for this suite. 02/06/23 17:20:05.496
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":1,"skipped":6,"failed":0}
------------------------------
• [SLOW TEST] [22.872 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:19:42.632
    Feb  6 17:19:42.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 17:19:42.633
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:19:42.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:19:42.669
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/06/23 17:19:42.673
    Feb  6 17:19:42.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-5054 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb  6 17:19:42.875: INFO: stderr: ""
    Feb  6 17:19:42.875: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 02/06/23 17:19:42.875
    Feb  6 17:19:42.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-5054 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Feb  6 17:19:45.299: INFO: stderr: ""
    Feb  6 17:19:45.299: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/06/23 17:19:45.299
    Feb  6 17:19:45.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-5054 delete pods e2e-test-httpd-pod'
    Feb  6 17:20:05.489: INFO: stderr: ""
    Feb  6 17:20:05.489: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 17:20:05.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5054" for this suite. 02/06/23 17:20:05.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:20:05.507
Feb  6 17:20:05.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename var-expansion 02/06/23 17:20:05.508
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:20:05.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:20:05.53
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 02/06/23 17:20:05.533
Feb  6 17:20:05.545: INFO: Waiting up to 2m0s for pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260" in namespace "var-expansion-2975" to be "running"
Feb  6 17:20:05.551: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016755ms
Feb  6 17:20:07.564: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019287845s
Feb  6 17:20:09.748: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 4.202868871s
Feb  6 17:20:11.654: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 6.10894319s
Feb  6 17:20:13.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012272757s
Feb  6 17:20:15.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012198752s
Feb  6 17:20:17.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010884881s
Feb  6 17:20:19.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010903721s
Feb  6 17:20:21.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010844427s
Feb  6 17:20:23.558: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013182925s
Feb  6 17:20:25.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 20.012210108s
Feb  6 17:20:27.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011592347s
Feb  6 17:20:29.558: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01315006s
Feb  6 17:20:31.555: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010278257s
Feb  6 17:20:33.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 28.011278488s
Feb  6 17:20:35.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011269955s
Feb  6 17:20:37.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011998626s
Feb  6 17:20:39.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011048658s
Feb  6 17:20:41.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010682682s
Feb  6 17:20:43.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011938312s
Feb  6 17:20:45.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011638816s
Feb  6 17:20:47.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012029746s
Feb  6 17:20:49.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 44.011594651s
Feb  6 17:20:51.558: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012977033s
Feb  6 17:20:53.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011316784s
Feb  6 17:20:55.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012527964s
Feb  6 17:20:57.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01098643s
Feb  6 17:20:59.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011618863s
Feb  6 17:21:01.558: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012651379s
Feb  6 17:21:03.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012201475s
Feb  6 17:21:05.555: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010487203s
Feb  6 17:21:07.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011621509s
Feb  6 17:21:09.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.011592673s
Feb  6 17:21:11.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011066737s
Feb  6 17:21:13.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011016814s
Feb  6 17:21:15.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010937705s
Feb  6 17:21:17.606: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.06119154s
Feb  6 17:21:19.555: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009945511s
Feb  6 17:21:21.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011018185s
Feb  6 17:21:23.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.01171449s
Feb  6 17:21:25.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011082063s
Feb  6 17:21:27.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011509266s
Feb  6 17:21:29.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010917883s
Feb  6 17:21:31.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01089409s
Feb  6 17:21:33.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.011778157s
Feb  6 17:21:35.555: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010599969s
Feb  6 17:21:37.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011169001s
Feb  6 17:21:39.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011638844s
Feb  6 17:21:41.555: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010513961s
Feb  6 17:21:43.558: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.01271762s
Feb  6 17:21:45.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.011701584s
Feb  6 17:21:47.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011232648s
Feb  6 17:21:49.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012010316s
Feb  6 17:21:51.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.012015175s
Feb  6 17:21:53.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011161822s
Feb  6 17:21:55.558: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013232279s
Feb  6 17:21:57.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011338192s
Feb  6 17:21:59.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01186767s
Feb  6 17:22:01.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.01222746s
Feb  6 17:22:03.559: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013793757s
Feb  6 17:22:05.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011520805s
Feb  6 17:22:05.560: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015112652s
STEP: updating the pod 02/06/23 17:22:05.56
Feb  6 17:22:06.073: INFO: Successfully updated pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260"
STEP: waiting for pod running 02/06/23 17:22:06.073
Feb  6 17:22:06.073: INFO: Waiting up to 2m0s for pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260" in namespace "var-expansion-2975" to be "running"
Feb  6 17:22:06.075: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 2.409014ms
Feb  6 17:22:08.079: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Running", Reason="", readiness=true. Elapsed: 2.005730357s
Feb  6 17:22:08.079: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260" satisfied condition "running"
STEP: deleting the pod gracefully 02/06/23 17:22:08.079
Feb  6 17:22:08.079: INFO: Deleting pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260" in namespace "var-expansion-2975"
Feb  6 17:22:08.086: INFO: Wait up to 5m0s for pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb  6 17:22:40.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2975" for this suite. 02/06/23 17:22:40.096
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":2,"skipped":48,"failed":0}
------------------------------
• [SLOW TEST] [154.600 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:20:05.507
    Feb  6 17:20:05.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename var-expansion 02/06/23 17:20:05.508
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:20:05.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:20:05.53
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 02/06/23 17:20:05.533
    Feb  6 17:20:05.545: INFO: Waiting up to 2m0s for pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260" in namespace "var-expansion-2975" to be "running"
    Feb  6 17:20:05.551: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016755ms
    Feb  6 17:20:07.564: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019287845s
    Feb  6 17:20:09.748: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 4.202868871s
    Feb  6 17:20:11.654: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 6.10894319s
    Feb  6 17:20:13.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012272757s
    Feb  6 17:20:15.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012198752s
    Feb  6 17:20:17.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010884881s
    Feb  6 17:20:19.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010903721s
    Feb  6 17:20:21.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010844427s
    Feb  6 17:20:23.558: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013182925s
    Feb  6 17:20:25.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 20.012210108s
    Feb  6 17:20:27.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011592347s
    Feb  6 17:20:29.558: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01315006s
    Feb  6 17:20:31.555: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010278257s
    Feb  6 17:20:33.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 28.011278488s
    Feb  6 17:20:35.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011269955s
    Feb  6 17:20:37.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011998626s
    Feb  6 17:20:39.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011048658s
    Feb  6 17:20:41.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010682682s
    Feb  6 17:20:43.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011938312s
    Feb  6 17:20:45.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011638816s
    Feb  6 17:20:47.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012029746s
    Feb  6 17:20:49.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 44.011594651s
    Feb  6 17:20:51.558: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012977033s
    Feb  6 17:20:53.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011316784s
    Feb  6 17:20:55.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012527964s
    Feb  6 17:20:57.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01098643s
    Feb  6 17:20:59.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011618863s
    Feb  6 17:21:01.558: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012651379s
    Feb  6 17:21:03.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012201475s
    Feb  6 17:21:05.555: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010487203s
    Feb  6 17:21:07.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011621509s
    Feb  6 17:21:09.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.011592673s
    Feb  6 17:21:11.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011066737s
    Feb  6 17:21:13.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011016814s
    Feb  6 17:21:15.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010937705s
    Feb  6 17:21:17.606: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.06119154s
    Feb  6 17:21:19.555: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009945511s
    Feb  6 17:21:21.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011018185s
    Feb  6 17:21:23.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.01171449s
    Feb  6 17:21:25.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011082063s
    Feb  6 17:21:27.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011509266s
    Feb  6 17:21:29.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010917883s
    Feb  6 17:21:31.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01089409s
    Feb  6 17:21:33.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.011778157s
    Feb  6 17:21:35.555: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010599969s
    Feb  6 17:21:37.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011169001s
    Feb  6 17:21:39.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011638844s
    Feb  6 17:21:41.555: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010513961s
    Feb  6 17:21:43.558: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.01271762s
    Feb  6 17:21:45.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.011701584s
    Feb  6 17:21:47.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011232648s
    Feb  6 17:21:49.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012010316s
    Feb  6 17:21:51.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.012015175s
    Feb  6 17:21:53.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011161822s
    Feb  6 17:21:55.558: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013232279s
    Feb  6 17:21:57.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011338192s
    Feb  6 17:21:59.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01186767s
    Feb  6 17:22:01.557: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.01222746s
    Feb  6 17:22:03.559: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013793757s
    Feb  6 17:22:05.556: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011520805s
    Feb  6 17:22:05.560: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015112652s
    STEP: updating the pod 02/06/23 17:22:05.56
    Feb  6 17:22:06.073: INFO: Successfully updated pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260"
    STEP: waiting for pod running 02/06/23 17:22:06.073
    Feb  6 17:22:06.073: INFO: Waiting up to 2m0s for pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260" in namespace "var-expansion-2975" to be "running"
    Feb  6 17:22:06.075: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Pending", Reason="", readiness=false. Elapsed: 2.409014ms
    Feb  6 17:22:08.079: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260": Phase="Running", Reason="", readiness=true. Elapsed: 2.005730357s
    Feb  6 17:22:08.079: INFO: Pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260" satisfied condition "running"
    STEP: deleting the pod gracefully 02/06/23 17:22:08.079
    Feb  6 17:22:08.079: INFO: Deleting pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260" in namespace "var-expansion-2975"
    Feb  6 17:22:08.086: INFO: Wait up to 5m0s for pod "var-expansion-9dc3c0d5-b374-4453-919a-d42adb2f7260" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb  6 17:22:40.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2975" for this suite. 02/06/23 17:22:40.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:22:40.113
Feb  6 17:22:40.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 17:22:40.114
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:22:40.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:22:40.132
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 02/06/23 17:22:40.135
Feb  6 17:22:40.144: INFO: Waiting up to 5m0s for pod "downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644" in namespace "downward-api-1372" to be "Succeeded or Failed"
Feb  6 17:22:40.149: INFO: Pod "downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.887999ms
Feb  6 17:22:42.158: INFO: Pod "downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014211392s
Feb  6 17:22:44.155: INFO: Pod "downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010517059s
Feb  6 17:22:46.153: INFO: Pod "downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008568726s
STEP: Saw pod success 02/06/23 17:22:46.153
Feb  6 17:22:46.153: INFO: Pod "downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644" satisfied condition "Succeeded or Failed"
Feb  6 17:22:46.156: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-6j94p pod downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644 container dapi-container: <nil>
STEP: delete the pod 02/06/23 17:22:46.173
Feb  6 17:22:46.184: INFO: Waiting for pod downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644 to disappear
Feb  6 17:22:46.186: INFO: Pod downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb  6 17:22:46.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1372" for this suite. 02/06/23 17:22:46.191
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":3,"skipped":62,"failed":0}
------------------------------
• [SLOW TEST] [6.086 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:22:40.113
    Feb  6 17:22:40.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 17:22:40.114
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:22:40.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:22:40.132
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 02/06/23 17:22:40.135
    Feb  6 17:22:40.144: INFO: Waiting up to 5m0s for pod "downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644" in namespace "downward-api-1372" to be "Succeeded or Failed"
    Feb  6 17:22:40.149: INFO: Pod "downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.887999ms
    Feb  6 17:22:42.158: INFO: Pod "downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014211392s
    Feb  6 17:22:44.155: INFO: Pod "downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010517059s
    Feb  6 17:22:46.153: INFO: Pod "downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008568726s
    STEP: Saw pod success 02/06/23 17:22:46.153
    Feb  6 17:22:46.153: INFO: Pod "downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644" satisfied condition "Succeeded or Failed"
    Feb  6 17:22:46.156: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-6j94p pod downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644 container dapi-container: <nil>
    STEP: delete the pod 02/06/23 17:22:46.173
    Feb  6 17:22:46.184: INFO: Waiting for pod downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644 to disappear
    Feb  6 17:22:46.186: INFO: Pod downward-api-8c3a7cdf-3433-43d0-9713-08dbbb2de644 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb  6 17:22:46.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1372" for this suite. 02/06/23 17:22:46.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:22:46.202
Feb  6 17:22:46.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 17:22:46.203
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:22:46.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:22:46.222
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 02/06/23 17:22:46.225
Feb  6 17:22:46.234: INFO: Waiting up to 5m0s for pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422" in namespace "downward-api-4731" to be "Succeeded or Failed"
Feb  6 17:22:46.237: INFO: Pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422": Phase="Pending", Reason="", readiness=false. Elapsed: 3.513764ms
Feb  6 17:22:48.243: INFO: Pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009065024s
Feb  6 17:22:50.243: INFO: Pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009218029s
Feb  6 17:22:52.241: INFO: Pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007130854s
Feb  6 17:22:54.243: INFO: Pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009096807s
STEP: Saw pod success 02/06/23 17:22:54.243
Feb  6 17:22:54.243: INFO: Pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422" satisfied condition "Succeeded or Failed"
Feb  6 17:22:54.247: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422 container dapi-container: <nil>
STEP: delete the pod 02/06/23 17:22:54.268
Feb  6 17:22:54.280: INFO: Waiting for pod downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422 to disappear
Feb  6 17:22:54.283: INFO: Pod downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb  6 17:22:54.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4731" for this suite. 02/06/23 17:22:54.287
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":4,"skipped":89,"failed":0}
------------------------------
• [SLOW TEST] [8.090 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:22:46.202
    Feb  6 17:22:46.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 17:22:46.203
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:22:46.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:22:46.222
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 02/06/23 17:22:46.225
    Feb  6 17:22:46.234: INFO: Waiting up to 5m0s for pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422" in namespace "downward-api-4731" to be "Succeeded or Failed"
    Feb  6 17:22:46.237: INFO: Pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422": Phase="Pending", Reason="", readiness=false. Elapsed: 3.513764ms
    Feb  6 17:22:48.243: INFO: Pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009065024s
    Feb  6 17:22:50.243: INFO: Pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009218029s
    Feb  6 17:22:52.241: INFO: Pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007130854s
    Feb  6 17:22:54.243: INFO: Pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009096807s
    STEP: Saw pod success 02/06/23 17:22:54.243
    Feb  6 17:22:54.243: INFO: Pod "downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422" satisfied condition "Succeeded or Failed"
    Feb  6 17:22:54.247: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422 container dapi-container: <nil>
    STEP: delete the pod 02/06/23 17:22:54.268
    Feb  6 17:22:54.280: INFO: Waiting for pod downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422 to disappear
    Feb  6 17:22:54.283: INFO: Pod downward-api-c6f39998-d243-4e7c-b0e8-9673a914a422 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb  6 17:22:54.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4731" for this suite. 02/06/23 17:22:54.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:22:54.3
Feb  6 17:22:54.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename init-container 02/06/23 17:22:54.301
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:22:54.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:22:54.32
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 02/06/23 17:22:54.323
Feb  6 17:22:54.323: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb  6 17:23:00.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4552" for this suite. 02/06/23 17:23:00.458
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":5,"skipped":110,"failed":0}
------------------------------
• [SLOW TEST] [6.166 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:22:54.3
    Feb  6 17:22:54.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename init-container 02/06/23 17:22:54.301
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:22:54.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:22:54.32
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 02/06/23 17:22:54.323
    Feb  6 17:22:54.323: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb  6 17:23:00.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4552" for this suite. 02/06/23 17:23:00.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:23:00.469
Feb  6 17:23:00.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir-wrapper 02/06/23 17:23:00.47
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:23:00.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:23:00.494
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 02/06/23 17:23:00.498
STEP: Creating RC which spawns configmap-volume pods 02/06/23 17:23:00.741
Feb  6 17:23:00.854: INFO: Pod name wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf: Found 3 pods out of 5
Feb  6 17:23:05.862: INFO: Pod name wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/06/23 17:23:05.862
Feb  6 17:23:05.862: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:23:05.866: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 3.256211ms
Feb  6 17:23:07.871: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008606019s
Feb  6 17:23:09.870: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007651946s
Feb  6 17:23:11.871: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009040957s
Feb  6 17:23:13.871: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008809075s
Feb  6 17:23:15.871: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008734377s
Feb  6 17:23:17.870: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007728918s
Feb  6 17:23:19.872: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009798925s
Feb  6 17:23:21.870: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Running", Reason="", readiness=true. Elapsed: 16.00824195s
Feb  6 17:23:21.871: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44" satisfied condition "running"
Feb  6 17:23:21.871: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-ddzqr" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:23:21.874: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-ddzqr": Phase="Running", Reason="", readiness=true. Elapsed: 3.79441ms
Feb  6 17:23:21.874: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-ddzqr" satisfied condition "running"
Feb  6 17:23:21.874: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-hhbkx" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:23:21.878: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-hhbkx": Phase="Running", Reason="", readiness=true. Elapsed: 3.267809ms
Feb  6 17:23:21.878: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-hhbkx" satisfied condition "running"
Feb  6 17:23:21.878: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-kfwmr" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:23:21.881: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-kfwmr": Phase="Running", Reason="", readiness=true. Elapsed: 2.996509ms
Feb  6 17:23:21.881: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-kfwmr" satisfied condition "running"
Feb  6 17:23:21.881: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-svk58" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:23:21.884: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-svk58": Phase="Running", Reason="", readiness=true. Elapsed: 3.148572ms
Feb  6 17:23:21.884: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-svk58" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf in namespace emptydir-wrapper-2957, will wait for the garbage collector to delete the pods 02/06/23 17:23:21.884
Feb  6 17:23:21.945: INFO: Deleting ReplicationController wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf took: 7.25377ms
Feb  6 17:23:22.046: INFO: Terminating ReplicationController wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf pods took: 101.11289ms
STEP: Creating RC which spawns configmap-volume pods 02/06/23 17:23:25.053
Feb  6 17:23:25.069: INFO: Pod name wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26: Found 0 pods out of 5
Feb  6 17:23:30.076: INFO: Pod name wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/06/23 17:23:30.076
Feb  6 17:23:30.076: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:23:30.080: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.418867ms
Feb  6 17:23:32.090: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013429139s
Feb  6 17:23:34.085: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008185039s
Feb  6 17:23:36.085: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008590549s
Feb  6 17:23:38.084: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007661927s
Feb  6 17:23:40.085: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00845977s
Feb  6 17:23:42.099: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 12.022584123s
Feb  6 17:23:44.090: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01399983s
Feb  6 17:23:46.085: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Running", Reason="", readiness=true. Elapsed: 16.008093208s
Feb  6 17:23:46.085: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt" satisfied condition "running"
Feb  6 17:23:46.085: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-5lxlw" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:23:46.088: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-5lxlw": Phase="Running", Reason="", readiness=true. Elapsed: 3.260885ms
Feb  6 17:23:46.088: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-5lxlw" satisfied condition "running"
Feb  6 17:23:46.088: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-hsjx5" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:23:46.092: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-hsjx5": Phase="Running", Reason="", readiness=true. Elapsed: 3.574545ms
Feb  6 17:23:46.092: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-hsjx5" satisfied condition "running"
Feb  6 17:23:46.092: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-j4lf5" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:23:46.095: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-j4lf5": Phase="Running", Reason="", readiness=true. Elapsed: 3.823261ms
Feb  6 17:23:46.096: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-j4lf5" satisfied condition "running"
Feb  6 17:23:46.096: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-mhmlb" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:23:46.099: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-mhmlb": Phase="Running", Reason="", readiness=true. Elapsed: 3.158801ms
Feb  6 17:23:46.099: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-mhmlb" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26 in namespace emptydir-wrapper-2957, will wait for the garbage collector to delete the pods 02/06/23 17:23:46.099
Feb  6 17:23:46.163: INFO: Deleting ReplicationController wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26 took: 9.767374ms
Feb  6 17:23:46.263: INFO: Terminating ReplicationController wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26 pods took: 100.765582ms
STEP: Creating RC which spawns configmap-volume pods 02/06/23 17:23:49.17
Feb  6 17:23:49.186: INFO: Pod name wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f: Found 0 pods out of 5
Feb  6 17:23:54.196: INFO: Pod name wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/06/23 17:23:54.196
Feb  6 17:23:54.196: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:23:54.200: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.434863ms
Feb  6 17:23:56.205: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00927902s
Feb  6 17:23:58.205: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008748564s
Feb  6 17:24:00.206: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009678589s
Feb  6 17:24:02.204: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007587434s
Feb  6 17:24:04.206: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00956559s
Feb  6 17:24:06.207: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011160281s
Feb  6 17:24:08.205: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Running", Reason="", readiness=true. Elapsed: 14.009215812s
Feb  6 17:24:08.205: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4" satisfied condition "running"
Feb  6 17:24:08.205: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-lmr9l" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:24:08.209: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-lmr9l": Phase="Running", Reason="", readiness=true. Elapsed: 3.366676ms
Feb  6 17:24:08.209: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-lmr9l" satisfied condition "running"
Feb  6 17:24:08.209: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-m552x" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:24:08.213: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-m552x": Phase="Pending", Reason="", readiness=false. Elapsed: 3.538041ms
Feb  6 17:24:10.217: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-m552x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007824361s
Feb  6 17:24:12.216: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-m552x": Phase="Running", Reason="", readiness=true. Elapsed: 4.006937241s
Feb  6 17:24:12.216: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-m552x" satisfied condition "running"
Feb  6 17:24:12.216: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-pz86s" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:24:12.220: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-pz86s": Phase="Running", Reason="", readiness=true. Elapsed: 3.609426ms
Feb  6 17:24:12.220: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-pz86s" satisfied condition "running"
Feb  6 17:24:12.220: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-s9z6d" in namespace "emptydir-wrapper-2957" to be "running"
Feb  6 17:24:12.223: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-s9z6d": Phase="Running", Reason="", readiness=true. Elapsed: 3.312857ms
Feb  6 17:24:12.223: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-s9z6d" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f in namespace emptydir-wrapper-2957, will wait for the garbage collector to delete the pods 02/06/23 17:24:12.223
Feb  6 17:24:12.287: INFO: Deleting ReplicationController wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f took: 8.454231ms
Feb  6 17:24:12.388: INFO: Terminating ReplicationController wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f pods took: 100.866054ms
STEP: Cleaning up the configMaps 02/06/23 17:24:15.188
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Feb  6 17:24:15.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2957" for this suite. 02/06/23 17:24:15.482
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":6,"skipped":117,"failed":0}
------------------------------
• [SLOW TEST] [75.019 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:23:00.469
    Feb  6 17:23:00.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir-wrapper 02/06/23 17:23:00.47
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:23:00.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:23:00.494
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 02/06/23 17:23:00.498
    STEP: Creating RC which spawns configmap-volume pods 02/06/23 17:23:00.741
    Feb  6 17:23:00.854: INFO: Pod name wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf: Found 3 pods out of 5
    Feb  6 17:23:05.862: INFO: Pod name wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/06/23 17:23:05.862
    Feb  6 17:23:05.862: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:23:05.866: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 3.256211ms
    Feb  6 17:23:07.871: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008606019s
    Feb  6 17:23:09.870: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007651946s
    Feb  6 17:23:11.871: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009040957s
    Feb  6 17:23:13.871: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008809075s
    Feb  6 17:23:15.871: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008734377s
    Feb  6 17:23:17.870: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007728918s
    Feb  6 17:23:19.872: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009798925s
    Feb  6 17:23:21.870: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44": Phase="Running", Reason="", readiness=true. Elapsed: 16.00824195s
    Feb  6 17:23:21.871: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-b5w44" satisfied condition "running"
    Feb  6 17:23:21.871: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-ddzqr" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:23:21.874: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-ddzqr": Phase="Running", Reason="", readiness=true. Elapsed: 3.79441ms
    Feb  6 17:23:21.874: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-ddzqr" satisfied condition "running"
    Feb  6 17:23:21.874: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-hhbkx" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:23:21.878: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-hhbkx": Phase="Running", Reason="", readiness=true. Elapsed: 3.267809ms
    Feb  6 17:23:21.878: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-hhbkx" satisfied condition "running"
    Feb  6 17:23:21.878: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-kfwmr" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:23:21.881: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-kfwmr": Phase="Running", Reason="", readiness=true. Elapsed: 2.996509ms
    Feb  6 17:23:21.881: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-kfwmr" satisfied condition "running"
    Feb  6 17:23:21.881: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-svk58" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:23:21.884: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-svk58": Phase="Running", Reason="", readiness=true. Elapsed: 3.148572ms
    Feb  6 17:23:21.884: INFO: Pod "wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf-svk58" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf in namespace emptydir-wrapper-2957, will wait for the garbage collector to delete the pods 02/06/23 17:23:21.884
    Feb  6 17:23:21.945: INFO: Deleting ReplicationController wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf took: 7.25377ms
    Feb  6 17:23:22.046: INFO: Terminating ReplicationController wrapped-volume-race-b2c7ad7c-1eb8-47b5-9a1f-203590d061cf pods took: 101.11289ms
    STEP: Creating RC which spawns configmap-volume pods 02/06/23 17:23:25.053
    Feb  6 17:23:25.069: INFO: Pod name wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26: Found 0 pods out of 5
    Feb  6 17:23:30.076: INFO: Pod name wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/06/23 17:23:30.076
    Feb  6 17:23:30.076: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:23:30.080: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.418867ms
    Feb  6 17:23:32.090: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013429139s
    Feb  6 17:23:34.085: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008185039s
    Feb  6 17:23:36.085: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008590549s
    Feb  6 17:23:38.084: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007661927s
    Feb  6 17:23:40.085: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00845977s
    Feb  6 17:23:42.099: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 12.022584123s
    Feb  6 17:23:44.090: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01399983s
    Feb  6 17:23:46.085: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt": Phase="Running", Reason="", readiness=true. Elapsed: 16.008093208s
    Feb  6 17:23:46.085: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-4jrtt" satisfied condition "running"
    Feb  6 17:23:46.085: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-5lxlw" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:23:46.088: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-5lxlw": Phase="Running", Reason="", readiness=true. Elapsed: 3.260885ms
    Feb  6 17:23:46.088: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-5lxlw" satisfied condition "running"
    Feb  6 17:23:46.088: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-hsjx5" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:23:46.092: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-hsjx5": Phase="Running", Reason="", readiness=true. Elapsed: 3.574545ms
    Feb  6 17:23:46.092: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-hsjx5" satisfied condition "running"
    Feb  6 17:23:46.092: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-j4lf5" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:23:46.095: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-j4lf5": Phase="Running", Reason="", readiness=true. Elapsed: 3.823261ms
    Feb  6 17:23:46.096: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-j4lf5" satisfied condition "running"
    Feb  6 17:23:46.096: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-mhmlb" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:23:46.099: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-mhmlb": Phase="Running", Reason="", readiness=true. Elapsed: 3.158801ms
    Feb  6 17:23:46.099: INFO: Pod "wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26-mhmlb" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26 in namespace emptydir-wrapper-2957, will wait for the garbage collector to delete the pods 02/06/23 17:23:46.099
    Feb  6 17:23:46.163: INFO: Deleting ReplicationController wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26 took: 9.767374ms
    Feb  6 17:23:46.263: INFO: Terminating ReplicationController wrapped-volume-race-be991244-af75-4c8b-9a2b-a7c150711b26 pods took: 100.765582ms
    STEP: Creating RC which spawns configmap-volume pods 02/06/23 17:23:49.17
    Feb  6 17:23:49.186: INFO: Pod name wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f: Found 0 pods out of 5
    Feb  6 17:23:54.196: INFO: Pod name wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/06/23 17:23:54.196
    Feb  6 17:23:54.196: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:23:54.200: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.434863ms
    Feb  6 17:23:56.205: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00927902s
    Feb  6 17:23:58.205: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008748564s
    Feb  6 17:24:00.206: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009678589s
    Feb  6 17:24:02.204: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007587434s
    Feb  6 17:24:04.206: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00956559s
    Feb  6 17:24:06.207: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011160281s
    Feb  6 17:24:08.205: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4": Phase="Running", Reason="", readiness=true. Elapsed: 14.009215812s
    Feb  6 17:24:08.205: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-28lr4" satisfied condition "running"
    Feb  6 17:24:08.205: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-lmr9l" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:24:08.209: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-lmr9l": Phase="Running", Reason="", readiness=true. Elapsed: 3.366676ms
    Feb  6 17:24:08.209: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-lmr9l" satisfied condition "running"
    Feb  6 17:24:08.209: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-m552x" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:24:08.213: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-m552x": Phase="Pending", Reason="", readiness=false. Elapsed: 3.538041ms
    Feb  6 17:24:10.217: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-m552x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007824361s
    Feb  6 17:24:12.216: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-m552x": Phase="Running", Reason="", readiness=true. Elapsed: 4.006937241s
    Feb  6 17:24:12.216: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-m552x" satisfied condition "running"
    Feb  6 17:24:12.216: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-pz86s" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:24:12.220: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-pz86s": Phase="Running", Reason="", readiness=true. Elapsed: 3.609426ms
    Feb  6 17:24:12.220: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-pz86s" satisfied condition "running"
    Feb  6 17:24:12.220: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-s9z6d" in namespace "emptydir-wrapper-2957" to be "running"
    Feb  6 17:24:12.223: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-s9z6d": Phase="Running", Reason="", readiness=true. Elapsed: 3.312857ms
    Feb  6 17:24:12.223: INFO: Pod "wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f-s9z6d" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f in namespace emptydir-wrapper-2957, will wait for the garbage collector to delete the pods 02/06/23 17:24:12.223
    Feb  6 17:24:12.287: INFO: Deleting ReplicationController wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f took: 8.454231ms
    Feb  6 17:24:12.388: INFO: Terminating ReplicationController wrapped-volume-race-b40feaf8-1602-4c52-a1b4-73ab055fe50f pods took: 100.866054ms
    STEP: Cleaning up the configMaps 02/06/23 17:24:15.188
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Feb  6 17:24:15.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-2957" for this suite. 02/06/23 17:24:15.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:24:15.491
Feb  6 17:24:15.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 17:24:15.492
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:24:15.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:24:15.511
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 02/06/23 17:24:15.514
Feb  6 17:24:15.514: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb  6 17:24:15.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 create -f -'
Feb  6 17:24:17.759: INFO: stderr: ""
Feb  6 17:24:17.759: INFO: stdout: "service/agnhost-replica created\n"
Feb  6 17:24:17.759: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb  6 17:24:17.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 create -f -'
Feb  6 17:24:19.894: INFO: stderr: ""
Feb  6 17:24:19.894: INFO: stdout: "service/agnhost-primary created\n"
Feb  6 17:24:19.894: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb  6 17:24:19.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 create -f -'
Feb  6 17:24:20.337: INFO: stderr: ""
Feb  6 17:24:20.337: INFO: stdout: "service/frontend created\n"
Feb  6 17:24:20.337: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb  6 17:24:20.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 create -f -'
Feb  6 17:24:20.744: INFO: stderr: ""
Feb  6 17:24:20.744: INFO: stdout: "deployment.apps/frontend created\n"
Feb  6 17:24:20.744: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb  6 17:24:20.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 create -f -'
Feb  6 17:24:21.185: INFO: stderr: ""
Feb  6 17:24:21.186: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb  6 17:24:21.186: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb  6 17:24:21.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 create -f -'
Feb  6 17:24:21.661: INFO: stderr: ""
Feb  6 17:24:21.661: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 02/06/23 17:24:21.661
Feb  6 17:24:21.661: INFO: Waiting for all frontend pods to be Running.
Feb  6 17:24:31.713: INFO: Waiting for frontend to serve content.
Feb  6 17:24:31.727: INFO: Trying to add a new entry to the guestbook.
Feb  6 17:24:31.742: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 02/06/23 17:24:31.754
Feb  6 17:24:31.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 delete --grace-period=0 --force -f -'
Feb  6 17:24:31.897: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  6 17:24:31.897: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 02/06/23 17:24:31.897
Feb  6 17:24:31.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 delete --grace-period=0 --force -f -'
Feb  6 17:24:32.037: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  6 17:24:32.037: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/06/23 17:24:32.037
Feb  6 17:24:32.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 delete --grace-period=0 --force -f -'
Feb  6 17:24:32.163: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  6 17:24:32.163: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/06/23 17:24:32.163
Feb  6 17:24:32.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 delete --grace-period=0 --force -f -'
Feb  6 17:24:32.250: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  6 17:24:32.250: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/06/23 17:24:32.25
Feb  6 17:24:32.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 delete --grace-period=0 --force -f -'
Feb  6 17:24:32.357: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  6 17:24:32.357: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/06/23 17:24:32.358
Feb  6 17:24:32.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 delete --grace-period=0 --force -f -'
Feb  6 17:24:32.460: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  6 17:24:32.460: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 17:24:32.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2277" for this suite. 02/06/23 17:24:32.469
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":7,"skipped":129,"failed":0}
------------------------------
• [SLOW TEST] [16.988 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:24:15.491
    Feb  6 17:24:15.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 17:24:15.492
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:24:15.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:24:15.511
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 02/06/23 17:24:15.514
    Feb  6 17:24:15.514: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Feb  6 17:24:15.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 create -f -'
    Feb  6 17:24:17.759: INFO: stderr: ""
    Feb  6 17:24:17.759: INFO: stdout: "service/agnhost-replica created\n"
    Feb  6 17:24:17.759: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Feb  6 17:24:17.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 create -f -'
    Feb  6 17:24:19.894: INFO: stderr: ""
    Feb  6 17:24:19.894: INFO: stdout: "service/agnhost-primary created\n"
    Feb  6 17:24:19.894: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Feb  6 17:24:19.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 create -f -'
    Feb  6 17:24:20.337: INFO: stderr: ""
    Feb  6 17:24:20.337: INFO: stdout: "service/frontend created\n"
    Feb  6 17:24:20.337: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Feb  6 17:24:20.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 create -f -'
    Feb  6 17:24:20.744: INFO: stderr: ""
    Feb  6 17:24:20.744: INFO: stdout: "deployment.apps/frontend created\n"
    Feb  6 17:24:20.744: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb  6 17:24:20.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 create -f -'
    Feb  6 17:24:21.185: INFO: stderr: ""
    Feb  6 17:24:21.186: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Feb  6 17:24:21.186: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb  6 17:24:21.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 create -f -'
    Feb  6 17:24:21.661: INFO: stderr: ""
    Feb  6 17:24:21.661: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 02/06/23 17:24:21.661
    Feb  6 17:24:21.661: INFO: Waiting for all frontend pods to be Running.
    Feb  6 17:24:31.713: INFO: Waiting for frontend to serve content.
    Feb  6 17:24:31.727: INFO: Trying to add a new entry to the guestbook.
    Feb  6 17:24:31.742: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 02/06/23 17:24:31.754
    Feb  6 17:24:31.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 delete --grace-period=0 --force -f -'
    Feb  6 17:24:31.897: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb  6 17:24:31.897: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 02/06/23 17:24:31.897
    Feb  6 17:24:31.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 delete --grace-period=0 --force -f -'
    Feb  6 17:24:32.037: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb  6 17:24:32.037: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/06/23 17:24:32.037
    Feb  6 17:24:32.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 delete --grace-period=0 --force -f -'
    Feb  6 17:24:32.163: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb  6 17:24:32.163: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/06/23 17:24:32.163
    Feb  6 17:24:32.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 delete --grace-period=0 --force -f -'
    Feb  6 17:24:32.250: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb  6 17:24:32.250: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/06/23 17:24:32.25
    Feb  6 17:24:32.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 delete --grace-period=0 --force -f -'
    Feb  6 17:24:32.357: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb  6 17:24:32.357: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/06/23 17:24:32.358
    Feb  6 17:24:32.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2277 delete --grace-period=0 --force -f -'
    Feb  6 17:24:32.460: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb  6 17:24:32.460: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 17:24:32.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2277" for this suite. 02/06/23 17:24:32.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:24:32.479
Feb  6 17:24:32.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename aggregator 02/06/23 17:24:32.48
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:24:32.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:24:32.507
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Feb  6 17:24:32.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 02/06/23 17:24:32.511
Feb  6 17:24:33.366: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb  6 17:24:35.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:24:37.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:24:39.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:24:41.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:24:43.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:24:45.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:24:47.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:24:49.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:24:51.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:24:53.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:24:55.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:24:57.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:24:59.559: INFO: Waited 122.153394ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 02/06/23 17:25:00.062
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/06/23 17:25:00.097
STEP: List APIServices 02/06/23 17:25:00.151
Feb  6 17:25:00.201: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Feb  6 17:25:00.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7192" for this suite. 02/06/23 17:25:01.051
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":8,"skipped":135,"failed":0}
------------------------------
• [SLOW TEST] [28.624 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:24:32.479
    Feb  6 17:24:32.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename aggregator 02/06/23 17:24:32.48
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:24:32.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:24:32.507
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Feb  6 17:24:32.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 02/06/23 17:24:32.511
    Feb  6 17:24:33.366: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Feb  6 17:24:35.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:24:37.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:24:39.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:24:41.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:24:43.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:24:45.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:24:47.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:24:49.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:24:51.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:24:53.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:24:55.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:24:57.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 24, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:24:59.559: INFO: Waited 122.153394ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 02/06/23 17:25:00.062
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/06/23 17:25:00.097
    STEP: List APIServices 02/06/23 17:25:00.151
    Feb  6 17:25:00.201: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Feb  6 17:25:00.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-7192" for this suite. 02/06/23 17:25:01.051
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:25:01.104
Feb  6 17:25:01.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 17:25:01.105
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:01.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:01.129
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-8660cfb9-0963-4729-bc8a-05e89f18416d 02/06/23 17:25:01.132
STEP: Creating a pod to test consume configMaps 02/06/23 17:25:01.136
Feb  6 17:25:01.146: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d" in namespace "configmap-8353" to be "Succeeded or Failed"
Feb  6 17:25:01.150: INFO: Pod "pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.305975ms
Feb  6 17:25:03.157: INFO: Pod "pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010637466s
Feb  6 17:25:05.155: INFO: Pod "pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008282246s
Feb  6 17:25:07.156: INFO: Pod "pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010009208s
STEP: Saw pod success 02/06/23 17:25:07.157
Feb  6 17:25:07.157: INFO: Pod "pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d" satisfied condition "Succeeded or Failed"
Feb  6 17:25:07.165: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-6j94p pod pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d container agnhost-container: <nil>
STEP: delete the pod 02/06/23 17:25:07.177
Feb  6 17:25:07.191: INFO: Waiting for pod pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d to disappear
Feb  6 17:25:07.194: INFO: Pod pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 17:25:07.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8353" for this suite. 02/06/23 17:25:07.2
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":9,"skipped":135,"failed":0}
------------------------------
• [SLOW TEST] [6.103 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:25:01.104
    Feb  6 17:25:01.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 17:25:01.105
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:01.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:01.129
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-8660cfb9-0963-4729-bc8a-05e89f18416d 02/06/23 17:25:01.132
    STEP: Creating a pod to test consume configMaps 02/06/23 17:25:01.136
    Feb  6 17:25:01.146: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d" in namespace "configmap-8353" to be "Succeeded or Failed"
    Feb  6 17:25:01.150: INFO: Pod "pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.305975ms
    Feb  6 17:25:03.157: INFO: Pod "pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010637466s
    Feb  6 17:25:05.155: INFO: Pod "pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008282246s
    Feb  6 17:25:07.156: INFO: Pod "pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010009208s
    STEP: Saw pod success 02/06/23 17:25:07.157
    Feb  6 17:25:07.157: INFO: Pod "pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d" satisfied condition "Succeeded or Failed"
    Feb  6 17:25:07.165: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-6j94p pod pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 17:25:07.177
    Feb  6 17:25:07.191: INFO: Waiting for pod pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d to disappear
    Feb  6 17:25:07.194: INFO: Pod pod-configmaps-6c114d21-ade5-4ab8-b408-7bbc0775578d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 17:25:07.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8353" for this suite. 02/06/23 17:25:07.2
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:25:07.21
Feb  6 17:25:07.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename security-context-test 02/06/23 17:25:07.211
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:07.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:07.231
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Feb  6 17:25:07.244: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-57588c9d-817b-4e85-8980-98312cce2212" in namespace "security-context-test-607" to be "Succeeded or Failed"
Feb  6 17:25:07.257: INFO: Pod "busybox-readonly-false-57588c9d-817b-4e85-8980-98312cce2212": Phase="Pending", Reason="", readiness=false. Elapsed: 13.095325ms
Feb  6 17:25:09.263: INFO: Pod "busybox-readonly-false-57588c9d-817b-4e85-8980-98312cce2212": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018105255s
Feb  6 17:25:11.261: INFO: Pod "busybox-readonly-false-57588c9d-817b-4e85-8980-98312cce2212": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016879207s
Feb  6 17:25:13.261: INFO: Pod "busybox-readonly-false-57588c9d-817b-4e85-8980-98312cce2212": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016737513s
Feb  6 17:25:13.261: INFO: Pod "busybox-readonly-false-57588c9d-817b-4e85-8980-98312cce2212" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb  6 17:25:13.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-607" for this suite. 02/06/23 17:25:13.266
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":10,"skipped":135,"failed":0}
------------------------------
• [SLOW TEST] [6.062 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:25:07.21
    Feb  6 17:25:07.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename security-context-test 02/06/23 17:25:07.211
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:07.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:07.231
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Feb  6 17:25:07.244: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-57588c9d-817b-4e85-8980-98312cce2212" in namespace "security-context-test-607" to be "Succeeded or Failed"
    Feb  6 17:25:07.257: INFO: Pod "busybox-readonly-false-57588c9d-817b-4e85-8980-98312cce2212": Phase="Pending", Reason="", readiness=false. Elapsed: 13.095325ms
    Feb  6 17:25:09.263: INFO: Pod "busybox-readonly-false-57588c9d-817b-4e85-8980-98312cce2212": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018105255s
    Feb  6 17:25:11.261: INFO: Pod "busybox-readonly-false-57588c9d-817b-4e85-8980-98312cce2212": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016879207s
    Feb  6 17:25:13.261: INFO: Pod "busybox-readonly-false-57588c9d-817b-4e85-8980-98312cce2212": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016737513s
    Feb  6 17:25:13.261: INFO: Pod "busybox-readonly-false-57588c9d-817b-4e85-8980-98312cce2212" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb  6 17:25:13.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-607" for this suite. 02/06/23 17:25:13.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:25:13.273
Feb  6 17:25:13.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename resourcequota 02/06/23 17:25:13.274
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:13.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:13.297
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 02/06/23 17:25:13.3
STEP: Creating a ResourceQuota 02/06/23 17:25:18.304
STEP: Ensuring resource quota status is calculated 02/06/23 17:25:18.312
STEP: Creating a ReplicationController 02/06/23 17:25:20.316
STEP: Ensuring resource quota status captures replication controller creation 02/06/23 17:25:20.333
STEP: Deleting a ReplicationController 02/06/23 17:25:22.337
STEP: Ensuring resource quota status released usage 02/06/23 17:25:22.343
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb  6 17:25:24.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5361" for this suite. 02/06/23 17:25:24.353
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":11,"skipped":152,"failed":0}
------------------------------
• [SLOW TEST] [11.086 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:25:13.273
    Feb  6 17:25:13.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename resourcequota 02/06/23 17:25:13.274
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:13.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:13.297
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 02/06/23 17:25:13.3
    STEP: Creating a ResourceQuota 02/06/23 17:25:18.304
    STEP: Ensuring resource quota status is calculated 02/06/23 17:25:18.312
    STEP: Creating a ReplicationController 02/06/23 17:25:20.316
    STEP: Ensuring resource quota status captures replication controller creation 02/06/23 17:25:20.333
    STEP: Deleting a ReplicationController 02/06/23 17:25:22.337
    STEP: Ensuring resource quota status released usage 02/06/23 17:25:22.343
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb  6 17:25:24.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5361" for this suite. 02/06/23 17:25:24.353
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:25:24.359
Feb  6 17:25:24.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename deployment 02/06/23 17:25:24.36
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:24.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:24.378
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Feb  6 17:25:24.381: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb  6 17:25:24.389: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb  6 17:25:29.397: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/06/23 17:25:29.397
Feb  6 17:25:29.397: INFO: Creating deployment "test-rolling-update-deployment"
Feb  6 17:25:29.402: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb  6 17:25:29.412: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb  6 17:25:31.419: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb  6 17:25:31.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 25, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 25, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 25, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 25, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:25:33.426: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb  6 17:25:33.436: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3401  8c53c512-13e8-44ab-b869-8746afdc852c 10650 1 2023-02-06 17:25:29 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-06 17:25:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b844e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-06 17:25:29 +0000 UTC,LastTransitionTime:2023-02-06 17:25:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-02-06 17:25:32 +0000 UTC,LastTransitionTime:2023-02-06 17:25:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb  6 17:25:33.440: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-3401  c986ce7d-0acb-40f0-a76f-1718f3067f85 10639 1 2023-02-06 17:25:29 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 8c53c512-13e8-44ab-b869-8746afdc852c 0xc002b849e7 0xc002b849e8}] [] [{kube-controller-manager Update apps/v1 2023-02-06 17:25:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c53c512-13e8-44ab-b869-8746afdc852c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:25:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b84a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb  6 17:25:33.440: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb  6 17:25:33.440: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3401  238b5046-656a-4c82-a1ff-87bb9d5caebc 10649 2 2023-02-06 17:25:24 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 8c53c512-13e8-44ab-b869-8746afdc852c 0xc002b848b7 0xc002b848b8}] [] [{e2e.test Update apps/v1 2023-02-06 17:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c53c512-13e8-44ab-b869-8746afdc852c\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:25:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002b84978 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb  6 17:25:33.445: INFO: Pod "test-rolling-update-deployment-78f575d8ff-tfng6" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-tfng6 test-rolling-update-deployment-78f575d8ff- deployment-3401  10b208a4-cf2d-4ac3-b8f2-4ca0129fcafb 10638 0 2023-02-06 17:25:29 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff c986ce7d-0acb-40f0-a76f-1718f3067f85 0xc002c1a337 0xc002c1a338}] [] [{kube-controller-manager Update v1 2023-02-06 17:25:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c986ce7d-0acb-40f0-a76f-1718f3067f85\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:25:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqvkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqvkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:25:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:25:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:25:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:25:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:192.168.3.253,StartTime:2023-02-06 17:25:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:25:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://34b965a8b54f70eb6ddc6e3c76a8c0af41157f4142df21bdf8286139b2a7bd06,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.3.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb  6 17:25:33.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3401" for this suite. 02/06/23 17:25:33.45
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":12,"skipped":153,"failed":0}
------------------------------
• [SLOW TEST] [9.097 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:25:24.359
    Feb  6 17:25:24.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename deployment 02/06/23 17:25:24.36
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:24.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:24.378
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Feb  6 17:25:24.381: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Feb  6 17:25:24.389: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb  6 17:25:29.397: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/06/23 17:25:29.397
    Feb  6 17:25:29.397: INFO: Creating deployment "test-rolling-update-deployment"
    Feb  6 17:25:29.402: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Feb  6 17:25:29.412: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Feb  6 17:25:31.419: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Feb  6 17:25:31.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 25, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 25, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 25, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 25, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:25:33.426: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb  6 17:25:33.436: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3401  8c53c512-13e8-44ab-b869-8746afdc852c 10650 1 2023-02-06 17:25:29 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-06 17:25:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b844e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-06 17:25:29 +0000 UTC,LastTransitionTime:2023-02-06 17:25:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-02-06 17:25:32 +0000 UTC,LastTransitionTime:2023-02-06 17:25:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb  6 17:25:33.440: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-3401  c986ce7d-0acb-40f0-a76f-1718f3067f85 10639 1 2023-02-06 17:25:29 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 8c53c512-13e8-44ab-b869-8746afdc852c 0xc002b849e7 0xc002b849e8}] [] [{kube-controller-manager Update apps/v1 2023-02-06 17:25:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c53c512-13e8-44ab-b869-8746afdc852c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:25:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b84a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb  6 17:25:33.440: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Feb  6 17:25:33.440: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3401  238b5046-656a-4c82-a1ff-87bb9d5caebc 10649 2 2023-02-06 17:25:24 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 8c53c512-13e8-44ab-b869-8746afdc852c 0xc002b848b7 0xc002b848b8}] [] [{e2e.test Update apps/v1 2023-02-06 17:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c53c512-13e8-44ab-b869-8746afdc852c\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:25:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002b84978 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb  6 17:25:33.445: INFO: Pod "test-rolling-update-deployment-78f575d8ff-tfng6" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-tfng6 test-rolling-update-deployment-78f575d8ff- deployment-3401  10b208a4-cf2d-4ac3-b8f2-4ca0129fcafb 10638 0 2023-02-06 17:25:29 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff c986ce7d-0acb-40f0-a76f-1718f3067f85 0xc002c1a337 0xc002c1a338}] [] [{kube-controller-manager Update v1 2023-02-06 17:25:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c986ce7d-0acb-40f0-a76f-1718f3067f85\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:25:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqvkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqvkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:25:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:25:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:25:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:25:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:192.168.3.253,StartTime:2023-02-06 17:25:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:25:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://34b965a8b54f70eb6ddc6e3c76a8c0af41157f4142df21bdf8286139b2a7bd06,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.3.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb  6 17:25:33.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3401" for this suite. 02/06/23 17:25:33.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:25:33.464
Feb  6 17:25:33.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 17:25:33.465
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:33.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:33.486
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8826 02/06/23 17:25:33.489
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/06/23 17:25:33.511
STEP: creating service externalsvc in namespace services-8826 02/06/23 17:25:33.511
STEP: creating replication controller externalsvc in namespace services-8826 02/06/23 17:25:33.536
I0206 17:25:33.543879      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8826, replica count: 2
I0206 17:25:36.595747      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 17:25:39.598047      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 02/06/23 17:25:39.602
Feb  6 17:25:39.639: INFO: Creating new exec pod
Feb  6 17:25:39.648: INFO: Waiting up to 5m0s for pod "execpodwdd2p" in namespace "services-8826" to be "running"
Feb  6 17:25:39.654: INFO: Pod "execpodwdd2p": Phase="Pending", Reason="", readiness=false. Elapsed: 5.854074ms
Feb  6 17:25:41.658: INFO: Pod "execpodwdd2p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009831343s
Feb  6 17:25:43.659: INFO: Pod "execpodwdd2p": Phase="Running", Reason="", readiness=true. Elapsed: 4.010954457s
Feb  6 17:25:43.659: INFO: Pod "execpodwdd2p" satisfied condition "running"
Feb  6 17:25:43.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8826 exec execpodwdd2p -- /bin/sh -x -c nslookup nodeport-service.services-8826.svc.cluster.local'
Feb  6 17:25:43.862: INFO: stderr: "+ nslookup nodeport-service.services-8826.svc.cluster.local\n"
Feb  6 17:25:43.862: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8826.svc.cluster.local\tcanonical name = externalsvc.services-8826.svc.cluster.local.\nName:\texternalsvc.services-8826.svc.cluster.local\nAddress: 10.106.156.157\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8826, will wait for the garbage collector to delete the pods 02/06/23 17:25:43.862
Feb  6 17:25:43.926: INFO: Deleting ReplicationController externalsvc took: 9.881692ms
Feb  6 17:25:44.027: INFO: Terminating ReplicationController externalsvc pods took: 100.524194ms
Feb  6 17:25:45.960: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 17:25:45.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8826" for this suite. 02/06/23 17:25:45.987
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":13,"skipped":199,"failed":0}
------------------------------
• [SLOW TEST] [12.533 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:25:33.464
    Feb  6 17:25:33.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 17:25:33.465
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:33.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:33.486
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8826 02/06/23 17:25:33.489
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/06/23 17:25:33.511
    STEP: creating service externalsvc in namespace services-8826 02/06/23 17:25:33.511
    STEP: creating replication controller externalsvc in namespace services-8826 02/06/23 17:25:33.536
    I0206 17:25:33.543879      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8826, replica count: 2
    I0206 17:25:36.595747      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 17:25:39.598047      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 02/06/23 17:25:39.602
    Feb  6 17:25:39.639: INFO: Creating new exec pod
    Feb  6 17:25:39.648: INFO: Waiting up to 5m0s for pod "execpodwdd2p" in namespace "services-8826" to be "running"
    Feb  6 17:25:39.654: INFO: Pod "execpodwdd2p": Phase="Pending", Reason="", readiness=false. Elapsed: 5.854074ms
    Feb  6 17:25:41.658: INFO: Pod "execpodwdd2p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009831343s
    Feb  6 17:25:43.659: INFO: Pod "execpodwdd2p": Phase="Running", Reason="", readiness=true. Elapsed: 4.010954457s
    Feb  6 17:25:43.659: INFO: Pod "execpodwdd2p" satisfied condition "running"
    Feb  6 17:25:43.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8826 exec execpodwdd2p -- /bin/sh -x -c nslookup nodeport-service.services-8826.svc.cluster.local'
    Feb  6 17:25:43.862: INFO: stderr: "+ nslookup nodeport-service.services-8826.svc.cluster.local\n"
    Feb  6 17:25:43.862: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8826.svc.cluster.local\tcanonical name = externalsvc.services-8826.svc.cluster.local.\nName:\texternalsvc.services-8826.svc.cluster.local\nAddress: 10.106.156.157\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8826, will wait for the garbage collector to delete the pods 02/06/23 17:25:43.862
    Feb  6 17:25:43.926: INFO: Deleting ReplicationController externalsvc took: 9.881692ms
    Feb  6 17:25:44.027: INFO: Terminating ReplicationController externalsvc pods took: 100.524194ms
    Feb  6 17:25:45.960: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 17:25:45.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8826" for this suite. 02/06/23 17:25:45.987
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:25:45.999
Feb  6 17:25:45.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename server-version 02/06/23 17:25:46.001
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:46.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:46.019
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 02/06/23 17:25:46.023
STEP: Confirm major version 02/06/23 17:25:46.024
Feb  6 17:25:46.025: INFO: Major version: 1
STEP: Confirm minor version 02/06/23 17:25:46.025
Feb  6 17:25:46.025: INFO: cleanMinorVersion: 25
Feb  6 17:25:46.025: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Feb  6 17:25:46.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-4194" for this suite. 02/06/23 17:25:46.032
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":14,"skipped":211,"failed":0}
------------------------------
• [0.040 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:25:45.999
    Feb  6 17:25:45.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename server-version 02/06/23 17:25:46.001
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:46.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:46.019
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 02/06/23 17:25:46.023
    STEP: Confirm major version 02/06/23 17:25:46.024
    Feb  6 17:25:46.025: INFO: Major version: 1
    STEP: Confirm minor version 02/06/23 17:25:46.025
    Feb  6 17:25:46.025: INFO: cleanMinorVersion: 25
    Feb  6 17:25:46.025: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Feb  6 17:25:46.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-4194" for this suite. 02/06/23 17:25:46.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:25:46.041
Feb  6 17:25:46.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename job 02/06/23 17:25:46.043
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:46.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:46.061
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 02/06/23 17:25:46.064
STEP: Ensure pods equal to paralellism count is attached to the job 02/06/23 17:25:46.072
STEP: patching /status 02/06/23 17:25:50.077
STEP: updating /status 02/06/23 17:25:50.088
STEP: get /status 02/06/23 17:25:50.118
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb  6 17:25:50.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3787" for this suite. 02/06/23 17:25:50.126
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":15,"skipped":235,"failed":0}
------------------------------
• [4.091 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:25:46.041
    Feb  6 17:25:46.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename job 02/06/23 17:25:46.043
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:46.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:46.061
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 02/06/23 17:25:46.064
    STEP: Ensure pods equal to paralellism count is attached to the job 02/06/23 17:25:46.072
    STEP: patching /status 02/06/23 17:25:50.077
    STEP: updating /status 02/06/23 17:25:50.088
    STEP: get /status 02/06/23 17:25:50.118
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb  6 17:25:50.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3787" for this suite. 02/06/23 17:25:50.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:25:50.133
Feb  6 17:25:50.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 17:25:50.134
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:50.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:50.154
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 02/06/23 17:25:50.157
Feb  6 17:25:50.165: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd" in namespace "projected-1800" to be "Succeeded or Failed"
Feb  6 17:25:50.169: INFO: Pod "downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0176ms
Feb  6 17:25:52.174: INFO: Pod "downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009077143s
Feb  6 17:25:54.175: INFO: Pod "downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009974775s
Feb  6 17:25:56.173: INFO: Pod "downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00758818s
STEP: Saw pod success 02/06/23 17:25:56.173
Feb  6 17:25:56.173: INFO: Pod "downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd" satisfied condition "Succeeded or Failed"
Feb  6 17:25:56.176: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd container client-container: <nil>
STEP: delete the pod 02/06/23 17:25:56.188
Feb  6 17:25:56.204: INFO: Waiting for pod downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd to disappear
Feb  6 17:25:56.207: INFO: Pod downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb  6 17:25:56.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1800" for this suite. 02/06/23 17:25:56.211
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":16,"skipped":244,"failed":0}
------------------------------
• [SLOW TEST] [6.086 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:25:50.133
    Feb  6 17:25:50.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 17:25:50.134
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:50.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:50.154
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 02/06/23 17:25:50.157
    Feb  6 17:25:50.165: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd" in namespace "projected-1800" to be "Succeeded or Failed"
    Feb  6 17:25:50.169: INFO: Pod "downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0176ms
    Feb  6 17:25:52.174: INFO: Pod "downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009077143s
    Feb  6 17:25:54.175: INFO: Pod "downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009974775s
    Feb  6 17:25:56.173: INFO: Pod "downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00758818s
    STEP: Saw pod success 02/06/23 17:25:56.173
    Feb  6 17:25:56.173: INFO: Pod "downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd" satisfied condition "Succeeded or Failed"
    Feb  6 17:25:56.176: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd container client-container: <nil>
    STEP: delete the pod 02/06/23 17:25:56.188
    Feb  6 17:25:56.204: INFO: Waiting for pod downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd to disappear
    Feb  6 17:25:56.207: INFO: Pod downwardapi-volume-b44b95fe-c1b7-4caf-a0bf-3bb6c86fd0cd no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb  6 17:25:56.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1800" for this suite. 02/06/23 17:25:56.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:25:56.221
Feb  6 17:25:56.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 17:25:56.222
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:56.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:56.238
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 02/06/23 17:25:56.241
Feb  6 17:25:56.250: INFO: Waiting up to 5m0s for pod "labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5" in namespace "projected-2179" to be "running and ready"
Feb  6 17:25:56.258: INFO: Pod "labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.063316ms
Feb  6 17:25:56.258: INFO: The phase of Pod labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:25:58.262: INFO: Pod "labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011141418s
Feb  6 17:25:58.262: INFO: The phase of Pod labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:26:00.262: INFO: Pod "labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5": Phase="Running", Reason="", readiness=true. Elapsed: 4.011007261s
Feb  6 17:26:00.262: INFO: The phase of Pod labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5 is Running (Ready = true)
Feb  6 17:26:00.262: INFO: Pod "labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5" satisfied condition "running and ready"
Feb  6 17:26:00.797: INFO: Successfully updated pod "labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb  6 17:26:04.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2179" for this suite. 02/06/23 17:26:04.84
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":17,"skipped":296,"failed":0}
------------------------------
• [SLOW TEST] [8.629 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:25:56.221
    Feb  6 17:25:56.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 17:25:56.222
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:25:56.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:25:56.238
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 02/06/23 17:25:56.241
    Feb  6 17:25:56.250: INFO: Waiting up to 5m0s for pod "labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5" in namespace "projected-2179" to be "running and ready"
    Feb  6 17:25:56.258: INFO: Pod "labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.063316ms
    Feb  6 17:25:56.258: INFO: The phase of Pod labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:25:58.262: INFO: Pod "labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011141418s
    Feb  6 17:25:58.262: INFO: The phase of Pod labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:26:00.262: INFO: Pod "labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5": Phase="Running", Reason="", readiness=true. Elapsed: 4.011007261s
    Feb  6 17:26:00.262: INFO: The phase of Pod labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5 is Running (Ready = true)
    Feb  6 17:26:00.262: INFO: Pod "labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5" satisfied condition "running and ready"
    Feb  6 17:26:00.797: INFO: Successfully updated pod "labelsupdate32dbe331-5ac1-4579-9596-d788f989e8f5"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb  6 17:26:04.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2179" for this suite. 02/06/23 17:26:04.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:26:04.851
Feb  6 17:26:04.851: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename podtemplate 02/06/23 17:26:04.852
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:26:04.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:26:04.867
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 02/06/23 17:26:04.87
Feb  6 17:26:04.876: INFO: created test-podtemplate-1
Feb  6 17:26:04.881: INFO: created test-podtemplate-2
Feb  6 17:26:04.886: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 02/06/23 17:26:04.886
STEP: delete collection of pod templates 02/06/23 17:26:04.889
Feb  6 17:26:04.889: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 02/06/23 17:26:04.905
Feb  6 17:26:04.905: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb  6 17:26:04.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3143" for this suite. 02/06/23 17:26:04.913
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":18,"skipped":318,"failed":0}
------------------------------
• [0.075 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:26:04.851
    Feb  6 17:26:04.851: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename podtemplate 02/06/23 17:26:04.852
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:26:04.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:26:04.867
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 02/06/23 17:26:04.87
    Feb  6 17:26:04.876: INFO: created test-podtemplate-1
    Feb  6 17:26:04.881: INFO: created test-podtemplate-2
    Feb  6 17:26:04.886: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 02/06/23 17:26:04.886
    STEP: delete collection of pod templates 02/06/23 17:26:04.889
    Feb  6 17:26:04.889: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 02/06/23 17:26:04.905
    Feb  6 17:26:04.905: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb  6 17:26:04.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3143" for this suite. 02/06/23 17:26:04.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:26:04.929
Feb  6 17:26:04.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename var-expansion 02/06/23 17:26:04.93
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:26:04.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:26:04.948
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 02/06/23 17:26:04.95
Feb  6 17:26:04.959: INFO: Waiting up to 5m0s for pod "var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098" in namespace "var-expansion-3778" to be "Succeeded or Failed"
Feb  6 17:26:04.965: INFO: Pod "var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098": Phase="Pending", Reason="", readiness=false. Elapsed: 5.364156ms
Feb  6 17:26:06.969: INFO: Pod "var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009197491s
Feb  6 17:26:08.969: INFO: Pod "var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009811112s
Feb  6 17:26:10.971: INFO: Pod "var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011891926s
STEP: Saw pod success 02/06/23 17:26:10.971
Feb  6 17:26:10.972: INFO: Pod "var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098" satisfied condition "Succeeded or Failed"
Feb  6 17:26:10.975: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098 container dapi-container: <nil>
STEP: delete the pod 02/06/23 17:26:10.983
Feb  6 17:26:10.993: INFO: Waiting for pod var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098 to disappear
Feb  6 17:26:10.996: INFO: Pod var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb  6 17:26:10.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3778" for this suite. 02/06/23 17:26:11.001
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":19,"skipped":328,"failed":0}
------------------------------
• [SLOW TEST] [6.078 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:26:04.929
    Feb  6 17:26:04.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename var-expansion 02/06/23 17:26:04.93
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:26:04.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:26:04.948
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 02/06/23 17:26:04.95
    Feb  6 17:26:04.959: INFO: Waiting up to 5m0s for pod "var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098" in namespace "var-expansion-3778" to be "Succeeded or Failed"
    Feb  6 17:26:04.965: INFO: Pod "var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098": Phase="Pending", Reason="", readiness=false. Elapsed: 5.364156ms
    Feb  6 17:26:06.969: INFO: Pod "var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009197491s
    Feb  6 17:26:08.969: INFO: Pod "var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009811112s
    Feb  6 17:26:10.971: INFO: Pod "var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011891926s
    STEP: Saw pod success 02/06/23 17:26:10.971
    Feb  6 17:26:10.972: INFO: Pod "var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098" satisfied condition "Succeeded or Failed"
    Feb  6 17:26:10.975: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098 container dapi-container: <nil>
    STEP: delete the pod 02/06/23 17:26:10.983
    Feb  6 17:26:10.993: INFO: Waiting for pod var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098 to disappear
    Feb  6 17:26:10.996: INFO: Pod var-expansion-48e393f9-1e78-4254-8f94-7c1714e34098 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb  6 17:26:10.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3778" for this suite. 02/06/23 17:26:11.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:26:11.013
Feb  6 17:26:11.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename cronjob 02/06/23 17:26:11.015
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:26:11.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:26:11.034
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 02/06/23 17:26:11.037
STEP: Ensuring more than one job is running at a time 02/06/23 17:26:11.044
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/06/23 17:28:01.049
STEP: Removing cronjob 02/06/23 17:28:01.052
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb  6 17:28:01.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2319" for this suite. 02/06/23 17:28:01.072
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":20,"skipped":355,"failed":0}
------------------------------
• [SLOW TEST] [110.083 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:26:11.013
    Feb  6 17:26:11.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename cronjob 02/06/23 17:26:11.015
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:26:11.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:26:11.034
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 02/06/23 17:26:11.037
    STEP: Ensuring more than one job is running at a time 02/06/23 17:26:11.044
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/06/23 17:28:01.049
    STEP: Removing cronjob 02/06/23 17:28:01.052
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb  6 17:28:01.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2319" for this suite. 02/06/23 17:28:01.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:28:01.108
Feb  6 17:28:01.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-probe 02/06/23 17:28:01.11
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:28:01.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:28:01.151
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb  6 17:29:01.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9001" for this suite. 02/06/23 17:29:01.183
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":21,"skipped":378,"failed":0}
------------------------------
• [SLOW TEST] [60.084 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:28:01.108
    Feb  6 17:28:01.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-probe 02/06/23 17:28:01.11
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:28:01.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:28:01.151
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb  6 17:29:01.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9001" for this suite. 02/06/23 17:29:01.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:29:01.199
Feb  6 17:29:01.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 17:29:01.201
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:29:01.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:29:01.225
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 02/06/23 17:29:01.228
Feb  6 17:29:01.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb  6 17:29:01.341: INFO: stderr: ""
Feb  6 17:29:01.341: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 02/06/23 17:29:01.342
Feb  6 17:29:01.342: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb  6 17:29:01.342: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3870" to be "running and ready, or succeeded"
Feb  6 17:29:01.348: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.761476ms
Feb  6 17:29:01.348: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'tneyla25-md-0-68cbcb4798-6j94p' to be 'Running' but was 'Pending'
Feb  6 17:29:03.354: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012736025s
Feb  6 17:29:03.354: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'tneyla25-md-0-68cbcb4798-6j94p' to be 'Running' but was 'Pending'
Feb  6 17:29:05.354: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.012483283s
Feb  6 17:29:05.354: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb  6 17:29:05.354: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 02/06/23 17:29:05.355
Feb  6 17:29:05.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 logs logs-generator logs-generator'
Feb  6 17:29:05.465: INFO: stderr: ""
Feb  6 17:29:05.465: INFO: stdout: "I0206 17:29:04.064540       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/949 365\nI0206 17:29:04.264769       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/ls78 590\nI0206 17:29:04.465333       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/wn6 425\nI0206 17:29:04.664623       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/2g7 362\nI0206 17:29:04.865099       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/drz 259\nI0206 17:29:05.065478       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/gh5 305\nI0206 17:29:05.264761       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xw8 479\n"
STEP: limiting log lines 02/06/23 17:29:05.465
Feb  6 17:29:05.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 logs logs-generator logs-generator --tail=1'
Feb  6 17:29:05.563: INFO: stderr: ""
Feb  6 17:29:05.563: INFO: stdout: "I0206 17:29:05.465244       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/fjp 561\n"
Feb  6 17:29:05.563: INFO: got output "I0206 17:29:05.465244       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/fjp 561\n"
STEP: limiting log bytes 02/06/23 17:29:05.563
Feb  6 17:29:05.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 logs logs-generator logs-generator --limit-bytes=1'
Feb  6 17:29:05.663: INFO: stderr: ""
Feb  6 17:29:05.663: INFO: stdout: "I"
Feb  6 17:29:05.663: INFO: got output "I"
STEP: exposing timestamps 02/06/23 17:29:05.663
Feb  6 17:29:05.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 logs logs-generator logs-generator --tail=1 --timestamps'
Feb  6 17:29:05.756: INFO: stderr: ""
Feb  6 17:29:05.757: INFO: stdout: "2023-02-06T17:29:05.664760433Z I0206 17:29:05.664566       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4m5 448\n"
Feb  6 17:29:05.757: INFO: got output "2023-02-06T17:29:05.664760433Z I0206 17:29:05.664566       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4m5 448\n"
STEP: restricting to a time range 02/06/23 17:29:05.757
Feb  6 17:29:08.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 logs logs-generator logs-generator --since=1s'
Feb  6 17:29:08.350: INFO: stderr: ""
Feb  6 17:29:08.350: INFO: stdout: "I0206 17:29:07.465145       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/vbfq 577\nI0206 17:29:07.665552       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/jcg 398\nI0206 17:29:07.864993       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/8kl 395\nI0206 17:29:08.065619       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/h6xs 372\nI0206 17:29:08.265071       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/ndzw 321\n"
Feb  6 17:29:08.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 logs logs-generator logs-generator --since=24h'
Feb  6 17:29:08.444: INFO: stderr: ""
Feb  6 17:29:08.444: INFO: stdout: "I0206 17:29:04.064540       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/949 365\nI0206 17:29:04.264769       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/ls78 590\nI0206 17:29:04.465333       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/wn6 425\nI0206 17:29:04.664623       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/2g7 362\nI0206 17:29:04.865099       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/drz 259\nI0206 17:29:05.065478       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/gh5 305\nI0206 17:29:05.264761       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xw8 479\nI0206 17:29:05.465244       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/fjp 561\nI0206 17:29:05.664566       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4m5 448\nI0206 17:29:05.864959       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/mdcf 578\nI0206 17:29:06.065437       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/6gfp 474\nI0206 17:29:06.264802       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/s6dt 416\nI0206 17:29:06.465238       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/k8mj 365\nI0206 17:29:06.665564       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/f8z 254\nI0206 17:29:06.864964       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/6l8 551\nI0206 17:29:07.065369       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/nw2g 489\nI0206 17:29:07.264704       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/9cwn 212\nI0206 17:29:07.465145       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/vbfq 577\nI0206 17:29:07.665552       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/jcg 398\nI0206 17:29:07.864993       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/8kl 395\nI0206 17:29:08.065619       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/h6xs 372\nI0206 17:29:08.265071       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/ndzw 321\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Feb  6 17:29:08.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 delete pod logs-generator'
Feb  6 17:29:09.364: INFO: stderr: ""
Feb  6 17:29:09.364: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 17:29:09.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3870" for this suite. 02/06/23 17:29:09.369
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":22,"skipped":394,"failed":0}
------------------------------
• [SLOW TEST] [8.176 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:29:01.199
    Feb  6 17:29:01.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 17:29:01.201
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:29:01.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:29:01.225
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 02/06/23 17:29:01.228
    Feb  6 17:29:01.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Feb  6 17:29:01.341: INFO: stderr: ""
    Feb  6 17:29:01.341: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 02/06/23 17:29:01.342
    Feb  6 17:29:01.342: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Feb  6 17:29:01.342: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3870" to be "running and ready, or succeeded"
    Feb  6 17:29:01.348: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.761476ms
    Feb  6 17:29:01.348: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'tneyla25-md-0-68cbcb4798-6j94p' to be 'Running' but was 'Pending'
    Feb  6 17:29:03.354: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012736025s
    Feb  6 17:29:03.354: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'tneyla25-md-0-68cbcb4798-6j94p' to be 'Running' but was 'Pending'
    Feb  6 17:29:05.354: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.012483283s
    Feb  6 17:29:05.354: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Feb  6 17:29:05.354: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 02/06/23 17:29:05.355
    Feb  6 17:29:05.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 logs logs-generator logs-generator'
    Feb  6 17:29:05.465: INFO: stderr: ""
    Feb  6 17:29:05.465: INFO: stdout: "I0206 17:29:04.064540       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/949 365\nI0206 17:29:04.264769       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/ls78 590\nI0206 17:29:04.465333       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/wn6 425\nI0206 17:29:04.664623       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/2g7 362\nI0206 17:29:04.865099       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/drz 259\nI0206 17:29:05.065478       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/gh5 305\nI0206 17:29:05.264761       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xw8 479\n"
    STEP: limiting log lines 02/06/23 17:29:05.465
    Feb  6 17:29:05.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 logs logs-generator logs-generator --tail=1'
    Feb  6 17:29:05.563: INFO: stderr: ""
    Feb  6 17:29:05.563: INFO: stdout: "I0206 17:29:05.465244       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/fjp 561\n"
    Feb  6 17:29:05.563: INFO: got output "I0206 17:29:05.465244       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/fjp 561\n"
    STEP: limiting log bytes 02/06/23 17:29:05.563
    Feb  6 17:29:05.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 logs logs-generator logs-generator --limit-bytes=1'
    Feb  6 17:29:05.663: INFO: stderr: ""
    Feb  6 17:29:05.663: INFO: stdout: "I"
    Feb  6 17:29:05.663: INFO: got output "I"
    STEP: exposing timestamps 02/06/23 17:29:05.663
    Feb  6 17:29:05.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 logs logs-generator logs-generator --tail=1 --timestamps'
    Feb  6 17:29:05.756: INFO: stderr: ""
    Feb  6 17:29:05.757: INFO: stdout: "2023-02-06T17:29:05.664760433Z I0206 17:29:05.664566       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4m5 448\n"
    Feb  6 17:29:05.757: INFO: got output "2023-02-06T17:29:05.664760433Z I0206 17:29:05.664566       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4m5 448\n"
    STEP: restricting to a time range 02/06/23 17:29:05.757
    Feb  6 17:29:08.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 logs logs-generator logs-generator --since=1s'
    Feb  6 17:29:08.350: INFO: stderr: ""
    Feb  6 17:29:08.350: INFO: stdout: "I0206 17:29:07.465145       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/vbfq 577\nI0206 17:29:07.665552       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/jcg 398\nI0206 17:29:07.864993       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/8kl 395\nI0206 17:29:08.065619       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/h6xs 372\nI0206 17:29:08.265071       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/ndzw 321\n"
    Feb  6 17:29:08.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 logs logs-generator logs-generator --since=24h'
    Feb  6 17:29:08.444: INFO: stderr: ""
    Feb  6 17:29:08.444: INFO: stdout: "I0206 17:29:04.064540       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/949 365\nI0206 17:29:04.264769       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/ls78 590\nI0206 17:29:04.465333       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/wn6 425\nI0206 17:29:04.664623       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/2g7 362\nI0206 17:29:04.865099       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/drz 259\nI0206 17:29:05.065478       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/gh5 305\nI0206 17:29:05.264761       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xw8 479\nI0206 17:29:05.465244       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/fjp 561\nI0206 17:29:05.664566       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4m5 448\nI0206 17:29:05.864959       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/mdcf 578\nI0206 17:29:06.065437       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/6gfp 474\nI0206 17:29:06.264802       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/s6dt 416\nI0206 17:29:06.465238       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/k8mj 365\nI0206 17:29:06.665564       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/f8z 254\nI0206 17:29:06.864964       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/6l8 551\nI0206 17:29:07.065369       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/nw2g 489\nI0206 17:29:07.264704       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/9cwn 212\nI0206 17:29:07.465145       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/vbfq 577\nI0206 17:29:07.665552       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/jcg 398\nI0206 17:29:07.864993       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/8kl 395\nI0206 17:29:08.065619       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/h6xs 372\nI0206 17:29:08.265071       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/ndzw 321\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Feb  6 17:29:08.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3870 delete pod logs-generator'
    Feb  6 17:29:09.364: INFO: stderr: ""
    Feb  6 17:29:09.364: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 17:29:09.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3870" for this suite. 02/06/23 17:29:09.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:29:09.377
Feb  6 17:29:09.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename var-expansion 02/06/23 17:29:09.378
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:29:09.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:29:09.394
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 02/06/23 17:29:09.443
STEP: waiting for pod running 02/06/23 17:29:09.457
Feb  6 17:29:09.457: INFO: Waiting up to 2m0s for pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f" in namespace "var-expansion-8801" to be "running"
Feb  6 17:29:09.459: INFO: Pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67099ms
Feb  6 17:29:11.464: INFO: Pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007297337s
Feb  6 17:29:13.464: INFO: Pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f": Phase="Running", Reason="", readiness=true. Elapsed: 4.007155156s
Feb  6 17:29:13.464: INFO: Pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f" satisfied condition "running"
STEP: creating a file in subpath 02/06/23 17:29:13.464
Feb  6 17:29:13.467: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8801 PodName:var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 17:29:13.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 17:29:13.468: INFO: ExecWithOptions: Clientset creation
Feb  6 17:29:13.468: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8801/pods/var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 02/06/23 17:29:13.539
Feb  6 17:29:13.543: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8801 PodName:var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 17:29:13.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 17:29:13.544: INFO: ExecWithOptions: Clientset creation
Feb  6 17:29:13.544: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8801/pods/var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 02/06/23 17:29:13.615
Feb  6 17:29:14.131: INFO: Successfully updated pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f"
STEP: waiting for annotated pod running 02/06/23 17:29:14.131
Feb  6 17:29:14.131: INFO: Waiting up to 2m0s for pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f" in namespace "var-expansion-8801" to be "running"
Feb  6 17:29:14.135: INFO: Pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f": Phase="Running", Reason="", readiness=true. Elapsed: 3.554474ms
Feb  6 17:29:14.135: INFO: Pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f" satisfied condition "running"
STEP: deleting the pod gracefully 02/06/23 17:29:14.135
Feb  6 17:29:14.135: INFO: Deleting pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f" in namespace "var-expansion-8801"
Feb  6 17:29:14.145: INFO: Wait up to 5m0s for pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb  6 17:29:48.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8801" for this suite. 02/06/23 17:29:48.158
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":23,"skipped":412,"failed":0}
------------------------------
• [SLOW TEST] [38.787 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:29:09.377
    Feb  6 17:29:09.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename var-expansion 02/06/23 17:29:09.378
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:29:09.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:29:09.394
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 02/06/23 17:29:09.443
    STEP: waiting for pod running 02/06/23 17:29:09.457
    Feb  6 17:29:09.457: INFO: Waiting up to 2m0s for pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f" in namespace "var-expansion-8801" to be "running"
    Feb  6 17:29:09.459: INFO: Pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67099ms
    Feb  6 17:29:11.464: INFO: Pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007297337s
    Feb  6 17:29:13.464: INFO: Pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f": Phase="Running", Reason="", readiness=true. Elapsed: 4.007155156s
    Feb  6 17:29:13.464: INFO: Pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f" satisfied condition "running"
    STEP: creating a file in subpath 02/06/23 17:29:13.464
    Feb  6 17:29:13.467: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8801 PodName:var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 17:29:13.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 17:29:13.468: INFO: ExecWithOptions: Clientset creation
    Feb  6 17:29:13.468: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8801/pods/var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 02/06/23 17:29:13.539
    Feb  6 17:29:13.543: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8801 PodName:var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 17:29:13.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 17:29:13.544: INFO: ExecWithOptions: Clientset creation
    Feb  6 17:29:13.544: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8801/pods/var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 02/06/23 17:29:13.615
    Feb  6 17:29:14.131: INFO: Successfully updated pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f"
    STEP: waiting for annotated pod running 02/06/23 17:29:14.131
    Feb  6 17:29:14.131: INFO: Waiting up to 2m0s for pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f" in namespace "var-expansion-8801" to be "running"
    Feb  6 17:29:14.135: INFO: Pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f": Phase="Running", Reason="", readiness=true. Elapsed: 3.554474ms
    Feb  6 17:29:14.135: INFO: Pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f" satisfied condition "running"
    STEP: deleting the pod gracefully 02/06/23 17:29:14.135
    Feb  6 17:29:14.135: INFO: Deleting pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f" in namespace "var-expansion-8801"
    Feb  6 17:29:14.145: INFO: Wait up to 5m0s for pod "var-expansion-87fe36c4-5374-4598-8ddf-3ed3a962270f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb  6 17:29:48.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8801" for this suite. 02/06/23 17:29:48.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:29:48.167
Feb  6 17:29:48.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename taint-multiple-pods 02/06/23 17:29:48.168
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:29:48.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:29:48.185
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Feb  6 17:29:48.187: INFO: Waiting up to 1m0s for all nodes to be ready
Feb  6 17:30:48.255: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Feb  6 17:30:48.260: INFO: Starting informer...
STEP: Starting pods... 02/06/23 17:30:48.26
Feb  6 17:30:48.479: INFO: Pod1 is running on tneyla25-md-0-68cbcb4798-5xxmw. Tainting Node
Feb  6 17:30:48.694: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9731" to be "running"
Feb  6 17:30:48.697: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572044ms
Feb  6 17:30:50.703: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009437887s
Feb  6 17:30:52.702: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.008014672s
Feb  6 17:30:52.702: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Feb  6 17:30:52.702: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9731" to be "running"
Feb  6 17:30:52.706: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.774513ms
Feb  6 17:30:52.706: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Feb  6 17:30:52.706: INFO: Pod2 is running on tneyla25-md-0-68cbcb4798-5xxmw. Tainting Node
STEP: Trying to apply a taint on the Node 02/06/23 17:30:52.706
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/06/23 17:30:52.721
STEP: Waiting for Pod1 and Pod2 to be deleted 02/06/23 17:30:52.727
Feb  6 17:30:58.455: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb  6 17:31:18.487: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/06/23 17:31:18.502
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Feb  6 17:31:18.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9731" for this suite. 02/06/23 17:31:18.514
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":24,"skipped":418,"failed":0}
------------------------------
• [SLOW TEST] [90.362 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:29:48.167
    Feb  6 17:29:48.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename taint-multiple-pods 02/06/23 17:29:48.168
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:29:48.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:29:48.185
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Feb  6 17:29:48.187: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb  6 17:30:48.255: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Feb  6 17:30:48.260: INFO: Starting informer...
    STEP: Starting pods... 02/06/23 17:30:48.26
    Feb  6 17:30:48.479: INFO: Pod1 is running on tneyla25-md-0-68cbcb4798-5xxmw. Tainting Node
    Feb  6 17:30:48.694: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9731" to be "running"
    Feb  6 17:30:48.697: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572044ms
    Feb  6 17:30:50.703: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009437887s
    Feb  6 17:30:52.702: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.008014672s
    Feb  6 17:30:52.702: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Feb  6 17:30:52.702: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9731" to be "running"
    Feb  6 17:30:52.706: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.774513ms
    Feb  6 17:30:52.706: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Feb  6 17:30:52.706: INFO: Pod2 is running on tneyla25-md-0-68cbcb4798-5xxmw. Tainting Node
    STEP: Trying to apply a taint on the Node 02/06/23 17:30:52.706
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/06/23 17:30:52.721
    STEP: Waiting for Pod1 and Pod2 to be deleted 02/06/23 17:30:52.727
    Feb  6 17:30:58.455: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Feb  6 17:31:18.487: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/06/23 17:31:18.502
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 17:31:18.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-9731" for this suite. 02/06/23 17:31:18.514
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:31:18.529
Feb  6 17:31:18.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename namespaces 02/06/23 17:31:18.53
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:18.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:18.562
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 02/06/23 17:31:18.564
Feb  6 17:31:18.568: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 02/06/23 17:31:18.568
Feb  6 17:31:18.573: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 02/06/23 17:31:18.573
Feb  6 17:31:18.583: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb  6 17:31:18.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4527" for this suite. 02/06/23 17:31:18.588
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":25,"skipped":419,"failed":0}
------------------------------
• [0.064 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:31:18.529
    Feb  6 17:31:18.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename namespaces 02/06/23 17:31:18.53
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:18.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:18.562
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 02/06/23 17:31:18.564
    Feb  6 17:31:18.568: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 02/06/23 17:31:18.568
    Feb  6 17:31:18.573: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 02/06/23 17:31:18.573
    Feb  6 17:31:18.583: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 17:31:18.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4527" for this suite. 02/06/23 17:31:18.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:31:18.598
Feb  6 17:31:18.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 17:31:18.6
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:18.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:18.626
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 02/06/23 17:31:18.629
Feb  6 17:31:18.637: INFO: Waiting up to 5m0s for pod "pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057" in namespace "emptydir-8013" to be "Succeeded or Failed"
Feb  6 17:31:18.645: INFO: Pod "pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057": Phase="Pending", Reason="", readiness=false. Elapsed: 8.214034ms
Feb  6 17:31:20.649: INFO: Pod "pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012237738s
Feb  6 17:31:22.649: INFO: Pod "pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012305925s
Feb  6 17:31:24.652: INFO: Pod "pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015189081s
STEP: Saw pod success 02/06/23 17:31:24.652
Feb  6 17:31:24.652: INFO: Pod "pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057" satisfied condition "Succeeded or Failed"
Feb  6 17:31:24.657: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057 container test-container: <nil>
STEP: delete the pod 02/06/23 17:31:24.674
Feb  6 17:31:24.690: INFO: Waiting for pod pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057 to disappear
Feb  6 17:31:24.694: INFO: Pod pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 17:31:24.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8013" for this suite. 02/06/23 17:31:24.699
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":26,"skipped":446,"failed":0}
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:31:18.598
    Feb  6 17:31:18.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 17:31:18.6
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:18.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:18.626
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/06/23 17:31:18.629
    Feb  6 17:31:18.637: INFO: Waiting up to 5m0s for pod "pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057" in namespace "emptydir-8013" to be "Succeeded or Failed"
    Feb  6 17:31:18.645: INFO: Pod "pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057": Phase="Pending", Reason="", readiness=false. Elapsed: 8.214034ms
    Feb  6 17:31:20.649: INFO: Pod "pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012237738s
    Feb  6 17:31:22.649: INFO: Pod "pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012305925s
    Feb  6 17:31:24.652: INFO: Pod "pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015189081s
    STEP: Saw pod success 02/06/23 17:31:24.652
    Feb  6 17:31:24.652: INFO: Pod "pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057" satisfied condition "Succeeded or Failed"
    Feb  6 17:31:24.657: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057 container test-container: <nil>
    STEP: delete the pod 02/06/23 17:31:24.674
    Feb  6 17:31:24.690: INFO: Waiting for pod pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057 to disappear
    Feb  6 17:31:24.694: INFO: Pod pod-7e55d8e5-a0a5-40e3-9061-e336f4df2057 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 17:31:24.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8013" for this suite. 02/06/23 17:31:24.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:31:24.708
Feb  6 17:31:24.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir-wrapper 02/06/23 17:31:24.709
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:24.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:24.728
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Feb  6 17:31:24.752: INFO: Waiting up to 5m0s for pod "pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd" in namespace "emptydir-wrapper-7079" to be "running and ready"
Feb  6 17:31:24.757: INFO: Pod "pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181821ms
Feb  6 17:31:24.757: INFO: The phase of Pod pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:31:26.762: INFO: Pod "pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009468017s
Feb  6 17:31:26.762: INFO: The phase of Pod pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:31:28.762: INFO: Pod "pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd": Phase="Running", Reason="", readiness=true. Elapsed: 4.009067639s
Feb  6 17:31:28.762: INFO: The phase of Pod pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd is Running (Ready = true)
Feb  6 17:31:28.762: INFO: Pod "pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd" satisfied condition "running and ready"
STEP: Cleaning up the secret 02/06/23 17:31:28.766
STEP: Cleaning up the configmap 02/06/23 17:31:28.778
STEP: Cleaning up the pod 02/06/23 17:31:28.785
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Feb  6 17:31:28.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7079" for this suite. 02/06/23 17:31:28.812
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":27,"skipped":466,"failed":0}
------------------------------
• [4.130 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:31:24.708
    Feb  6 17:31:24.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir-wrapper 02/06/23 17:31:24.709
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:24.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:24.728
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Feb  6 17:31:24.752: INFO: Waiting up to 5m0s for pod "pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd" in namespace "emptydir-wrapper-7079" to be "running and ready"
    Feb  6 17:31:24.757: INFO: Pod "pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181821ms
    Feb  6 17:31:24.757: INFO: The phase of Pod pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:31:26.762: INFO: Pod "pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009468017s
    Feb  6 17:31:26.762: INFO: The phase of Pod pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:31:28.762: INFO: Pod "pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd": Phase="Running", Reason="", readiness=true. Elapsed: 4.009067639s
    Feb  6 17:31:28.762: INFO: The phase of Pod pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd is Running (Ready = true)
    Feb  6 17:31:28.762: INFO: Pod "pod-secrets-c53ee027-87be-459a-abe4-49fe121a44dd" satisfied condition "running and ready"
    STEP: Cleaning up the secret 02/06/23 17:31:28.766
    STEP: Cleaning up the configmap 02/06/23 17:31:28.778
    STEP: Cleaning up the pod 02/06/23 17:31:28.785
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Feb  6 17:31:28.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-7079" for this suite. 02/06/23 17:31:28.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:31:28.84
Feb  6 17:31:28.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename lease-test 02/06/23 17:31:28.841
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:28.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:28.893
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Feb  6 17:31:28.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-7634" for this suite. 02/06/23 17:31:28.961
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":28,"skipped":476,"failed":0}
------------------------------
• [0.130 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:31:28.84
    Feb  6 17:31:28.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename lease-test 02/06/23 17:31:28.841
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:28.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:28.893
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Feb  6 17:31:28.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-7634" for this suite. 02/06/23 17:31:28.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:31:28.973
Feb  6 17:31:28.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 17:31:28.974
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:28.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:28.993
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-5243/configmap-test-0109e163-fed2-4d0d-897f-4b03ca09d9fb 02/06/23 17:31:28.999
STEP: Creating a pod to test consume configMaps 02/06/23 17:31:29.005
Feb  6 17:31:29.014: INFO: Waiting up to 5m0s for pod "pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39" in namespace "configmap-5243" to be "Succeeded or Failed"
Feb  6 17:31:29.021: INFO: Pod "pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39": Phase="Pending", Reason="", readiness=false. Elapsed: 6.436956ms
Feb  6 17:31:31.030: INFO: Pod "pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01571135s
Feb  6 17:31:33.026: INFO: Pod "pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011252945s
Feb  6 17:31:35.025: INFO: Pod "pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010759226s
STEP: Saw pod success 02/06/23 17:31:35.025
Feb  6 17:31:35.026: INFO: Pod "pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39" satisfied condition "Succeeded or Failed"
Feb  6 17:31:35.029: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39 container env-test: <nil>
STEP: delete the pod 02/06/23 17:31:35.037
Feb  6 17:31:35.051: INFO: Waiting for pod pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39 to disappear
Feb  6 17:31:35.055: INFO: Pod pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 17:31:35.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5243" for this suite. 02/06/23 17:31:35.06
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":29,"skipped":500,"failed":0}
------------------------------
• [SLOW TEST] [6.093 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:31:28.973
    Feb  6 17:31:28.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 17:31:28.974
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:28.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:28.993
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-5243/configmap-test-0109e163-fed2-4d0d-897f-4b03ca09d9fb 02/06/23 17:31:28.999
    STEP: Creating a pod to test consume configMaps 02/06/23 17:31:29.005
    Feb  6 17:31:29.014: INFO: Waiting up to 5m0s for pod "pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39" in namespace "configmap-5243" to be "Succeeded or Failed"
    Feb  6 17:31:29.021: INFO: Pod "pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39": Phase="Pending", Reason="", readiness=false. Elapsed: 6.436956ms
    Feb  6 17:31:31.030: INFO: Pod "pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01571135s
    Feb  6 17:31:33.026: INFO: Pod "pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011252945s
    Feb  6 17:31:35.025: INFO: Pod "pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010759226s
    STEP: Saw pod success 02/06/23 17:31:35.025
    Feb  6 17:31:35.026: INFO: Pod "pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39" satisfied condition "Succeeded or Failed"
    Feb  6 17:31:35.029: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39 container env-test: <nil>
    STEP: delete the pod 02/06/23 17:31:35.037
    Feb  6 17:31:35.051: INFO: Waiting for pod pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39 to disappear
    Feb  6 17:31:35.055: INFO: Pod pod-configmaps-b5ab0e84-55ec-46be-b338-0646d1192c39 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 17:31:35.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5243" for this suite. 02/06/23 17:31:35.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:31:35.07
Feb  6 17:31:35.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename job 02/06/23 17:31:35.072
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:35.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:35.09
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 02/06/23 17:31:35.093
STEP: Ensuring job reaches completions 02/06/23 17:31:35.101
STEP: Ensuring pods with index for job exist 02/06/23 17:31:49.105
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb  6 17:31:49.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7220" for this suite. 02/06/23 17:31:49.114
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":30,"skipped":546,"failed":0}
------------------------------
• [SLOW TEST] [14.051 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:31:35.07
    Feb  6 17:31:35.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename job 02/06/23 17:31:35.072
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:35.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:35.09
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 02/06/23 17:31:35.093
    STEP: Ensuring job reaches completions 02/06/23 17:31:35.101
    STEP: Ensuring pods with index for job exist 02/06/23 17:31:49.105
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb  6 17:31:49.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7220" for this suite. 02/06/23 17:31:49.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:31:49.125
Feb  6 17:31:49.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 17:31:49.126
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:49.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:49.146
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-6cf054ad-2e6e-40b5-89ee-3df3708aaf96 02/06/23 17:31:49.156
STEP: Creating the pod 02/06/23 17:31:49.162
Feb  6 17:31:49.170: INFO: Waiting up to 5m0s for pod "pod-configmaps-0d871a8f-f823-42ae-ba7c-f087dda9440c" in namespace "configmap-9616" to be "running"
Feb  6 17:31:49.174: INFO: Pod "pod-configmaps-0d871a8f-f823-42ae-ba7c-f087dda9440c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.597535ms
Feb  6 17:31:51.178: INFO: Pod "pod-configmaps-0d871a8f-f823-42ae-ba7c-f087dda9440c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00750728s
Feb  6 17:31:53.178: INFO: Pod "pod-configmaps-0d871a8f-f823-42ae-ba7c-f087dda9440c": Phase="Running", Reason="", readiness=false. Elapsed: 4.007754926s
Feb  6 17:31:53.178: INFO: Pod "pod-configmaps-0d871a8f-f823-42ae-ba7c-f087dda9440c" satisfied condition "running"
STEP: Waiting for pod with text data 02/06/23 17:31:53.178
STEP: Waiting for pod with binary data 02/06/23 17:31:53.185
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 17:31:53.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9616" for this suite. 02/06/23 17:31:53.198
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":31,"skipped":558,"failed":0}
------------------------------
• [4.083 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:31:49.125
    Feb  6 17:31:49.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 17:31:49.126
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:49.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:49.146
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-6cf054ad-2e6e-40b5-89ee-3df3708aaf96 02/06/23 17:31:49.156
    STEP: Creating the pod 02/06/23 17:31:49.162
    Feb  6 17:31:49.170: INFO: Waiting up to 5m0s for pod "pod-configmaps-0d871a8f-f823-42ae-ba7c-f087dda9440c" in namespace "configmap-9616" to be "running"
    Feb  6 17:31:49.174: INFO: Pod "pod-configmaps-0d871a8f-f823-42ae-ba7c-f087dda9440c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.597535ms
    Feb  6 17:31:51.178: INFO: Pod "pod-configmaps-0d871a8f-f823-42ae-ba7c-f087dda9440c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00750728s
    Feb  6 17:31:53.178: INFO: Pod "pod-configmaps-0d871a8f-f823-42ae-ba7c-f087dda9440c": Phase="Running", Reason="", readiness=false. Elapsed: 4.007754926s
    Feb  6 17:31:53.178: INFO: Pod "pod-configmaps-0d871a8f-f823-42ae-ba7c-f087dda9440c" satisfied condition "running"
    STEP: Waiting for pod with text data 02/06/23 17:31:53.178
    STEP: Waiting for pod with binary data 02/06/23 17:31:53.185
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 17:31:53.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9616" for this suite. 02/06/23 17:31:53.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:31:53.214
Feb  6 17:31:53.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 17:31:53.215
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:53.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:53.233
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Feb  6 17:31:53.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/06/23 17:32:02.422
Feb  6 17:32:02.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-5971 --namespace=crd-publish-openapi-5971 create -f -'
Feb  6 17:32:04.004: INFO: stderr: ""
Feb  6 17:32:04.004: INFO: stdout: "e2e-test-crd-publish-openapi-2606-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb  6 17:32:04.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-5971 --namespace=crd-publish-openapi-5971 delete e2e-test-crd-publish-openapi-2606-crds test-cr'
Feb  6 17:32:04.105: INFO: stderr: ""
Feb  6 17:32:04.105: INFO: stdout: "e2e-test-crd-publish-openapi-2606-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb  6 17:32:04.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-5971 --namespace=crd-publish-openapi-5971 apply -f -'
Feb  6 17:32:05.586: INFO: stderr: ""
Feb  6 17:32:05.586: INFO: stdout: "e2e-test-crd-publish-openapi-2606-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb  6 17:32:05.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-5971 --namespace=crd-publish-openapi-5971 delete e2e-test-crd-publish-openapi-2606-crds test-cr'
Feb  6 17:32:05.679: INFO: stderr: ""
Feb  6 17:32:05.679: INFO: stdout: "e2e-test-crd-publish-openapi-2606-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 02/06/23 17:32:05.679
Feb  6 17:32:05.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-5971 explain e2e-test-crd-publish-openapi-2606-crds'
Feb  6 17:32:07.071: INFO: stderr: ""
Feb  6 17:32:07.071: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2606-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:32:15.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5971" for this suite. 02/06/23 17:32:15.157
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":32,"skipped":573,"failed":0}
------------------------------
• [SLOW TEST] [21.952 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:31:53.214
    Feb  6 17:31:53.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 17:31:53.215
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:31:53.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:31:53.233
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Feb  6 17:31:53.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/06/23 17:32:02.422
    Feb  6 17:32:02.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-5971 --namespace=crd-publish-openapi-5971 create -f -'
    Feb  6 17:32:04.004: INFO: stderr: ""
    Feb  6 17:32:04.004: INFO: stdout: "e2e-test-crd-publish-openapi-2606-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb  6 17:32:04.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-5971 --namespace=crd-publish-openapi-5971 delete e2e-test-crd-publish-openapi-2606-crds test-cr'
    Feb  6 17:32:04.105: INFO: stderr: ""
    Feb  6 17:32:04.105: INFO: stdout: "e2e-test-crd-publish-openapi-2606-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Feb  6 17:32:04.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-5971 --namespace=crd-publish-openapi-5971 apply -f -'
    Feb  6 17:32:05.586: INFO: stderr: ""
    Feb  6 17:32:05.586: INFO: stdout: "e2e-test-crd-publish-openapi-2606-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb  6 17:32:05.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-5971 --namespace=crd-publish-openapi-5971 delete e2e-test-crd-publish-openapi-2606-crds test-cr'
    Feb  6 17:32:05.679: INFO: stderr: ""
    Feb  6 17:32:05.679: INFO: stdout: "e2e-test-crd-publish-openapi-2606-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 02/06/23 17:32:05.679
    Feb  6 17:32:05.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-5971 explain e2e-test-crd-publish-openapi-2606-crds'
    Feb  6 17:32:07.071: INFO: stderr: ""
    Feb  6 17:32:07.071: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2606-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:32:15.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5971" for this suite. 02/06/23 17:32:15.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:32:15.168
Feb  6 17:32:15.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 17:32:15.169
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:32:15.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:32:15.19
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 17:32:15.207
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:32:15.498
STEP: Deploying the webhook pod 02/06/23 17:32:15.506
STEP: Wait for the deployment to be ready 02/06/23 17:32:15.521
Feb  6 17:32:15.537: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 17:32:17.549: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 32, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 32, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 32, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 32, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 17:32:19.555
STEP: Verifying the service has paired with the endpoint 02/06/23 17:32:19.571
Feb  6 17:32:20.571: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Feb  6 17:32:20.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/06/23 17:32:21.086
STEP: Creating a custom resource that should be denied by the webhook 02/06/23 17:32:21.103
STEP: Creating a custom resource whose deletion would be denied by the webhook 02/06/23 17:32:23.163
STEP: Updating the custom resource with disallowed data should be denied 02/06/23 17:32:23.177
STEP: Deleting the custom resource should be denied 02/06/23 17:32:23.215
STEP: Remove the offending key and value from the custom resource data 02/06/23 17:32:23.229
STEP: Deleting the updated custom resource should be successful 02/06/23 17:32:23.288
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:32:23.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-152" for this suite. 02/06/23 17:32:23.898
STEP: Destroying namespace "webhook-152-markers" for this suite. 02/06/23 17:32:23.906
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":33,"skipped":589,"failed":0}
------------------------------
• [SLOW TEST] [8.814 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:32:15.168
    Feb  6 17:32:15.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 17:32:15.169
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:32:15.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:32:15.19
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 17:32:15.207
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:32:15.498
    STEP: Deploying the webhook pod 02/06/23 17:32:15.506
    STEP: Wait for the deployment to be ready 02/06/23 17:32:15.521
    Feb  6 17:32:15.537: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 17:32:17.549: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 32, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 32, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 32, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 32, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 17:32:19.555
    STEP: Verifying the service has paired with the endpoint 02/06/23 17:32:19.571
    Feb  6 17:32:20.571: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Feb  6 17:32:20.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/06/23 17:32:21.086
    STEP: Creating a custom resource that should be denied by the webhook 02/06/23 17:32:21.103
    STEP: Creating a custom resource whose deletion would be denied by the webhook 02/06/23 17:32:23.163
    STEP: Updating the custom resource with disallowed data should be denied 02/06/23 17:32:23.177
    STEP: Deleting the custom resource should be denied 02/06/23 17:32:23.215
    STEP: Remove the offending key and value from the custom resource data 02/06/23 17:32:23.229
    STEP: Deleting the updated custom resource should be successful 02/06/23 17:32:23.288
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:32:23.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-152" for this suite. 02/06/23 17:32:23.898
    STEP: Destroying namespace "webhook-152-markers" for this suite. 02/06/23 17:32:23.906
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:32:23.997
Feb  6 17:32:23.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 17:32:23.999
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:32:24.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:32:24.031
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 02/06/23 17:32:24.036
Feb  6 17:32:24.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87" in namespace "downward-api-7120" to be "Succeeded or Failed"
Feb  6 17:32:24.057: INFO: Pod "downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.984974ms
Feb  6 17:32:26.064: INFO: Pod "downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012246109s
Feb  6 17:32:28.062: INFO: Pod "downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010019384s
Feb  6 17:32:30.061: INFO: Pod "downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009577216s
STEP: Saw pod success 02/06/23 17:32:30.061
Feb  6 17:32:30.062: INFO: Pod "downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87" satisfied condition "Succeeded or Failed"
Feb  6 17:32:30.065: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87 container client-container: <nil>
STEP: delete the pod 02/06/23 17:32:30.071
Feb  6 17:32:30.083: INFO: Waiting for pod downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87 to disappear
Feb  6 17:32:30.086: INFO: Pod downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb  6 17:32:30.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7120" for this suite. 02/06/23 17:32:30.091
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":34,"skipped":618,"failed":0}
------------------------------
• [SLOW TEST] [6.102 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:32:23.997
    Feb  6 17:32:23.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 17:32:23.999
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:32:24.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:32:24.031
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 02/06/23 17:32:24.036
    Feb  6 17:32:24.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87" in namespace "downward-api-7120" to be "Succeeded or Failed"
    Feb  6 17:32:24.057: INFO: Pod "downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.984974ms
    Feb  6 17:32:26.064: INFO: Pod "downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012246109s
    Feb  6 17:32:28.062: INFO: Pod "downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010019384s
    Feb  6 17:32:30.061: INFO: Pod "downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009577216s
    STEP: Saw pod success 02/06/23 17:32:30.061
    Feb  6 17:32:30.062: INFO: Pod "downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87" satisfied condition "Succeeded or Failed"
    Feb  6 17:32:30.065: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87 container client-container: <nil>
    STEP: delete the pod 02/06/23 17:32:30.071
    Feb  6 17:32:30.083: INFO: Waiting for pod downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87 to disappear
    Feb  6 17:32:30.086: INFO: Pod downwardapi-volume-9699dfb3-fb43-410d-9653-e7a88f635c87 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb  6 17:32:30.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7120" for this suite. 02/06/23 17:32:30.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:32:30.101
Feb  6 17:32:30.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename sched-pred 02/06/23 17:32:30.102
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:32:30.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:32:30.126
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb  6 17:32:30.131: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  6 17:32:30.143: INFO: Waiting for terminating namespaces to be deleted...
Feb  6 17:32:30.147: INFO: 
Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-5xxmw before test
Feb  6 17:32:30.160: INFO: cilium-9gdv4 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.160: INFO: 	Container cilium-agent ready: true, restart count 0
Feb  6 17:32:30.160: INFO: kube-proxy-mnvm8 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.160: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  6 17:32:30.160: INFO: vsphere-cloud-controller-manager-7dmv6 from kube-system started at 2023-02-06 17:31:18 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.160: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Feb  6 17:32:30.160: INFO: vsphere-csi-node-7l6qr from kube-system started at 2023-02-06 17:12:22 +0000 UTC (3 container statuses recorded)
Feb  6 17:32:30.160: INFO: 	Container liveness-probe ready: true, restart count 0
Feb  6 17:32:30.160: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  6 17:32:30.160: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Feb  6 17:32:30.160: INFO: sonobuoy from sonobuoy started at 2023-02-06 17:18:54 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.160: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  6 17:32:30.160: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-cr8qs from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 17:32:30.160: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 17:32:30.160: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  6 17:32:30.160: INFO: 
Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-6j94p before test
Feb  6 17:32:30.175: INFO: capi-kubeadm-control-plane-controller-manager-5f686d9d47-lkld2 from capi-kubeadm-control-plane-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.175: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:32:30.175: INFO: capi-controller-manager-5b68b88d59-vcpp2 from capi-system started at 2023-02-06 17:15:34 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.175: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:32:30.175: INFO: cert-manager-7b9fcd6db9-rz78g from cert-manager started at 2023-02-06 17:15:16 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.175: INFO: 	Container cert-manager ready: true, restart count 0
Feb  6 17:32:30.175: INFO: cert-manager-cainjector-75b587b679-rjbzj from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.175: INFO: 	Container cert-manager ready: true, restart count 0
Feb  6 17:32:30.175: INFO: cert-manager-webhook-896b549c4-hj9c4 from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.175: INFO: 	Container cert-manager ready: true, restart count 0
Feb  6 17:32:30.175: INFO: eks-anywhere-packages-8589bd5d9c-4pc9v from eksa-packages started at 2023-02-06 17:17:15 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.175: INFO: 	Container controller ready: true, restart count 0
Feb  6 17:32:30.175: INFO: etcdadm-bootstrap-provider-controller-manager-84ff796964-d6tt8 from etcdadm-bootstrap-provider-system started at 2023-02-06 17:15:37 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.175: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:32:30.175: INFO: cilium-nb887 from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.175: INFO: 	Container cilium-agent ready: true, restart count 0
Feb  6 17:32:30.175: INFO: kube-proxy-ld4nv from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.175: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  6 17:32:30.175: INFO: vsphere-cloud-controller-manager-76l5k from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.175: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Feb  6 17:32:30.175: INFO: vsphere-csi-node-2tfxk from kube-system started at 2023-02-06 17:12:26 +0000 UTC (3 container statuses recorded)
Feb  6 17:32:30.175: INFO: 	Container liveness-probe ready: true, restart count 0
Feb  6 17:32:30.175: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  6 17:32:30.175: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Feb  6 17:32:30.175: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-hcvcv from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 17:32:30.175: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 17:32:30.176: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  6 17:32:30.176: INFO: 
Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-lz542 before test
Feb  6 17:32:30.188: INFO: capi-kubeadm-bootstrap-controller-manager-79d4bdf548-4xmjq from capi-kubeadm-bootstrap-system started at 2023-02-06 17:15:36 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.188: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:32:30.188: INFO: capv-controller-manager-598f9bf684-4qdws from capv-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.188: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:32:30.188: INFO: eksa-controller-manager-5588899966-wtcpc from eksa-system started at 2023-02-06 17:16:14 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.188: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:32:30.188: INFO: etcdadm-controller-controller-manager-767b7fb9b5-mt7lx from etcdadm-controller-system started at 2023-02-06 17:15:38 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.188: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:32:30.188: INFO: cilium-ndf4k from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.188: INFO: 	Container cilium-agent ready: true, restart count 0
Feb  6 17:32:30.188: INFO: cilium-operator-7f966b595f-pzn9c from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.188: INFO: 	Container cilium-operator ready: true, restart count 0
Feb  6 17:32:30.188: INFO: kube-proxy-hfblf from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.188: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  6 17:32:30.188: INFO: vsphere-cloud-controller-manager-x8k8p from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 17:32:30.188: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Feb  6 17:32:30.188: INFO: vsphere-csi-node-v4zd6 from kube-system started at 2023-02-06 17:12:15 +0000 UTC (3 container statuses recorded)
Feb  6 17:32:30.188: INFO: 	Container liveness-probe ready: true, restart count 0
Feb  6 17:32:30.188: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  6 17:32:30.188: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Feb  6 17:32:30.188: INFO: sonobuoy-e2e-job-b0e9e31f5d894166 from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 17:32:30.188: INFO: 	Container e2e ready: true, restart count 0
Feb  6 17:32:30.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 17:32:30.188: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-fncmf from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 17:32:30.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 17:32:30.188: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/06/23 17:32:30.188
Feb  6 17:32:30.203: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1406" to be "running"
Feb  6 17:32:30.212: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.651814ms
Feb  6 17:32:32.217: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014186456s
Feb  6 17:32:34.216: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.012960288s
Feb  6 17:32:34.216: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/06/23 17:32:34.219
STEP: Trying to apply a random label on the found node. 02/06/23 17:32:34.235
STEP: verifying the node has the label kubernetes.io/e2e-74ecc861-ceb4-476c-b746-330517fff94d 42 02/06/23 17:32:34.254
STEP: Trying to relaunch the pod, now with labels. 02/06/23 17:32:34.269
Feb  6 17:32:34.278: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1406" to be "not pending"
Feb  6 17:32:34.286: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024969ms
Feb  6 17:32:36.292: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013444281s
Feb  6 17:32:38.291: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.012448028s
Feb  6 17:32:38.291: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-74ecc861-ceb4-476c-b746-330517fff94d off the node tneyla25-md-0-68cbcb4798-5xxmw 02/06/23 17:32:38.294
STEP: verifying the node doesn't have the label kubernetes.io/e2e-74ecc861-ceb4-476c-b746-330517fff94d 02/06/23 17:32:38.315
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb  6 17:32:38.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1406" for this suite. 02/06/23 17:32:38.328
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":35,"skipped":640,"failed":0}
------------------------------
• [SLOW TEST] [8.238 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:32:30.101
    Feb  6 17:32:30.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename sched-pred 02/06/23 17:32:30.102
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:32:30.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:32:30.126
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb  6 17:32:30.131: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb  6 17:32:30.143: INFO: Waiting for terminating namespaces to be deleted...
    Feb  6 17:32:30.147: INFO: 
    Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-5xxmw before test
    Feb  6 17:32:30.160: INFO: cilium-9gdv4 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.160: INFO: 	Container cilium-agent ready: true, restart count 0
    Feb  6 17:32:30.160: INFO: kube-proxy-mnvm8 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.160: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb  6 17:32:30.160: INFO: vsphere-cloud-controller-manager-7dmv6 from kube-system started at 2023-02-06 17:31:18 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.160: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
    Feb  6 17:32:30.160: INFO: vsphere-csi-node-7l6qr from kube-system started at 2023-02-06 17:12:22 +0000 UTC (3 container statuses recorded)
    Feb  6 17:32:30.160: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb  6 17:32:30.160: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb  6 17:32:30.160: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Feb  6 17:32:30.160: INFO: sonobuoy from sonobuoy started at 2023-02-06 17:18:54 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.160: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb  6 17:32:30.160: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-cr8qs from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 17:32:30.160: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 17:32:30.160: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb  6 17:32:30.160: INFO: 
    Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-6j94p before test
    Feb  6 17:32:30.175: INFO: capi-kubeadm-control-plane-controller-manager-5f686d9d47-lkld2 from capi-kubeadm-control-plane-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.175: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: capi-controller-manager-5b68b88d59-vcpp2 from capi-system started at 2023-02-06 17:15:34 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.175: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: cert-manager-7b9fcd6db9-rz78g from cert-manager started at 2023-02-06 17:15:16 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.175: INFO: 	Container cert-manager ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: cert-manager-cainjector-75b587b679-rjbzj from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.175: INFO: 	Container cert-manager ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: cert-manager-webhook-896b549c4-hj9c4 from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.175: INFO: 	Container cert-manager ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: eks-anywhere-packages-8589bd5d9c-4pc9v from eksa-packages started at 2023-02-06 17:17:15 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.175: INFO: 	Container controller ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: etcdadm-bootstrap-provider-controller-manager-84ff796964-d6tt8 from etcdadm-bootstrap-provider-system started at 2023-02-06 17:15:37 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.175: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: cilium-nb887 from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.175: INFO: 	Container cilium-agent ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: kube-proxy-ld4nv from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.175: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: vsphere-cloud-controller-manager-76l5k from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.175: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: vsphere-csi-node-2tfxk from kube-system started at 2023-02-06 17:12:26 +0000 UTC (3 container statuses recorded)
    Feb  6 17:32:30.175: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Feb  6 17:32:30.175: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-hcvcv from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 17:32:30.175: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 17:32:30.176: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb  6 17:32:30.176: INFO: 
    Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-lz542 before test
    Feb  6 17:32:30.188: INFO: capi-kubeadm-bootstrap-controller-manager-79d4bdf548-4xmjq from capi-kubeadm-bootstrap-system started at 2023-02-06 17:15:36 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.188: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: capv-controller-manager-598f9bf684-4qdws from capv-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.188: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: eksa-controller-manager-5588899966-wtcpc from eksa-system started at 2023-02-06 17:16:14 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.188: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: etcdadm-controller-controller-manager-767b7fb9b5-mt7lx from etcdadm-controller-system started at 2023-02-06 17:15:38 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.188: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: cilium-ndf4k from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.188: INFO: 	Container cilium-agent ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: cilium-operator-7f966b595f-pzn9c from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.188: INFO: 	Container cilium-operator ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: kube-proxy-hfblf from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.188: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: vsphere-cloud-controller-manager-x8k8p from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 17:32:30.188: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: vsphere-csi-node-v4zd6 from kube-system started at 2023-02-06 17:12:15 +0000 UTC (3 container statuses recorded)
    Feb  6 17:32:30.188: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: sonobuoy-e2e-job-b0e9e31f5d894166 from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 17:32:30.188: INFO: 	Container e2e ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-fncmf from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 17:32:30.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 17:32:30.188: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/06/23 17:32:30.188
    Feb  6 17:32:30.203: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1406" to be "running"
    Feb  6 17:32:30.212: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.651814ms
    Feb  6 17:32:32.217: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014186456s
    Feb  6 17:32:34.216: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.012960288s
    Feb  6 17:32:34.216: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/06/23 17:32:34.219
    STEP: Trying to apply a random label on the found node. 02/06/23 17:32:34.235
    STEP: verifying the node has the label kubernetes.io/e2e-74ecc861-ceb4-476c-b746-330517fff94d 42 02/06/23 17:32:34.254
    STEP: Trying to relaunch the pod, now with labels. 02/06/23 17:32:34.269
    Feb  6 17:32:34.278: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1406" to be "not pending"
    Feb  6 17:32:34.286: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024969ms
    Feb  6 17:32:36.292: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013444281s
    Feb  6 17:32:38.291: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.012448028s
    Feb  6 17:32:38.291: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-74ecc861-ceb4-476c-b746-330517fff94d off the node tneyla25-md-0-68cbcb4798-5xxmw 02/06/23 17:32:38.294
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-74ecc861-ceb4-476c-b746-330517fff94d 02/06/23 17:32:38.315
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 17:32:38.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1406" for this suite. 02/06/23 17:32:38.328
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:32:38.34
Feb  6 17:32:38.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 17:32:38.341
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:32:38.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:32:38.359
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-1720 02/06/23 17:32:38.364
Feb  6 17:32:38.371: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1720" to be "running and ready"
Feb  6 17:32:38.375: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.903783ms
Feb  6 17:32:38.375: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:32:40.379: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.008198086s
Feb  6 17:32:40.380: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb  6 17:32:40.380: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Feb  6 17:32:40.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb  6 17:32:40.552: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb  6 17:32:40.552: INFO: stdout: "iptables"
Feb  6 17:32:40.552: INFO: proxyMode: iptables
Feb  6 17:32:40.569: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb  6 17:32:40.574: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-1720 02/06/23 17:32:40.574
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1720 02/06/23 17:32:40.597
I0206 17:32:40.609147      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1720, replica count: 3
I0206 17:32:43.660188      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 17:32:46.662065      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  6 17:32:46.676: INFO: Creating new exec pod
Feb  6 17:32:46.684: INFO: Waiting up to 5m0s for pod "execpod-affinitytrv5m" in namespace "services-1720" to be "running"
Feb  6 17:32:46.691: INFO: Pod "execpod-affinitytrv5m": Phase="Pending", Reason="", readiness=false. Elapsed: 7.081294ms
Feb  6 17:32:48.694: INFO: Pod "execpod-affinitytrv5m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010663154s
Feb  6 17:32:50.695: INFO: Pod "execpod-affinitytrv5m": Phase="Running", Reason="", readiness=true. Elapsed: 4.011293758s
Feb  6 17:32:50.695: INFO: Pod "execpod-affinitytrv5m" satisfied condition "running"
Feb  6 17:32:51.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Feb  6 17:32:51.870: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Feb  6 17:32:51.870: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 17:32:51.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.248.56 80'
Feb  6 17:32:52.033: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.248.56 80\nConnection to 10.99.248.56 80 port [tcp/http] succeeded!\n"
Feb  6 17:32:52.033: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 17:32:52.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.90 31592'
Feb  6 17:32:52.188: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.148.90 31592\nConnection to 195.17.148.90 31592 port [tcp/*] succeeded!\n"
Feb  6 17:32:52.188: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 17:32:52.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.89 31592'
Feb  6 17:32:52.353: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.148.89 31592\nConnection to 195.17.148.89 31592 port [tcp/*] succeeded!\n"
Feb  6 17:32:52.353: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 17:32:52.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://195.17.4.133:31592/ ; done'
Feb  6 17:32:52.622: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n"
Feb  6 17:32:52.622: INFO: stdout: "\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v"
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
Feb  6 17:32:52.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://195.17.4.133:31592/'
Feb  6 17:32:52.790: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n"
Feb  6 17:32:52.790: INFO: stdout: "affinity-nodeport-timeout-8zl8v"
Feb  6 17:33:12.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://195.17.4.133:31592/'
Feb  6 17:33:12.972: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n"
Feb  6 17:33:12.972: INFO: stdout: "affinity-nodeport-timeout-8zl8v"
Feb  6 17:33:32.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://195.17.4.133:31592/'
Feb  6 17:33:33.137: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n"
Feb  6 17:33:33.137: INFO: stdout: "affinity-nodeport-timeout-4rljk"
Feb  6 17:33:33.137: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1720, will wait for the garbage collector to delete the pods 02/06/23 17:33:33.155
Feb  6 17:33:33.224: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 8.699957ms
Feb  6 17:33:33.324: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.556783ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 17:33:35.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1720" for this suite. 02/06/23 17:33:35.368
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":36,"skipped":646,"failed":0}
------------------------------
• [SLOW TEST] [57.035 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:32:38.34
    Feb  6 17:32:38.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 17:32:38.341
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:32:38.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:32:38.359
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-1720 02/06/23 17:32:38.364
    Feb  6 17:32:38.371: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1720" to be "running and ready"
    Feb  6 17:32:38.375: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.903783ms
    Feb  6 17:32:38.375: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:32:40.379: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.008198086s
    Feb  6 17:32:40.380: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Feb  6 17:32:40.380: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Feb  6 17:32:40.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Feb  6 17:32:40.552: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Feb  6 17:32:40.552: INFO: stdout: "iptables"
    Feb  6 17:32:40.552: INFO: proxyMode: iptables
    Feb  6 17:32:40.569: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Feb  6 17:32:40.574: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-1720 02/06/23 17:32:40.574
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-1720 02/06/23 17:32:40.597
    I0206 17:32:40.609147      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1720, replica count: 3
    I0206 17:32:43.660188      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 17:32:46.662065      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb  6 17:32:46.676: INFO: Creating new exec pod
    Feb  6 17:32:46.684: INFO: Waiting up to 5m0s for pod "execpod-affinitytrv5m" in namespace "services-1720" to be "running"
    Feb  6 17:32:46.691: INFO: Pod "execpod-affinitytrv5m": Phase="Pending", Reason="", readiness=false. Elapsed: 7.081294ms
    Feb  6 17:32:48.694: INFO: Pod "execpod-affinitytrv5m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010663154s
    Feb  6 17:32:50.695: INFO: Pod "execpod-affinitytrv5m": Phase="Running", Reason="", readiness=true. Elapsed: 4.011293758s
    Feb  6 17:32:50.695: INFO: Pod "execpod-affinitytrv5m" satisfied condition "running"
    Feb  6 17:32:51.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Feb  6 17:32:51.870: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Feb  6 17:32:51.870: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 17:32:51.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.248.56 80'
    Feb  6 17:32:52.033: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.248.56 80\nConnection to 10.99.248.56 80 port [tcp/http] succeeded!\n"
    Feb  6 17:32:52.033: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 17:32:52.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.90 31592'
    Feb  6 17:32:52.188: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.148.90 31592\nConnection to 195.17.148.90 31592 port [tcp/*] succeeded!\n"
    Feb  6 17:32:52.188: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 17:32:52.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.89 31592'
    Feb  6 17:32:52.353: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.148.89 31592\nConnection to 195.17.148.89 31592 port [tcp/*] succeeded!\n"
    Feb  6 17:32:52.353: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 17:32:52.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://195.17.4.133:31592/ ; done'
    Feb  6 17:32:52.622: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n"
    Feb  6 17:32:52.622: INFO: stdout: "\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v\naffinity-nodeport-timeout-8zl8v"
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Received response from host: affinity-nodeport-timeout-8zl8v
    Feb  6 17:32:52.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://195.17.4.133:31592/'
    Feb  6 17:32:52.790: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n"
    Feb  6 17:32:52.790: INFO: stdout: "affinity-nodeport-timeout-8zl8v"
    Feb  6 17:33:12.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://195.17.4.133:31592/'
    Feb  6 17:33:12.972: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n"
    Feb  6 17:33:12.972: INFO: stdout: "affinity-nodeport-timeout-8zl8v"
    Feb  6 17:33:32.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1720 exec execpod-affinitytrv5m -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://195.17.4.133:31592/'
    Feb  6 17:33:33.137: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://195.17.4.133:31592/\n"
    Feb  6 17:33:33.137: INFO: stdout: "affinity-nodeport-timeout-4rljk"
    Feb  6 17:33:33.137: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1720, will wait for the garbage collector to delete the pods 02/06/23 17:33:33.155
    Feb  6 17:33:33.224: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 8.699957ms
    Feb  6 17:33:33.324: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.556783ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 17:33:35.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1720" for this suite. 02/06/23 17:33:35.368
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:33:35.376
Feb  6 17:33:35.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename watch 02/06/23 17:33:35.377
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:33:35.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:33:35.397
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 02/06/23 17:33:35.4
STEP: modifying the configmap once 02/06/23 17:33:35.407
STEP: modifying the configmap a second time 02/06/23 17:33:35.415
STEP: deleting the configmap 02/06/23 17:33:35.424
STEP: creating a watch on configmaps from the resource version returned by the first update 02/06/23 17:33:35.432
STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/06/23 17:33:35.434
Feb  6 17:33:35.434: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4469  14c29bd7-9abf-43bf-8e12-9a7d2d1a1f95 16544 0 2023-02-06 17:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-06 17:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 17:33:35.435: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4469  14c29bd7-9abf-43bf-8e12-9a7d2d1a1f95 16545 0 2023-02-06 17:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-06 17:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb  6 17:33:35.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4469" for this suite. 02/06/23 17:33:35.441
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":37,"skipped":658,"failed":0}
------------------------------
• [0.071 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:33:35.376
    Feb  6 17:33:35.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename watch 02/06/23 17:33:35.377
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:33:35.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:33:35.397
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 02/06/23 17:33:35.4
    STEP: modifying the configmap once 02/06/23 17:33:35.407
    STEP: modifying the configmap a second time 02/06/23 17:33:35.415
    STEP: deleting the configmap 02/06/23 17:33:35.424
    STEP: creating a watch on configmaps from the resource version returned by the first update 02/06/23 17:33:35.432
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/06/23 17:33:35.434
    Feb  6 17:33:35.434: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4469  14c29bd7-9abf-43bf-8e12-9a7d2d1a1f95 16544 0 2023-02-06 17:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-06 17:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 17:33:35.435: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4469  14c29bd7-9abf-43bf-8e12-9a7d2d1a1f95 16545 0 2023-02-06 17:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-06 17:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb  6 17:33:35.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4469" for this suite. 02/06/23 17:33:35.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:33:35.452
Feb  6 17:33:35.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename disruption 02/06/23 17:33:35.453
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:33:35.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:33:35.47
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 02/06/23 17:33:35.474
STEP: Waiting for the pdb to be processed 02/06/23 17:33:35.48
STEP: First trying to evict a pod which shouldn't be evictable 02/06/23 17:33:37.492
STEP: Waiting for all pods to be running 02/06/23 17:33:37.492
Feb  6 17:33:37.497: INFO: pods: 0 < 3
Feb  6 17:33:39.501: INFO: running pods: 0 < 3
STEP: locating a running pod 02/06/23 17:33:41.501
STEP: Updating the pdb to allow a pod to be evicted 02/06/23 17:33:41.51
STEP: Waiting for the pdb to be processed 02/06/23 17:33:41.517
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/06/23 17:33:43.528
STEP: Waiting for all pods to be running 02/06/23 17:33:43.529
STEP: Waiting for the pdb to observed all healthy pods 02/06/23 17:33:43.532
STEP: Patching the pdb to disallow a pod to be evicted 02/06/23 17:33:43.556
STEP: Waiting for the pdb to be processed 02/06/23 17:33:43.58
STEP: Waiting for all pods to be running 02/06/23 17:33:45.595
Feb  6 17:33:45.598: INFO: running pods: 2 < 3
STEP: locating a running pod 02/06/23 17:33:47.603
STEP: Deleting the pdb to allow a pod to be evicted 02/06/23 17:33:47.612
STEP: Waiting for the pdb to be deleted 02/06/23 17:33:47.618
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/06/23 17:33:47.621
STEP: Waiting for all pods to be running 02/06/23 17:33:47.621
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb  6 17:33:47.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9152" for this suite. 02/06/23 17:33:47.646
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":38,"skipped":669,"failed":0}
------------------------------
• [SLOW TEST] [12.210 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:33:35.452
    Feb  6 17:33:35.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename disruption 02/06/23 17:33:35.453
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:33:35.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:33:35.47
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 02/06/23 17:33:35.474
    STEP: Waiting for the pdb to be processed 02/06/23 17:33:35.48
    STEP: First trying to evict a pod which shouldn't be evictable 02/06/23 17:33:37.492
    STEP: Waiting for all pods to be running 02/06/23 17:33:37.492
    Feb  6 17:33:37.497: INFO: pods: 0 < 3
    Feb  6 17:33:39.501: INFO: running pods: 0 < 3
    STEP: locating a running pod 02/06/23 17:33:41.501
    STEP: Updating the pdb to allow a pod to be evicted 02/06/23 17:33:41.51
    STEP: Waiting for the pdb to be processed 02/06/23 17:33:41.517
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/06/23 17:33:43.528
    STEP: Waiting for all pods to be running 02/06/23 17:33:43.529
    STEP: Waiting for the pdb to observed all healthy pods 02/06/23 17:33:43.532
    STEP: Patching the pdb to disallow a pod to be evicted 02/06/23 17:33:43.556
    STEP: Waiting for the pdb to be processed 02/06/23 17:33:43.58
    STEP: Waiting for all pods to be running 02/06/23 17:33:45.595
    Feb  6 17:33:45.598: INFO: running pods: 2 < 3
    STEP: locating a running pod 02/06/23 17:33:47.603
    STEP: Deleting the pdb to allow a pod to be evicted 02/06/23 17:33:47.612
    STEP: Waiting for the pdb to be deleted 02/06/23 17:33:47.618
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/06/23 17:33:47.621
    STEP: Waiting for all pods to be running 02/06/23 17:33:47.621
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb  6 17:33:47.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9152" for this suite. 02/06/23 17:33:47.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:33:47.67
Feb  6 17:33:47.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename custom-resource-definition 02/06/23 17:33:47.671
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:33:47.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:33:47.691
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Feb  6 17:33:47.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:33:50.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6468" for this suite. 02/06/23 17:33:50.874
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":39,"skipped":701,"failed":0}
------------------------------
• [3.212 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:33:47.67
    Feb  6 17:33:47.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename custom-resource-definition 02/06/23 17:33:47.671
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:33:47.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:33:47.691
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Feb  6 17:33:47.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:33:50.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6468" for this suite. 02/06/23 17:33:50.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:33:50.888
Feb  6 17:33:50.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename custom-resource-definition 02/06/23 17:33:50.889
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:33:50.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:33:50.907
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Feb  6 17:33:50.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:33:51.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8352" for this suite. 02/06/23 17:33:51.488
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":40,"skipped":715,"failed":0}
------------------------------
• [0.619 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:33:50.888
    Feb  6 17:33:50.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename custom-resource-definition 02/06/23 17:33:50.889
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:33:50.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:33:50.907
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Feb  6 17:33:50.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:33:51.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8352" for this suite. 02/06/23 17:33:51.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:33:51.509
Feb  6 17:33:51.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename security-context 02/06/23 17:33:51.511
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:33:51.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:33:51.542
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/06/23 17:33:51.545
Feb  6 17:33:51.555: INFO: Waiting up to 5m0s for pod "security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c" in namespace "security-context-7372" to be "Succeeded or Failed"
Feb  6 17:33:51.563: INFO: Pod "security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.454309ms
Feb  6 17:33:53.567: INFO: Pod "security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011565765s
Feb  6 17:33:55.567: INFO: Pod "security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012310988s
Feb  6 17:33:57.568: INFO: Pod "security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012563373s
STEP: Saw pod success 02/06/23 17:33:57.568
Feb  6 17:33:57.568: INFO: Pod "security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c" satisfied condition "Succeeded or Failed"
Feb  6 17:33:57.571: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c container test-container: <nil>
STEP: delete the pod 02/06/23 17:33:57.583
Feb  6 17:33:57.598: INFO: Waiting for pod security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c to disappear
Feb  6 17:33:57.600: INFO: Pod security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb  6 17:33:57.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7372" for this suite. 02/06/23 17:33:57.605
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":41,"skipped":734,"failed":0}
------------------------------
• [SLOW TEST] [6.101 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:33:51.509
    Feb  6 17:33:51.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename security-context 02/06/23 17:33:51.511
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:33:51.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:33:51.542
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/06/23 17:33:51.545
    Feb  6 17:33:51.555: INFO: Waiting up to 5m0s for pod "security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c" in namespace "security-context-7372" to be "Succeeded or Failed"
    Feb  6 17:33:51.563: INFO: Pod "security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.454309ms
    Feb  6 17:33:53.567: INFO: Pod "security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011565765s
    Feb  6 17:33:55.567: INFO: Pod "security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012310988s
    Feb  6 17:33:57.568: INFO: Pod "security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012563373s
    STEP: Saw pod success 02/06/23 17:33:57.568
    Feb  6 17:33:57.568: INFO: Pod "security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c" satisfied condition "Succeeded or Failed"
    Feb  6 17:33:57.571: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c container test-container: <nil>
    STEP: delete the pod 02/06/23 17:33:57.583
    Feb  6 17:33:57.598: INFO: Waiting for pod security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c to disappear
    Feb  6 17:33:57.600: INFO: Pod security-context-9066a1c5-fa73-4266-bfa6-e8f0ca21794c no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb  6 17:33:57.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-7372" for this suite. 02/06/23 17:33:57.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:33:57.614
Feb  6 17:33:57.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename controllerrevisions 02/06/23 17:33:57.615
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:33:57.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:33:57.633
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-gbv2s-daemon-set" 02/06/23 17:33:57.653
STEP: Check that daemon pods launch on every node of the cluster. 02/06/23 17:33:57.659
Feb  6 17:33:57.669: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:33:57.669: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:33:57.670: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:33:57.675: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 0
Feb  6 17:33:57.675: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:33:58.680: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:33:58.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:33:58.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:33:58.687: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 0
Feb  6 17:33:58.687: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:33:59.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:33:59.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:33:59.682: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:33:59.685: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 0
Feb  6 17:33:59.686: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:34:00.683: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:00.683: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:00.683: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:00.687: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 0
Feb  6 17:34:00.687: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:34:01.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:01.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:01.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:01.684: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
Feb  6 17:34:01.685: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:34:02.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:02.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:02.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:02.686: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
Feb  6 17:34:02.686: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:34:03.682: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:03.682: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:03.682: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:03.686: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
Feb  6 17:34:03.686: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:34:04.683: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:04.683: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:04.683: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:04.687: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
Feb  6 17:34:04.687: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:34:05.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:05.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:05.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:05.685: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
Feb  6 17:34:05.685: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:34:06.683: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:06.683: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:06.683: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:06.687: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
Feb  6 17:34:06.687: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:34:07.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:07.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:07.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:07.684: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
Feb  6 17:34:07.684: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:34:08.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:08.682: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:08.682: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:08.685: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
Feb  6 17:34:08.685: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:34:09.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:09.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:09.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:09.686: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
Feb  6 17:34:09.686: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:34:10.680: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:10.680: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:10.680: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:10.684: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
Feb  6 17:34:10.684: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:34:11.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:11.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:11.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:11.684: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
Feb  6 17:34:11.685: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:34:12.685: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:12.685: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:12.685: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:12.696: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
Feb  6 17:34:12.696: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:34:13.684: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:13.685: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:13.685: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:34:13.691: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 3
Feb  6 17:34:13.692: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-gbv2s-daemon-set
STEP: Confirm DaemonSet "e2e-gbv2s-daemon-set" successfully created with "daemonset-name=e2e-gbv2s-daemon-set" label 02/06/23 17:34:13.696
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-gbv2s-daemon-set" 02/06/23 17:34:13.712
Feb  6 17:34:13.716: INFO: Located ControllerRevision: "e2e-gbv2s-daemon-set-68f47684fc"
STEP: Patching ControllerRevision "e2e-gbv2s-daemon-set-68f47684fc" 02/06/23 17:34:13.718
Feb  6 17:34:13.726: INFO: e2e-gbv2s-daemon-set-68f47684fc has been patched
STEP: Create a new ControllerRevision 02/06/23 17:34:13.726
Feb  6 17:34:13.734: INFO: Created ControllerRevision: e2e-gbv2s-daemon-set-5dd89c5dc7
STEP: Confirm that there are two ControllerRevisions 02/06/23 17:34:13.734
Feb  6 17:34:13.735: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb  6 17:34:13.740: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-gbv2s-daemon-set-68f47684fc" 02/06/23 17:34:13.74
STEP: Confirm that there is only one ControllerRevision 02/06/23 17:34:13.753
Feb  6 17:34:13.753: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb  6 17:34:13.760: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-gbv2s-daemon-set-5dd89c5dc7" 02/06/23 17:34:13.767
Feb  6 17:34:13.776: INFO: e2e-gbv2s-daemon-set-5dd89c5dc7 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 02/06/23 17:34:13.776
W0206 17:34:13.788002      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 02/06/23 17:34:13.788
Feb  6 17:34:13.788: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb  6 17:34:14.793: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb  6 17:34:14.796: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-gbv2s-daemon-set-5dd89c5dc7=updated" 02/06/23 17:34:14.796
STEP: Confirm that there is only one ControllerRevision 02/06/23 17:34:14.805
Feb  6 17:34:14.806: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb  6 17:34:14.810: INFO: Found 1 ControllerRevisions
Feb  6 17:34:14.812: INFO: ControllerRevision "e2e-gbv2s-daemon-set-7644d5669b" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-gbv2s-daemon-set" 02/06/23 17:34:14.815
STEP: deleting DaemonSet.extensions e2e-gbv2s-daemon-set in namespace controllerrevisions-5060, will wait for the garbage collector to delete the pods 02/06/23 17:34:14.815
Feb  6 17:34:14.877: INFO: Deleting DaemonSet.extensions e2e-gbv2s-daemon-set took: 8.2429ms
Feb  6 17:34:14.978: INFO: Terminating DaemonSet.extensions e2e-gbv2s-daemon-set pods took: 101.239132ms
Feb  6 17:34:16.482: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 0
Feb  6 17:34:16.482: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-gbv2s-daemon-set
Feb  6 17:34:16.488: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17260"},"items":null}

Feb  6 17:34:16.491: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17260"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Feb  6 17:34:16.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-5060" for this suite. 02/06/23 17:34:16.518
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":42,"skipped":758,"failed":0}
------------------------------
• [SLOW TEST] [18.911 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:33:57.614
    Feb  6 17:33:57.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename controllerrevisions 02/06/23 17:33:57.615
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:33:57.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:33:57.633
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-gbv2s-daemon-set" 02/06/23 17:33:57.653
    STEP: Check that daemon pods launch on every node of the cluster. 02/06/23 17:33:57.659
    Feb  6 17:33:57.669: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:33:57.669: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:33:57.670: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:33:57.675: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 0
    Feb  6 17:33:57.675: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:33:58.680: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:33:58.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:33:58.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:33:58.687: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 0
    Feb  6 17:33:58.687: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:33:59.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:33:59.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:33:59.682: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:33:59.685: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 0
    Feb  6 17:33:59.686: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:34:00.683: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:00.683: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:00.683: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:00.687: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 0
    Feb  6 17:34:00.687: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:34:01.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:01.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:01.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:01.684: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
    Feb  6 17:34:01.685: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:34:02.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:02.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:02.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:02.686: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
    Feb  6 17:34:02.686: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:34:03.682: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:03.682: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:03.682: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:03.686: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
    Feb  6 17:34:03.686: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:34:04.683: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:04.683: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:04.683: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:04.687: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
    Feb  6 17:34:04.687: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:34:05.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:05.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:05.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:05.685: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
    Feb  6 17:34:05.685: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:34:06.683: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:06.683: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:06.683: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:06.687: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
    Feb  6 17:34:06.687: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:34:07.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:07.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:07.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:07.684: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
    Feb  6 17:34:07.684: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:34:08.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:08.682: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:08.682: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:08.685: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
    Feb  6 17:34:08.685: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:34:09.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:09.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:09.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:09.686: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
    Feb  6 17:34:09.686: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:34:10.680: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:10.680: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:10.680: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:10.684: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
    Feb  6 17:34:10.684: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:34:11.681: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:11.681: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:11.681: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:11.684: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
    Feb  6 17:34:11.685: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:34:12.685: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:12.685: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:12.685: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:12.696: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 1
    Feb  6 17:34:12.696: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:34:13.684: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:13.685: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:13.685: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:34:13.691: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 3
    Feb  6 17:34:13.692: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-gbv2s-daemon-set
    STEP: Confirm DaemonSet "e2e-gbv2s-daemon-set" successfully created with "daemonset-name=e2e-gbv2s-daemon-set" label 02/06/23 17:34:13.696
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-gbv2s-daemon-set" 02/06/23 17:34:13.712
    Feb  6 17:34:13.716: INFO: Located ControllerRevision: "e2e-gbv2s-daemon-set-68f47684fc"
    STEP: Patching ControllerRevision "e2e-gbv2s-daemon-set-68f47684fc" 02/06/23 17:34:13.718
    Feb  6 17:34:13.726: INFO: e2e-gbv2s-daemon-set-68f47684fc has been patched
    STEP: Create a new ControllerRevision 02/06/23 17:34:13.726
    Feb  6 17:34:13.734: INFO: Created ControllerRevision: e2e-gbv2s-daemon-set-5dd89c5dc7
    STEP: Confirm that there are two ControllerRevisions 02/06/23 17:34:13.734
    Feb  6 17:34:13.735: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb  6 17:34:13.740: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-gbv2s-daemon-set-68f47684fc" 02/06/23 17:34:13.74
    STEP: Confirm that there is only one ControllerRevision 02/06/23 17:34:13.753
    Feb  6 17:34:13.753: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb  6 17:34:13.760: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-gbv2s-daemon-set-5dd89c5dc7" 02/06/23 17:34:13.767
    Feb  6 17:34:13.776: INFO: e2e-gbv2s-daemon-set-5dd89c5dc7 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 02/06/23 17:34:13.776
    W0206 17:34:13.788002      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 02/06/23 17:34:13.788
    Feb  6 17:34:13.788: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb  6 17:34:14.793: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb  6 17:34:14.796: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-gbv2s-daemon-set-5dd89c5dc7=updated" 02/06/23 17:34:14.796
    STEP: Confirm that there is only one ControllerRevision 02/06/23 17:34:14.805
    Feb  6 17:34:14.806: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb  6 17:34:14.810: INFO: Found 1 ControllerRevisions
    Feb  6 17:34:14.812: INFO: ControllerRevision "e2e-gbv2s-daemon-set-7644d5669b" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-gbv2s-daemon-set" 02/06/23 17:34:14.815
    STEP: deleting DaemonSet.extensions e2e-gbv2s-daemon-set in namespace controllerrevisions-5060, will wait for the garbage collector to delete the pods 02/06/23 17:34:14.815
    Feb  6 17:34:14.877: INFO: Deleting DaemonSet.extensions e2e-gbv2s-daemon-set took: 8.2429ms
    Feb  6 17:34:14.978: INFO: Terminating DaemonSet.extensions e2e-gbv2s-daemon-set pods took: 101.239132ms
    Feb  6 17:34:16.482: INFO: Number of nodes with available pods controlled by daemonset e2e-gbv2s-daemon-set: 0
    Feb  6 17:34:16.482: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-gbv2s-daemon-set
    Feb  6 17:34:16.488: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17260"},"items":null}

    Feb  6 17:34:16.491: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17260"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 17:34:16.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-5060" for this suite. 02/06/23 17:34:16.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:34:16.53
Feb  6 17:34:16.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 17:34:16.531
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:16.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:16.551
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Feb  6 17:34:16.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 create -f -'
Feb  6 17:34:18.751: INFO: stderr: ""
Feb  6 17:34:18.751: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb  6 17:34:18.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 create -f -'
Feb  6 17:34:19.188: INFO: stderr: ""
Feb  6 17:34:19.188: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/06/23 17:34:19.188
Feb  6 17:34:20.193: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 17:34:20.193: INFO: Found 0 / 1
Feb  6 17:34:21.196: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 17:34:21.197: INFO: Found 0 / 1
Feb  6 17:34:22.192: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 17:34:22.192: INFO: Found 1 / 1
Feb  6 17:34:22.192: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  6 17:34:22.197: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 17:34:22.197: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  6 17:34:22.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 describe pod agnhost-primary-ldmsd'
Feb  6 17:34:22.304: INFO: stderr: ""
Feb  6 17:34:22.304: INFO: stdout: "Name:             agnhost-primary-ldmsd\nNamespace:        kubectl-3466\nPriority:         0\nService Account:  default\nNode:             tneyla25-md-0-68cbcb4798-5xxmw/195.17.4.133\nStart Time:       Mon, 06 Feb 2023 17:34:18 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.2.158\nIPs:\n  IP:           192.168.2.158\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://c8afffade7ffffacad143a4cd89cda382eccc80d00edcbec1b4ea9eb37115720\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 06 Feb 2023 17:34:21 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5xh5d (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5xh5d:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-3466/agnhost-primary-ldmsd to tneyla25-md-0-68cbcb4798-5xxmw\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Feb  6 17:34:22.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 describe rc agnhost-primary'
Feb  6 17:34:22.406: INFO: stderr: ""
Feb  6 17:34:22.406: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3466\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-ldmsd\n"
Feb  6 17:34:22.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 describe service agnhost-primary'
Feb  6 17:34:22.515: INFO: stderr: ""
Feb  6 17:34:22.515: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3466\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.106.24.108\nIPs:               10.106.24.108\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.2.158:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb  6 17:34:22.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 describe node tneyla25-bdbfp'
Feb  6 17:34:22.676: INFO: stderr: ""
Feb  6 17:34:22.676: INFO: stdout: "Name:               tneyla25-bdbfp\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-8gb.os-ubuntu\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=tneyla25-bdbfp\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-8gb.os-ubuntu\nAnnotations:        cluster.x-k8s.io/cluster-name: tneyla25\n                    cluster.x-k8s.io/cluster-namespace: eksa-system\n                    cluster.x-k8s.io/machine: tneyla25-bdbfp\n                    cluster.x-k8s.io/owner-kind: KubeadmControlPlane\n                    cluster.x-k8s.io/owner-name: tneyla25\n                    csi.volume.kubernetes.io/nodeid: {\"csi.vsphere.vmware.com\":\"tneyla25-bdbfp\"}\n                    io.cilium.network.ipv4-cilium-host: 192.168.5.246\n                    io.cilium.network.ipv4-pod-cidr: 192.168.5.0/24\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 06 Feb 2023 17:14:49 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  tneyla25-bdbfp\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 06 Feb 2023 17:34:21 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 06 Feb 2023 17:30:29 +0000   Mon, 06 Feb 2023 17:14:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 06 Feb 2023 17:30:29 +0000   Mon, 06 Feb 2023 17:14:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 06 Feb 2023 17:30:29 +0000   Mon, 06 Feb 2023 17:14:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 06 Feb 2023 17:30:29 +0000   Mon, 06 Feb 2023 17:15:30 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    tneyla25-bdbfp\n  InternalIP:  195.17.71.3\n  ExternalIP:  195.17.71.3\nCapacity:\n  cpu:                2\n  ephemeral-storage:  25625852Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8148708Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  23616785165\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8046308Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 b136faff6ce0448a9a3dc9f1c09a9662\n  System UUID:                18121a42-e57f-95b1-899f-9db29d9760dd\n  Boot ID:                    bc5b1be9-0b91-4f91-8f8a-6b155729abfd\n  Kernel Version:             5.4.0-137-generic\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.9\n  Kubelet Version:            v1.25.5-eks-c248520\n  Kube-Proxy Version:         v1.25.5-eks-c248520\nPodCIDR:                      192.168.5.0/24\nPodCIDRs:                     192.168.5.0/24\nProviderID:                   vsphere://421a1218-7fe5-b195-899f-9db29d9760dd\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-dg7th                                               100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         19m\n  kube-system                 etcd-tneyla25-bdbfp                                        100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         19m\n  kube-system                 kube-apiserver-tneyla25-bdbfp                              250m (12%)    0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                 kube-controller-manager-tneyla25-bdbfp                     200m (10%)    0 (0%)      0 (0%)           0 (0%)         18m\n  kube-system                 kube-proxy-57b4w                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                 kube-scheduler-tneyla25-bdbfp                              100m (5%)     0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                 kube-vip-tneyla25-bdbfp                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\n  kube-system                 vsphere-cloud-controller-manager-c4d6p                     200m (10%)    0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                 vsphere-csi-node-sxc8k                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-7rl9f    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                950m (47%)  0 (0%)\n  memory             200Mi (2%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason          Age   From                   Message\n  ----    ------          ----  ----                   -------\n  Normal  Starting        19m   kube-proxy             \n  Normal  RegisteredNode  19m   node-controller        Node tneyla25-bdbfp event: Registered Node tneyla25-bdbfp in Controller\n  Normal  Synced          19m   cloud-node-controller  Node synced successfully\n"
Feb  6 17:34:22.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 describe namespace kubectl-3466'
Feb  6 17:34:22.787: INFO: stderr: ""
Feb  6 17:34:22.787: INFO: stdout: "Name:         kubectl-3466\nLabels:       e2e-framework=kubectl\n              e2e-run=22cc322c-ea4b-415b-8190-e6eb23dafba3\n              kubernetes.io/metadata.name=kubectl-3466\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 17:34:22.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3466" for this suite. 02/06/23 17:34:22.798
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":43,"skipped":794,"failed":0}
------------------------------
• [SLOW TEST] [6.276 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:34:16.53
    Feb  6 17:34:16.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 17:34:16.531
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:16.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:16.551
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Feb  6 17:34:16.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 create -f -'
    Feb  6 17:34:18.751: INFO: stderr: ""
    Feb  6 17:34:18.751: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Feb  6 17:34:18.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 create -f -'
    Feb  6 17:34:19.188: INFO: stderr: ""
    Feb  6 17:34:19.188: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/06/23 17:34:19.188
    Feb  6 17:34:20.193: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 17:34:20.193: INFO: Found 0 / 1
    Feb  6 17:34:21.196: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 17:34:21.197: INFO: Found 0 / 1
    Feb  6 17:34:22.192: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 17:34:22.192: INFO: Found 1 / 1
    Feb  6 17:34:22.192: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb  6 17:34:22.197: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 17:34:22.197: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb  6 17:34:22.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 describe pod agnhost-primary-ldmsd'
    Feb  6 17:34:22.304: INFO: stderr: ""
    Feb  6 17:34:22.304: INFO: stdout: "Name:             agnhost-primary-ldmsd\nNamespace:        kubectl-3466\nPriority:         0\nService Account:  default\nNode:             tneyla25-md-0-68cbcb4798-5xxmw/195.17.4.133\nStart Time:       Mon, 06 Feb 2023 17:34:18 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.2.158\nIPs:\n  IP:           192.168.2.158\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://c8afffade7ffffacad143a4cd89cda382eccc80d00edcbec1b4ea9eb37115720\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 06 Feb 2023 17:34:21 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5xh5d (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5xh5d:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-3466/agnhost-primary-ldmsd to tneyla25-md-0-68cbcb4798-5xxmw\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Feb  6 17:34:22.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 describe rc agnhost-primary'
    Feb  6 17:34:22.406: INFO: stderr: ""
    Feb  6 17:34:22.406: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3466\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-ldmsd\n"
    Feb  6 17:34:22.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 describe service agnhost-primary'
    Feb  6 17:34:22.515: INFO: stderr: ""
    Feb  6 17:34:22.515: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3466\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.106.24.108\nIPs:               10.106.24.108\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.2.158:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Feb  6 17:34:22.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 describe node tneyla25-bdbfp'
    Feb  6 17:34:22.676: INFO: stderr: ""
    Feb  6 17:34:22.676: INFO: stdout: "Name:               tneyla25-bdbfp\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-8gb.os-ubuntu\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=tneyla25-bdbfp\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-8gb.os-ubuntu\nAnnotations:        cluster.x-k8s.io/cluster-name: tneyla25\n                    cluster.x-k8s.io/cluster-namespace: eksa-system\n                    cluster.x-k8s.io/machine: tneyla25-bdbfp\n                    cluster.x-k8s.io/owner-kind: KubeadmControlPlane\n                    cluster.x-k8s.io/owner-name: tneyla25\n                    csi.volume.kubernetes.io/nodeid: {\"csi.vsphere.vmware.com\":\"tneyla25-bdbfp\"}\n                    io.cilium.network.ipv4-cilium-host: 192.168.5.246\n                    io.cilium.network.ipv4-pod-cidr: 192.168.5.0/24\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 06 Feb 2023 17:14:49 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  tneyla25-bdbfp\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 06 Feb 2023 17:34:21 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 06 Feb 2023 17:30:29 +0000   Mon, 06 Feb 2023 17:14:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 06 Feb 2023 17:30:29 +0000   Mon, 06 Feb 2023 17:14:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 06 Feb 2023 17:30:29 +0000   Mon, 06 Feb 2023 17:14:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 06 Feb 2023 17:30:29 +0000   Mon, 06 Feb 2023 17:15:30 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    tneyla25-bdbfp\n  InternalIP:  195.17.71.3\n  ExternalIP:  195.17.71.3\nCapacity:\n  cpu:                2\n  ephemeral-storage:  25625852Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8148708Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  23616785165\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8046308Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 b136faff6ce0448a9a3dc9f1c09a9662\n  System UUID:                18121a42-e57f-95b1-899f-9db29d9760dd\n  Boot ID:                    bc5b1be9-0b91-4f91-8f8a-6b155729abfd\n  Kernel Version:             5.4.0-137-generic\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.9\n  Kubelet Version:            v1.25.5-eks-c248520\n  Kube-Proxy Version:         v1.25.5-eks-c248520\nPodCIDR:                      192.168.5.0/24\nPodCIDRs:                     192.168.5.0/24\nProviderID:                   vsphere://421a1218-7fe5-b195-899f-9db29d9760dd\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-dg7th                                               100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         19m\n  kube-system                 etcd-tneyla25-bdbfp                                        100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         19m\n  kube-system                 kube-apiserver-tneyla25-bdbfp                              250m (12%)    0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                 kube-controller-manager-tneyla25-bdbfp                     200m (10%)    0 (0%)      0 (0%)           0 (0%)         18m\n  kube-system                 kube-proxy-57b4w                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                 kube-scheduler-tneyla25-bdbfp                              100m (5%)     0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                 kube-vip-tneyla25-bdbfp                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\n  kube-system                 vsphere-cloud-controller-manager-c4d6p                     200m (10%)    0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                 vsphere-csi-node-sxc8k                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-7rl9f    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                950m (47%)  0 (0%)\n  memory             200Mi (2%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason          Age   From                   Message\n  ----    ------          ----  ----                   -------\n  Normal  Starting        19m   kube-proxy             \n  Normal  RegisteredNode  19m   node-controller        Node tneyla25-bdbfp event: Registered Node tneyla25-bdbfp in Controller\n  Normal  Synced          19m   cloud-node-controller  Node synced successfully\n"
    Feb  6 17:34:22.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3466 describe namespace kubectl-3466'
    Feb  6 17:34:22.787: INFO: stderr: ""
    Feb  6 17:34:22.787: INFO: stdout: "Name:         kubectl-3466\nLabels:       e2e-framework=kubectl\n              e2e-run=22cc322c-ea4b-415b-8190-e6eb23dafba3\n              kubernetes.io/metadata.name=kubectl-3466\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 17:34:22.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3466" for this suite. 02/06/23 17:34:22.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:34:22.808
Feb  6 17:34:22.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pods 02/06/23 17:34:22.81
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:22.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:22.834
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 02/06/23 17:34:22.838
STEP: submitting the pod to kubernetes 02/06/23 17:34:22.838
STEP: verifying QOS class is set on the pod 02/06/23 17:34:22.856
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Feb  6 17:34:22.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3940" for this suite. 02/06/23 17:34:22.872
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":44,"skipped":849,"failed":0}
------------------------------
• [0.078 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:34:22.808
    Feb  6 17:34:22.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pods 02/06/23 17:34:22.81
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:22.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:22.834
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 02/06/23 17:34:22.838
    STEP: submitting the pod to kubernetes 02/06/23 17:34:22.838
    STEP: verifying QOS class is set on the pod 02/06/23 17:34:22.856
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Feb  6 17:34:22.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3940" for this suite. 02/06/23 17:34:22.872
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:34:22.887
Feb  6 17:34:22.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 17:34:22.888
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:22.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:22.914
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-9fd32537-c9c0-4e1f-9c9f-b5adcdfe8800 02/06/23 17:34:22.916
STEP: Creating a pod to test consume secrets 02/06/23 17:34:22.922
Feb  6 17:34:22.932: INFO: Waiting up to 5m0s for pod "pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b" in namespace "secrets-2202" to be "Succeeded or Failed"
Feb  6 17:34:22.935: INFO: Pod "pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.749649ms
Feb  6 17:34:24.941: INFO: Pod "pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009163399s
Feb  6 17:34:26.940: INFO: Pod "pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008314675s
Feb  6 17:34:28.941: INFO: Pod "pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008952042s
STEP: Saw pod success 02/06/23 17:34:28.941
Feb  6 17:34:28.941: INFO: Pod "pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b" satisfied condition "Succeeded or Failed"
Feb  6 17:34:28.944: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b container secret-volume-test: <nil>
STEP: delete the pod 02/06/23 17:34:28.949
Feb  6 17:34:28.962: INFO: Waiting for pod pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b to disappear
Feb  6 17:34:28.964: INFO: Pod pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb  6 17:34:28.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2202" for this suite. 02/06/23 17:34:28.969
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":45,"skipped":853,"failed":0}
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:34:22.887
    Feb  6 17:34:22.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 17:34:22.888
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:22.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:22.914
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-9fd32537-c9c0-4e1f-9c9f-b5adcdfe8800 02/06/23 17:34:22.916
    STEP: Creating a pod to test consume secrets 02/06/23 17:34:22.922
    Feb  6 17:34:22.932: INFO: Waiting up to 5m0s for pod "pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b" in namespace "secrets-2202" to be "Succeeded or Failed"
    Feb  6 17:34:22.935: INFO: Pod "pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.749649ms
    Feb  6 17:34:24.941: INFO: Pod "pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009163399s
    Feb  6 17:34:26.940: INFO: Pod "pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008314675s
    Feb  6 17:34:28.941: INFO: Pod "pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008952042s
    STEP: Saw pod success 02/06/23 17:34:28.941
    Feb  6 17:34:28.941: INFO: Pod "pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b" satisfied condition "Succeeded or Failed"
    Feb  6 17:34:28.944: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b container secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 17:34:28.949
    Feb  6 17:34:28.962: INFO: Waiting for pod pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b to disappear
    Feb  6 17:34:28.964: INFO: Pod pod-secrets-bde68fbf-a327-4b01-960a-1f6b02fa796b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 17:34:28.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2202" for this suite. 02/06/23 17:34:28.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:34:28.977
Feb  6 17:34:28.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 17:34:28.978
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:28.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:28.996
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-245/configmap-test-7fb1ab04-00ca-4dff-9e59-d51ff3e4d931 02/06/23 17:34:28.999
STEP: Creating a pod to test consume configMaps 02/06/23 17:34:29.004
Feb  6 17:34:29.015: INFO: Waiting up to 5m0s for pod "pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b" in namespace "configmap-245" to be "Succeeded or Failed"
Feb  6 17:34:29.018: INFO: Pod "pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.622829ms
Feb  6 17:34:31.023: INFO: Pod "pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008809327s
Feb  6 17:34:33.025: INFO: Pod "pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010204846s
Feb  6 17:34:35.023: INFO: Pod "pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007928314s
STEP: Saw pod success 02/06/23 17:34:35.023
Feb  6 17:34:35.023: INFO: Pod "pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b" satisfied condition "Succeeded or Failed"
Feb  6 17:34:35.027: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b container env-test: <nil>
STEP: delete the pod 02/06/23 17:34:35.033
Feb  6 17:34:35.045: INFO: Waiting for pod pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b to disappear
Feb  6 17:34:35.048: INFO: Pod pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 17:34:35.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-245" for this suite. 02/06/23 17:34:35.053
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":46,"skipped":858,"failed":0}
------------------------------
• [SLOW TEST] [6.083 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:34:28.977
    Feb  6 17:34:28.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 17:34:28.978
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:28.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:28.996
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-245/configmap-test-7fb1ab04-00ca-4dff-9e59-d51ff3e4d931 02/06/23 17:34:28.999
    STEP: Creating a pod to test consume configMaps 02/06/23 17:34:29.004
    Feb  6 17:34:29.015: INFO: Waiting up to 5m0s for pod "pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b" in namespace "configmap-245" to be "Succeeded or Failed"
    Feb  6 17:34:29.018: INFO: Pod "pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.622829ms
    Feb  6 17:34:31.023: INFO: Pod "pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008809327s
    Feb  6 17:34:33.025: INFO: Pod "pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010204846s
    Feb  6 17:34:35.023: INFO: Pod "pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007928314s
    STEP: Saw pod success 02/06/23 17:34:35.023
    Feb  6 17:34:35.023: INFO: Pod "pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b" satisfied condition "Succeeded or Failed"
    Feb  6 17:34:35.027: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b container env-test: <nil>
    STEP: delete the pod 02/06/23 17:34:35.033
    Feb  6 17:34:35.045: INFO: Waiting for pod pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b to disappear
    Feb  6 17:34:35.048: INFO: Pod pod-configmaps-41fb689b-fe15-4665-8d8f-ce2606416b5b no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 17:34:35.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-245" for this suite. 02/06/23 17:34:35.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:34:35.071
Feb  6 17:34:35.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename replication-controller 02/06/23 17:34:35.072
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:35.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:35.089
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 02/06/23 17:34:35.097
STEP: waiting for RC to be added 02/06/23 17:34:35.103
STEP: waiting for available Replicas 02/06/23 17:34:35.103
STEP: patching ReplicationController 02/06/23 17:34:41.119
STEP: waiting for RC to be modified 02/06/23 17:34:41.127
STEP: patching ReplicationController status 02/06/23 17:34:41.127
STEP: waiting for RC to be modified 02/06/23 17:34:41.136
STEP: waiting for available Replicas 02/06/23 17:34:41.137
STEP: fetching ReplicationController status 02/06/23 17:34:41.143
STEP: patching ReplicationController scale 02/06/23 17:34:41.147
STEP: waiting for RC to be modified 02/06/23 17:34:41.157
STEP: waiting for ReplicationController's scale to be the max amount 02/06/23 17:34:41.157
STEP: fetching ReplicationController; ensuring that it's patched 02/06/23 17:34:47.471
STEP: updating ReplicationController status 02/06/23 17:34:47.474
STEP: waiting for RC to be modified 02/06/23 17:34:47.48
STEP: listing all ReplicationControllers 02/06/23 17:34:47.48
STEP: checking that ReplicationController has expected values 02/06/23 17:34:47.484
STEP: deleting ReplicationControllers by collection 02/06/23 17:34:47.484
STEP: waiting for ReplicationController to have a DELETED watchEvent 02/06/23 17:34:47.497
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb  6 17:34:47.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3171" for this suite. 02/06/23 17:34:47.569
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":47,"skipped":946,"failed":0}
------------------------------
• [SLOW TEST] [12.507 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:34:35.071
    Feb  6 17:34:35.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename replication-controller 02/06/23 17:34:35.072
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:35.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:35.089
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 02/06/23 17:34:35.097
    STEP: waiting for RC to be added 02/06/23 17:34:35.103
    STEP: waiting for available Replicas 02/06/23 17:34:35.103
    STEP: patching ReplicationController 02/06/23 17:34:41.119
    STEP: waiting for RC to be modified 02/06/23 17:34:41.127
    STEP: patching ReplicationController status 02/06/23 17:34:41.127
    STEP: waiting for RC to be modified 02/06/23 17:34:41.136
    STEP: waiting for available Replicas 02/06/23 17:34:41.137
    STEP: fetching ReplicationController status 02/06/23 17:34:41.143
    STEP: patching ReplicationController scale 02/06/23 17:34:41.147
    STEP: waiting for RC to be modified 02/06/23 17:34:41.157
    STEP: waiting for ReplicationController's scale to be the max amount 02/06/23 17:34:41.157
    STEP: fetching ReplicationController; ensuring that it's patched 02/06/23 17:34:47.471
    STEP: updating ReplicationController status 02/06/23 17:34:47.474
    STEP: waiting for RC to be modified 02/06/23 17:34:47.48
    STEP: listing all ReplicationControllers 02/06/23 17:34:47.48
    STEP: checking that ReplicationController has expected values 02/06/23 17:34:47.484
    STEP: deleting ReplicationControllers by collection 02/06/23 17:34:47.484
    STEP: waiting for ReplicationController to have a DELETED watchEvent 02/06/23 17:34:47.497
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb  6 17:34:47.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3171" for this suite. 02/06/23 17:34:47.569
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:34:47.579
Feb  6 17:34:47.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pods 02/06/23 17:34:47.58
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:47.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:47.6
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 02/06/23 17:34:47.602
STEP: setting up watch 02/06/23 17:34:47.602
STEP: submitting the pod to kubernetes 02/06/23 17:34:47.706
STEP: verifying the pod is in kubernetes 02/06/23 17:34:47.72
STEP: verifying pod creation was observed 02/06/23 17:34:47.724
Feb  6 17:34:47.724: INFO: Waiting up to 5m0s for pod "pod-submit-remove-614268bb-71ba-4822-9c10-2033d9a2971e" in namespace "pods-2956" to be "running"
Feb  6 17:34:47.730: INFO: Pod "pod-submit-remove-614268bb-71ba-4822-9c10-2033d9a2971e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.500519ms
Feb  6 17:34:49.734: INFO: Pod "pod-submit-remove-614268bb-71ba-4822-9c10-2033d9a2971e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010002118s
Feb  6 17:34:51.735: INFO: Pod "pod-submit-remove-614268bb-71ba-4822-9c10-2033d9a2971e": Phase="Running", Reason="", readiness=true. Elapsed: 4.010039358s
Feb  6 17:34:51.735: INFO: Pod "pod-submit-remove-614268bb-71ba-4822-9c10-2033d9a2971e" satisfied condition "running"
STEP: deleting the pod gracefully 02/06/23 17:34:51.738
STEP: verifying pod deletion was observed 02/06/23 17:34:51.746
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb  6 17:34:54.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2956" for this suite. 02/06/23 17:34:54.176
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":48,"skipped":949,"failed":0}
------------------------------
• [SLOW TEST] [6.602 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:34:47.579
    Feb  6 17:34:47.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pods 02/06/23 17:34:47.58
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:47.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:47.6
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 02/06/23 17:34:47.602
    STEP: setting up watch 02/06/23 17:34:47.602
    STEP: submitting the pod to kubernetes 02/06/23 17:34:47.706
    STEP: verifying the pod is in kubernetes 02/06/23 17:34:47.72
    STEP: verifying pod creation was observed 02/06/23 17:34:47.724
    Feb  6 17:34:47.724: INFO: Waiting up to 5m0s for pod "pod-submit-remove-614268bb-71ba-4822-9c10-2033d9a2971e" in namespace "pods-2956" to be "running"
    Feb  6 17:34:47.730: INFO: Pod "pod-submit-remove-614268bb-71ba-4822-9c10-2033d9a2971e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.500519ms
    Feb  6 17:34:49.734: INFO: Pod "pod-submit-remove-614268bb-71ba-4822-9c10-2033d9a2971e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010002118s
    Feb  6 17:34:51.735: INFO: Pod "pod-submit-remove-614268bb-71ba-4822-9c10-2033d9a2971e": Phase="Running", Reason="", readiness=true. Elapsed: 4.010039358s
    Feb  6 17:34:51.735: INFO: Pod "pod-submit-remove-614268bb-71ba-4822-9c10-2033d9a2971e" satisfied condition "running"
    STEP: deleting the pod gracefully 02/06/23 17:34:51.738
    STEP: verifying pod deletion was observed 02/06/23 17:34:51.746
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb  6 17:34:54.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2956" for this suite. 02/06/23 17:34:54.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:34:54.187
Feb  6 17:34:54.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 17:34:54.188
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:54.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:54.208
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-632cb390-4d66-4a48-a274-481e3a2d103a 02/06/23 17:34:54.211
STEP: Creating a pod to test consume secrets 02/06/23 17:34:54.215
Feb  6 17:34:54.223: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4" in namespace "projected-1144" to be "Succeeded or Failed"
Feb  6 17:34:54.228: INFO: Pod "pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.166622ms
Feb  6 17:34:56.234: INFO: Pod "pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010868106s
Feb  6 17:34:58.233: INFO: Pod "pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009340177s
Feb  6 17:35:00.232: INFO: Pod "pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008677543s
STEP: Saw pod success 02/06/23 17:35:00.232
Feb  6 17:35:00.232: INFO: Pod "pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4" satisfied condition "Succeeded or Failed"
Feb  6 17:35:00.235: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/06/23 17:35:00.242
Feb  6 17:35:00.254: INFO: Waiting for pod pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4 to disappear
Feb  6 17:35:00.257: INFO: Pod pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb  6 17:35:00.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1144" for this suite. 02/06/23 17:35:00.264
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":49,"skipped":972,"failed":0}
------------------------------
• [SLOW TEST] [6.083 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:34:54.187
    Feb  6 17:34:54.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 17:34:54.188
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:34:54.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:34:54.208
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-632cb390-4d66-4a48-a274-481e3a2d103a 02/06/23 17:34:54.211
    STEP: Creating a pod to test consume secrets 02/06/23 17:34:54.215
    Feb  6 17:34:54.223: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4" in namespace "projected-1144" to be "Succeeded or Failed"
    Feb  6 17:34:54.228: INFO: Pod "pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.166622ms
    Feb  6 17:34:56.234: INFO: Pod "pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010868106s
    Feb  6 17:34:58.233: INFO: Pod "pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009340177s
    Feb  6 17:35:00.232: INFO: Pod "pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008677543s
    STEP: Saw pod success 02/06/23 17:35:00.232
    Feb  6 17:35:00.232: INFO: Pod "pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4" satisfied condition "Succeeded or Failed"
    Feb  6 17:35:00.235: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 17:35:00.242
    Feb  6 17:35:00.254: INFO: Waiting for pod pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4 to disappear
    Feb  6 17:35:00.257: INFO: Pod pod-projected-secrets-1f28e536-d76e-405b-ae58-b745bb319aa4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb  6 17:35:00.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1144" for this suite. 02/06/23 17:35:00.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:35:00.274
Feb  6 17:35:00.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 17:35:00.275
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:00.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:00.293
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6663 02/06/23 17:35:00.296
STEP: changing the ExternalName service to type=ClusterIP 02/06/23 17:35:00.302
STEP: creating replication controller externalname-service in namespace services-6663 02/06/23 17:35:00.333
I0206 17:35:00.346036      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6663, replica count: 2
I0206 17:35:03.398316      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 17:35:06.399740      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  6 17:35:06.399: INFO: Creating new exec pod
Feb  6 17:35:06.409: INFO: Waiting up to 5m0s for pod "execpodskltb" in namespace "services-6663" to be "running"
Feb  6 17:35:06.412: INFO: Pod "execpodskltb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520127ms
Feb  6 17:35:08.417: INFO: Pod "execpodskltb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007859439s
Feb  6 17:35:10.416: INFO: Pod "execpodskltb": Phase="Running", Reason="", readiness=true. Elapsed: 4.007187453s
Feb  6 17:35:10.416: INFO: Pod "execpodskltb" satisfied condition "running"
Feb  6 17:35:11.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-6663 exec execpodskltb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb  6 17:35:11.581: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb  6 17:35:11.581: INFO: stdout: ""
Feb  6 17:35:12.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-6663 exec execpodskltb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb  6 17:35:12.768: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb  6 17:35:12.768: INFO: stdout: "externalname-service-ftxwc"
Feb  6 17:35:12.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-6663 exec execpodskltb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.160.194 80'
Feb  6 17:35:12.965: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.160.194 80\nConnection to 10.98.160.194 80 port [tcp/http] succeeded!\n"
Feb  6 17:35:12.965: INFO: stdout: "externalname-service-ftxwc"
Feb  6 17:35:12.965: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 17:35:13.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6663" for this suite. 02/06/23 17:35:13.022
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":50,"skipped":1015,"failed":0}
------------------------------
• [SLOW TEST] [12.756 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:35:00.274
    Feb  6 17:35:00.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 17:35:00.275
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:00.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:00.293
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6663 02/06/23 17:35:00.296
    STEP: changing the ExternalName service to type=ClusterIP 02/06/23 17:35:00.302
    STEP: creating replication controller externalname-service in namespace services-6663 02/06/23 17:35:00.333
    I0206 17:35:00.346036      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6663, replica count: 2
    I0206 17:35:03.398316      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 17:35:06.399740      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb  6 17:35:06.399: INFO: Creating new exec pod
    Feb  6 17:35:06.409: INFO: Waiting up to 5m0s for pod "execpodskltb" in namespace "services-6663" to be "running"
    Feb  6 17:35:06.412: INFO: Pod "execpodskltb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520127ms
    Feb  6 17:35:08.417: INFO: Pod "execpodskltb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007859439s
    Feb  6 17:35:10.416: INFO: Pod "execpodskltb": Phase="Running", Reason="", readiness=true. Elapsed: 4.007187453s
    Feb  6 17:35:10.416: INFO: Pod "execpodskltb" satisfied condition "running"
    Feb  6 17:35:11.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-6663 exec execpodskltb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb  6 17:35:11.581: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb  6 17:35:11.581: INFO: stdout: ""
    Feb  6 17:35:12.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-6663 exec execpodskltb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb  6 17:35:12.768: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb  6 17:35:12.768: INFO: stdout: "externalname-service-ftxwc"
    Feb  6 17:35:12.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-6663 exec execpodskltb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.160.194 80'
    Feb  6 17:35:12.965: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.160.194 80\nConnection to 10.98.160.194 80 port [tcp/http] succeeded!\n"
    Feb  6 17:35:12.965: INFO: stdout: "externalname-service-ftxwc"
    Feb  6 17:35:12.965: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 17:35:13.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6663" for this suite. 02/06/23 17:35:13.022
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:35:13.031
Feb  6 17:35:13.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename podtemplate 02/06/23 17:35:13.032
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:13.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:13.052
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb  6 17:35:13.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3141" for this suite. 02/06/23 17:35:13.101
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":51,"skipped":1032,"failed":0}
------------------------------
• [0.079 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:35:13.031
    Feb  6 17:35:13.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename podtemplate 02/06/23 17:35:13.032
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:13.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:13.052
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb  6 17:35:13.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3141" for this suite. 02/06/23 17:35:13.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:35:13.112
Feb  6 17:35:13.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 17:35:13.113
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:13.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:13.139
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 17:35:13.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2360" for this suite. 02/06/23 17:35:13.156
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":52,"skipped":1083,"failed":0}
------------------------------
• [0.055 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:35:13.112
    Feb  6 17:35:13.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 17:35:13.113
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:13.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:13.139
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 17:35:13.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2360" for this suite. 02/06/23 17:35:13.156
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:35:13.176
Feb  6 17:35:13.176: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename init-container 02/06/23 17:35:13.177
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:13.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:13.197
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 02/06/23 17:35:13.203
Feb  6 17:35:13.203: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb  6 17:35:20.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2837" for this suite. 02/06/23 17:35:20.232
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":53,"skipped":1088,"failed":0}
------------------------------
• [SLOW TEST] [7.066 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:35:13.176
    Feb  6 17:35:13.176: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename init-container 02/06/23 17:35:13.177
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:13.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:13.197
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 02/06/23 17:35:13.203
    Feb  6 17:35:13.203: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb  6 17:35:20.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-2837" for this suite. 02/06/23 17:35:20.232
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:35:20.242
Feb  6 17:35:20.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 17:35:20.244
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:20.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:20.269
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 02/06/23 17:35:20.272
Feb  6 17:35:20.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 create -f -'
Feb  6 17:35:22.597: INFO: stderr: ""
Feb  6 17:35:22.597: INFO: stdout: "pod/pause created\n"
Feb  6 17:35:22.597: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb  6 17:35:22.597: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8022" to be "running and ready"
Feb  6 17:35:22.610: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.668576ms
Feb  6 17:35:22.610: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'tneyla25-md-0-68cbcb4798-5xxmw' to be 'Running' but was 'Pending'
Feb  6 17:35:24.615: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017158775s
Feb  6 17:35:24.615: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'tneyla25-md-0-68cbcb4798-5xxmw' to be 'Running' but was 'Pending'
Feb  6 17:35:26.616: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.018271124s
Feb  6 17:35:26.616: INFO: Pod "pause" satisfied condition "running and ready"
Feb  6 17:35:26.616: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 02/06/23 17:35:26.616
Feb  6 17:35:26.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 label pods pause testing-label=testing-label-value'
Feb  6 17:35:26.714: INFO: stderr: ""
Feb  6 17:35:26.714: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 02/06/23 17:35:26.714
Feb  6 17:35:26.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 get pod pause -L testing-label'
Feb  6 17:35:26.802: INFO: stderr: ""
Feb  6 17:35:26.802: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod 02/06/23 17:35:26.802
Feb  6 17:35:26.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 label pods pause testing-label-'
Feb  6 17:35:26.902: INFO: stderr: ""
Feb  6 17:35:26.902: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 02/06/23 17:35:26.902
Feb  6 17:35:26.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 get pod pause -L testing-label'
Feb  6 17:35:26.988: INFO: stderr: ""
Feb  6 17:35:26.988: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 02/06/23 17:35:26.989
Feb  6 17:35:26.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 delete --grace-period=0 --force -f -'
Feb  6 17:35:27.090: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  6 17:35:27.090: INFO: stdout: "pod \"pause\" force deleted\n"
Feb  6 17:35:27.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 get rc,svc -l name=pause --no-headers'
Feb  6 17:35:27.196: INFO: stderr: "No resources found in kubectl-8022 namespace.\n"
Feb  6 17:35:27.196: INFO: stdout: ""
Feb  6 17:35:27.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  6 17:35:27.280: INFO: stderr: ""
Feb  6 17:35:27.280: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 17:35:27.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8022" for this suite. 02/06/23 17:35:27.287
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":54,"skipped":1088,"failed":0}
------------------------------
• [SLOW TEST] [7.055 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:35:20.242
    Feb  6 17:35:20.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 17:35:20.244
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:20.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:20.269
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 02/06/23 17:35:20.272
    Feb  6 17:35:20.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 create -f -'
    Feb  6 17:35:22.597: INFO: stderr: ""
    Feb  6 17:35:22.597: INFO: stdout: "pod/pause created\n"
    Feb  6 17:35:22.597: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Feb  6 17:35:22.597: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8022" to be "running and ready"
    Feb  6 17:35:22.610: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.668576ms
    Feb  6 17:35:22.610: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'tneyla25-md-0-68cbcb4798-5xxmw' to be 'Running' but was 'Pending'
    Feb  6 17:35:24.615: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017158775s
    Feb  6 17:35:24.615: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'tneyla25-md-0-68cbcb4798-5xxmw' to be 'Running' but was 'Pending'
    Feb  6 17:35:26.616: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.018271124s
    Feb  6 17:35:26.616: INFO: Pod "pause" satisfied condition "running and ready"
    Feb  6 17:35:26.616: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 02/06/23 17:35:26.616
    Feb  6 17:35:26.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 label pods pause testing-label=testing-label-value'
    Feb  6 17:35:26.714: INFO: stderr: ""
    Feb  6 17:35:26.714: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 02/06/23 17:35:26.714
    Feb  6 17:35:26.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 get pod pause -L testing-label'
    Feb  6 17:35:26.802: INFO: stderr: ""
    Feb  6 17:35:26.802: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 02/06/23 17:35:26.802
    Feb  6 17:35:26.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 label pods pause testing-label-'
    Feb  6 17:35:26.902: INFO: stderr: ""
    Feb  6 17:35:26.902: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 02/06/23 17:35:26.902
    Feb  6 17:35:26.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 get pod pause -L testing-label'
    Feb  6 17:35:26.988: INFO: stderr: ""
    Feb  6 17:35:26.988: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 02/06/23 17:35:26.989
    Feb  6 17:35:26.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 delete --grace-period=0 --force -f -'
    Feb  6 17:35:27.090: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb  6 17:35:27.090: INFO: stdout: "pod \"pause\" force deleted\n"
    Feb  6 17:35:27.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 get rc,svc -l name=pause --no-headers'
    Feb  6 17:35:27.196: INFO: stderr: "No resources found in kubectl-8022 namespace.\n"
    Feb  6 17:35:27.196: INFO: stdout: ""
    Feb  6 17:35:27.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8022 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb  6 17:35:27.280: INFO: stderr: ""
    Feb  6 17:35:27.280: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 17:35:27.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8022" for this suite. 02/06/23 17:35:27.287
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:35:27.298
Feb  6 17:35:27.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename replicaset 02/06/23 17:35:27.299
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:27.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:27.317
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Feb  6 17:35:27.331: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb  6 17:35:32.335: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/06/23 17:35:32.335
STEP: Scaling up "test-rs" replicaset  02/06/23 17:35:32.335
Feb  6 17:35:32.345: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 02/06/23 17:35:32.345
W0206 17:35:32.357862      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb  6 17:35:32.360: INFO: observed ReplicaSet test-rs in namespace replicaset-2161 with ReadyReplicas 1, AvailableReplicas 1
Feb  6 17:35:32.376: INFO: observed ReplicaSet test-rs in namespace replicaset-2161 with ReadyReplicas 1, AvailableReplicas 1
Feb  6 17:35:32.397: INFO: observed ReplicaSet test-rs in namespace replicaset-2161 with ReadyReplicas 1, AvailableReplicas 1
Feb  6 17:35:32.420: INFO: observed ReplicaSet test-rs in namespace replicaset-2161 with ReadyReplicas 1, AvailableReplicas 1
Feb  6 17:35:35.617: INFO: observed ReplicaSet test-rs in namespace replicaset-2161 with ReadyReplicas 2, AvailableReplicas 2
Feb  6 17:35:36.293: INFO: observed Replicaset test-rs in namespace replicaset-2161 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb  6 17:35:36.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2161" for this suite. 02/06/23 17:35:36.299
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":55,"skipped":1089,"failed":0}
------------------------------
• [SLOW TEST] [9.008 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:35:27.298
    Feb  6 17:35:27.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename replicaset 02/06/23 17:35:27.299
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:27.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:27.317
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Feb  6 17:35:27.331: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb  6 17:35:32.335: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/06/23 17:35:32.335
    STEP: Scaling up "test-rs" replicaset  02/06/23 17:35:32.335
    Feb  6 17:35:32.345: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 02/06/23 17:35:32.345
    W0206 17:35:32.357862      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb  6 17:35:32.360: INFO: observed ReplicaSet test-rs in namespace replicaset-2161 with ReadyReplicas 1, AvailableReplicas 1
    Feb  6 17:35:32.376: INFO: observed ReplicaSet test-rs in namespace replicaset-2161 with ReadyReplicas 1, AvailableReplicas 1
    Feb  6 17:35:32.397: INFO: observed ReplicaSet test-rs in namespace replicaset-2161 with ReadyReplicas 1, AvailableReplicas 1
    Feb  6 17:35:32.420: INFO: observed ReplicaSet test-rs in namespace replicaset-2161 with ReadyReplicas 1, AvailableReplicas 1
    Feb  6 17:35:35.617: INFO: observed ReplicaSet test-rs in namespace replicaset-2161 with ReadyReplicas 2, AvailableReplicas 2
    Feb  6 17:35:36.293: INFO: observed Replicaset test-rs in namespace replicaset-2161 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb  6 17:35:36.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2161" for this suite. 02/06/23 17:35:36.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:35:36.314
Feb  6 17:35:36.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 17:35:36.315
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:36.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:36.333
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 02/06/23 17:35:36.336
Feb  6 17:35:36.345: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25" in namespace "downward-api-8851" to be "Succeeded or Failed"
Feb  6 17:35:36.351: INFO: Pod "downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25": Phase="Pending", Reason="", readiness=false. Elapsed: 5.080552ms
Feb  6 17:35:38.356: INFO: Pod "downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01057984s
Feb  6 17:35:40.354: INFO: Pod "downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008961046s
Feb  6 17:35:42.356: INFO: Pod "downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010063606s
STEP: Saw pod success 02/06/23 17:35:42.356
Feb  6 17:35:42.356: INFO: Pod "downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25" satisfied condition "Succeeded or Failed"
Feb  6 17:35:42.359: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25 container client-container: <nil>
STEP: delete the pod 02/06/23 17:35:42.365
Feb  6 17:35:42.379: INFO: Waiting for pod downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25 to disappear
Feb  6 17:35:42.382: INFO: Pod downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb  6 17:35:42.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8851" for this suite. 02/06/23 17:35:42.387
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":56,"skipped":1137,"failed":0}
------------------------------
• [SLOW TEST] [6.079 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:35:36.314
    Feb  6 17:35:36.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 17:35:36.315
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:36.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:36.333
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 02/06/23 17:35:36.336
    Feb  6 17:35:36.345: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25" in namespace "downward-api-8851" to be "Succeeded or Failed"
    Feb  6 17:35:36.351: INFO: Pod "downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25": Phase="Pending", Reason="", readiness=false. Elapsed: 5.080552ms
    Feb  6 17:35:38.356: INFO: Pod "downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01057984s
    Feb  6 17:35:40.354: INFO: Pod "downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008961046s
    Feb  6 17:35:42.356: INFO: Pod "downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010063606s
    STEP: Saw pod success 02/06/23 17:35:42.356
    Feb  6 17:35:42.356: INFO: Pod "downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25" satisfied condition "Succeeded or Failed"
    Feb  6 17:35:42.359: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25 container client-container: <nil>
    STEP: delete the pod 02/06/23 17:35:42.365
    Feb  6 17:35:42.379: INFO: Waiting for pod downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25 to disappear
    Feb  6 17:35:42.382: INFO: Pod downwardapi-volume-24d26d97-51f2-45e1-be73-ee6d885b9c25 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb  6 17:35:42.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8851" for this suite. 02/06/23 17:35:42.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:35:42.398
Feb  6 17:35:42.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pod-network-test 02/06/23 17:35:42.399
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:42.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:42.423
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-895 02/06/23 17:35:42.426
STEP: creating a selector 02/06/23 17:35:42.426
STEP: Creating the service pods in kubernetes 02/06/23 17:35:42.426
Feb  6 17:35:42.426: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb  6 17:35:42.468: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-895" to be "running and ready"
Feb  6 17:35:42.482: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.048184ms
Feb  6 17:35:42.482: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:35:44.487: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018290985s
Feb  6 17:35:44.487: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:35:46.490: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.022029414s
Feb  6 17:35:46.490: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 17:35:48.488: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019403394s
Feb  6 17:35:48.488: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 17:35:50.491: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.02255412s
Feb  6 17:35:50.491: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 17:35:52.487: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018410709s
Feb  6 17:35:52.487: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 17:35:54.488: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019377362s
Feb  6 17:35:54.488: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 17:35:56.487: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.018379762s
Feb  6 17:35:56.487: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 17:35:58.490: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021330345s
Feb  6 17:35:58.490: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 17:36:00.487: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.01892591s
Feb  6 17:36:00.487: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 17:36:02.488: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019755612s
Feb  6 17:36:02.488: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 17:36:04.487: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018264626s
Feb  6 17:36:04.487: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb  6 17:36:04.487: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb  6 17:36:04.490: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-895" to be "running and ready"
Feb  6 17:36:04.493: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.895272ms
Feb  6 17:36:04.493: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb  6 17:36:04.493: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb  6 17:36:04.495: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-895" to be "running and ready"
Feb  6 17:36:04.499: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.354509ms
Feb  6 17:36:04.499: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb  6 17:36:04.499: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/06/23 17:36:04.501
Feb  6 17:36:04.507: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-895" to be "running"
Feb  6 17:36:04.515: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.35489ms
Feb  6 17:36:06.520: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012977261s
Feb  6 17:36:08.519: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012143788s
Feb  6 17:36:08.519: INFO: Pod "test-container-pod" satisfied condition "running"
Feb  6 17:36:08.522: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb  6 17:36:08.522: INFO: Breadth first check of 192.168.2.241 on host 195.17.4.133...
Feb  6 17:36:08.525: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.9:9080/dial?request=hostname&protocol=http&host=192.168.2.241&port=8083&tries=1'] Namespace:pod-network-test-895 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 17:36:08.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 17:36:08.525: INFO: ExecWithOptions: Clientset creation
Feb  6 17:36:08.525: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-895/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.2.241%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb  6 17:36:08.601: INFO: Waiting for responses: map[]
Feb  6 17:36:08.601: INFO: reached 192.168.2.241 after 0/1 tries
Feb  6 17:36:08.601: INFO: Breadth first check of 192.168.3.211 on host 195.17.148.89...
Feb  6 17:36:08.604: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.9:9080/dial?request=hostname&protocol=http&host=192.168.3.211&port=8083&tries=1'] Namespace:pod-network-test-895 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 17:36:08.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 17:36:08.605: INFO: ExecWithOptions: Clientset creation
Feb  6 17:36:08.605: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-895/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.3.211%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb  6 17:36:08.680: INFO: Waiting for responses: map[]
Feb  6 17:36:08.681: INFO: reached 192.168.3.211 after 0/1 tries
Feb  6 17:36:08.681: INFO: Breadth first check of 192.168.1.156 on host 195.17.148.90...
Feb  6 17:36:08.685: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.9:9080/dial?request=hostname&protocol=http&host=192.168.1.156&port=8083&tries=1'] Namespace:pod-network-test-895 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 17:36:08.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 17:36:08.686: INFO: ExecWithOptions: Clientset creation
Feb  6 17:36:08.686: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-895/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.1.156%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb  6 17:36:08.770: INFO: Waiting for responses: map[]
Feb  6 17:36:08.770: INFO: reached 192.168.1.156 after 0/1 tries
Feb  6 17:36:08.770: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb  6 17:36:08.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-895" for this suite. 02/06/23 17:36:08.776
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":57,"skipped":1145,"failed":0}
------------------------------
• [SLOW TEST] [26.383 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:35:42.398
    Feb  6 17:35:42.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pod-network-test 02/06/23 17:35:42.399
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:35:42.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:35:42.423
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-895 02/06/23 17:35:42.426
    STEP: creating a selector 02/06/23 17:35:42.426
    STEP: Creating the service pods in kubernetes 02/06/23 17:35:42.426
    Feb  6 17:35:42.426: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb  6 17:35:42.468: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-895" to be "running and ready"
    Feb  6 17:35:42.482: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.048184ms
    Feb  6 17:35:42.482: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:35:44.487: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018290985s
    Feb  6 17:35:44.487: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:35:46.490: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.022029414s
    Feb  6 17:35:46.490: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 17:35:48.488: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019403394s
    Feb  6 17:35:48.488: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 17:35:50.491: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.02255412s
    Feb  6 17:35:50.491: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 17:35:52.487: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018410709s
    Feb  6 17:35:52.487: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 17:35:54.488: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019377362s
    Feb  6 17:35:54.488: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 17:35:56.487: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.018379762s
    Feb  6 17:35:56.487: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 17:35:58.490: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021330345s
    Feb  6 17:35:58.490: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 17:36:00.487: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.01892591s
    Feb  6 17:36:00.487: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 17:36:02.488: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019755612s
    Feb  6 17:36:02.488: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 17:36:04.487: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018264626s
    Feb  6 17:36:04.487: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb  6 17:36:04.487: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb  6 17:36:04.490: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-895" to be "running and ready"
    Feb  6 17:36:04.493: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.895272ms
    Feb  6 17:36:04.493: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb  6 17:36:04.493: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb  6 17:36:04.495: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-895" to be "running and ready"
    Feb  6 17:36:04.499: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.354509ms
    Feb  6 17:36:04.499: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb  6 17:36:04.499: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/06/23 17:36:04.501
    Feb  6 17:36:04.507: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-895" to be "running"
    Feb  6 17:36:04.515: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.35489ms
    Feb  6 17:36:06.520: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012977261s
    Feb  6 17:36:08.519: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012143788s
    Feb  6 17:36:08.519: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb  6 17:36:08.522: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb  6 17:36:08.522: INFO: Breadth first check of 192.168.2.241 on host 195.17.4.133...
    Feb  6 17:36:08.525: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.9:9080/dial?request=hostname&protocol=http&host=192.168.2.241&port=8083&tries=1'] Namespace:pod-network-test-895 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 17:36:08.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 17:36:08.525: INFO: ExecWithOptions: Clientset creation
    Feb  6 17:36:08.525: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-895/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.2.241%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb  6 17:36:08.601: INFO: Waiting for responses: map[]
    Feb  6 17:36:08.601: INFO: reached 192.168.2.241 after 0/1 tries
    Feb  6 17:36:08.601: INFO: Breadth first check of 192.168.3.211 on host 195.17.148.89...
    Feb  6 17:36:08.604: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.9:9080/dial?request=hostname&protocol=http&host=192.168.3.211&port=8083&tries=1'] Namespace:pod-network-test-895 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 17:36:08.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 17:36:08.605: INFO: ExecWithOptions: Clientset creation
    Feb  6 17:36:08.605: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-895/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.3.211%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb  6 17:36:08.680: INFO: Waiting for responses: map[]
    Feb  6 17:36:08.681: INFO: reached 192.168.3.211 after 0/1 tries
    Feb  6 17:36:08.681: INFO: Breadth first check of 192.168.1.156 on host 195.17.148.90...
    Feb  6 17:36:08.685: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.9:9080/dial?request=hostname&protocol=http&host=192.168.1.156&port=8083&tries=1'] Namespace:pod-network-test-895 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 17:36:08.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 17:36:08.686: INFO: ExecWithOptions: Clientset creation
    Feb  6 17:36:08.686: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-895/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.9%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.1.156%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb  6 17:36:08.770: INFO: Waiting for responses: map[]
    Feb  6 17:36:08.770: INFO: reached 192.168.1.156 after 0/1 tries
    Feb  6 17:36:08.770: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb  6 17:36:08.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-895" for this suite. 02/06/23 17:36:08.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:36:08.785
Feb  6 17:36:08.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename daemonsets 02/06/23 17:36:08.786
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:36:08.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:36:08.805
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 02/06/23 17:36:08.838
STEP: Check that daemon pods launch on every node of the cluster. 02/06/23 17:36:08.844
Feb  6 17:36:08.853: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:08.854: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:08.854: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:08.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:36:08.862: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:36:09.869: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:09.869: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:09.869: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:09.873: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:36:09.873: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:36:10.880: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:10.887: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:10.891: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:10.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:36:10.906: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:36:11.868: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:11.869: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:11.869: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:11.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:36:11.874: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:36:12.871: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:12.871: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:12.871: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:12.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb  6 17:36:12.877: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 17:36:13.868: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:13.868: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:13.868: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:13.872: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb  6 17:36:13.872: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/06/23 17:36:13.877
Feb  6 17:36:13.903: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:13.903: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:13.903: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:36:13.912: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb  6 17:36:13.912: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 02/06/23 17:36:13.912
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/06/23 17:36:13.943
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-580, will wait for the garbage collector to delete the pods 02/06/23 17:36:13.943
Feb  6 17:36:14.005: INFO: Deleting DaemonSet.extensions daemon-set took: 7.179261ms
Feb  6 17:36:14.106: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.977107ms
Feb  6 17:36:18.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:36:18.509: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb  6 17:36:18.513: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19203"},"items":null}

Feb  6 17:36:18.517: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19203"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb  6 17:36:18.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-580" for this suite. 02/06/23 17:36:18.537
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":58,"skipped":1152,"failed":0}
------------------------------
• [SLOW TEST] [9.758 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:36:08.785
    Feb  6 17:36:08.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename daemonsets 02/06/23 17:36:08.786
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:36:08.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:36:08.805
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 02/06/23 17:36:08.838
    STEP: Check that daemon pods launch on every node of the cluster. 02/06/23 17:36:08.844
    Feb  6 17:36:08.853: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:08.854: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:08.854: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:08.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:36:08.862: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:36:09.869: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:09.869: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:09.869: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:09.873: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:36:09.873: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:36:10.880: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:10.887: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:10.891: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:10.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:36:10.906: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:36:11.868: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:11.869: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:11.869: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:11.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:36:11.874: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:36:12.871: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:12.871: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:12.871: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:12.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb  6 17:36:12.877: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 17:36:13.868: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:13.868: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:13.868: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:13.872: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb  6 17:36:13.872: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/06/23 17:36:13.877
    Feb  6 17:36:13.903: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:13.903: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:13.903: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:36:13.912: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb  6 17:36:13.912: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 02/06/23 17:36:13.912
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/06/23 17:36:13.943
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-580, will wait for the garbage collector to delete the pods 02/06/23 17:36:13.943
    Feb  6 17:36:14.005: INFO: Deleting DaemonSet.extensions daemon-set took: 7.179261ms
    Feb  6 17:36:14.106: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.977107ms
    Feb  6 17:36:18.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:36:18.509: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb  6 17:36:18.513: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19203"},"items":null}

    Feb  6 17:36:18.517: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19203"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 17:36:18.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-580" for this suite. 02/06/23 17:36:18.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:36:18.546
Feb  6 17:36:18.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-probe 02/06/23 17:36:18.548
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:36:18.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:36:18.567
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 in namespace container-probe-8462 02/06/23 17:36:18.57
Feb  6 17:36:18.579: INFO: Waiting up to 5m0s for pod "liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469" in namespace "container-probe-8462" to be "not pending"
Feb  6 17:36:18.586: INFO: Pod "liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469": Phase="Pending", Reason="", readiness=false. Elapsed: 6.88976ms
Feb  6 17:36:20.591: INFO: Pod "liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011703498s
Feb  6 17:36:22.590: INFO: Pod "liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469": Phase="Running", Reason="", readiness=true. Elapsed: 4.010987534s
Feb  6 17:36:22.590: INFO: Pod "liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469" satisfied condition "not pending"
Feb  6 17:36:22.590: INFO: Started pod liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 in namespace container-probe-8462
STEP: checking the pod's current state and verifying that restartCount is present 02/06/23 17:36:22.59
Feb  6 17:36:22.593: INFO: Initial restart count of pod liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 is 0
Feb  6 17:36:40.642: INFO: Restart count of pod container-probe-8462/liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 is now 1 (18.048619704s elapsed)
Feb  6 17:37:00.698: INFO: Restart count of pod container-probe-8462/liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 is now 2 (38.104931537s elapsed)
Feb  6 17:37:20.758: INFO: Restart count of pod container-probe-8462/liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 is now 3 (58.164830404s elapsed)
Feb  6 17:37:40.811: INFO: Restart count of pod container-probe-8462/liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 is now 4 (1m18.217358741s elapsed)
Feb  6 17:38:50.983: INFO: Restart count of pod container-probe-8462/liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 is now 5 (2m28.389588202s elapsed)
STEP: deleting the pod 02/06/23 17:38:50.983
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb  6 17:38:50.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8462" for this suite. 02/06/23 17:38:51.004
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":59,"skipped":1179,"failed":0}
------------------------------
• [SLOW TEST] [152.484 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:36:18.546
    Feb  6 17:36:18.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-probe 02/06/23 17:36:18.548
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:36:18.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:36:18.567
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 in namespace container-probe-8462 02/06/23 17:36:18.57
    Feb  6 17:36:18.579: INFO: Waiting up to 5m0s for pod "liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469" in namespace "container-probe-8462" to be "not pending"
    Feb  6 17:36:18.586: INFO: Pod "liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469": Phase="Pending", Reason="", readiness=false. Elapsed: 6.88976ms
    Feb  6 17:36:20.591: INFO: Pod "liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011703498s
    Feb  6 17:36:22.590: INFO: Pod "liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469": Phase="Running", Reason="", readiness=true. Elapsed: 4.010987534s
    Feb  6 17:36:22.590: INFO: Pod "liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469" satisfied condition "not pending"
    Feb  6 17:36:22.590: INFO: Started pod liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 in namespace container-probe-8462
    STEP: checking the pod's current state and verifying that restartCount is present 02/06/23 17:36:22.59
    Feb  6 17:36:22.593: INFO: Initial restart count of pod liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 is 0
    Feb  6 17:36:40.642: INFO: Restart count of pod container-probe-8462/liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 is now 1 (18.048619704s elapsed)
    Feb  6 17:37:00.698: INFO: Restart count of pod container-probe-8462/liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 is now 2 (38.104931537s elapsed)
    Feb  6 17:37:20.758: INFO: Restart count of pod container-probe-8462/liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 is now 3 (58.164830404s elapsed)
    Feb  6 17:37:40.811: INFO: Restart count of pod container-probe-8462/liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 is now 4 (1m18.217358741s elapsed)
    Feb  6 17:38:50.983: INFO: Restart count of pod container-probe-8462/liveness-8ba5e8d3-0bbe-4e80-ab05-eaa21e6e2469 is now 5 (2m28.389588202s elapsed)
    STEP: deleting the pod 02/06/23 17:38:50.983
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb  6 17:38:50.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8462" for this suite. 02/06/23 17:38:51.004
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:38:51.034
Feb  6 17:38:51.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename subpath 02/06/23 17:38:51.035
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:38:51.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:38:51.057
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/06/23 17:38:51.06
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-p9mv 02/06/23 17:38:51.068
STEP: Creating a pod to test atomic-volume-subpath 02/06/23 17:38:51.068
Feb  6 17:38:51.081: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-p9mv" in namespace "subpath-196" to be "Succeeded or Failed"
Feb  6 17:38:51.089: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.365752ms
Feb  6 17:38:53.095: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013545707s
Feb  6 17:38:55.096: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 4.014574681s
Feb  6 17:38:57.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 6.012650161s
Feb  6 17:38:59.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 8.013316319s
Feb  6 17:39:01.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 10.013010828s
Feb  6 17:39:03.095: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 12.013983886s
Feb  6 17:39:05.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 14.012624756s
Feb  6 17:39:07.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 16.012852121s
Feb  6 17:39:09.095: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 18.014342524s
Feb  6 17:39:11.096: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 20.014757112s
Feb  6 17:39:13.095: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 22.014189344s
Feb  6 17:39:15.097: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=false. Elapsed: 24.015865582s
Feb  6 17:39:17.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.013007383s
STEP: Saw pod success 02/06/23 17:39:17.094
Feb  6 17:39:17.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv" satisfied condition "Succeeded or Failed"
Feb  6 17:39:17.098: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-subpath-test-downwardapi-p9mv container test-container-subpath-downwardapi-p9mv: <nil>
STEP: delete the pod 02/06/23 17:39:17.115
Feb  6 17:39:17.128: INFO: Waiting for pod pod-subpath-test-downwardapi-p9mv to disappear
Feb  6 17:39:17.131: INFO: Pod pod-subpath-test-downwardapi-p9mv no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-p9mv 02/06/23 17:39:17.131
Feb  6 17:39:17.131: INFO: Deleting pod "pod-subpath-test-downwardapi-p9mv" in namespace "subpath-196"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb  6 17:39:17.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-196" for this suite. 02/06/23 17:39:17.139
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":60,"skipped":1183,"failed":0}
------------------------------
• [SLOW TEST] [26.112 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:38:51.034
    Feb  6 17:38:51.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename subpath 02/06/23 17:38:51.035
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:38:51.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:38:51.057
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/06/23 17:38:51.06
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-p9mv 02/06/23 17:38:51.068
    STEP: Creating a pod to test atomic-volume-subpath 02/06/23 17:38:51.068
    Feb  6 17:38:51.081: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-p9mv" in namespace "subpath-196" to be "Succeeded or Failed"
    Feb  6 17:38:51.089: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.365752ms
    Feb  6 17:38:53.095: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013545707s
    Feb  6 17:38:55.096: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 4.014574681s
    Feb  6 17:38:57.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 6.012650161s
    Feb  6 17:38:59.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 8.013316319s
    Feb  6 17:39:01.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 10.013010828s
    Feb  6 17:39:03.095: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 12.013983886s
    Feb  6 17:39:05.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 14.012624756s
    Feb  6 17:39:07.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 16.012852121s
    Feb  6 17:39:09.095: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 18.014342524s
    Feb  6 17:39:11.096: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 20.014757112s
    Feb  6 17:39:13.095: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=true. Elapsed: 22.014189344s
    Feb  6 17:39:15.097: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Running", Reason="", readiness=false. Elapsed: 24.015865582s
    Feb  6 17:39:17.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.013007383s
    STEP: Saw pod success 02/06/23 17:39:17.094
    Feb  6 17:39:17.094: INFO: Pod "pod-subpath-test-downwardapi-p9mv" satisfied condition "Succeeded or Failed"
    Feb  6 17:39:17.098: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-subpath-test-downwardapi-p9mv container test-container-subpath-downwardapi-p9mv: <nil>
    STEP: delete the pod 02/06/23 17:39:17.115
    Feb  6 17:39:17.128: INFO: Waiting for pod pod-subpath-test-downwardapi-p9mv to disappear
    Feb  6 17:39:17.131: INFO: Pod pod-subpath-test-downwardapi-p9mv no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-p9mv 02/06/23 17:39:17.131
    Feb  6 17:39:17.131: INFO: Deleting pod "pod-subpath-test-downwardapi-p9mv" in namespace "subpath-196"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb  6 17:39:17.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-196" for this suite. 02/06/23 17:39:17.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:39:17.153
Feb  6 17:39:17.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 17:39:17.154
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:39:17.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:39:17.174
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 17:39:17.192
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:39:18.034
STEP: Deploying the webhook pod 02/06/23 17:39:18.049
STEP: Wait for the deployment to be ready 02/06/23 17:39:18.061
Feb  6 17:39:18.077: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 17:39:20.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 39, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 39, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 39, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 39, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 17:39:22.094
STEP: Verifying the service has paired with the endpoint 02/06/23 17:39:22.112
Feb  6 17:39:23.112: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 02/06/23 17:39:23.198
STEP: Creating a configMap that does not comply to the validation webhook rules 02/06/23 17:39:23.242
STEP: Deleting the collection of validation webhooks 02/06/23 17:39:23.277
STEP: Creating a configMap that does not comply to the validation webhook rules 02/06/23 17:39:23.331
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:39:23.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6792" for this suite. 02/06/23 17:39:23.349
STEP: Destroying namespace "webhook-6792-markers" for this suite. 02/06/23 17:39:23.355
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":61,"skipped":1204,"failed":0}
------------------------------
• [SLOW TEST] [6.285 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:39:17.153
    Feb  6 17:39:17.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 17:39:17.154
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:39:17.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:39:17.174
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 17:39:17.192
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:39:18.034
    STEP: Deploying the webhook pod 02/06/23 17:39:18.049
    STEP: Wait for the deployment to be ready 02/06/23 17:39:18.061
    Feb  6 17:39:18.077: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 17:39:20.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 39, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 39, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 39, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 39, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 17:39:22.094
    STEP: Verifying the service has paired with the endpoint 02/06/23 17:39:22.112
    Feb  6 17:39:23.112: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 02/06/23 17:39:23.198
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/06/23 17:39:23.242
    STEP: Deleting the collection of validation webhooks 02/06/23 17:39:23.277
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/06/23 17:39:23.331
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:39:23.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6792" for this suite. 02/06/23 17:39:23.349
    STEP: Destroying namespace "webhook-6792-markers" for this suite. 02/06/23 17:39:23.355
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:39:23.449
Feb  6 17:39:23.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename replicaset 02/06/23 17:39:23.45
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:39:23.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:39:23.475
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Feb  6 17:39:23.478: INFO: Creating ReplicaSet my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64
Feb  6 17:39:23.498: INFO: Pod name my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64: Found 0 pods out of 1
Feb  6 17:39:28.504: INFO: Pod name my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64: Found 1 pods out of 1
Feb  6 17:39:28.504: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64" is running
Feb  6 17:39:28.504: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64-jk5rp" in namespace "replicaset-3237" to be "running"
Feb  6 17:39:28.509: INFO: Pod "my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64-jk5rp": Phase="Running", Reason="", readiness=true. Elapsed: 5.536276ms
Feb  6 17:39:28.509: INFO: Pod "my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64-jk5rp" satisfied condition "running"
Feb  6 17:39:28.509: INFO: Pod "my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64-jk5rp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:39:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:39:26 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:39:26 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:39:23 +0000 UTC Reason: Message:}])
Feb  6 17:39:28.509: INFO: Trying to dial the pod
Feb  6 17:39:33.522: INFO: Controller my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64: Got expected result from replica 1 [my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64-jk5rp]: "my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64-jk5rp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb  6 17:39:33.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3237" for this suite. 02/06/23 17:39:33.527
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":62,"skipped":1220,"failed":0}
------------------------------
• [SLOW TEST] [10.084 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:39:23.449
    Feb  6 17:39:23.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename replicaset 02/06/23 17:39:23.45
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:39:23.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:39:23.475
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Feb  6 17:39:23.478: INFO: Creating ReplicaSet my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64
    Feb  6 17:39:23.498: INFO: Pod name my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64: Found 0 pods out of 1
    Feb  6 17:39:28.504: INFO: Pod name my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64: Found 1 pods out of 1
    Feb  6 17:39:28.504: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64" is running
    Feb  6 17:39:28.504: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64-jk5rp" in namespace "replicaset-3237" to be "running"
    Feb  6 17:39:28.509: INFO: Pod "my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64-jk5rp": Phase="Running", Reason="", readiness=true. Elapsed: 5.536276ms
    Feb  6 17:39:28.509: INFO: Pod "my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64-jk5rp" satisfied condition "running"
    Feb  6 17:39:28.509: INFO: Pod "my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64-jk5rp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:39:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:39:26 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:39:26 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:39:23 +0000 UTC Reason: Message:}])
    Feb  6 17:39:28.509: INFO: Trying to dial the pod
    Feb  6 17:39:33.522: INFO: Controller my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64: Got expected result from replica 1 [my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64-jk5rp]: "my-hostname-basic-f4c21403-26c9-4580-a7fc-0cf64450ee64-jk5rp", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb  6 17:39:33.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3237" for this suite. 02/06/23 17:39:33.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:39:33.536
Feb  6 17:39:33.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename statefulset 02/06/23 17:39:33.537
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:39:33.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:39:33.561
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9159 02/06/23 17:39:33.564
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 02/06/23 17:39:33.571
Feb  6 17:39:33.588: INFO: Found 0 stateful pods, waiting for 3
Feb  6 17:39:43.595: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 17:39:43.595: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 17:39:43.595: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb  6 17:39:53.594: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 17:39:53.594: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 17:39:53.594: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 17:39:53.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9159 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  6 17:39:53.802: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  6 17:39:53.802: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  6 17:39:53.802: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/06/23 17:40:03.819
Feb  6 17:40:03.839: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/06/23 17:40:03.839
STEP: Updating Pods in reverse ordinal order 02/06/23 17:40:13.86
Feb  6 17:40:13.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9159 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  6 17:40:14.042: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  6 17:40:14.042: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  6 17:40:14.043: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  6 17:40:24.065: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
Feb  6 17:40:24.065: INFO: Waiting for Pod statefulset-9159/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Feb  6 17:40:24.065: INFO: Waiting for Pod statefulset-9159/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Feb  6 17:40:34.075: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
Feb  6 17:40:34.075: INFO: Waiting for Pod statefulset-9159/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Feb  6 17:40:44.074: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
Feb  6 17:40:44.074: INFO: Waiting for Pod statefulset-9159/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Feb  6 17:40:54.073: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
Feb  6 17:41:04.074: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
STEP: Rolling back to a previous revision 02/06/23 17:41:14.075
Feb  6 17:41:14.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9159 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  6 17:41:14.257: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  6 17:41:14.257: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  6 17:41:14.257: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  6 17:41:24.291: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 02/06/23 17:41:34.314
Feb  6 17:41:34.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9159 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  6 17:41:34.506: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  6 17:41:34.506: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  6 17:41:34.506: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  6 17:41:44.527: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb  6 17:41:54.537: INFO: Deleting all statefulset in ns statefulset-9159
Feb  6 17:41:54.540: INFO: Scaling statefulset ss2 to 0
Feb  6 17:42:04.561: INFO: Waiting for statefulset status.replicas updated to 0
Feb  6 17:42:04.565: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb  6 17:42:04.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9159" for this suite. 02/06/23 17:42:04.606
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":63,"skipped":1246,"failed":0}
------------------------------
• [SLOW TEST] [151.080 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:39:33.536
    Feb  6 17:39:33.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename statefulset 02/06/23 17:39:33.537
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:39:33.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:39:33.561
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9159 02/06/23 17:39:33.564
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 02/06/23 17:39:33.571
    Feb  6 17:39:33.588: INFO: Found 0 stateful pods, waiting for 3
    Feb  6 17:39:43.595: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 17:39:43.595: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 17:39:43.595: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
    Feb  6 17:39:53.594: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 17:39:53.594: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 17:39:53.594: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 17:39:53.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9159 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb  6 17:39:53.802: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb  6 17:39:53.802: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb  6 17:39:53.802: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/06/23 17:40:03.819
    Feb  6 17:40:03.839: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/06/23 17:40:03.839
    STEP: Updating Pods in reverse ordinal order 02/06/23 17:40:13.86
    Feb  6 17:40:13.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9159 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb  6 17:40:14.042: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb  6 17:40:14.042: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb  6 17:40:14.043: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb  6 17:40:24.065: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
    Feb  6 17:40:24.065: INFO: Waiting for Pod statefulset-9159/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Feb  6 17:40:24.065: INFO: Waiting for Pod statefulset-9159/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Feb  6 17:40:34.075: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
    Feb  6 17:40:34.075: INFO: Waiting for Pod statefulset-9159/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Feb  6 17:40:44.074: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
    Feb  6 17:40:44.074: INFO: Waiting for Pod statefulset-9159/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Feb  6 17:40:54.073: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
    Feb  6 17:41:04.074: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
    STEP: Rolling back to a previous revision 02/06/23 17:41:14.075
    Feb  6 17:41:14.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9159 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb  6 17:41:14.257: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb  6 17:41:14.257: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb  6 17:41:14.257: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb  6 17:41:24.291: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 02/06/23 17:41:34.314
    Feb  6 17:41:34.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9159 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb  6 17:41:34.506: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb  6 17:41:34.506: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb  6 17:41:34.506: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb  6 17:41:44.527: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb  6 17:41:54.537: INFO: Deleting all statefulset in ns statefulset-9159
    Feb  6 17:41:54.540: INFO: Scaling statefulset ss2 to 0
    Feb  6 17:42:04.561: INFO: Waiting for statefulset status.replicas updated to 0
    Feb  6 17:42:04.565: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb  6 17:42:04.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9159" for this suite. 02/06/23 17:42:04.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:42:04.618
Feb  6 17:42:04.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename runtimeclass 02/06/23 17:42:04.619
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:04.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:04.64
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Feb  6 17:42:04.657: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8208 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb  6 17:42:04.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8208" for this suite. 02/06/23 17:42:04.688
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":64,"skipped":1263,"failed":0}
------------------------------
• [0.083 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:42:04.618
    Feb  6 17:42:04.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename runtimeclass 02/06/23 17:42:04.619
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:04.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:04.64
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Feb  6 17:42:04.657: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8208 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb  6 17:42:04.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8208" for this suite. 02/06/23 17:42:04.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:42:04.702
Feb  6 17:42:04.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename deployment 02/06/23 17:42:04.704
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:04.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:04.734
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Feb  6 17:42:04.737: INFO: Creating deployment "webserver-deployment"
Feb  6 17:42:04.743: INFO: Waiting for observed generation 1
Feb  6 17:42:06.767: INFO: Waiting for all required pods to come up
Feb  6 17:42:06.824: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 02/06/23 17:42:06.936
Feb  6 17:42:06.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-zhbd6" in namespace "deployment-8120" to be "running"
Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4q2jc" in namespace "deployment-8120" to be "running"
Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-ct5md" in namespace "deployment-8120" to be "running"
Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-dh7dm" in namespace "deployment-8120" to be "running"
Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hx2qr" in namespace "deployment-8120" to be "running"
Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jpw4l" in namespace "deployment-8120" to be "running"
Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mddnt" in namespace "deployment-8120" to be "running"
Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-n7bk6" in namespace "deployment-8120" to be "running"
Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rb8mm" in namespace "deployment-8120" to be "running"
Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-v94qd" in namespace "deployment-8120" to be "running"
Feb  6 17:42:07.071: INFO: Pod "webserver-deployment-845c8977d9-zhbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 68.23234ms
Feb  6 17:42:07.147: INFO: Pod "webserver-deployment-845c8977d9-4q2jc": Phase="Pending", Reason="", readiness=false. Elapsed: 48.185837ms
Feb  6 17:42:07.155: INFO: Pod "webserver-deployment-845c8977d9-ct5md": Phase="Pending", Reason="", readiness=false. Elapsed: 5.097068ms
Feb  6 17:42:07.155: INFO: Pod "webserver-deployment-845c8977d9-hx2qr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394908ms
Feb  6 17:42:07.156: INFO: Pod "webserver-deployment-845c8977d9-dh7dm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.382908ms
Feb  6 17:42:07.156: INFO: Pod "webserver-deployment-845c8977d9-jpw4l": Phase="Pending", Reason="", readiness=false. Elapsed: 5.098962ms
Feb  6 17:42:07.159: INFO: Pod "webserver-deployment-845c8977d9-v94qd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025551ms
Feb  6 17:42:07.159: INFO: Pod "webserver-deployment-845c8977d9-rb8mm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.798599ms
Feb  6 17:42:07.159: INFO: Pod "webserver-deployment-845c8977d9-n7bk6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.271772ms
Feb  6 17:42:07.159: INFO: Pod "webserver-deployment-845c8977d9-mddnt": Phase="Pending", Reason="", readiness=false. Elapsed: 7.716668ms
Feb  6 17:42:09.160: INFO: Pod "webserver-deployment-845c8977d9-zhbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.157143706s
Feb  6 17:42:09.162: INFO: Pod "webserver-deployment-845c8977d9-ct5md": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011818482s
Feb  6 17:42:09.162: INFO: Pod "webserver-deployment-845c8977d9-4q2jc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063096008s
Feb  6 17:42:09.162: INFO: Pod "webserver-deployment-845c8977d9-dh7dm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011659095s
Feb  6 17:42:09.164: INFO: Pod "webserver-deployment-845c8977d9-hx2qr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013276256s
Feb  6 17:42:09.165: INFO: Pod "webserver-deployment-845c8977d9-jpw4l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013428867s
Feb  6 17:42:09.166: INFO: Pod "webserver-deployment-845c8977d9-v94qd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012990444s
Feb  6 17:42:09.168: INFO: Pod "webserver-deployment-845c8977d9-n7bk6": Phase="Running", Reason="", readiness=true. Elapsed: 2.015600555s
Feb  6 17:42:09.168: INFO: Pod "webserver-deployment-845c8977d9-n7bk6" satisfied condition "running"
Feb  6 17:42:09.168: INFO: Pod "webserver-deployment-845c8977d9-rb8mm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015676077s
Feb  6 17:42:09.168: INFO: Pod "webserver-deployment-845c8977d9-mddnt": Phase="Running", Reason="", readiness=true. Elapsed: 2.016766629s
Feb  6 17:42:09.169: INFO: Pod "webserver-deployment-845c8977d9-mddnt" satisfied condition "running"
Feb  6 17:42:11.167: INFO: Pod "webserver-deployment-845c8977d9-4q2jc": Phase="Running", Reason="", readiness=true. Elapsed: 4.067646922s
Feb  6 17:42:11.167: INFO: Pod "webserver-deployment-845c8977d9-4q2jc" satisfied condition "running"
Feb  6 17:42:11.167: INFO: Pod "webserver-deployment-845c8977d9-zhbd6": Phase="Running", Reason="", readiness=true. Elapsed: 4.164479875s
Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-zhbd6" satisfied condition "running"
Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-dh7dm": Phase="Running", Reason="", readiness=true. Elapsed: 4.017451857s
Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-dh7dm" satisfied condition "running"
Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-hx2qr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017067895s
Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-ct5md": Phase="Running", Reason="", readiness=true. Elapsed: 4.01814279s
Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-ct5md" satisfied condition "running"
Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-jpw4l": Phase="Running", Reason="", readiness=true. Elapsed: 4.016451421s
Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-jpw4l" satisfied condition "running"
Feb  6 17:42:11.170: INFO: Pod "webserver-deployment-845c8977d9-rb8mm": Phase="Running", Reason="", readiness=true. Elapsed: 4.017541508s
Feb  6 17:42:11.170: INFO: Pod "webserver-deployment-845c8977d9-rb8mm" satisfied condition "running"
Feb  6 17:42:11.170: INFO: Pod "webserver-deployment-845c8977d9-v94qd": Phase="Running", Reason="", readiness=true. Elapsed: 4.017634882s
Feb  6 17:42:11.170: INFO: Pod "webserver-deployment-845c8977d9-v94qd" satisfied condition "running"
Feb  6 17:42:13.160: INFO: Pod "webserver-deployment-845c8977d9-hx2qr": Phase="Running", Reason="", readiness=true. Elapsed: 6.009457863s
Feb  6 17:42:13.161: INFO: Pod "webserver-deployment-845c8977d9-hx2qr" satisfied condition "running"
Feb  6 17:42:13.161: INFO: Waiting for deployment "webserver-deployment" to complete
Feb  6 17:42:13.169: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb  6 17:42:13.178: INFO: Updating deployment webserver-deployment
Feb  6 17:42:13.178: INFO: Waiting for observed generation 2
Feb  6 17:42:15.227: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb  6 17:42:15.291: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb  6 17:42:15.395: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb  6 17:42:15.587: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb  6 17:42:15.667: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb  6 17:42:15.709: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb  6 17:42:15.718: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb  6 17:42:15.719: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb  6 17:42:15.737: INFO: Updating deployment webserver-deployment
Feb  6 17:42:15.737: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb  6 17:42:15.758: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb  6 17:42:17.903: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb  6 17:42:18.892: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8120  8f9cf7a6-9151-42fb-824d-b4960a2db7dc 23649 3 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0010f9fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-06 17:42:15 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-02-06 17:42:16 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb  6 17:42:18.898: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-8120  7b153397-f8c7-4259-b462-ddb7294b51b6 23642 3 2023-02-06 17:42:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8f9cf7a6-9151-42fb-824d-b4960a2db7dc 0xc006b1c327 0xc006b1c328}] [] [{kube-controller-manager Update apps/v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f9cf7a6-9151-42fb-824d-b4960a2db7dc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b1c3c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb  6 17:42:18.898: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb  6 17:42:18.898: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-8120  72a91ac9-866f-445b-a242-eef5cdb37722 23628 3 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8f9cf7a6-9151-42fb-824d-b4960a2db7dc 0xc006b1c427 0xc006b1c428}] [] [{kube-controller-manager Update apps/v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f9cf7a6-9151-42fb-824d-b4960a2db7dc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b1c4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb  6 17:42:18.905: INFO: Pod "webserver-deployment-69b7448995-4gwct" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-4gwct webserver-deployment-69b7448995- deployment-8120  ca078a26-e148-4151-9ca2-60934400021f 23693 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc0006c4827 0xc0006c4828}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wg295,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wg295,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:18.905: INFO: Pod "webserver-deployment-69b7448995-5qbkf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5qbkf webserver-deployment-69b7448995- deployment-8120  6cd5b7e1-3a2c-4f62-a84a-58f6cc263a5f 23532 0 2023-02-06 17:42:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000a9e0b7 0xc000a9e0b8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-brr2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brr2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:18.905: INFO: Pod "webserver-deployment-69b7448995-6z9tk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6z9tk webserver-deployment-69b7448995- deployment-8120  c1f9d29f-9e17-4313-9e3f-6b05204432e2 23722 0 2023-02-06 17:42:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000a9e927 0xc000a9e928}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hvqrz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hvqrz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:192.168.3.23,StartTime:2023-02-06 17:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.3.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:18.905: INFO: Pod "webserver-deployment-69b7448995-96lww" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-96lww webserver-deployment-69b7448995- deployment-8120  cdeea2cc-f30c-4568-a835-c498d81c77a4 23632 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000a9f117 0xc000a9f118}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8r4jr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8r4jr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:18.906: INFO: Pod "webserver-deployment-69b7448995-9h9dz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-9h9dz webserver-deployment-69b7448995- deployment-8120  88070240-db0c-4a5e-b582-fc75d79d099c 23720 0 2023-02-06 17:42:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000281157 0xc000281158}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2vpqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2vpqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:192.168.1.55,StartTime:2023-02-06 17:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.1.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:18.906: INFO: Pod "webserver-deployment-69b7448995-fmfwq" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-fmfwq webserver-deployment-69b7448995- deployment-8120  3ffe0fc3-22da-4f28-aed4-80dd7e57c8e6 23734 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000b7a417 0xc000b7a418}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wq6zl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wq6zl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:18.906: INFO: Pod "webserver-deployment-69b7448995-jjj7l" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-jjj7l webserver-deployment-69b7448995- deployment-8120  37b53e03-935e-4ac4-a13f-648df84d3c57 23599 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000b7acb7 0xc000b7acb8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ndrgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ndrgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:18.906: INFO: Pod "webserver-deployment-69b7448995-ll7ng" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-ll7ng webserver-deployment-69b7448995- deployment-8120  eb7181b0-269b-4e93-be28-44aa5c26fdbe 23646 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000b7b597 0xc000b7b598}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tjjtf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tjjtf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:18.906: INFO: Pod "webserver-deployment-69b7448995-p29gs" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-p29gs webserver-deployment-69b7448995- deployment-8120  8a0e7f2c-65c1-4290-ba6c-249705120070 23630 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000b7bb87 0xc000b7bb88}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bclvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bclvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:19.592: INFO: Pod "webserver-deployment-69b7448995-qkdjn" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qkdjn webserver-deployment-69b7448995- deployment-8120  e05d17ce-8788-4688-acf3-89016cb69795 23530 0 2023-02-06 17:42:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc00289c337 0xc00289c338}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-spnct,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-spnct,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:19.655: INFO: Pod "webserver-deployment-69b7448995-sm26x" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-sm26x webserver-deployment-69b7448995- deployment-8120  acb68b9d-3024-4c9b-8186-fda4a754067c 23644 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc00289c9c7 0xc00289c9c8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n7f5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n7f5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:19.699: INFO: Pod "webserver-deployment-69b7448995-ttz2l" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-ttz2l webserver-deployment-69b7448995- deployment-8120  c67cfc3f-c248-44cc-9c93-bca2fb363190 23497 0 2023-02-06 17:42:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc00289d047 0xc00289d048}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6cvn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6cvn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:20.366: INFO: Pod "webserver-deployment-69b7448995-vncw8" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-vncw8 webserver-deployment-69b7448995- deployment-8120  fb50a7d4-26d7-4313-bbc3-5ffa6bee7a19 23709 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc00289d3d7 0xc00289d3d8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ffqjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ffqjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:20.375: INFO: Pod "webserver-deployment-845c8977d9-4q2jc" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4q2jc webserver-deployment-845c8977d9- deployment-8120  bca4e373-5b6c-4f71-86de-1d44f84eb987 23417 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc00289d847 0xc00289d848}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ghhgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ghhgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.62,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ee9b543b03c56e07bf683d11d22aee12d9eab3cc43433b0eb8ced22b7112bfef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:20.571: INFO: Pod "webserver-deployment-845c8977d9-5w5kd" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5w5kd webserver-deployment-845c8977d9- deployment-8120  e2d4e12e-172d-4af1-831d-5bb1bbd45830 23726 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc00289dd67 0xc00289dd68}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4hpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4hpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.005: INFO: Pod "webserver-deployment-845c8977d9-7skw8" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7skw8 webserver-deployment-845c8977d9- deployment-8120  e577dfb2-8270-44d9-96e6-ac887fe20105 23637 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc000542857 0xc000542858}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r7ntz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r7ntz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.009: INFO: Pod "webserver-deployment-845c8977d9-ct5md" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ct5md webserver-deployment-845c8977d9- deployment-8120  df7b20c2-f277-4868-a7b8-51f22f8e8033 23331 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc0005437b7 0xc0005437b8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vksbz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vksbz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:192.168.3.85,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5cf534c31c29877c5cfa0dfe9ebdc69d8db9b01bcc4f48cd92b91a0ddbea31db,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.3.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.044: INFO: Pod "webserver-deployment-845c8977d9-dh7dm" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dh7dm webserver-deployment-845c8977d9- deployment-8120  7c502397-be55-4315-8624-2473c37f0281 23446 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc000543b77 0xc000543b78}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mr7gq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mr7gq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:192.168.1.90,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://67d3dc52fa006fdc34cc0d7ccdfc3dca01002bc5d9cfded7daad97171137e4a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.1.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.068: INFO: Pod "webserver-deployment-845c8977d9-fctzj" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-fctzj webserver-deployment-845c8977d9- deployment-8120  0683b077-3d7d-4a7c-bc49-cb4f75361368 23589 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc000543de7 0xc000543de8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rw6jb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rw6jb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.079: INFO: Pod "webserver-deployment-845c8977d9-gvxx9" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gvxx9 webserver-deployment-845c8977d9- deployment-8120  2027ee67-ce5e-4229-8f03-737d408863bc 23640 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc60e7 0xc002bc60e8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwkpm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwkpm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.093: INFO: Pod "webserver-deployment-845c8977d9-hctrq" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hctrq webserver-deployment-845c8977d9- deployment-8120  0a1d2c31-ff1b-4b95-8e3b-cfc981f31a79 23610 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc62a7 0xc002bc62a8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gtls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gtls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.105: INFO: Pod "webserver-deployment-845c8977d9-jsfbc" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jsfbc webserver-deployment-845c8977d9- deployment-8120  fc21579f-4e17-4505-9d51-c5601ccc5d38 23613 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6477 0xc002bc6478}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2vhm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2vhm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.129: INFO: Pod "webserver-deployment-845c8977d9-mddnt" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mddnt webserver-deployment-845c8977d9- deployment-8120  90784af7-a1f6-4df6-a967-315964272e8b 23311 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6647 0xc002bc6648}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-brf4x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brf4x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:192.168.1.132,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6ffd0689f33457d7deccae089a1aa7188d670344975147d99a9063d9adc34f61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.1.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.164: INFO: Pod "webserver-deployment-845c8977d9-n7bk6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-n7bk6 webserver-deployment-845c8977d9- deployment-8120  304f2bde-3509-48e2-ad32-c7d427edf5fe 23314 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6827 0xc002bc6828}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-swlxq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-swlxq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:192.168.1.244,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f1dc1866dccc1e2bd21f7572e3b151654669c8d942d0a09ebfcf9538d5e2d3e5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.1.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.169: INFO: Pod "webserver-deployment-845c8977d9-nbqb5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-nbqb5 webserver-deployment-845c8977d9- deployment-8120  9f4ee717-f449-4b1f-8998-ad5a5ccd8646 23660 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6a07 0xc002bc6a08}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5mcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5mcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.214: INFO: Pod "webserver-deployment-845c8977d9-rb8mm" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rb8mm webserver-deployment-845c8977d9- deployment-8120  07c7e2c2-aad0-432a-82b9-9db3b52ea7aa 23329 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6bd7 0xc002bc6bd8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hvskm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hvskm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:192.168.3.161,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f51c728de7904b005f8b5dbe9667a87f7b6f5d5ec591c2a1814f737c09b0aacf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.3.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.484: INFO: Pod "webserver-deployment-845c8977d9-rzdrt" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rzdrt webserver-deployment-845c8977d9- deployment-8120  e02c779e-058e-45e0-b417-a89bb6380570 23626 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6dc7 0xc002bc6dc8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqkcw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqkcw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.491: INFO: Pod "webserver-deployment-845c8977d9-v94qd" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-v94qd webserver-deployment-845c8977d9- deployment-8120  47b845da-44fb-4a0c-a72e-f3f4fca1ab4a 23404 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6f87 0xc002bc6f88}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qd5f4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qd5f4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.89,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f9ce6401a841ebd051411288f38affcfb84e8143c32ff78b55ce0f515b9884bd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.498: INFO: Pod "webserver-deployment-845c8977d9-vlvww" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vlvww webserver-deployment-845c8977d9- deployment-8120  d6d946d5-12b5-4014-a3ae-cdf4a61b3371 23708 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc7167 0xc002bc7168}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tmmn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tmmn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.502: INFO: Pod "webserver-deployment-845c8977d9-wvlp6" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wvlp6 webserver-deployment-845c8977d9- deployment-8120  c423eae8-2846-45f5-9c21-449a2b74e2d8 23579 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc7327 0xc002bc7328}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rl54v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rl54v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.506: INFO: Pod "webserver-deployment-845c8977d9-wzz5h" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wzz5h webserver-deployment-845c8977d9- deployment-8120  8001a822-85b2-4d79-9a15-9d8114284f0b 23652 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc74e7 0xc002bc74e8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hqhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hqhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.508: INFO: Pod "webserver-deployment-845c8977d9-z99n6" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-z99n6 webserver-deployment-845c8977d9- deployment-8120  aeda9480-53a4-45f3-beb7-78164e7cd20c 23718 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc76c7 0xc002bc76c8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54n84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54n84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:42:21.515: INFO: Pod "webserver-deployment-845c8977d9-zhbd6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-zhbd6 webserver-deployment-845c8977d9- deployment-8120  18db16e6-bca9-4d2d-952a-dafaab070157 23386 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc7887 0xc002bc7888}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tl4fl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tl4fl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:192.168.3.158,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c9dad41a044a967c6dcc59861ae1fd3b86722b6f4a30c2df593d86c4302008f3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.3.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb  6 17:42:21.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8120" for this suite. 02/06/23 17:42:21.536
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":65,"skipped":1273,"failed":0}
------------------------------
• [SLOW TEST] [16.845 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:42:04.702
    Feb  6 17:42:04.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename deployment 02/06/23 17:42:04.704
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:04.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:04.734
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Feb  6 17:42:04.737: INFO: Creating deployment "webserver-deployment"
    Feb  6 17:42:04.743: INFO: Waiting for observed generation 1
    Feb  6 17:42:06.767: INFO: Waiting for all required pods to come up
    Feb  6 17:42:06.824: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 02/06/23 17:42:06.936
    Feb  6 17:42:06.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-zhbd6" in namespace "deployment-8120" to be "running"
    Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4q2jc" in namespace "deployment-8120" to be "running"
    Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-ct5md" in namespace "deployment-8120" to be "running"
    Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-dh7dm" in namespace "deployment-8120" to be "running"
    Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hx2qr" in namespace "deployment-8120" to be "running"
    Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jpw4l" in namespace "deployment-8120" to be "running"
    Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mddnt" in namespace "deployment-8120" to be "running"
    Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-n7bk6" in namespace "deployment-8120" to be "running"
    Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rb8mm" in namespace "deployment-8120" to be "running"
    Feb  6 17:42:06.987: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-v94qd" in namespace "deployment-8120" to be "running"
    Feb  6 17:42:07.071: INFO: Pod "webserver-deployment-845c8977d9-zhbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 68.23234ms
    Feb  6 17:42:07.147: INFO: Pod "webserver-deployment-845c8977d9-4q2jc": Phase="Pending", Reason="", readiness=false. Elapsed: 48.185837ms
    Feb  6 17:42:07.155: INFO: Pod "webserver-deployment-845c8977d9-ct5md": Phase="Pending", Reason="", readiness=false. Elapsed: 5.097068ms
    Feb  6 17:42:07.155: INFO: Pod "webserver-deployment-845c8977d9-hx2qr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394908ms
    Feb  6 17:42:07.156: INFO: Pod "webserver-deployment-845c8977d9-dh7dm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.382908ms
    Feb  6 17:42:07.156: INFO: Pod "webserver-deployment-845c8977d9-jpw4l": Phase="Pending", Reason="", readiness=false. Elapsed: 5.098962ms
    Feb  6 17:42:07.159: INFO: Pod "webserver-deployment-845c8977d9-v94qd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025551ms
    Feb  6 17:42:07.159: INFO: Pod "webserver-deployment-845c8977d9-rb8mm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.798599ms
    Feb  6 17:42:07.159: INFO: Pod "webserver-deployment-845c8977d9-n7bk6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.271772ms
    Feb  6 17:42:07.159: INFO: Pod "webserver-deployment-845c8977d9-mddnt": Phase="Pending", Reason="", readiness=false. Elapsed: 7.716668ms
    Feb  6 17:42:09.160: INFO: Pod "webserver-deployment-845c8977d9-zhbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.157143706s
    Feb  6 17:42:09.162: INFO: Pod "webserver-deployment-845c8977d9-ct5md": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011818482s
    Feb  6 17:42:09.162: INFO: Pod "webserver-deployment-845c8977d9-4q2jc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063096008s
    Feb  6 17:42:09.162: INFO: Pod "webserver-deployment-845c8977d9-dh7dm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011659095s
    Feb  6 17:42:09.164: INFO: Pod "webserver-deployment-845c8977d9-hx2qr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013276256s
    Feb  6 17:42:09.165: INFO: Pod "webserver-deployment-845c8977d9-jpw4l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013428867s
    Feb  6 17:42:09.166: INFO: Pod "webserver-deployment-845c8977d9-v94qd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012990444s
    Feb  6 17:42:09.168: INFO: Pod "webserver-deployment-845c8977d9-n7bk6": Phase="Running", Reason="", readiness=true. Elapsed: 2.015600555s
    Feb  6 17:42:09.168: INFO: Pod "webserver-deployment-845c8977d9-n7bk6" satisfied condition "running"
    Feb  6 17:42:09.168: INFO: Pod "webserver-deployment-845c8977d9-rb8mm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015676077s
    Feb  6 17:42:09.168: INFO: Pod "webserver-deployment-845c8977d9-mddnt": Phase="Running", Reason="", readiness=true. Elapsed: 2.016766629s
    Feb  6 17:42:09.169: INFO: Pod "webserver-deployment-845c8977d9-mddnt" satisfied condition "running"
    Feb  6 17:42:11.167: INFO: Pod "webserver-deployment-845c8977d9-4q2jc": Phase="Running", Reason="", readiness=true. Elapsed: 4.067646922s
    Feb  6 17:42:11.167: INFO: Pod "webserver-deployment-845c8977d9-4q2jc" satisfied condition "running"
    Feb  6 17:42:11.167: INFO: Pod "webserver-deployment-845c8977d9-zhbd6": Phase="Running", Reason="", readiness=true. Elapsed: 4.164479875s
    Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-zhbd6" satisfied condition "running"
    Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-dh7dm": Phase="Running", Reason="", readiness=true. Elapsed: 4.017451857s
    Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-dh7dm" satisfied condition "running"
    Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-hx2qr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017067895s
    Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-ct5md": Phase="Running", Reason="", readiness=true. Elapsed: 4.01814279s
    Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-ct5md" satisfied condition "running"
    Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-jpw4l": Phase="Running", Reason="", readiness=true. Elapsed: 4.016451421s
    Feb  6 17:42:11.168: INFO: Pod "webserver-deployment-845c8977d9-jpw4l" satisfied condition "running"
    Feb  6 17:42:11.170: INFO: Pod "webserver-deployment-845c8977d9-rb8mm": Phase="Running", Reason="", readiness=true. Elapsed: 4.017541508s
    Feb  6 17:42:11.170: INFO: Pod "webserver-deployment-845c8977d9-rb8mm" satisfied condition "running"
    Feb  6 17:42:11.170: INFO: Pod "webserver-deployment-845c8977d9-v94qd": Phase="Running", Reason="", readiness=true. Elapsed: 4.017634882s
    Feb  6 17:42:11.170: INFO: Pod "webserver-deployment-845c8977d9-v94qd" satisfied condition "running"
    Feb  6 17:42:13.160: INFO: Pod "webserver-deployment-845c8977d9-hx2qr": Phase="Running", Reason="", readiness=true. Elapsed: 6.009457863s
    Feb  6 17:42:13.161: INFO: Pod "webserver-deployment-845c8977d9-hx2qr" satisfied condition "running"
    Feb  6 17:42:13.161: INFO: Waiting for deployment "webserver-deployment" to complete
    Feb  6 17:42:13.169: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Feb  6 17:42:13.178: INFO: Updating deployment webserver-deployment
    Feb  6 17:42:13.178: INFO: Waiting for observed generation 2
    Feb  6 17:42:15.227: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Feb  6 17:42:15.291: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Feb  6 17:42:15.395: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb  6 17:42:15.587: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Feb  6 17:42:15.667: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Feb  6 17:42:15.709: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb  6 17:42:15.718: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Feb  6 17:42:15.719: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Feb  6 17:42:15.737: INFO: Updating deployment webserver-deployment
    Feb  6 17:42:15.737: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Feb  6 17:42:15.758: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Feb  6 17:42:17.903: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb  6 17:42:18.892: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-8120  8f9cf7a6-9151-42fb-824d-b4960a2db7dc 23649 3 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0010f9fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-06 17:42:15 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-02-06 17:42:16 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Feb  6 17:42:18.898: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-8120  7b153397-f8c7-4259-b462-ddb7294b51b6 23642 3 2023-02-06 17:42:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8f9cf7a6-9151-42fb-824d-b4960a2db7dc 0xc006b1c327 0xc006b1c328}] [] [{kube-controller-manager Update apps/v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f9cf7a6-9151-42fb-824d-b4960a2db7dc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b1c3c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb  6 17:42:18.898: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Feb  6 17:42:18.898: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-8120  72a91ac9-866f-445b-a242-eef5cdb37722 23628 3 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8f9cf7a6-9151-42fb-824d-b4960a2db7dc 0xc006b1c427 0xc006b1c428}] [] [{kube-controller-manager Update apps/v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f9cf7a6-9151-42fb-824d-b4960a2db7dc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b1c4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Feb  6 17:42:18.905: INFO: Pod "webserver-deployment-69b7448995-4gwct" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-4gwct webserver-deployment-69b7448995- deployment-8120  ca078a26-e148-4151-9ca2-60934400021f 23693 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc0006c4827 0xc0006c4828}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wg295,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wg295,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:18.905: INFO: Pod "webserver-deployment-69b7448995-5qbkf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5qbkf webserver-deployment-69b7448995- deployment-8120  6cd5b7e1-3a2c-4f62-a84a-58f6cc263a5f 23532 0 2023-02-06 17:42:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000a9e0b7 0xc000a9e0b8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-brr2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brr2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:18.905: INFO: Pod "webserver-deployment-69b7448995-6z9tk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6z9tk webserver-deployment-69b7448995- deployment-8120  c1f9d29f-9e17-4313-9e3f-6b05204432e2 23722 0 2023-02-06 17:42:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000a9e927 0xc000a9e928}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hvqrz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hvqrz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:192.168.3.23,StartTime:2023-02-06 17:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.3.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:18.905: INFO: Pod "webserver-deployment-69b7448995-96lww" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-96lww webserver-deployment-69b7448995- deployment-8120  cdeea2cc-f30c-4568-a835-c498d81c77a4 23632 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000a9f117 0xc000a9f118}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8r4jr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8r4jr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:18.906: INFO: Pod "webserver-deployment-69b7448995-9h9dz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-9h9dz webserver-deployment-69b7448995- deployment-8120  88070240-db0c-4a5e-b582-fc75d79d099c 23720 0 2023-02-06 17:42:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000281157 0xc000281158}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2vpqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2vpqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:192.168.1.55,StartTime:2023-02-06 17:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.1.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:18.906: INFO: Pod "webserver-deployment-69b7448995-fmfwq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-fmfwq webserver-deployment-69b7448995- deployment-8120  3ffe0fc3-22da-4f28-aed4-80dd7e57c8e6 23734 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000b7a417 0xc000b7a418}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wq6zl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wq6zl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:18.906: INFO: Pod "webserver-deployment-69b7448995-jjj7l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-jjj7l webserver-deployment-69b7448995- deployment-8120  37b53e03-935e-4ac4-a13f-648df84d3c57 23599 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000b7acb7 0xc000b7acb8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ndrgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ndrgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:18.906: INFO: Pod "webserver-deployment-69b7448995-ll7ng" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-ll7ng webserver-deployment-69b7448995- deployment-8120  eb7181b0-269b-4e93-be28-44aa5c26fdbe 23646 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000b7b597 0xc000b7b598}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tjjtf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tjjtf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:18.906: INFO: Pod "webserver-deployment-69b7448995-p29gs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-p29gs webserver-deployment-69b7448995- deployment-8120  8a0e7f2c-65c1-4290-ba6c-249705120070 23630 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc000b7bb87 0xc000b7bb88}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bclvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bclvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:19.592: INFO: Pod "webserver-deployment-69b7448995-qkdjn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qkdjn webserver-deployment-69b7448995- deployment-8120  e05d17ce-8788-4688-acf3-89016cb69795 23530 0 2023-02-06 17:42:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc00289c337 0xc00289c338}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-spnct,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-spnct,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:19.655: INFO: Pod "webserver-deployment-69b7448995-sm26x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-sm26x webserver-deployment-69b7448995- deployment-8120  acb68b9d-3024-4c9b-8186-fda4a754067c 23644 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc00289c9c7 0xc00289c9c8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n7f5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n7f5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:19.699: INFO: Pod "webserver-deployment-69b7448995-ttz2l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-ttz2l webserver-deployment-69b7448995- deployment-8120  c67cfc3f-c248-44cc-9c93-bca2fb363190 23497 0 2023-02-06 17:42:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc00289d047 0xc00289d048}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6cvn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6cvn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:20.366: INFO: Pod "webserver-deployment-69b7448995-vncw8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-vncw8 webserver-deployment-69b7448995- deployment-8120  fb50a7d4-26d7-4313-bbc3-5ffa6bee7a19 23709 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 7b153397-f8c7-4259-b462-ddb7294b51b6 0xc00289d3d7 0xc00289d3d8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b153397-f8c7-4259-b462-ddb7294b51b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ffqjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ffqjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:20.375: INFO: Pod "webserver-deployment-845c8977d9-4q2jc" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4q2jc webserver-deployment-845c8977d9- deployment-8120  bca4e373-5b6c-4f71-86de-1d44f84eb987 23417 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc00289d847 0xc00289d848}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ghhgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ghhgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.62,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ee9b543b03c56e07bf683d11d22aee12d9eab3cc43433b0eb8ced22b7112bfef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:20.571: INFO: Pod "webserver-deployment-845c8977d9-5w5kd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5w5kd webserver-deployment-845c8977d9- deployment-8120  e2d4e12e-172d-4af1-831d-5bb1bbd45830 23726 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc00289dd67 0xc00289dd68}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4hpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4hpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.005: INFO: Pod "webserver-deployment-845c8977d9-7skw8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7skw8 webserver-deployment-845c8977d9- deployment-8120  e577dfb2-8270-44d9-96e6-ac887fe20105 23637 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc000542857 0xc000542858}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r7ntz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r7ntz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.009: INFO: Pod "webserver-deployment-845c8977d9-ct5md" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ct5md webserver-deployment-845c8977d9- deployment-8120  df7b20c2-f277-4868-a7b8-51f22f8e8033 23331 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc0005437b7 0xc0005437b8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vksbz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vksbz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:192.168.3.85,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5cf534c31c29877c5cfa0dfe9ebdc69d8db9b01bcc4f48cd92b91a0ddbea31db,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.3.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.044: INFO: Pod "webserver-deployment-845c8977d9-dh7dm" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dh7dm webserver-deployment-845c8977d9- deployment-8120  7c502397-be55-4315-8624-2473c37f0281 23446 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc000543b77 0xc000543b78}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mr7gq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mr7gq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:192.168.1.90,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://67d3dc52fa006fdc34cc0d7ccdfc3dca01002bc5d9cfded7daad97171137e4a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.1.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.068: INFO: Pod "webserver-deployment-845c8977d9-fctzj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-fctzj webserver-deployment-845c8977d9- deployment-8120  0683b077-3d7d-4a7c-bc49-cb4f75361368 23589 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc000543de7 0xc000543de8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rw6jb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rw6jb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.079: INFO: Pod "webserver-deployment-845c8977d9-gvxx9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gvxx9 webserver-deployment-845c8977d9- deployment-8120  2027ee67-ce5e-4229-8f03-737d408863bc 23640 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc60e7 0xc002bc60e8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwkpm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwkpm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.093: INFO: Pod "webserver-deployment-845c8977d9-hctrq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hctrq webserver-deployment-845c8977d9- deployment-8120  0a1d2c31-ff1b-4b95-8e3b-cfc981f31a79 23610 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc62a7 0xc002bc62a8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gtls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gtls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.105: INFO: Pod "webserver-deployment-845c8977d9-jsfbc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jsfbc webserver-deployment-845c8977d9- deployment-8120  fc21579f-4e17-4505-9d51-c5601ccc5d38 23613 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6477 0xc002bc6478}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2vhm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2vhm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.129: INFO: Pod "webserver-deployment-845c8977d9-mddnt" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mddnt webserver-deployment-845c8977d9- deployment-8120  90784af7-a1f6-4df6-a967-315964272e8b 23311 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6647 0xc002bc6648}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-brf4x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brf4x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:192.168.1.132,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6ffd0689f33457d7deccae089a1aa7188d670344975147d99a9063d9adc34f61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.1.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.164: INFO: Pod "webserver-deployment-845c8977d9-n7bk6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-n7bk6 webserver-deployment-845c8977d9- deployment-8120  304f2bde-3509-48e2-ad32-c7d427edf5fe 23314 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6827 0xc002bc6828}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-swlxq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-swlxq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:192.168.1.244,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f1dc1866dccc1e2bd21f7572e3b151654669c8d942d0a09ebfcf9538d5e2d3e5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.1.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.169: INFO: Pod "webserver-deployment-845c8977d9-nbqb5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-nbqb5 webserver-deployment-845c8977d9- deployment-8120  9f4ee717-f449-4b1f-8998-ad5a5ccd8646 23660 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6a07 0xc002bc6a08}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5mcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5mcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.214: INFO: Pod "webserver-deployment-845c8977d9-rb8mm" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rb8mm webserver-deployment-845c8977d9- deployment-8120  07c7e2c2-aad0-432a-82b9-9db3b52ea7aa 23329 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6bd7 0xc002bc6bd8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hvskm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hvskm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:192.168.3.161,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f51c728de7904b005f8b5dbe9667a87f7b6f5d5ec591c2a1814f737c09b0aacf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.3.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.484: INFO: Pod "webserver-deployment-845c8977d9-rzdrt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rzdrt webserver-deployment-845c8977d9- deployment-8120  e02c779e-058e-45e0-b417-a89bb6380570 23626 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6dc7 0xc002bc6dc8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqkcw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqkcw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.491: INFO: Pod "webserver-deployment-845c8977d9-v94qd" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-v94qd webserver-deployment-845c8977d9- deployment-8120  47b845da-44fb-4a0c-a72e-f3f4fca1ab4a 23404 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc6f87 0xc002bc6f88}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qd5f4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qd5f4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.89,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f9ce6401a841ebd051411288f38affcfb84e8143c32ff78b55ce0f515b9884bd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.498: INFO: Pod "webserver-deployment-845c8977d9-vlvww" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vlvww webserver-deployment-845c8977d9- deployment-8120  d6d946d5-12b5-4014-a3ae-cdf4a61b3371 23708 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc7167 0xc002bc7168}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tmmn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tmmn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:,StartTime:2023-02-06 17:42:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.502: INFO: Pod "webserver-deployment-845c8977d9-wvlp6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wvlp6 webserver-deployment-845c8977d9- deployment-8120  c423eae8-2846-45f5-9c21-449a2b74e2d8 23579 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc7327 0xc002bc7328}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rl54v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rl54v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.506: INFO: Pod "webserver-deployment-845c8977d9-wzz5h" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wzz5h webserver-deployment-845c8977d9- deployment-8120  8001a822-85b2-4d79-9a15-9d8114284f0b 23652 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc74e7 0xc002bc74e8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hqhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hqhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.508: INFO: Pod "webserver-deployment-845c8977d9-z99n6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-z99n6 webserver-deployment-845c8977d9- deployment-8120  aeda9480-53a4-45f3-beb7-78164e7cd20c 23718 0 2023-02-06 17:42:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc76c7 0xc002bc76c8}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54n84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54n84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:42:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:42:21.515: INFO: Pod "webserver-deployment-845c8977d9-zhbd6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-zhbd6 webserver-deployment-845c8977d9- deployment-8120  18db16e6-bca9-4d2d-952a-dafaab070157 23386 0 2023-02-06 17:42:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 72a91ac9-866f-445b-a242-eef5cdb37722 0xc002bc7887 0xc002bc7888}] [] [{kube-controller-manager Update v1 2023-02-06 17:42:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72a91ac9-866f-445b-a242-eef5cdb37722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:42:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tl4fl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tl4fl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:42:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:192.168.3.158,StartTime:2023-02-06 17:42:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:42:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c9dad41a044a967c6dcc59861ae1fd3b86722b6f4a30c2df593d86c4302008f3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.3.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb  6 17:42:21.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8120" for this suite. 02/06/23 17:42:21.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:42:21.67
Feb  6 17:42:21.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename job 02/06/23 17:42:21.688
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:21.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:21.719
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 02/06/23 17:42:21.724
STEP: Ensuring job reaches completions 02/06/23 17:42:21.735
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb  6 17:42:49.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1758" for this suite. 02/06/23 17:42:49.748
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":66,"skipped":1280,"failed":0}
------------------------------
• [SLOW TEST] [28.086 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:42:21.67
    Feb  6 17:42:21.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename job 02/06/23 17:42:21.688
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:21.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:21.719
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 02/06/23 17:42:21.724
    STEP: Ensuring job reaches completions 02/06/23 17:42:21.735
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb  6 17:42:49.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1758" for this suite. 02/06/23 17:42:49.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:42:49.762
Feb  6 17:42:49.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 17:42:49.763
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:49.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:49.789
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 02/06/23 17:42:49.793
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 17:42:49.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7338" for this suite. 02/06/23 17:42:49.802
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":67,"skipped":1318,"failed":0}
------------------------------
• [0.049 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:42:49.762
    Feb  6 17:42:49.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 17:42:49.763
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:49.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:49.789
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 02/06/23 17:42:49.793
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 17:42:49.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7338" for this suite. 02/06/23 17:42:49.802
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:42:49.812
Feb  6 17:42:49.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename conformance-tests 02/06/23 17:42:49.813
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:49.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:49.839
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 02/06/23 17:42:49.842
Feb  6 17:42:49.842: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Feb  6 17:42:49.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-8001" for this suite. 02/06/23 17:42:49.854
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":68,"skipped":1318,"failed":0}
------------------------------
• [0.047 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:42:49.812
    Feb  6 17:42:49.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename conformance-tests 02/06/23 17:42:49.813
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:49.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:49.839
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 02/06/23 17:42:49.842
    Feb  6 17:42:49.842: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Feb  6 17:42:49.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-8001" for this suite. 02/06/23 17:42:49.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:42:49.863
Feb  6 17:42:49.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 17:42:49.864
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:49.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:49.886
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 02/06/23 17:42:49.889
Feb  6 17:42:49.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd" in namespace "downward-api-765" to be "Succeeded or Failed"
Feb  6 17:42:49.907: INFO: Pod "downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.619595ms
Feb  6 17:42:51.912: INFO: Pod "downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013928098s
Feb  6 17:42:53.911: INFO: Pod "downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd": Phase="Running", Reason="", readiness=false. Elapsed: 4.013701852s
Feb  6 17:42:55.911: INFO: Pod "downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013616148s
STEP: Saw pod success 02/06/23 17:42:55.911
Feb  6 17:42:55.911: INFO: Pod "downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd" satisfied condition "Succeeded or Failed"
Feb  6 17:42:55.915: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd container client-container: <nil>
STEP: delete the pod 02/06/23 17:42:55.93
Feb  6 17:42:55.942: INFO: Waiting for pod downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd to disappear
Feb  6 17:42:55.945: INFO: Pod downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb  6 17:42:55.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-765" for this suite. 02/06/23 17:42:55.951
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":69,"skipped":1338,"failed":0}
------------------------------
• [SLOW TEST] [6.097 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:42:49.863
    Feb  6 17:42:49.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 17:42:49.864
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:49.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:49.886
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 02/06/23 17:42:49.889
    Feb  6 17:42:49.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd" in namespace "downward-api-765" to be "Succeeded or Failed"
    Feb  6 17:42:49.907: INFO: Pod "downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.619595ms
    Feb  6 17:42:51.912: INFO: Pod "downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013928098s
    Feb  6 17:42:53.911: INFO: Pod "downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd": Phase="Running", Reason="", readiness=false. Elapsed: 4.013701852s
    Feb  6 17:42:55.911: INFO: Pod "downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013616148s
    STEP: Saw pod success 02/06/23 17:42:55.911
    Feb  6 17:42:55.911: INFO: Pod "downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd" satisfied condition "Succeeded or Failed"
    Feb  6 17:42:55.915: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd container client-container: <nil>
    STEP: delete the pod 02/06/23 17:42:55.93
    Feb  6 17:42:55.942: INFO: Waiting for pod downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd to disappear
    Feb  6 17:42:55.945: INFO: Pod downwardapi-volume-59f746b4-a10b-4478-9bf7-7d722d557ccd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb  6 17:42:55.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-765" for this suite. 02/06/23 17:42:55.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:42:55.965
Feb  6 17:42:55.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 17:42:55.966
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:55.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:55.984
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 02/06/23 17:42:55.986
Feb  6 17:42:55.996: INFO: Waiting up to 5m0s for pod "pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf" in namespace "emptydir-2990" to be "Succeeded or Failed"
Feb  6 17:42:56.003: INFO: Pod "pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.059578ms
Feb  6 17:42:58.008: INFO: Pod "pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012043203s
Feb  6 17:43:00.011: INFO: Pod "pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014658342s
Feb  6 17:43:02.007: INFO: Pod "pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011079733s
STEP: Saw pod success 02/06/23 17:43:02.007
Feb  6 17:43:02.007: INFO: Pod "pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf" satisfied condition "Succeeded or Failed"
Feb  6 17:43:02.010: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf container test-container: <nil>
STEP: delete the pod 02/06/23 17:43:02.019
Feb  6 17:43:02.043: INFO: Waiting for pod pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf to disappear
Feb  6 17:43:02.047: INFO: Pod pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 17:43:02.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2990" for this suite. 02/06/23 17:43:02.054
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":70,"skipped":1371,"failed":0}
------------------------------
• [SLOW TEST] [6.096 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:42:55.965
    Feb  6 17:42:55.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 17:42:55.966
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:42:55.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:42:55.984
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 02/06/23 17:42:55.986
    Feb  6 17:42:55.996: INFO: Waiting up to 5m0s for pod "pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf" in namespace "emptydir-2990" to be "Succeeded or Failed"
    Feb  6 17:42:56.003: INFO: Pod "pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.059578ms
    Feb  6 17:42:58.008: INFO: Pod "pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012043203s
    Feb  6 17:43:00.011: INFO: Pod "pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014658342s
    Feb  6 17:43:02.007: INFO: Pod "pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011079733s
    STEP: Saw pod success 02/06/23 17:43:02.007
    Feb  6 17:43:02.007: INFO: Pod "pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf" satisfied condition "Succeeded or Failed"
    Feb  6 17:43:02.010: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf container test-container: <nil>
    STEP: delete the pod 02/06/23 17:43:02.019
    Feb  6 17:43:02.043: INFO: Waiting for pod pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf to disappear
    Feb  6 17:43:02.047: INFO: Pod pod-1fd55885-409c-4cbf-bfc8-d9537654fbaf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 17:43:02.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2990" for this suite. 02/06/23 17:43:02.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:43:02.064
Feb  6 17:43:02.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pods 02/06/23 17:43:02.066
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:43:02.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:43:02.085
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 02/06/23 17:43:02.087
Feb  6 17:43:02.099: INFO: Waiting up to 5m0s for pod "pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5" in namespace "pods-5383" to be "running and ready"
Feb  6 17:43:02.103: INFO: Pod "pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.913158ms
Feb  6 17:43:02.103: INFO: The phase of Pod pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:43:04.107: INFO: Pod "pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008149892s
Feb  6 17:43:04.107: INFO: The phase of Pod pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:43:06.108: INFO: Pod "pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5": Phase="Running", Reason="", readiness=true. Elapsed: 4.009088713s
Feb  6 17:43:06.108: INFO: The phase of Pod pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5 is Running (Ready = true)
Feb  6 17:43:06.108: INFO: Pod "pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5" satisfied condition "running and ready"
Feb  6 17:43:06.114: INFO: Pod pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5 has hostIP: 195.17.4.133
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb  6 17:43:06.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5383" for this suite. 02/06/23 17:43:06.123
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":71,"skipped":1389,"failed":0}
------------------------------
• [4.068 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:43:02.064
    Feb  6 17:43:02.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pods 02/06/23 17:43:02.066
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:43:02.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:43:02.085
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 02/06/23 17:43:02.087
    Feb  6 17:43:02.099: INFO: Waiting up to 5m0s for pod "pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5" in namespace "pods-5383" to be "running and ready"
    Feb  6 17:43:02.103: INFO: Pod "pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.913158ms
    Feb  6 17:43:02.103: INFO: The phase of Pod pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:43:04.107: INFO: Pod "pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008149892s
    Feb  6 17:43:04.107: INFO: The phase of Pod pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:43:06.108: INFO: Pod "pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5": Phase="Running", Reason="", readiness=true. Elapsed: 4.009088713s
    Feb  6 17:43:06.108: INFO: The phase of Pod pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5 is Running (Ready = true)
    Feb  6 17:43:06.108: INFO: Pod "pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5" satisfied condition "running and ready"
    Feb  6 17:43:06.114: INFO: Pod pod-hostip-14132af6-60f1-42c6-8e76-63fe0ce383f5 has hostIP: 195.17.4.133
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb  6 17:43:06.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5383" for this suite. 02/06/23 17:43:06.123
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:43:06.138
Feb  6 17:43:06.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename svcaccounts 02/06/23 17:43:06.14
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:43:06.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:43:06.159
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Feb  6 17:43:06.190: INFO: created pod pod-service-account-defaultsa
Feb  6 17:43:06.191: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb  6 17:43:06.204: INFO: created pod pod-service-account-mountsa
Feb  6 17:43:06.204: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb  6 17:43:06.229: INFO: created pod pod-service-account-nomountsa
Feb  6 17:43:06.229: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb  6 17:43:06.246: INFO: created pod pod-service-account-defaultsa-mountspec
Feb  6 17:43:06.246: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb  6 17:43:06.265: INFO: created pod pod-service-account-mountsa-mountspec
Feb  6 17:43:06.266: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb  6 17:43:06.282: INFO: created pod pod-service-account-nomountsa-mountspec
Feb  6 17:43:06.282: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb  6 17:43:06.297: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb  6 17:43:06.297: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb  6 17:43:06.317: INFO: created pod pod-service-account-mountsa-nomountspec
Feb  6 17:43:06.317: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb  6 17:43:06.333: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb  6 17:43:06.333: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb  6 17:43:06.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4334" for this suite. 02/06/23 17:43:06.343
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":72,"skipped":1393,"failed":0}
------------------------------
• [0.236 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:43:06.138
    Feb  6 17:43:06.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename svcaccounts 02/06/23 17:43:06.14
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:43:06.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:43:06.159
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Feb  6 17:43:06.190: INFO: created pod pod-service-account-defaultsa
    Feb  6 17:43:06.191: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Feb  6 17:43:06.204: INFO: created pod pod-service-account-mountsa
    Feb  6 17:43:06.204: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Feb  6 17:43:06.229: INFO: created pod pod-service-account-nomountsa
    Feb  6 17:43:06.229: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Feb  6 17:43:06.246: INFO: created pod pod-service-account-defaultsa-mountspec
    Feb  6 17:43:06.246: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Feb  6 17:43:06.265: INFO: created pod pod-service-account-mountsa-mountspec
    Feb  6 17:43:06.266: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Feb  6 17:43:06.282: INFO: created pod pod-service-account-nomountsa-mountspec
    Feb  6 17:43:06.282: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Feb  6 17:43:06.297: INFO: created pod pod-service-account-defaultsa-nomountspec
    Feb  6 17:43:06.297: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Feb  6 17:43:06.317: INFO: created pod pod-service-account-mountsa-nomountspec
    Feb  6 17:43:06.317: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Feb  6 17:43:06.333: INFO: created pod pod-service-account-nomountsa-nomountspec
    Feb  6 17:43:06.333: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb  6 17:43:06.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4334" for this suite. 02/06/23 17:43:06.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:43:06.384
Feb  6 17:43:06.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename resourcequota 02/06/23 17:43:06.385
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:43:06.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:43:06.402
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 02/06/23 17:43:06.405
STEP: Creating a ResourceQuota 02/06/23 17:43:11.413
STEP: Ensuring resource quota status is calculated 02/06/23 17:43:11.42
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb  6 17:43:13.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7753" for this suite. 02/06/23 17:43:13.431
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":73,"skipped":1475,"failed":0}
------------------------------
• [SLOW TEST] [7.056 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:43:06.384
    Feb  6 17:43:06.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename resourcequota 02/06/23 17:43:06.385
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:43:06.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:43:06.402
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 02/06/23 17:43:06.405
    STEP: Creating a ResourceQuota 02/06/23 17:43:11.413
    STEP: Ensuring resource quota status is calculated 02/06/23 17:43:11.42
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb  6 17:43:13.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7753" for this suite. 02/06/23 17:43:13.431
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:43:13.443
Feb  6 17:43:13.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 17:43:13.444
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:43:13.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:43:13.475
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 17:43:13.499
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:43:13.974
STEP: Deploying the webhook pod 02/06/23 17:43:13.983
STEP: Wait for the deployment to be ready 02/06/23 17:43:13.997
Feb  6 17:43:14.012: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 17:43:16.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 43, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 43, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 43, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 43, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 17:43:18.05
STEP: Verifying the service has paired with the endpoint 02/06/23 17:43:18.068
Feb  6 17:43:19.069: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 02/06/23 17:43:19.074
STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/06/23 17:43:19.096
STEP: Creating a configMap that should not be mutated 02/06/23 17:43:19.105
STEP: Patching a mutating webhook configuration's rules to include the create operation 02/06/23 17:43:19.118
STEP: Creating a configMap that should be mutated 02/06/23 17:43:19.128
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:43:19.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4413" for this suite. 02/06/23 17:43:19.162
STEP: Destroying namespace "webhook-4413-markers" for this suite. 02/06/23 17:43:19.169
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":74,"skipped":1477,"failed":0}
------------------------------
• [SLOW TEST] [5.802 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:43:13.443
    Feb  6 17:43:13.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 17:43:13.444
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:43:13.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:43:13.475
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 17:43:13.499
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:43:13.974
    STEP: Deploying the webhook pod 02/06/23 17:43:13.983
    STEP: Wait for the deployment to be ready 02/06/23 17:43:13.997
    Feb  6 17:43:14.012: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 17:43:16.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 43, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 43, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 43, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 43, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 17:43:18.05
    STEP: Verifying the service has paired with the endpoint 02/06/23 17:43:18.068
    Feb  6 17:43:19.069: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 02/06/23 17:43:19.074
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/06/23 17:43:19.096
    STEP: Creating a configMap that should not be mutated 02/06/23 17:43:19.105
    STEP: Patching a mutating webhook configuration's rules to include the create operation 02/06/23 17:43:19.118
    STEP: Creating a configMap that should be mutated 02/06/23 17:43:19.128
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:43:19.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4413" for this suite. 02/06/23 17:43:19.162
    STEP: Destroying namespace "webhook-4413-markers" for this suite. 02/06/23 17:43:19.169
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:43:19.252
Feb  6 17:43:19.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 17:43:19.254
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:43:19.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:43:19.289
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-8da0df72-bb36-4ee4-9242-bd862e09b22a 02/06/23 17:43:19.299
STEP: Creating configMap with name cm-test-opt-upd-6128a20d-c03d-4f6d-9e8e-e08d663d8dbc 02/06/23 17:43:19.307
STEP: Creating the pod 02/06/23 17:43:19.312
Feb  6 17:43:19.328: INFO: Waiting up to 5m0s for pod "pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467" in namespace "configmap-6321" to be "running and ready"
Feb  6 17:43:19.338: INFO: Pod "pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467": Phase="Pending", Reason="", readiness=false. Elapsed: 10.113414ms
Feb  6 17:43:19.338: INFO: The phase of Pod pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:43:21.344: INFO: Pod "pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015456926s
Feb  6 17:43:21.344: INFO: The phase of Pod pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:43:23.344: INFO: Pod "pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467": Phase="Running", Reason="", readiness=true. Elapsed: 4.016185456s
Feb  6 17:43:23.344: INFO: The phase of Pod pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467 is Running (Ready = true)
Feb  6 17:43:23.344: INFO: Pod "pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-8da0df72-bb36-4ee4-9242-bd862e09b22a 02/06/23 17:43:23.365
STEP: Updating configmap cm-test-opt-upd-6128a20d-c03d-4f6d-9e8e-e08d663d8dbc 02/06/23 17:43:23.372
STEP: Creating configMap with name cm-test-opt-create-8e3b9f7f-232b-438d-843e-dcb227c5ba87 02/06/23 17:43:23.378
STEP: waiting to observe update in volume 02/06/23 17:43:23.384
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 17:43:25.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6321" for this suite. 02/06/23 17:43:25.418
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":75,"skipped":1478,"failed":0}
------------------------------
• [SLOW TEST] [6.174 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:43:19.252
    Feb  6 17:43:19.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 17:43:19.254
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:43:19.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:43:19.289
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-8da0df72-bb36-4ee4-9242-bd862e09b22a 02/06/23 17:43:19.299
    STEP: Creating configMap with name cm-test-opt-upd-6128a20d-c03d-4f6d-9e8e-e08d663d8dbc 02/06/23 17:43:19.307
    STEP: Creating the pod 02/06/23 17:43:19.312
    Feb  6 17:43:19.328: INFO: Waiting up to 5m0s for pod "pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467" in namespace "configmap-6321" to be "running and ready"
    Feb  6 17:43:19.338: INFO: Pod "pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467": Phase="Pending", Reason="", readiness=false. Elapsed: 10.113414ms
    Feb  6 17:43:19.338: INFO: The phase of Pod pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:43:21.344: INFO: Pod "pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015456926s
    Feb  6 17:43:21.344: INFO: The phase of Pod pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:43:23.344: INFO: Pod "pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467": Phase="Running", Reason="", readiness=true. Elapsed: 4.016185456s
    Feb  6 17:43:23.344: INFO: The phase of Pod pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467 is Running (Ready = true)
    Feb  6 17:43:23.344: INFO: Pod "pod-configmaps-dba555a3-592c-436a-b26e-2c14e75cc467" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-8da0df72-bb36-4ee4-9242-bd862e09b22a 02/06/23 17:43:23.365
    STEP: Updating configmap cm-test-opt-upd-6128a20d-c03d-4f6d-9e8e-e08d663d8dbc 02/06/23 17:43:23.372
    STEP: Creating configMap with name cm-test-opt-create-8e3b9f7f-232b-438d-843e-dcb227c5ba87 02/06/23 17:43:23.378
    STEP: waiting to observe update in volume 02/06/23 17:43:23.384
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 17:43:25.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6321" for this suite. 02/06/23 17:43:25.418
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:43:25.429
Feb  6 17:43:25.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename dns 02/06/23 17:43:25.43
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:43:25.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:43:25.456
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 02/06/23 17:43:25.46
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 103.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.103_udp@PTR;check="$$(dig +tcp +noall +answer +search 103.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.103_tcp@PTR;sleep 1; done
 02/06/23 17:43:25.495
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 103.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.103_udp@PTR;check="$$(dig +tcp +noall +answer +search 103.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.103_tcp@PTR;sleep 1; done
 02/06/23 17:43:25.496
STEP: creating a pod to probe DNS 02/06/23 17:43:25.497
STEP: submitting the pod to kubernetes 02/06/23 17:43:25.497
Feb  6 17:43:25.518: INFO: Waiting up to 15m0s for pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f" in namespace "dns-4018" to be "running"
Feb  6 17:43:25.525: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.238984ms
Feb  6 17:43:27.530: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012351201s
Feb  6 17:43:29.530: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012764391s
Feb  6 17:43:31.535: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017388447s
Feb  6 17:43:33.531: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013514212s
Feb  6 17:43:35.529: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011093512s
Feb  6 17:43:37.533: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.015354572s
Feb  6 17:43:39.529: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011257971s
Feb  6 17:43:41.529: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011662185s
Feb  6 17:43:43.528: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010688672s
Feb  6 17:43:45.530: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.012847233s
Feb  6 17:43:47.530: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012032677s
Feb  6 17:43:49.532: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014102913s
Feb  6 17:43:51.529: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Running", Reason="", readiness=true. Elapsed: 26.011766633s
Feb  6 17:43:51.529: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f" satisfied condition "running"
STEP: retrieving the pod 02/06/23 17:43:51.529
STEP: looking for the results for each expected name from probers 02/06/23 17:43:51.533
Feb  6 17:43:51.539: INFO: Unable to read wheezy_udp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:51.543: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:51.547: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:51.551: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:51.568: INFO: Unable to read jessie_udp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:51.572: INFO: Unable to read jessie_tcp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:51.576: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:51.579: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:51.593: INFO: Lookups using dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f failed for: [wheezy_udp@dns-test-service.dns-4018.svc.cluster.local wheezy_tcp@dns-test-service.dns-4018.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local jessie_udp@dns-test-service.dns-4018.svc.cluster.local jessie_tcp@dns-test-service.dns-4018.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local]

Feb  6 17:43:56.600: INFO: Unable to read wheezy_udp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:56.604: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:56.608: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:56.612: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:56.639: INFO: Unable to read jessie_udp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:56.643: INFO: Unable to read jessie_tcp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:56.647: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:56.651: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
Feb  6 17:43:56.669: INFO: Lookups using dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f failed for: [wheezy_udp@dns-test-service.dns-4018.svc.cluster.local wheezy_tcp@dns-test-service.dns-4018.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local jessie_udp@dns-test-service.dns-4018.svc.cluster.local jessie_tcp@dns-test-service.dns-4018.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local]

Feb  6 17:44:01.650: INFO: DNS probes using dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f succeeded

STEP: deleting the pod 02/06/23 17:44:01.65
STEP: deleting the test service 02/06/23 17:44:01.677
STEP: deleting the test headless service 02/06/23 17:44:01.766
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb  6 17:44:01.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4018" for this suite. 02/06/23 17:44:01.797
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":76,"skipped":1482,"failed":0}
------------------------------
• [SLOW TEST] [36.378 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:43:25.429
    Feb  6 17:43:25.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename dns 02/06/23 17:43:25.43
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:43:25.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:43:25.456
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 02/06/23 17:43:25.46
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 103.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.103_udp@PTR;check="$$(dig +tcp +noall +answer +search 103.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.103_tcp@PTR;sleep 1; done
     02/06/23 17:43:25.495
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4018.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 103.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.103_udp@PTR;check="$$(dig +tcp +noall +answer +search 103.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.103_tcp@PTR;sleep 1; done
     02/06/23 17:43:25.496
    STEP: creating a pod to probe DNS 02/06/23 17:43:25.497
    STEP: submitting the pod to kubernetes 02/06/23 17:43:25.497
    Feb  6 17:43:25.518: INFO: Waiting up to 15m0s for pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f" in namespace "dns-4018" to be "running"
    Feb  6 17:43:25.525: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.238984ms
    Feb  6 17:43:27.530: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012351201s
    Feb  6 17:43:29.530: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012764391s
    Feb  6 17:43:31.535: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017388447s
    Feb  6 17:43:33.531: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013514212s
    Feb  6 17:43:35.529: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011093512s
    Feb  6 17:43:37.533: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.015354572s
    Feb  6 17:43:39.529: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011257971s
    Feb  6 17:43:41.529: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011662185s
    Feb  6 17:43:43.528: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010688672s
    Feb  6 17:43:45.530: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.012847233s
    Feb  6 17:43:47.530: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012032677s
    Feb  6 17:43:49.532: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014102913s
    Feb  6 17:43:51.529: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f": Phase="Running", Reason="", readiness=true. Elapsed: 26.011766633s
    Feb  6 17:43:51.529: INFO: Pod "dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f" satisfied condition "running"
    STEP: retrieving the pod 02/06/23 17:43:51.529
    STEP: looking for the results for each expected name from probers 02/06/23 17:43:51.533
    Feb  6 17:43:51.539: INFO: Unable to read wheezy_udp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:51.543: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:51.547: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:51.551: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:51.568: INFO: Unable to read jessie_udp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:51.572: INFO: Unable to read jessie_tcp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:51.576: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:51.579: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:51.593: INFO: Lookups using dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f failed for: [wheezy_udp@dns-test-service.dns-4018.svc.cluster.local wheezy_tcp@dns-test-service.dns-4018.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local jessie_udp@dns-test-service.dns-4018.svc.cluster.local jessie_tcp@dns-test-service.dns-4018.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local]

    Feb  6 17:43:56.600: INFO: Unable to read wheezy_udp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:56.604: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:56.608: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:56.612: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:56.639: INFO: Unable to read jessie_udp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:56.643: INFO: Unable to read jessie_tcp@dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:56.647: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:56.651: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local from pod dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f: the server could not find the requested resource (get pods dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f)
    Feb  6 17:43:56.669: INFO: Lookups using dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f failed for: [wheezy_udp@dns-test-service.dns-4018.svc.cluster.local wheezy_tcp@dns-test-service.dns-4018.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local jessie_udp@dns-test-service.dns-4018.svc.cluster.local jessie_tcp@dns-test-service.dns-4018.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4018.svc.cluster.local]

    Feb  6 17:44:01.650: INFO: DNS probes using dns-4018/dns-test-73c392c5-3ce9-4798-afe0-a309b3ca888f succeeded

    STEP: deleting the pod 02/06/23 17:44:01.65
    STEP: deleting the test service 02/06/23 17:44:01.677
    STEP: deleting the test headless service 02/06/23 17:44:01.766
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb  6 17:44:01.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4018" for this suite. 02/06/23 17:44:01.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:44:01.808
Feb  6 17:44:01.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename job 02/06/23 17:44:01.809
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:44:01.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:44:01.839
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 02/06/23 17:44:01.842
STEP: Ensuring active pods == parallelism 02/06/23 17:44:01.849
STEP: Orphaning one of the Job's Pods 02/06/23 17:44:05.855
Feb  6 17:44:06.376: INFO: Successfully updated pod "adopt-release-bjzkw"
STEP: Checking that the Job readopts the Pod 02/06/23 17:44:06.376
Feb  6 17:44:06.376: INFO: Waiting up to 15m0s for pod "adopt-release-bjzkw" in namespace "job-7906" to be "adopted"
Feb  6 17:44:06.390: INFO: Pod "adopt-release-bjzkw": Phase="Running", Reason="", readiness=true. Elapsed: 13.946548ms
Feb  6 17:44:08.394: INFO: Pod "adopt-release-bjzkw": Phase="Running", Reason="", readiness=true. Elapsed: 2.018523716s
Feb  6 17:44:08.394: INFO: Pod "adopt-release-bjzkw" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 02/06/23 17:44:08.394
Feb  6 17:44:08.910: INFO: Successfully updated pod "adopt-release-bjzkw"
STEP: Checking that the Job releases the Pod 02/06/23 17:44:08.91
Feb  6 17:44:08.910: INFO: Waiting up to 15m0s for pod "adopt-release-bjzkw" in namespace "job-7906" to be "released"
Feb  6 17:44:08.924: INFO: Pod "adopt-release-bjzkw": Phase="Running", Reason="", readiness=true. Elapsed: 13.082106ms
Feb  6 17:44:10.928: INFO: Pod "adopt-release-bjzkw": Phase="Running", Reason="", readiness=true. Elapsed: 2.01778666s
Feb  6 17:44:10.928: INFO: Pod "adopt-release-bjzkw" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb  6 17:44:10.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7906" for this suite. 02/06/23 17:44:10.936
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":77,"skipped":1489,"failed":0}
------------------------------
• [SLOW TEST] [9.134 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:44:01.808
    Feb  6 17:44:01.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename job 02/06/23 17:44:01.809
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:44:01.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:44:01.839
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 02/06/23 17:44:01.842
    STEP: Ensuring active pods == parallelism 02/06/23 17:44:01.849
    STEP: Orphaning one of the Job's Pods 02/06/23 17:44:05.855
    Feb  6 17:44:06.376: INFO: Successfully updated pod "adopt-release-bjzkw"
    STEP: Checking that the Job readopts the Pod 02/06/23 17:44:06.376
    Feb  6 17:44:06.376: INFO: Waiting up to 15m0s for pod "adopt-release-bjzkw" in namespace "job-7906" to be "adopted"
    Feb  6 17:44:06.390: INFO: Pod "adopt-release-bjzkw": Phase="Running", Reason="", readiness=true. Elapsed: 13.946548ms
    Feb  6 17:44:08.394: INFO: Pod "adopt-release-bjzkw": Phase="Running", Reason="", readiness=true. Elapsed: 2.018523716s
    Feb  6 17:44:08.394: INFO: Pod "adopt-release-bjzkw" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 02/06/23 17:44:08.394
    Feb  6 17:44:08.910: INFO: Successfully updated pod "adopt-release-bjzkw"
    STEP: Checking that the Job releases the Pod 02/06/23 17:44:08.91
    Feb  6 17:44:08.910: INFO: Waiting up to 15m0s for pod "adopt-release-bjzkw" in namespace "job-7906" to be "released"
    Feb  6 17:44:08.924: INFO: Pod "adopt-release-bjzkw": Phase="Running", Reason="", readiness=true. Elapsed: 13.082106ms
    Feb  6 17:44:10.928: INFO: Pod "adopt-release-bjzkw": Phase="Running", Reason="", readiness=true. Elapsed: 2.01778666s
    Feb  6 17:44:10.928: INFO: Pod "adopt-release-bjzkw" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb  6 17:44:10.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7906" for this suite. 02/06/23 17:44:10.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:44:10.983
Feb  6 17:44:10.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename deployment 02/06/23 17:44:10.985
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:44:10.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:44:11.005
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Feb  6 17:44:11.011: INFO: Creating simple deployment test-new-deployment
Feb  6 17:44:11.025: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
Feb  6 17:44:13.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 44, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 44, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 44, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 44, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 02/06/23 17:44:15.047
STEP: updating a scale subresource 02/06/23 17:44:15.05
STEP: verifying the deployment Spec.Replicas was modified 02/06/23 17:44:15.071
STEP: Patch a scale subresource 02/06/23 17:44:15.078
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb  6 17:44:15.129: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2317  32a23a99-be43-4bc9-8752-d823554345a9 25735 3 2023-02-06 17:44:11 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-06 17:44:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:44:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032ab0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-06 17:44:14 +0000 UTC,LastTransitionTime:2023-02-06 17:44:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-02-06 17:44:14 +0000 UTC,LastTransitionTime:2023-02-06 17:44:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb  6 17:44:15.141: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-2317  8b702614-7792-44fc-9625-f0864cc27845 25741 2 2023-02-06 17:44:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 32a23a99-be43-4bc9-8752-d823554345a9 0xc0032ab487 0xc0032ab488}] [] [{kube-controller-manager Update apps/v1 2023-02-06 17:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32a23a99-be43-4bc9-8752-d823554345a9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:44:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032ab528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb  6 17:44:15.152: INFO: Pod "test-new-deployment-845c8977d9-4gwx8" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-4gwx8 test-new-deployment-845c8977d9- deployment-2317  297f83ae-a129-4ce5-b713-7f8ad103ea06 25743 0 2023-02-06 17:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 8b702614-7792-44fc-9625-f0864cc27845 0xc0032ab8b7 0xc0032ab8b8}] [] [{kube-controller-manager Update v1 2023-02-06 17:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b702614-7792-44fc-9625-f0864cc27845\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-whntd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-whntd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:44:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 17:44:15.153: INFO: Pod "test-new-deployment-845c8977d9-78m7f" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-78m7f test-new-deployment-845c8977d9- deployment-2317  9a13f42d-61fb-4fc5-a710-3b86448dc610 25716 0 2023-02-06 17:44:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 8b702614-7792-44fc-9625-f0864cc27845 0xc0032aba77 0xc0032aba78}] [] [{kube-controller-manager Update v1 2023-02-06 17:44:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b702614-7792-44fc-9625-f0864cc27845\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:44:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wrxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wrxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.188,StartTime:2023-02-06 17:44:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:44:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4c173789ccb96c5610057691c71b939623eeb3763598612c21d39e17e8e6e8a3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb  6 17:44:15.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2317" for this suite. 02/06/23 17:44:15.173
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":78,"skipped":1538,"failed":0}
------------------------------
• [4.248 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:44:10.983
    Feb  6 17:44:10.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename deployment 02/06/23 17:44:10.985
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:44:10.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:44:11.005
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Feb  6 17:44:11.011: INFO: Creating simple deployment test-new-deployment
    Feb  6 17:44:11.025: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    Feb  6 17:44:13.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 44, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 44, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 44, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 44, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 02/06/23 17:44:15.047
    STEP: updating a scale subresource 02/06/23 17:44:15.05
    STEP: verifying the deployment Spec.Replicas was modified 02/06/23 17:44:15.071
    STEP: Patch a scale subresource 02/06/23 17:44:15.078
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb  6 17:44:15.129: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-2317  32a23a99-be43-4bc9-8752-d823554345a9 25735 3 2023-02-06 17:44:11 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-06 17:44:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:44:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032ab0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-06 17:44:14 +0000 UTC,LastTransitionTime:2023-02-06 17:44:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-02-06 17:44:14 +0000 UTC,LastTransitionTime:2023-02-06 17:44:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb  6 17:44:15.141: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-2317  8b702614-7792-44fc-9625-f0864cc27845 25741 2 2023-02-06 17:44:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 32a23a99-be43-4bc9-8752-d823554345a9 0xc0032ab487 0xc0032ab488}] [] [{kube-controller-manager Update apps/v1 2023-02-06 17:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32a23a99-be43-4bc9-8752-d823554345a9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:44:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032ab528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb  6 17:44:15.152: INFO: Pod "test-new-deployment-845c8977d9-4gwx8" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-4gwx8 test-new-deployment-845c8977d9- deployment-2317  297f83ae-a129-4ce5-b713-7f8ad103ea06 25743 0 2023-02-06 17:44:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 8b702614-7792-44fc-9625-f0864cc27845 0xc0032ab8b7 0xc0032ab8b8}] [] [{kube-controller-manager Update v1 2023-02-06 17:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b702614-7792-44fc-9625-f0864cc27845\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-whntd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-whntd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:,StartTime:2023-02-06 17:44:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 17:44:15.153: INFO: Pod "test-new-deployment-845c8977d9-78m7f" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-78m7f test-new-deployment-845c8977d9- deployment-2317  9a13f42d-61fb-4fc5-a710-3b86448dc610 25716 0 2023-02-06 17:44:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 8b702614-7792-44fc-9625-f0864cc27845 0xc0032aba77 0xc0032aba78}] [] [{kube-controller-manager Update v1 2023-02-06 17:44:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b702614-7792-44fc-9625-f0864cc27845\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:44:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wrxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wrxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:44:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.188,StartTime:2023-02-06 17:44:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 17:44:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4c173789ccb96c5610057691c71b939623eeb3763598612c21d39e17e8e6e8a3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb  6 17:44:15.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2317" for this suite. 02/06/23 17:44:15.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:44:15.237
Feb  6 17:44:15.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 17:44:15.238
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:44:15.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:44:15.295
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 02/06/23 17:44:15.298
STEP: fetching the ConfigMap 02/06/23 17:44:15.311
STEP: patching the ConfigMap 02/06/23 17:44:15.32
STEP: listing all ConfigMaps in all namespaces with a label selector 02/06/23 17:44:15.336
STEP: deleting the ConfigMap by collection with a label selector 02/06/23 17:44:15.35
STEP: listing all ConfigMaps in test namespace 02/06/23 17:44:15.376
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 17:44:15.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6746" for this suite. 02/06/23 17:44:15.388
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":79,"skipped":1556,"failed":0}
------------------------------
• [0.159 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:44:15.237
    Feb  6 17:44:15.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 17:44:15.238
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:44:15.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:44:15.295
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 02/06/23 17:44:15.298
    STEP: fetching the ConfigMap 02/06/23 17:44:15.311
    STEP: patching the ConfigMap 02/06/23 17:44:15.32
    STEP: listing all ConfigMaps in all namespaces with a label selector 02/06/23 17:44:15.336
    STEP: deleting the ConfigMap by collection with a label selector 02/06/23 17:44:15.35
    STEP: listing all ConfigMaps in test namespace 02/06/23 17:44:15.376
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 17:44:15.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6746" for this suite. 02/06/23 17:44:15.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:44:15.402
Feb  6 17:44:15.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 17:44:15.403
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:44:15.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:44:15.419
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 02/06/23 17:44:15.422
Feb  6 17:44:15.431: INFO: Waiting up to 5m0s for pod "labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee" in namespace "downward-api-3804" to be "running and ready"
Feb  6 17:44:15.437: INFO: Pod "labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370109ms
Feb  6 17:44:15.437: INFO: The phase of Pod labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:44:17.442: INFO: Pod "labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010907924s
Feb  6 17:44:17.442: INFO: The phase of Pod labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:44:19.443: INFO: Pod "labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee": Phase="Running", Reason="", readiness=true. Elapsed: 4.011964895s
Feb  6 17:44:19.443: INFO: The phase of Pod labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee is Running (Ready = true)
Feb  6 17:44:19.443: INFO: Pod "labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee" satisfied condition "running and ready"
Feb  6 17:44:19.979: INFO: Successfully updated pod "labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb  6 17:44:24.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3804" for this suite. 02/06/23 17:44:24.029
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":80,"skipped":1588,"failed":0}
------------------------------
• [SLOW TEST] [8.634 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:44:15.402
    Feb  6 17:44:15.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 17:44:15.403
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:44:15.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:44:15.419
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 02/06/23 17:44:15.422
    Feb  6 17:44:15.431: INFO: Waiting up to 5m0s for pod "labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee" in namespace "downward-api-3804" to be "running and ready"
    Feb  6 17:44:15.437: INFO: Pod "labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370109ms
    Feb  6 17:44:15.437: INFO: The phase of Pod labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:44:17.442: INFO: Pod "labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010907924s
    Feb  6 17:44:17.442: INFO: The phase of Pod labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:44:19.443: INFO: Pod "labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee": Phase="Running", Reason="", readiness=true. Elapsed: 4.011964895s
    Feb  6 17:44:19.443: INFO: The phase of Pod labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee is Running (Ready = true)
    Feb  6 17:44:19.443: INFO: Pod "labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee" satisfied condition "running and ready"
    Feb  6 17:44:19.979: INFO: Successfully updated pod "labelsupdate76f91540-6fe7-47ed-9586-22c514d354ee"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb  6 17:44:24.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3804" for this suite. 02/06/23 17:44:24.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:44:24.042
Feb  6 17:44:24.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename sched-preemption 02/06/23 17:44:24.043
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:44:24.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:44:24.066
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb  6 17:44:24.083: INFO: Waiting up to 1m0s for all nodes to be ready
Feb  6 17:45:24.150: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:45:24.154
Feb  6 17:45:24.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename sched-preemption-path 02/06/23 17:45:24.155
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:45:24.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:45:24.18
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Feb  6 17:45:24.196: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Feb  6 17:45:24.199: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Feb  6 17:45:24.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6318" for this suite. 02/06/23 17:45:24.228
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb  6 17:45:24.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9025" for this suite. 02/06/23 17:45:24.254
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":81,"skipped":1600,"failed":0}
------------------------------
• [SLOW TEST] [60.269 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:44:24.042
    Feb  6 17:44:24.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename sched-preemption 02/06/23 17:44:24.043
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:44:24.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:44:24.066
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb  6 17:44:24.083: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb  6 17:45:24.150: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:45:24.154
    Feb  6 17:45:24.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename sched-preemption-path 02/06/23 17:45:24.155
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:45:24.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:45:24.18
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Feb  6 17:45:24.196: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Feb  6 17:45:24.199: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Feb  6 17:45:24.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-6318" for this suite. 02/06/23 17:45:24.228
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 17:45:24.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9025" for this suite. 02/06/23 17:45:24.254
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:45:24.316
Feb  6 17:45:24.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-runtime 02/06/23 17:45:24.317
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:45:24.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:45:24.338
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 02/06/23 17:45:24.341
STEP: wait for the container to reach Succeeded 02/06/23 17:45:24.354
STEP: get the container status 02/06/23 17:45:30.386
STEP: the container should be terminated 02/06/23 17:45:30.39
STEP: the termination message should be set 02/06/23 17:45:30.391
Feb  6 17:45:30.391: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 02/06/23 17:45:30.391
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb  6 17:45:30.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8742" for this suite. 02/06/23 17:45:30.414
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":82,"skipped":1614,"failed":0}
------------------------------
• [SLOW TEST] [6.105 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:45:24.316
    Feb  6 17:45:24.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-runtime 02/06/23 17:45:24.317
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:45:24.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:45:24.338
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 02/06/23 17:45:24.341
    STEP: wait for the container to reach Succeeded 02/06/23 17:45:24.354
    STEP: get the container status 02/06/23 17:45:30.386
    STEP: the container should be terminated 02/06/23 17:45:30.39
    STEP: the termination message should be set 02/06/23 17:45:30.391
    Feb  6 17:45:30.391: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 02/06/23 17:45:30.391
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb  6 17:45:30.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8742" for this suite. 02/06/23 17:45:30.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:45:30.425
Feb  6 17:45:30.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 17:45:30.426
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:45:30.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:45:30.445
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 02/06/23 17:45:30.449
Feb  6 17:45:30.460: INFO: Waiting up to 5m0s for pod "pod-485323a6-ac3d-41ac-bba3-72c3aec25f55" in namespace "emptydir-3543" to be "Succeeded or Failed"
Feb  6 17:45:30.471: INFO: Pod "pod-485323a6-ac3d-41ac-bba3-72c3aec25f55": Phase="Pending", Reason="", readiness=false. Elapsed: 11.041829ms
Feb  6 17:45:32.477: INFO: Pod "pod-485323a6-ac3d-41ac-bba3-72c3aec25f55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016527911s
Feb  6 17:45:34.476: INFO: Pod "pod-485323a6-ac3d-41ac-bba3-72c3aec25f55": Phase="Running", Reason="", readiness=false. Elapsed: 4.016176011s
Feb  6 17:45:36.476: INFO: Pod "pod-485323a6-ac3d-41ac-bba3-72c3aec25f55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016273364s
STEP: Saw pod success 02/06/23 17:45:36.477
Feb  6 17:45:36.477: INFO: Pod "pod-485323a6-ac3d-41ac-bba3-72c3aec25f55" satisfied condition "Succeeded or Failed"
Feb  6 17:45:36.492: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-485323a6-ac3d-41ac-bba3-72c3aec25f55 container test-container: <nil>
STEP: delete the pod 02/06/23 17:45:36.521
Feb  6 17:45:36.554: INFO: Waiting for pod pod-485323a6-ac3d-41ac-bba3-72c3aec25f55 to disappear
Feb  6 17:45:36.562: INFO: Pod pod-485323a6-ac3d-41ac-bba3-72c3aec25f55 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 17:45:36.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3543" for this suite. 02/06/23 17:45:36.572
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":83,"skipped":1645,"failed":0}
------------------------------
• [SLOW TEST] [6.167 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:45:30.425
    Feb  6 17:45:30.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 17:45:30.426
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:45:30.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:45:30.445
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/06/23 17:45:30.449
    Feb  6 17:45:30.460: INFO: Waiting up to 5m0s for pod "pod-485323a6-ac3d-41ac-bba3-72c3aec25f55" in namespace "emptydir-3543" to be "Succeeded or Failed"
    Feb  6 17:45:30.471: INFO: Pod "pod-485323a6-ac3d-41ac-bba3-72c3aec25f55": Phase="Pending", Reason="", readiness=false. Elapsed: 11.041829ms
    Feb  6 17:45:32.477: INFO: Pod "pod-485323a6-ac3d-41ac-bba3-72c3aec25f55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016527911s
    Feb  6 17:45:34.476: INFO: Pod "pod-485323a6-ac3d-41ac-bba3-72c3aec25f55": Phase="Running", Reason="", readiness=false. Elapsed: 4.016176011s
    Feb  6 17:45:36.476: INFO: Pod "pod-485323a6-ac3d-41ac-bba3-72c3aec25f55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016273364s
    STEP: Saw pod success 02/06/23 17:45:36.477
    Feb  6 17:45:36.477: INFO: Pod "pod-485323a6-ac3d-41ac-bba3-72c3aec25f55" satisfied condition "Succeeded or Failed"
    Feb  6 17:45:36.492: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-485323a6-ac3d-41ac-bba3-72c3aec25f55 container test-container: <nil>
    STEP: delete the pod 02/06/23 17:45:36.521
    Feb  6 17:45:36.554: INFO: Waiting for pod pod-485323a6-ac3d-41ac-bba3-72c3aec25f55 to disappear
    Feb  6 17:45:36.562: INFO: Pod pod-485323a6-ac3d-41ac-bba3-72c3aec25f55 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 17:45:36.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3543" for this suite. 02/06/23 17:45:36.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:45:36.605
Feb  6 17:45:36.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename security-context-test 02/06/23 17:45:36.606
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:45:36.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:45:36.687
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Feb  6 17:45:36.700: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78" in namespace "security-context-test-6098" to be "Succeeded or Failed"
Feb  6 17:45:36.704: INFO: Pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980315ms
Feb  6 17:45:38.710: INFO: Pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010281764s
Feb  6 17:45:40.709: INFO: Pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009597256s
Feb  6 17:45:42.708: INFO: Pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008674231s
Feb  6 17:45:42.708: INFO: Pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78" satisfied condition "Succeeded or Failed"
Feb  6 17:45:42.716: INFO: Got logs for pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb  6 17:45:42.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6098" for this suite. 02/06/23 17:45:42.72
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":84,"skipped":1700,"failed":0}
------------------------------
• [SLOW TEST] [6.123 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:45:36.605
    Feb  6 17:45:36.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename security-context-test 02/06/23 17:45:36.606
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:45:36.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:45:36.687
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Feb  6 17:45:36.700: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78" in namespace "security-context-test-6098" to be "Succeeded or Failed"
    Feb  6 17:45:36.704: INFO: Pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980315ms
    Feb  6 17:45:38.710: INFO: Pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010281764s
    Feb  6 17:45:40.709: INFO: Pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009597256s
    Feb  6 17:45:42.708: INFO: Pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008674231s
    Feb  6 17:45:42.708: INFO: Pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78" satisfied condition "Succeeded or Failed"
    Feb  6 17:45:42.716: INFO: Got logs for pod "busybox-privileged-false-5db829a0-3690-4e25-b6be-a3eebeeb2b78": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb  6 17:45:42.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-6098" for this suite. 02/06/23 17:45:42.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:45:42.729
Feb  6 17:45:42.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 17:45:42.732
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:45:42.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:45:42.757
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-8556 02/06/23 17:45:42.76
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8556 to expose endpoints map[] 02/06/23 17:45:42.785
Feb  6 17:45:42.795: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Feb  6 17:45:43.804: INFO: successfully validated that service multi-endpoint-test in namespace services-8556 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8556 02/06/23 17:45:43.804
Feb  6 17:45:43.813: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8556" to be "running and ready"
Feb  6 17:45:43.818: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.499533ms
Feb  6 17:45:43.818: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:45:45.822: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009088476s
Feb  6 17:45:45.823: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:45:47.821: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.008243028s
Feb  6 17:45:47.821: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb  6 17:45:47.821: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8556 to expose endpoints map[pod1:[100]] 02/06/23 17:45:47.825
Feb  6 17:45:47.843: INFO: successfully validated that service multi-endpoint-test in namespace services-8556 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8556 02/06/23 17:45:47.843
Feb  6 17:45:47.851: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8556" to be "running and ready"
Feb  6 17:45:47.857: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.566911ms
Feb  6 17:45:47.857: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:45:49.861: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009779248s
Feb  6 17:45:49.861: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:45:51.862: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.010820029s
Feb  6 17:45:51.862: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb  6 17:45:51.862: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8556 to expose endpoints map[pod1:[100] pod2:[101]] 02/06/23 17:45:51.865
Feb  6 17:45:51.878: INFO: successfully validated that service multi-endpoint-test in namespace services-8556 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 02/06/23 17:45:51.878
Feb  6 17:45:51.878: INFO: Creating new exec pod
Feb  6 17:45:51.885: INFO: Waiting up to 5m0s for pod "execpodz58z5" in namespace "services-8556" to be "running"
Feb  6 17:45:51.889: INFO: Pod "execpodz58z5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023449ms
Feb  6 17:45:53.895: INFO: Pod "execpodz58z5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009492189s
Feb  6 17:45:55.894: INFO: Pod "execpodz58z5": Phase="Running", Reason="", readiness=true. Elapsed: 4.008942095s
Feb  6 17:45:55.894: INFO: Pod "execpodz58z5" satisfied condition "running"
Feb  6 17:45:56.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8556 exec execpodz58z5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Feb  6 17:45:57.068: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Feb  6 17:45:57.068: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 17:45:57.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8556 exec execpodz58z5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.243.234 80'
Feb  6 17:45:57.231: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.243.234 80\nConnection to 10.100.243.234 80 port [tcp/http] succeeded!\n"
Feb  6 17:45:57.231: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 17:45:57.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8556 exec execpodz58z5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Feb  6 17:45:57.395: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Feb  6 17:45:57.395: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 17:45:57.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8556 exec execpodz58z5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.243.234 81'
Feb  6 17:45:57.561: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.243.234 81\nConnection to 10.100.243.234 81 port [tcp/*] succeeded!\n"
Feb  6 17:45:57.561: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8556 02/06/23 17:45:57.561
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8556 to expose endpoints map[pod2:[101]] 02/06/23 17:45:57.586
Feb  6 17:45:58.662: INFO: successfully validated that service multi-endpoint-test in namespace services-8556 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8556 02/06/23 17:45:58.663
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8556 to expose endpoints map[] 02/06/23 17:45:58.687
Feb  6 17:45:58.703: INFO: successfully validated that service multi-endpoint-test in namespace services-8556 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 17:45:58.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8556" for this suite. 02/06/23 17:45:58.741
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":85,"skipped":1710,"failed":0}
------------------------------
• [SLOW TEST] [16.034 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:45:42.729
    Feb  6 17:45:42.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 17:45:42.732
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:45:42.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:45:42.757
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-8556 02/06/23 17:45:42.76
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8556 to expose endpoints map[] 02/06/23 17:45:42.785
    Feb  6 17:45:42.795: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Feb  6 17:45:43.804: INFO: successfully validated that service multi-endpoint-test in namespace services-8556 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8556 02/06/23 17:45:43.804
    Feb  6 17:45:43.813: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8556" to be "running and ready"
    Feb  6 17:45:43.818: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.499533ms
    Feb  6 17:45:43.818: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:45:45.822: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009088476s
    Feb  6 17:45:45.823: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:45:47.821: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.008243028s
    Feb  6 17:45:47.821: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb  6 17:45:47.821: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8556 to expose endpoints map[pod1:[100]] 02/06/23 17:45:47.825
    Feb  6 17:45:47.843: INFO: successfully validated that service multi-endpoint-test in namespace services-8556 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-8556 02/06/23 17:45:47.843
    Feb  6 17:45:47.851: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8556" to be "running and ready"
    Feb  6 17:45:47.857: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.566911ms
    Feb  6 17:45:47.857: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:45:49.861: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009779248s
    Feb  6 17:45:49.861: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:45:51.862: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.010820029s
    Feb  6 17:45:51.862: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb  6 17:45:51.862: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8556 to expose endpoints map[pod1:[100] pod2:[101]] 02/06/23 17:45:51.865
    Feb  6 17:45:51.878: INFO: successfully validated that service multi-endpoint-test in namespace services-8556 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 02/06/23 17:45:51.878
    Feb  6 17:45:51.878: INFO: Creating new exec pod
    Feb  6 17:45:51.885: INFO: Waiting up to 5m0s for pod "execpodz58z5" in namespace "services-8556" to be "running"
    Feb  6 17:45:51.889: INFO: Pod "execpodz58z5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023449ms
    Feb  6 17:45:53.895: INFO: Pod "execpodz58z5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009492189s
    Feb  6 17:45:55.894: INFO: Pod "execpodz58z5": Phase="Running", Reason="", readiness=true. Elapsed: 4.008942095s
    Feb  6 17:45:55.894: INFO: Pod "execpodz58z5" satisfied condition "running"
    Feb  6 17:45:56.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8556 exec execpodz58z5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Feb  6 17:45:57.068: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Feb  6 17:45:57.068: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 17:45:57.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8556 exec execpodz58z5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.243.234 80'
    Feb  6 17:45:57.231: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.243.234 80\nConnection to 10.100.243.234 80 port [tcp/http] succeeded!\n"
    Feb  6 17:45:57.231: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 17:45:57.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8556 exec execpodz58z5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Feb  6 17:45:57.395: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Feb  6 17:45:57.395: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 17:45:57.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8556 exec execpodz58z5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.243.234 81'
    Feb  6 17:45:57.561: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.243.234 81\nConnection to 10.100.243.234 81 port [tcp/*] succeeded!\n"
    Feb  6 17:45:57.561: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-8556 02/06/23 17:45:57.561
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8556 to expose endpoints map[pod2:[101]] 02/06/23 17:45:57.586
    Feb  6 17:45:58.662: INFO: successfully validated that service multi-endpoint-test in namespace services-8556 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-8556 02/06/23 17:45:58.663
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8556 to expose endpoints map[] 02/06/23 17:45:58.687
    Feb  6 17:45:58.703: INFO: successfully validated that service multi-endpoint-test in namespace services-8556 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 17:45:58.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8556" for this suite. 02/06/23 17:45:58.741
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:45:58.773
Feb  6 17:45:58.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename subpath 02/06/23 17:45:58.776
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:45:58.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:45:58.804
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/06/23 17:45:58.807
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-jntg 02/06/23 17:45:58.818
STEP: Creating a pod to test atomic-volume-subpath 02/06/23 17:45:58.818
Feb  6 17:45:58.829: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jntg" in namespace "subpath-604" to be "Succeeded or Failed"
Feb  6 17:45:58.839: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Pending", Reason="", readiness=false. Elapsed: 10.346725ms
Feb  6 17:46:00.843: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014094156s
Feb  6 17:46:02.844: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 4.014732372s
Feb  6 17:46:04.844: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 6.014606562s
Feb  6 17:46:06.843: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 8.01409697s
Feb  6 17:46:08.845: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 10.016435652s
Feb  6 17:46:10.844: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 12.015468806s
Feb  6 17:46:12.843: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 14.014394073s
Feb  6 17:46:14.843: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 16.014354801s
Feb  6 17:46:16.844: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 18.014808234s
Feb  6 17:46:18.844: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 20.015324521s
Feb  6 17:46:20.845: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 22.015702371s
Feb  6 17:46:22.843: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=false. Elapsed: 24.013885265s
Feb  6 17:46:24.843: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.014135422s
STEP: Saw pod success 02/06/23 17:46:24.843
Feb  6 17:46:24.843: INFO: Pod "pod-subpath-test-configmap-jntg" satisfied condition "Succeeded or Failed"
Feb  6 17:46:24.847: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-subpath-test-configmap-jntg container test-container-subpath-configmap-jntg: <nil>
STEP: delete the pod 02/06/23 17:46:24.855
Feb  6 17:46:24.868: INFO: Waiting for pod pod-subpath-test-configmap-jntg to disappear
Feb  6 17:46:24.871: INFO: Pod pod-subpath-test-configmap-jntg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jntg 02/06/23 17:46:24.871
Feb  6 17:46:24.872: INFO: Deleting pod "pod-subpath-test-configmap-jntg" in namespace "subpath-604"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb  6 17:46:24.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-604" for this suite. 02/06/23 17:46:24.879
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":86,"skipped":1743,"failed":0}
------------------------------
• [SLOW TEST] [26.113 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:45:58.773
    Feb  6 17:45:58.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename subpath 02/06/23 17:45:58.776
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:45:58.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:45:58.804
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/06/23 17:45:58.807
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-jntg 02/06/23 17:45:58.818
    STEP: Creating a pod to test atomic-volume-subpath 02/06/23 17:45:58.818
    Feb  6 17:45:58.829: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jntg" in namespace "subpath-604" to be "Succeeded or Failed"
    Feb  6 17:45:58.839: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Pending", Reason="", readiness=false. Elapsed: 10.346725ms
    Feb  6 17:46:00.843: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014094156s
    Feb  6 17:46:02.844: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 4.014732372s
    Feb  6 17:46:04.844: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 6.014606562s
    Feb  6 17:46:06.843: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 8.01409697s
    Feb  6 17:46:08.845: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 10.016435652s
    Feb  6 17:46:10.844: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 12.015468806s
    Feb  6 17:46:12.843: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 14.014394073s
    Feb  6 17:46:14.843: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 16.014354801s
    Feb  6 17:46:16.844: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 18.014808234s
    Feb  6 17:46:18.844: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 20.015324521s
    Feb  6 17:46:20.845: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=true. Elapsed: 22.015702371s
    Feb  6 17:46:22.843: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Running", Reason="", readiness=false. Elapsed: 24.013885265s
    Feb  6 17:46:24.843: INFO: Pod "pod-subpath-test-configmap-jntg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.014135422s
    STEP: Saw pod success 02/06/23 17:46:24.843
    Feb  6 17:46:24.843: INFO: Pod "pod-subpath-test-configmap-jntg" satisfied condition "Succeeded or Failed"
    Feb  6 17:46:24.847: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-subpath-test-configmap-jntg container test-container-subpath-configmap-jntg: <nil>
    STEP: delete the pod 02/06/23 17:46:24.855
    Feb  6 17:46:24.868: INFO: Waiting for pod pod-subpath-test-configmap-jntg to disappear
    Feb  6 17:46:24.871: INFO: Pod pod-subpath-test-configmap-jntg no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-jntg 02/06/23 17:46:24.871
    Feb  6 17:46:24.872: INFO: Deleting pod "pod-subpath-test-configmap-jntg" in namespace "subpath-604"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb  6 17:46:24.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-604" for this suite. 02/06/23 17:46:24.879
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:46:24.889
Feb  6 17:46:24.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename disruption 02/06/23 17:46:24.891
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:24.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:24.91
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:46:24.913
Feb  6 17:46:24.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename disruption-2 02/06/23 17:46:24.914
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:24.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:24.931
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 02/06/23 17:46:24.939
STEP: Waiting for the pdb to be processed 02/06/23 17:46:26.96
STEP: Waiting for the pdb to be processed 02/06/23 17:46:28.979
STEP: listing a collection of PDBs across all namespaces 02/06/23 17:46:30.992
STEP: listing a collection of PDBs in namespace disruption-9065 02/06/23 17:46:30.996
STEP: deleting a collection of PDBs 02/06/23 17:46:30.998
STEP: Waiting for the PDB collection to be deleted 02/06/23 17:46:31.01
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Feb  6 17:46:31.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-8929" for this suite. 02/06/23 17:46:31.017
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb  6 17:46:31.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9065" for this suite. 02/06/23 17:46:31.029
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":87,"skipped":1745,"failed":0}
------------------------------
• [SLOW TEST] [6.145 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:46:24.889
    Feb  6 17:46:24.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename disruption 02/06/23 17:46:24.891
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:24.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:24.91
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:46:24.913
    Feb  6 17:46:24.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename disruption-2 02/06/23 17:46:24.914
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:24.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:24.931
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 02/06/23 17:46:24.939
    STEP: Waiting for the pdb to be processed 02/06/23 17:46:26.96
    STEP: Waiting for the pdb to be processed 02/06/23 17:46:28.979
    STEP: listing a collection of PDBs across all namespaces 02/06/23 17:46:30.992
    STEP: listing a collection of PDBs in namespace disruption-9065 02/06/23 17:46:30.996
    STEP: deleting a collection of PDBs 02/06/23 17:46:30.998
    STEP: Waiting for the PDB collection to be deleted 02/06/23 17:46:31.01
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Feb  6 17:46:31.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-8929" for this suite. 02/06/23 17:46:31.017
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb  6 17:46:31.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9065" for this suite. 02/06/23 17:46:31.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:46:31.035
Feb  6 17:46:31.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 17:46:31.037
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:31.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:31.059
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 02/06/23 17:46:31.062
Feb  6 17:46:31.071: INFO: Waiting up to 5m0s for pod "annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3" in namespace "downward-api-5905" to be "running and ready"
Feb  6 17:46:31.080: INFO: Pod "annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.237428ms
Feb  6 17:46:31.080: INFO: The phase of Pod annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:46:33.084: INFO: Pod "annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013551264s
Feb  6 17:46:33.084: INFO: The phase of Pod annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:46:35.087: INFO: Pod "annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.01575522s
Feb  6 17:46:35.087: INFO: The phase of Pod annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3 is Running (Ready = true)
Feb  6 17:46:35.087: INFO: Pod "annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3" satisfied condition "running and ready"
Feb  6 17:46:35.612: INFO: Successfully updated pod "annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb  6 17:46:37.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5905" for this suite. 02/06/23 17:46:37.645
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":88,"skipped":1757,"failed":0}
------------------------------
• [SLOW TEST] [6.618 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:46:31.035
    Feb  6 17:46:31.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 17:46:31.037
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:31.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:31.059
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 02/06/23 17:46:31.062
    Feb  6 17:46:31.071: INFO: Waiting up to 5m0s for pod "annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3" in namespace "downward-api-5905" to be "running and ready"
    Feb  6 17:46:31.080: INFO: Pod "annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.237428ms
    Feb  6 17:46:31.080: INFO: The phase of Pod annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:46:33.084: INFO: Pod "annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013551264s
    Feb  6 17:46:33.084: INFO: The phase of Pod annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:46:35.087: INFO: Pod "annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.01575522s
    Feb  6 17:46:35.087: INFO: The phase of Pod annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3 is Running (Ready = true)
    Feb  6 17:46:35.087: INFO: Pod "annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3" satisfied condition "running and ready"
    Feb  6 17:46:35.612: INFO: Successfully updated pod "annotationupdate189be4ca-8a6a-4627-8409-fb0c8964e1d3"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb  6 17:46:37.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5905" for this suite. 02/06/23 17:46:37.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:46:37.658
Feb  6 17:46:37.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 17:46:37.659
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:37.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:37.676
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/06/23 17:46:37.68
Feb  6 17:46:37.690: INFO: Waiting up to 5m0s for pod "pod-4dd4d0d3-3c44-4db6-802d-681dc0869966" in namespace "emptydir-7543" to be "Succeeded or Failed"
Feb  6 17:46:37.696: INFO: Pod "pod-4dd4d0d3-3c44-4db6-802d-681dc0869966": Phase="Pending", Reason="", readiness=false. Elapsed: 6.279314ms
Feb  6 17:46:39.703: INFO: Pod "pod-4dd4d0d3-3c44-4db6-802d-681dc0869966": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013425404s
Feb  6 17:46:41.700: INFO: Pod "pod-4dd4d0d3-3c44-4db6-802d-681dc0869966": Phase="Running", Reason="", readiness=false. Elapsed: 4.010462969s
Feb  6 17:46:43.701: INFO: Pod "pod-4dd4d0d3-3c44-4db6-802d-681dc0869966": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01127778s
STEP: Saw pod success 02/06/23 17:46:43.701
Feb  6 17:46:43.702: INFO: Pod "pod-4dd4d0d3-3c44-4db6-802d-681dc0869966" satisfied condition "Succeeded or Failed"
Feb  6 17:46:43.706: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-4dd4d0d3-3c44-4db6-802d-681dc0869966 container test-container: <nil>
STEP: delete the pod 02/06/23 17:46:43.713
Feb  6 17:46:43.724: INFO: Waiting for pod pod-4dd4d0d3-3c44-4db6-802d-681dc0869966 to disappear
Feb  6 17:46:43.728: INFO: Pod pod-4dd4d0d3-3c44-4db6-802d-681dc0869966 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 17:46:43.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7543" for this suite. 02/06/23 17:46:43.733
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":89,"skipped":1768,"failed":0}
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:46:37.658
    Feb  6 17:46:37.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 17:46:37.659
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:37.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:37.676
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/06/23 17:46:37.68
    Feb  6 17:46:37.690: INFO: Waiting up to 5m0s for pod "pod-4dd4d0d3-3c44-4db6-802d-681dc0869966" in namespace "emptydir-7543" to be "Succeeded or Failed"
    Feb  6 17:46:37.696: INFO: Pod "pod-4dd4d0d3-3c44-4db6-802d-681dc0869966": Phase="Pending", Reason="", readiness=false. Elapsed: 6.279314ms
    Feb  6 17:46:39.703: INFO: Pod "pod-4dd4d0d3-3c44-4db6-802d-681dc0869966": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013425404s
    Feb  6 17:46:41.700: INFO: Pod "pod-4dd4d0d3-3c44-4db6-802d-681dc0869966": Phase="Running", Reason="", readiness=false. Elapsed: 4.010462969s
    Feb  6 17:46:43.701: INFO: Pod "pod-4dd4d0d3-3c44-4db6-802d-681dc0869966": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01127778s
    STEP: Saw pod success 02/06/23 17:46:43.701
    Feb  6 17:46:43.702: INFO: Pod "pod-4dd4d0d3-3c44-4db6-802d-681dc0869966" satisfied condition "Succeeded or Failed"
    Feb  6 17:46:43.706: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-4dd4d0d3-3c44-4db6-802d-681dc0869966 container test-container: <nil>
    STEP: delete the pod 02/06/23 17:46:43.713
    Feb  6 17:46:43.724: INFO: Waiting for pod pod-4dd4d0d3-3c44-4db6-802d-681dc0869966 to disappear
    Feb  6 17:46:43.728: INFO: Pod pod-4dd4d0d3-3c44-4db6-802d-681dc0869966 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 17:46:43.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7543" for this suite. 02/06/23 17:46:43.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:46:43.744
Feb  6 17:46:43.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename replication-controller 02/06/23 17:46:43.745
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:43.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:43.77
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 02/06/23 17:46:43.772
Feb  6 17:46:43.780: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8630" to be "running and ready"
Feb  6 17:46:43.786: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.812659ms
Feb  6 17:46:43.786: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:46:45.790: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010034101s
Feb  6 17:46:45.790: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:46:47.794: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.01389998s
Feb  6 17:46:47.794: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Feb  6 17:46:47.794: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 02/06/23 17:46:47.798
STEP: Then the orphan pod is adopted 02/06/23 17:46:47.806
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb  6 17:46:48.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8630" for this suite. 02/06/23 17:46:48.818
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":90,"skipped":1783,"failed":0}
------------------------------
• [SLOW TEST] [5.082 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:46:43.744
    Feb  6 17:46:43.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename replication-controller 02/06/23 17:46:43.745
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:43.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:43.77
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 02/06/23 17:46:43.772
    Feb  6 17:46:43.780: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8630" to be "running and ready"
    Feb  6 17:46:43.786: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.812659ms
    Feb  6 17:46:43.786: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:46:45.790: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010034101s
    Feb  6 17:46:45.790: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:46:47.794: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.01389998s
    Feb  6 17:46:47.794: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Feb  6 17:46:47.794: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 02/06/23 17:46:47.798
    STEP: Then the orphan pod is adopted 02/06/23 17:46:47.806
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb  6 17:46:48.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8630" for this suite. 02/06/23 17:46:48.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:46:48.829
Feb  6 17:46:48.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 17:46:48.83
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:48.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:48.86
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 02/06/23 17:46:48.865
Feb  6 17:46:48.877: INFO: Waiting up to 5m0s for pod "downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895" in namespace "projected-3874" to be "Succeeded or Failed"
Feb  6 17:46:48.883: INFO: Pod "downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895": Phase="Pending", Reason="", readiness=false. Elapsed: 6.119752ms
Feb  6 17:46:50.889: INFO: Pod "downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012418322s
Feb  6 17:46:52.888: INFO: Pod "downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895": Phase="Running", Reason="", readiness=false. Elapsed: 4.011581715s
Feb  6 17:46:54.888: INFO: Pod "downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01102745s
STEP: Saw pod success 02/06/23 17:46:54.888
Feb  6 17:46:54.888: INFO: Pod "downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895" satisfied condition "Succeeded or Failed"
Feb  6 17:46:54.891: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895 container client-container: <nil>
STEP: delete the pod 02/06/23 17:46:54.897
Feb  6 17:46:54.908: INFO: Waiting for pod downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895 to disappear
Feb  6 17:46:54.911: INFO: Pod downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb  6 17:46:54.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3874" for this suite. 02/06/23 17:46:54.916
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":91,"skipped":1818,"failed":0}
------------------------------
• [SLOW TEST] [6.094 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:46:48.829
    Feb  6 17:46:48.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 17:46:48.83
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:48.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:48.86
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 02/06/23 17:46:48.865
    Feb  6 17:46:48.877: INFO: Waiting up to 5m0s for pod "downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895" in namespace "projected-3874" to be "Succeeded or Failed"
    Feb  6 17:46:48.883: INFO: Pod "downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895": Phase="Pending", Reason="", readiness=false. Elapsed: 6.119752ms
    Feb  6 17:46:50.889: INFO: Pod "downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012418322s
    Feb  6 17:46:52.888: INFO: Pod "downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895": Phase="Running", Reason="", readiness=false. Elapsed: 4.011581715s
    Feb  6 17:46:54.888: INFO: Pod "downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01102745s
    STEP: Saw pod success 02/06/23 17:46:54.888
    Feb  6 17:46:54.888: INFO: Pod "downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895" satisfied condition "Succeeded or Failed"
    Feb  6 17:46:54.891: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895 container client-container: <nil>
    STEP: delete the pod 02/06/23 17:46:54.897
    Feb  6 17:46:54.908: INFO: Waiting for pod downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895 to disappear
    Feb  6 17:46:54.911: INFO: Pod downwardapi-volume-62c924f3-ca08-4209-ba61-47106a857895 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb  6 17:46:54.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3874" for this suite. 02/06/23 17:46:54.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:46:54.927
Feb  6 17:46:54.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 17:46:54.928
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:54.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:54.949
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 02/06/23 17:46:54.952
Feb  6 17:46:54.959: INFO: Waiting up to 5m0s for pod "pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a" in namespace "emptydir-2252" to be "Succeeded or Failed"
Feb  6 17:46:54.966: INFO: Pod "pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.052123ms
Feb  6 17:46:56.970: INFO: Pod "pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010333696s
Feb  6 17:46:58.971: INFO: Pod "pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011775385s
Feb  6 17:47:00.971: INFO: Pod "pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011934089s
STEP: Saw pod success 02/06/23 17:47:00.971
Feb  6 17:47:00.972: INFO: Pod "pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a" satisfied condition "Succeeded or Failed"
Feb  6 17:47:00.974: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a container test-container: <nil>
STEP: delete the pod 02/06/23 17:47:00.981
Feb  6 17:47:00.995: INFO: Waiting for pod pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a to disappear
Feb  6 17:47:00.998: INFO: Pod pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 17:47:00.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2252" for this suite. 02/06/23 17:47:01.003
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":92,"skipped":1827,"failed":0}
------------------------------
• [SLOW TEST] [6.082 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:46:54.927
    Feb  6 17:46:54.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 17:46:54.928
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:46:54.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:46:54.949
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/06/23 17:46:54.952
    Feb  6 17:46:54.959: INFO: Waiting up to 5m0s for pod "pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a" in namespace "emptydir-2252" to be "Succeeded or Failed"
    Feb  6 17:46:54.966: INFO: Pod "pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.052123ms
    Feb  6 17:46:56.970: INFO: Pod "pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010333696s
    Feb  6 17:46:58.971: INFO: Pod "pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011775385s
    Feb  6 17:47:00.971: INFO: Pod "pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011934089s
    STEP: Saw pod success 02/06/23 17:47:00.971
    Feb  6 17:47:00.972: INFO: Pod "pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a" satisfied condition "Succeeded or Failed"
    Feb  6 17:47:00.974: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a container test-container: <nil>
    STEP: delete the pod 02/06/23 17:47:00.981
    Feb  6 17:47:00.995: INFO: Waiting for pod pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a to disappear
    Feb  6 17:47:00.998: INFO: Pod pod-e5ac671d-f565-4cf6-9f37-b387fa3b585a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 17:47:00.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2252" for this suite. 02/06/23 17:47:01.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:47:01.012
Feb  6 17:47:01.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 17:47:01.014
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:47:01.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:47:01.032
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-45ec8674-7f88-42fa-b77a-8b6c4d9ba580 02/06/23 17:47:01.034
STEP: Creating a pod to test consume configMaps 02/06/23 17:47:01.039
Feb  6 17:47:01.048: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70" in namespace "projected-6074" to be "Succeeded or Failed"
Feb  6 17:47:01.053: INFO: Pod "pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.95459ms
Feb  6 17:47:03.059: INFO: Pod "pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010545113s
Feb  6 17:47:05.057: INFO: Pod "pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009078147s
Feb  6 17:47:07.059: INFO: Pod "pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010893542s
STEP: Saw pod success 02/06/23 17:47:07.059
Feb  6 17:47:07.059: INFO: Pod "pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70" satisfied condition "Succeeded or Failed"
Feb  6 17:47:07.063: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70 container agnhost-container: <nil>
STEP: delete the pod 02/06/23 17:47:07.069
Feb  6 17:47:07.085: INFO: Waiting for pod pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70 to disappear
Feb  6 17:47:07.089: INFO: Pod pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb  6 17:47:07.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6074" for this suite. 02/06/23 17:47:07.094
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":93,"skipped":1833,"failed":0}
------------------------------
• [SLOW TEST] [6.092 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:47:01.012
    Feb  6 17:47:01.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 17:47:01.014
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:47:01.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:47:01.032
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-45ec8674-7f88-42fa-b77a-8b6c4d9ba580 02/06/23 17:47:01.034
    STEP: Creating a pod to test consume configMaps 02/06/23 17:47:01.039
    Feb  6 17:47:01.048: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70" in namespace "projected-6074" to be "Succeeded or Failed"
    Feb  6 17:47:01.053: INFO: Pod "pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.95459ms
    Feb  6 17:47:03.059: INFO: Pod "pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010545113s
    Feb  6 17:47:05.057: INFO: Pod "pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009078147s
    Feb  6 17:47:07.059: INFO: Pod "pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010893542s
    STEP: Saw pod success 02/06/23 17:47:07.059
    Feb  6 17:47:07.059: INFO: Pod "pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70" satisfied condition "Succeeded or Failed"
    Feb  6 17:47:07.063: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70 container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 17:47:07.069
    Feb  6 17:47:07.085: INFO: Waiting for pod pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70 to disappear
    Feb  6 17:47:07.089: INFO: Pod pod-projected-configmaps-7c624872-b0a2-4a57-8e28-f30ee20a2a70 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb  6 17:47:07.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6074" for this suite. 02/06/23 17:47:07.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:47:07.104
Feb  6 17:47:07.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 17:47:07.105
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:47:07.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:47:07.124
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 17:47:07.144
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:47:07.595
STEP: Deploying the webhook pod 02/06/23 17:47:07.603
STEP: Wait for the deployment to be ready 02/06/23 17:47:07.617
Feb  6 17:47:07.635: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 17:47:09.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 47, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 47, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 47, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 47, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 17:47:11.65
STEP: Verifying the service has paired with the endpoint 02/06/23 17:47:11.674
Feb  6 17:47:12.674: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/06/23 17:47:12.681
STEP: create a pod that should be updated by the webhook 02/06/23 17:47:12.699
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:47:12.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4821" for this suite. 02/06/23 17:47:12.732
STEP: Destroying namespace "webhook-4821-markers" for this suite. 02/06/23 17:47:12.74
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":94,"skipped":1840,"failed":0}
------------------------------
• [SLOW TEST] [5.720 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:47:07.104
    Feb  6 17:47:07.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 17:47:07.105
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:47:07.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:47:07.124
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 17:47:07.144
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:47:07.595
    STEP: Deploying the webhook pod 02/06/23 17:47:07.603
    STEP: Wait for the deployment to be ready 02/06/23 17:47:07.617
    Feb  6 17:47:07.635: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 17:47:09.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 47, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 47, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 47, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 47, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 17:47:11.65
    STEP: Verifying the service has paired with the endpoint 02/06/23 17:47:11.674
    Feb  6 17:47:12.674: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/06/23 17:47:12.681
    STEP: create a pod that should be updated by the webhook 02/06/23 17:47:12.699
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:47:12.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4821" for this suite. 02/06/23 17:47:12.732
    STEP: Destroying namespace "webhook-4821-markers" for this suite. 02/06/23 17:47:12.74
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:47:12.841
Feb  6 17:47:12.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename ephemeral-containers-test 02/06/23 17:47:12.842
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:47:12.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:47:12.867
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 02/06/23 17:47:12.87
Feb  6 17:47:12.880: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2303" to be "running and ready"
Feb  6 17:47:12.888: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.71745ms
Feb  6 17:47:12.888: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:47:14.892: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012124953s
Feb  6 17:47:14.892: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:47:16.892: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.0119433s
Feb  6 17:47:16.892: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Feb  6 17:47:16.892: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 02/06/23 17:47:16.895
Feb  6 17:47:16.906: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2303" to be "container debugger running"
Feb  6 17:47:16.910: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.391074ms
Feb  6 17:47:18.915: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008581061s
Feb  6 17:47:18.915: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 02/06/23 17:47:18.915
Feb  6 17:47:18.915: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2303 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 17:47:18.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 17:47:18.916: INFO: ExecWithOptions: Clientset creation
Feb  6 17:47:18.916: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-2303/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Feb  6 17:47:18.996: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Feb  6 17:47:19.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-2303" for this suite. 02/06/23 17:47:19.007
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":95,"skipped":1892,"failed":0}
------------------------------
• [SLOW TEST] [6.172 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:47:12.841
    Feb  6 17:47:12.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename ephemeral-containers-test 02/06/23 17:47:12.842
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:47:12.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:47:12.867
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 02/06/23 17:47:12.87
    Feb  6 17:47:12.880: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2303" to be "running and ready"
    Feb  6 17:47:12.888: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.71745ms
    Feb  6 17:47:12.888: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:47:14.892: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012124953s
    Feb  6 17:47:14.892: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:47:16.892: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.0119433s
    Feb  6 17:47:16.892: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Feb  6 17:47:16.892: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 02/06/23 17:47:16.895
    Feb  6 17:47:16.906: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2303" to be "container debugger running"
    Feb  6 17:47:16.910: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.391074ms
    Feb  6 17:47:18.915: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008581061s
    Feb  6 17:47:18.915: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 02/06/23 17:47:18.915
    Feb  6 17:47:18.915: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2303 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 17:47:18.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 17:47:18.916: INFO: ExecWithOptions: Clientset creation
    Feb  6 17:47:18.916: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-2303/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Feb  6 17:47:18.996: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb  6 17:47:19.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-2303" for this suite. 02/06/23 17:47:19.007
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:47:19.015
Feb  6 17:47:19.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename resourcequota 02/06/23 17:47:19.016
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:47:19.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:47:19.033
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 02/06/23 17:47:19.036
STEP: Counting existing ResourceQuota 02/06/23 17:47:24.073
STEP: Creating a ResourceQuota 02/06/23 17:47:29.078
STEP: Ensuring resource quota status is calculated 02/06/23 17:47:29.087
STEP: Creating a Secret 02/06/23 17:47:31.091
STEP: Ensuring resource quota status captures secret creation 02/06/23 17:47:31.106
STEP: Deleting a secret 02/06/23 17:47:33.111
STEP: Ensuring resource quota status released usage 02/06/23 17:47:33.117
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb  6 17:47:35.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3116" for this suite. 02/06/23 17:47:35.127
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":96,"skipped":1895,"failed":0}
------------------------------
• [SLOW TEST] [16.119 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:47:19.015
    Feb  6 17:47:19.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename resourcequota 02/06/23 17:47:19.016
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:47:19.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:47:19.033
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 02/06/23 17:47:19.036
    STEP: Counting existing ResourceQuota 02/06/23 17:47:24.073
    STEP: Creating a ResourceQuota 02/06/23 17:47:29.078
    STEP: Ensuring resource quota status is calculated 02/06/23 17:47:29.087
    STEP: Creating a Secret 02/06/23 17:47:31.091
    STEP: Ensuring resource quota status captures secret creation 02/06/23 17:47:31.106
    STEP: Deleting a secret 02/06/23 17:47:33.111
    STEP: Ensuring resource quota status released usage 02/06/23 17:47:33.117
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb  6 17:47:35.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3116" for this suite. 02/06/23 17:47:35.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:47:35.135
Feb  6 17:47:35.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename endpointslice 02/06/23 17:47:35.136
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:47:35.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:47:35.156
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 02/06/23 17:47:40.275
STEP: referencing matching pods with named port 02/06/23 17:47:45.285
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/06/23 17:47:50.296
STEP: recreating EndpointSlices after they've been deleted 02/06/23 17:47:55.304
Feb  6 17:47:55.326: INFO: EndpointSlice for Service endpointslice-9464/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb  6 17:48:05.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9464" for this suite. 02/06/23 17:48:05.344
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":97,"skipped":1900,"failed":0}
------------------------------
• [SLOW TEST] [30.218 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:47:35.135
    Feb  6 17:47:35.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename endpointslice 02/06/23 17:47:35.136
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:47:35.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:47:35.156
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 02/06/23 17:47:40.275
    STEP: referencing matching pods with named port 02/06/23 17:47:45.285
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/06/23 17:47:50.296
    STEP: recreating EndpointSlices after they've been deleted 02/06/23 17:47:55.304
    Feb  6 17:47:55.326: INFO: EndpointSlice for Service endpointslice-9464/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb  6 17:48:05.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9464" for this suite. 02/06/23 17:48:05.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:48:05.355
Feb  6 17:48:05.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubelet-test 02/06/23 17:48:05.357
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:05.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:05.373
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb  6 17:48:09.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4311" for this suite. 02/06/23 17:48:09.399
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":98,"skipped":1914,"failed":0}
------------------------------
• [4.050 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:48:05.355
    Feb  6 17:48:05.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubelet-test 02/06/23 17:48:05.357
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:05.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:05.373
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb  6 17:48:09.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4311" for this suite. 02/06/23 17:48:09.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:48:09.408
Feb  6 17:48:09.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 17:48:09.41
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:09.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:09.428
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Feb  6 17:48:09.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1406 version'
Feb  6 17:48:09.506: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Feb  6 17:48:09.506: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:02Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5-eks-c248520\", GitCommit:\"c24852001be7f1dd35e6756ea0e1e58b4d6d10c5\", GitTreeState:\"archive\", BuildDate:\"2022-12-08T10:06:36Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 17:48:09.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1406" for this suite. 02/06/23 17:48:09.512
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":99,"skipped":1933,"failed":0}
------------------------------
• [0.110 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:48:09.408
    Feb  6 17:48:09.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 17:48:09.41
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:09.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:09.428
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Feb  6 17:48:09.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1406 version'
    Feb  6 17:48:09.506: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Feb  6 17:48:09.506: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:02Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5-eks-c248520\", GitCommit:\"c24852001be7f1dd35e6756ea0e1e58b4d6d10c5\", GitTreeState:\"archive\", BuildDate:\"2022-12-08T10:06:36Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 17:48:09.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1406" for this suite. 02/06/23 17:48:09.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:48:09.519
Feb  6 17:48:09.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename watch 02/06/23 17:48:09.52
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:09.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:09.537
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 02/06/23 17:48:09.539
STEP: creating a new configmap 02/06/23 17:48:09.54
STEP: modifying the configmap once 02/06/23 17:48:09.545
STEP: changing the label value of the configmap 02/06/23 17:48:09.553
STEP: Expecting to observe a delete notification for the watched object 02/06/23 17:48:09.56
Feb  6 17:48:09.560: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3091  1c79784f-568e-4bf3-825f-aa932fce5a6e 28784 0 2023-02-06 17:48:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-06 17:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 17:48:09.560: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3091  1c79784f-568e-4bf3-825f-aa932fce5a6e 28785 0 2023-02-06 17:48:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-06 17:48:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 17:48:09.561: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3091  1c79784f-568e-4bf3-825f-aa932fce5a6e 28786 0 2023-02-06 17:48:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-06 17:48:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 02/06/23 17:48:09.561
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/06/23 17:48:09.568
STEP: changing the label value of the configmap back 02/06/23 17:48:19.569
STEP: modifying the configmap a third time 02/06/23 17:48:19.578
STEP: deleting the configmap 02/06/23 17:48:19.587
STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/06/23 17:48:19.593
Feb  6 17:48:19.593: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3091  1c79784f-568e-4bf3-825f-aa932fce5a6e 28940 0 2023-02-06 17:48:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-06 17:48:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 17:48:19.594: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3091  1c79784f-568e-4bf3-825f-aa932fce5a6e 28941 0 2023-02-06 17:48:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-06 17:48:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 17:48:19.594: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3091  1c79784f-568e-4bf3-825f-aa932fce5a6e 28942 0 2023-02-06 17:48:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-06 17:48:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb  6 17:48:19.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3091" for this suite. 02/06/23 17:48:19.599
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":100,"skipped":1952,"failed":0}
------------------------------
• [SLOW TEST] [10.086 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:48:09.519
    Feb  6 17:48:09.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename watch 02/06/23 17:48:09.52
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:09.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:09.537
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 02/06/23 17:48:09.539
    STEP: creating a new configmap 02/06/23 17:48:09.54
    STEP: modifying the configmap once 02/06/23 17:48:09.545
    STEP: changing the label value of the configmap 02/06/23 17:48:09.553
    STEP: Expecting to observe a delete notification for the watched object 02/06/23 17:48:09.56
    Feb  6 17:48:09.560: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3091  1c79784f-568e-4bf3-825f-aa932fce5a6e 28784 0 2023-02-06 17:48:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-06 17:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 17:48:09.560: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3091  1c79784f-568e-4bf3-825f-aa932fce5a6e 28785 0 2023-02-06 17:48:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-06 17:48:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 17:48:09.561: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3091  1c79784f-568e-4bf3-825f-aa932fce5a6e 28786 0 2023-02-06 17:48:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-06 17:48:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 02/06/23 17:48:09.561
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/06/23 17:48:09.568
    STEP: changing the label value of the configmap back 02/06/23 17:48:19.569
    STEP: modifying the configmap a third time 02/06/23 17:48:19.578
    STEP: deleting the configmap 02/06/23 17:48:19.587
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/06/23 17:48:19.593
    Feb  6 17:48:19.593: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3091  1c79784f-568e-4bf3-825f-aa932fce5a6e 28940 0 2023-02-06 17:48:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-06 17:48:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 17:48:19.594: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3091  1c79784f-568e-4bf3-825f-aa932fce5a6e 28941 0 2023-02-06 17:48:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-06 17:48:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 17:48:19.594: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3091  1c79784f-568e-4bf3-825f-aa932fce5a6e 28942 0 2023-02-06 17:48:09 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-06 17:48:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb  6 17:48:19.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3091" for this suite. 02/06/23 17:48:19.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:48:19.607
Feb  6 17:48:19.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename resourcequota 02/06/23 17:48:19.608
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:19.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:19.632
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 02/06/23 17:48:19.634
STEP: Creating a ResourceQuota 02/06/23 17:48:24.638
STEP: Ensuring resource quota status is calculated 02/06/23 17:48:24.646
STEP: Creating a Service 02/06/23 17:48:26.652
STEP: Creating a NodePort Service 02/06/23 17:48:26.691
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/06/23 17:48:26.72
STEP: Ensuring resource quota status captures service creation 02/06/23 17:48:26.761
STEP: Deleting Services 02/06/23 17:48:28.765
STEP: Ensuring resource quota status released usage 02/06/23 17:48:28.855
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb  6 17:48:30.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5674" for this suite. 02/06/23 17:48:30.868
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":101,"skipped":1984,"failed":0}
------------------------------
• [SLOW TEST] [11.269 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:48:19.607
    Feb  6 17:48:19.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename resourcequota 02/06/23 17:48:19.608
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:19.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:19.632
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 02/06/23 17:48:19.634
    STEP: Creating a ResourceQuota 02/06/23 17:48:24.638
    STEP: Ensuring resource quota status is calculated 02/06/23 17:48:24.646
    STEP: Creating a Service 02/06/23 17:48:26.652
    STEP: Creating a NodePort Service 02/06/23 17:48:26.691
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/06/23 17:48:26.72
    STEP: Ensuring resource quota status captures service creation 02/06/23 17:48:26.761
    STEP: Deleting Services 02/06/23 17:48:28.765
    STEP: Ensuring resource quota status released usage 02/06/23 17:48:28.855
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb  6 17:48:30.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5674" for this suite. 02/06/23 17:48:30.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:48:30.88
Feb  6 17:48:30.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename daemonsets 02/06/23 17:48:30.881
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:30.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:30.898
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 02/06/23 17:48:30.924
STEP: Check that daemon pods launch on every node of the cluster. 02/06/23 17:48:30.931
Feb  6 17:48:30.946: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:30.946: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:30.946: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:30.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:48:30.950: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:48:31.957: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:31.957: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:31.957: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:31.960: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:48:31.961: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:48:32.956: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:32.956: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:32.957: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:32.960: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:48:32.961: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:48:33.957: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:33.957: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:33.957: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:33.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb  6 17:48:33.963: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:48:34.956: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:34.956: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:34.956: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:48:34.959: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb  6 17:48:34.959: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 02/06/23 17:48:34.962
STEP: DeleteCollection of the DaemonSets 02/06/23 17:48:34.966
STEP: Verify that ReplicaSets have been deleted 02/06/23 17:48:34.975
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Feb  6 17:48:35.012: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29162"},"items":null}

Feb  6 17:48:35.020: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29162"},"items":[{"metadata":{"name":"daemon-set-lcwrn","generateName":"daemon-set-","namespace":"daemonsets-3043","uid":"0ddecba5-7769-4edc-b724-338a9ac5d7f3","resourceVersion":"29152","creationTimestamp":"2023-02-06T17:48:30Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d3faaae9-8e02-42d0-b67e-ed66689da3c5","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-06T17:48:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3faaae9-8e02-42d0-b67e-ed66689da3c5\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-06T17:48:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4j7p7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4j7p7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tneyla25-md-0-68cbcb4798-6j94p","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tneyla25-md-0-68cbcb4798-6j94p"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:30Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:30Z"}],"hostIP":"195.17.148.89","podIP":"192.168.3.148","podIPs":[{"ip":"192.168.3.148"}],"startTime":"2023-02-06T17:48:30Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-06T17:48:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://0deb067a8cce0273f9c2d28037380818b36b6166c4660765a73795567a5c50d8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tmrz9","generateName":"daemon-set-","namespace":"daemonsets-3043","uid":"4421ac0c-4012-4599-bee2-a5ca1d321b29","resourceVersion":"29150","creationTimestamp":"2023-02-06T17:48:30Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d3faaae9-8e02-42d0-b67e-ed66689da3c5","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-06T17:48:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3faaae9-8e02-42d0-b67e-ed66689da3c5\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-06T17:48:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.193\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zvzr8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zvzr8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tneyla25-md-0-68cbcb4798-5xxmw","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tneyla25-md-0-68cbcb4798-5xxmw"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:30Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:30Z"}],"hostIP":"195.17.4.133","podIP":"192.168.2.193","podIPs":[{"ip":"192.168.2.193"}],"startTime":"2023-02-06T17:48:30Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-06T17:48:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://78813f23b245db29592c8ff5b1c090283026c6c25663acf90706649293508ede","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-x4c7q","generateName":"daemon-set-","namespace":"daemonsets-3043","uid":"13634d80-3866-46cd-9c4f-bc24d87a51ac","resourceVersion":"29147","creationTimestamp":"2023-02-06T17:48:30Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d3faaae9-8e02-42d0-b67e-ed66689da3c5","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-06T17:48:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3faaae9-8e02-42d0-b67e-ed66689da3c5\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-06T17:48:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7gmjt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7gmjt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tneyla25-md-0-68cbcb4798-lz542","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tneyla25-md-0-68cbcb4798-lz542"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:30Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:33Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:33Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:30Z"}],"hostIP":"195.17.148.90","podIP":"192.168.1.143","podIPs":[{"ip":"192.168.1.143"}],"startTime":"2023-02-06T17:48:30Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-06T17:48:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://3e843fb6e8e652e50aee8bf9fd116c4384bec06dbee72f3dc65be856a69dd232","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb  6 17:48:35.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3043" for this suite. 02/06/23 17:48:35.057
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":102,"skipped":2003,"failed":0}
------------------------------
• [4.183 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:48:30.88
    Feb  6 17:48:30.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename daemonsets 02/06/23 17:48:30.881
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:30.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:30.898
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 02/06/23 17:48:30.924
    STEP: Check that daemon pods launch on every node of the cluster. 02/06/23 17:48:30.931
    Feb  6 17:48:30.946: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:30.946: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:30.946: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:30.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:48:30.950: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:48:31.957: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:31.957: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:31.957: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:31.960: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:48:31.961: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:48:32.956: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:32.956: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:32.957: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:32.960: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:48:32.961: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:48:33.957: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:33.957: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:33.957: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:33.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb  6 17:48:33.963: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:48:34.956: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:34.956: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:34.956: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:48:34.959: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb  6 17:48:34.959: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 02/06/23 17:48:34.962
    STEP: DeleteCollection of the DaemonSets 02/06/23 17:48:34.966
    STEP: Verify that ReplicaSets have been deleted 02/06/23 17:48:34.975
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Feb  6 17:48:35.012: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29162"},"items":null}

    Feb  6 17:48:35.020: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29162"},"items":[{"metadata":{"name":"daemon-set-lcwrn","generateName":"daemon-set-","namespace":"daemonsets-3043","uid":"0ddecba5-7769-4edc-b724-338a9ac5d7f3","resourceVersion":"29152","creationTimestamp":"2023-02-06T17:48:30Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d3faaae9-8e02-42d0-b67e-ed66689da3c5","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-06T17:48:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3faaae9-8e02-42d0-b67e-ed66689da3c5\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-06T17:48:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4j7p7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4j7p7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tneyla25-md-0-68cbcb4798-6j94p","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tneyla25-md-0-68cbcb4798-6j94p"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:30Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:30Z"}],"hostIP":"195.17.148.89","podIP":"192.168.3.148","podIPs":[{"ip":"192.168.3.148"}],"startTime":"2023-02-06T17:48:30Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-06T17:48:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://0deb067a8cce0273f9c2d28037380818b36b6166c4660765a73795567a5c50d8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tmrz9","generateName":"daemon-set-","namespace":"daemonsets-3043","uid":"4421ac0c-4012-4599-bee2-a5ca1d321b29","resourceVersion":"29150","creationTimestamp":"2023-02-06T17:48:30Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d3faaae9-8e02-42d0-b67e-ed66689da3c5","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-06T17:48:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3faaae9-8e02-42d0-b67e-ed66689da3c5\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-06T17:48:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.193\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zvzr8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zvzr8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tneyla25-md-0-68cbcb4798-5xxmw","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tneyla25-md-0-68cbcb4798-5xxmw"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:30Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:30Z"}],"hostIP":"195.17.4.133","podIP":"192.168.2.193","podIPs":[{"ip":"192.168.2.193"}],"startTime":"2023-02-06T17:48:30Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-06T17:48:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://78813f23b245db29592c8ff5b1c090283026c6c25663acf90706649293508ede","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-x4c7q","generateName":"daemon-set-","namespace":"daemonsets-3043","uid":"13634d80-3866-46cd-9c4f-bc24d87a51ac","resourceVersion":"29147","creationTimestamp":"2023-02-06T17:48:30Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d3faaae9-8e02-42d0-b67e-ed66689da3c5","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-06T17:48:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3faaae9-8e02-42d0-b67e-ed66689da3c5\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-06T17:48:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7gmjt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7gmjt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tneyla25-md-0-68cbcb4798-lz542","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tneyla25-md-0-68cbcb4798-lz542"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:30Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:33Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:33Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-06T17:48:30Z"}],"hostIP":"195.17.148.90","podIP":"192.168.1.143","podIPs":[{"ip":"192.168.1.143"}],"startTime":"2023-02-06T17:48:30Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-06T17:48:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://3e843fb6e8e652e50aee8bf9fd116c4384bec06dbee72f3dc65be856a69dd232","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 17:48:35.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3043" for this suite. 02/06/23 17:48:35.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:48:35.068
Feb  6 17:48:35.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 17:48:35.069
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:35.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:35.087
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 17:48:35.101
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:48:35.423
STEP: Deploying the webhook pod 02/06/23 17:48:35.432
STEP: Wait for the deployment to be ready 02/06/23 17:48:35.447
Feb  6 17:48:35.459: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 17:48:37.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 48, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 48, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 48, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 48, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 17:48:39.473
STEP: Verifying the service has paired with the endpoint 02/06/23 17:48:39.489
Feb  6 17:48:40.490: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 02/06/23 17:48:40.494
STEP: Creating a custom resource definition that should be denied by the webhook 02/06/23 17:48:40.516
Feb  6 17:48:40.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:48:40.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5124" for this suite. 02/06/23 17:48:40.547
STEP: Destroying namespace "webhook-5124-markers" for this suite. 02/06/23 17:48:40.554
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":103,"skipped":2050,"failed":0}
------------------------------
• [SLOW TEST] [5.571 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:48:35.068
    Feb  6 17:48:35.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 17:48:35.069
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:35.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:35.087
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 17:48:35.101
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:48:35.423
    STEP: Deploying the webhook pod 02/06/23 17:48:35.432
    STEP: Wait for the deployment to be ready 02/06/23 17:48:35.447
    Feb  6 17:48:35.459: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 17:48:37.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 48, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 48, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 48, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 48, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 17:48:39.473
    STEP: Verifying the service has paired with the endpoint 02/06/23 17:48:39.489
    Feb  6 17:48:40.490: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 02/06/23 17:48:40.494
    STEP: Creating a custom resource definition that should be denied by the webhook 02/06/23 17:48:40.516
    Feb  6 17:48:40.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:48:40.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5124" for this suite. 02/06/23 17:48:40.547
    STEP: Destroying namespace "webhook-5124-markers" for this suite. 02/06/23 17:48:40.554
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:48:40.639
Feb  6 17:48:40.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename namespaces 02/06/23 17:48:40.641
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:40.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:40.675
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 02/06/23 17:48:40.679
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:40.703
STEP: Creating a service in the namespace 02/06/23 17:48:40.709
STEP: Deleting the namespace 02/06/23 17:48:40.738
STEP: Waiting for the namespace to be removed. 02/06/23 17:48:40.753
STEP: Recreating the namespace 02/06/23 17:48:46.757
STEP: Verifying there is no service in the namespace 02/06/23 17:48:46.775
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb  6 17:48:46.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-434" for this suite. 02/06/23 17:48:46.784
STEP: Destroying namespace "nsdeletetest-6505" for this suite. 02/06/23 17:48:46.789
Feb  6 17:48:46.792: INFO: Namespace nsdeletetest-6505 was already deleted
STEP: Destroying namespace "nsdeletetest-2202" for this suite. 02/06/23 17:48:46.792
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":104,"skipped":2053,"failed":0}
------------------------------
• [SLOW TEST] [6.160 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:48:40.639
    Feb  6 17:48:40.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename namespaces 02/06/23 17:48:40.641
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:40.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:40.675
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 02/06/23 17:48:40.679
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:40.703
    STEP: Creating a service in the namespace 02/06/23 17:48:40.709
    STEP: Deleting the namespace 02/06/23 17:48:40.738
    STEP: Waiting for the namespace to be removed. 02/06/23 17:48:40.753
    STEP: Recreating the namespace 02/06/23 17:48:46.757
    STEP: Verifying there is no service in the namespace 02/06/23 17:48:46.775
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 17:48:46.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-434" for this suite. 02/06/23 17:48:46.784
    STEP: Destroying namespace "nsdeletetest-6505" for this suite. 02/06/23 17:48:46.789
    Feb  6 17:48:46.792: INFO: Namespace nsdeletetest-6505 was already deleted
    STEP: Destroying namespace "nsdeletetest-2202" for this suite. 02/06/23 17:48:46.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:48:46.807
Feb  6 17:48:46.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 17:48:46.808
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:46.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:46.83
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/06/23 17:48:46.836
Feb  6 17:48:46.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-4156 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb  6 17:48:46.955: INFO: stderr: ""
Feb  6 17:48:46.955: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 02/06/23 17:48:46.955
STEP: verifying the pod e2e-test-httpd-pod was created 02/06/23 17:48:52.007
Feb  6 17:48:52.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-4156 get pod e2e-test-httpd-pod -o json'
Feb  6 17:48:52.094: INFO: stderr: ""
Feb  6 17:48:52.094: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-02-06T17:48:46Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4156\",\n        \"resourceVersion\": \"29457\",\n        \"uid\": \"6454a3f6-c088-4b7e-a5a5-419387947e57\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-tp529\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"tneyla25-md-0-68cbcb4798-5xxmw\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-tp529\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-06T17:48:46Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-06T17:48:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-06T17:48:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-06T17:48:46Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://d2a451a907033d81b8d763d28e50179e8196264ce29c4c5d62e3bebf12991278\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-06T17:48:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"195.17.4.133\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.2.27\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.2.27\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-06T17:48:46Z\"\n    }\n}\n"
STEP: replace the image in the pod 02/06/23 17:48:52.095
Feb  6 17:48:52.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-4156 replace -f -'
Feb  6 17:48:52.545: INFO: stderr: ""
Feb  6 17:48:52.545: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 02/06/23 17:48:52.545
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Feb  6 17:48:52.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-4156 delete pods e2e-test-httpd-pod'
Feb  6 17:48:54.175: INFO: stderr: ""
Feb  6 17:48:54.175: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 17:48:54.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4156" for this suite. 02/06/23 17:48:54.181
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":105,"skipped":2076,"failed":0}
------------------------------
• [SLOW TEST] [7.379 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:48:46.807
    Feb  6 17:48:46.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 17:48:46.808
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:46.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:46.83
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/06/23 17:48:46.836
    Feb  6 17:48:46.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-4156 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb  6 17:48:46.955: INFO: stderr: ""
    Feb  6 17:48:46.955: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 02/06/23 17:48:46.955
    STEP: verifying the pod e2e-test-httpd-pod was created 02/06/23 17:48:52.007
    Feb  6 17:48:52.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-4156 get pod e2e-test-httpd-pod -o json'
    Feb  6 17:48:52.094: INFO: stderr: ""
    Feb  6 17:48:52.094: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-02-06T17:48:46Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4156\",\n        \"resourceVersion\": \"29457\",\n        \"uid\": \"6454a3f6-c088-4b7e-a5a5-419387947e57\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-tp529\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"tneyla25-md-0-68cbcb4798-5xxmw\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-tp529\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-06T17:48:46Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-06T17:48:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-06T17:48:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-06T17:48:46Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://d2a451a907033d81b8d763d28e50179e8196264ce29c4c5d62e3bebf12991278\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-06T17:48:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"195.17.4.133\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.2.27\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.2.27\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-06T17:48:46Z\"\n    }\n}\n"
    STEP: replace the image in the pod 02/06/23 17:48:52.095
    Feb  6 17:48:52.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-4156 replace -f -'
    Feb  6 17:48:52.545: INFO: stderr: ""
    Feb  6 17:48:52.545: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 02/06/23 17:48:52.545
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Feb  6 17:48:52.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-4156 delete pods e2e-test-httpd-pod'
    Feb  6 17:48:54.175: INFO: stderr: ""
    Feb  6 17:48:54.175: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 17:48:54.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4156" for this suite. 02/06/23 17:48:54.181
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:48:54.187
Feb  6 17:48:54.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-lifecycle-hook 02/06/23 17:48:54.188
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:54.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:54.205
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/06/23 17:48:54.218
Feb  6 17:48:54.229: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4460" to be "running and ready"
Feb  6 17:48:54.236: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.889736ms
Feb  6 17:48:54.236: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:48:56.241: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011733675s
Feb  6 17:48:56.241: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:48:58.240: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.011253515s
Feb  6 17:48:58.240: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb  6 17:48:58.241: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 02/06/23 17:48:58.244
Feb  6 17:48:58.251: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4460" to be "running and ready"
Feb  6 17:48:58.254: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.865906ms
Feb  6 17:48:58.254: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:49:00.259: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007796589s
Feb  6 17:49:00.259: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:49:02.260: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.008196231s
Feb  6 17:49:02.260: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Feb  6 17:49:02.260: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/06/23 17:49:02.263
Feb  6 17:49:02.270: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  6 17:49:02.273: INFO: Pod pod-with-prestop-http-hook still exists
Feb  6 17:49:04.274: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  6 17:49:04.279: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 02/06/23 17:49:04.279
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb  6 17:49:04.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4460" for this suite. 02/06/23 17:49:04.304
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":106,"skipped":2078,"failed":0}
------------------------------
• [SLOW TEST] [10.125 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:48:54.187
    Feb  6 17:48:54.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/06/23 17:48:54.188
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:48:54.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:48:54.205
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/06/23 17:48:54.218
    Feb  6 17:48:54.229: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4460" to be "running and ready"
    Feb  6 17:48:54.236: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.889736ms
    Feb  6 17:48:54.236: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:48:56.241: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011733675s
    Feb  6 17:48:56.241: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:48:58.240: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.011253515s
    Feb  6 17:48:58.240: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb  6 17:48:58.241: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 02/06/23 17:48:58.244
    Feb  6 17:48:58.251: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4460" to be "running and ready"
    Feb  6 17:48:58.254: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.865906ms
    Feb  6 17:48:58.254: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:49:00.259: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007796589s
    Feb  6 17:49:00.259: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:49:02.260: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.008196231s
    Feb  6 17:49:02.260: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Feb  6 17:49:02.260: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/06/23 17:49:02.263
    Feb  6 17:49:02.270: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb  6 17:49:02.273: INFO: Pod pod-with-prestop-http-hook still exists
    Feb  6 17:49:04.274: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb  6 17:49:04.279: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 02/06/23 17:49:04.279
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb  6 17:49:04.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4460" for this suite. 02/06/23 17:49:04.304
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:49:04.312
Feb  6 17:49:04.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 17:49:04.314
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:49:04.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:49:04.332
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 02/06/23 17:49:04.334
Feb  6 17:49:04.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1891 api-versions'
Feb  6 17:49:04.408: INFO: stderr: ""
Feb  6 17:49:04.408: INFO: stdout: "acme.cert-manager.io/v1\naddons.cluster.x-k8s.io/v1alpha3\naddons.cluster.x-k8s.io/v1alpha4\naddons.cluster.x-k8s.io/v1beta1\nadmissionregistration.k8s.io/v1\nanywhere.eks.amazonaws.com/v1alpha1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbootstrap.cluster.x-k8s.io/v1alpha3\nbootstrap.cluster.x-k8s.io/v1alpha4\nbootstrap.cluster.x-k8s.io/v1beta1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncluster.x-k8s.io/v1alpha3\ncluster.x-k8s.io/v1alpha4\ncluster.x-k8s.io/v1beta1\nclusterctl.cluster.x-k8s.io/v1alpha3\ncontrolplane.cluster.x-k8s.io/v1alpha3\ncontrolplane.cluster.x-k8s.io/v1alpha4\ncontrolplane.cluster.x-k8s.io/v1beta1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\ndistro.eks.amazonaws.com/v1alpha1\netcdcluster.cluster.x-k8s.io/v1alpha3\netcdcluster.cluster.x-k8s.io/v1beta1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninfrastructure.cluster.x-k8s.io/v1alpha3\ninfrastructure.cluster.x-k8s.io/v1alpha4\ninfrastructure.cluster.x-k8s.io/v1beta1\nipam.cluster.x-k8s.io/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npackages.eks.amazonaws.com/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nruntime.cluster.x-k8s.io/v1alpha1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 17:49:04.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1891" for this suite. 02/06/23 17:49:04.413
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":107,"skipped":2082,"failed":0}
------------------------------
• [0.108 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:49:04.312
    Feb  6 17:49:04.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 17:49:04.314
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:49:04.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:49:04.332
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 02/06/23 17:49:04.334
    Feb  6 17:49:04.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1891 api-versions'
    Feb  6 17:49:04.408: INFO: stderr: ""
    Feb  6 17:49:04.408: INFO: stdout: "acme.cert-manager.io/v1\naddons.cluster.x-k8s.io/v1alpha3\naddons.cluster.x-k8s.io/v1alpha4\naddons.cluster.x-k8s.io/v1beta1\nadmissionregistration.k8s.io/v1\nanywhere.eks.amazonaws.com/v1alpha1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbootstrap.cluster.x-k8s.io/v1alpha3\nbootstrap.cluster.x-k8s.io/v1alpha4\nbootstrap.cluster.x-k8s.io/v1beta1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncluster.x-k8s.io/v1alpha3\ncluster.x-k8s.io/v1alpha4\ncluster.x-k8s.io/v1beta1\nclusterctl.cluster.x-k8s.io/v1alpha3\ncontrolplane.cluster.x-k8s.io/v1alpha3\ncontrolplane.cluster.x-k8s.io/v1alpha4\ncontrolplane.cluster.x-k8s.io/v1beta1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\ndistro.eks.amazonaws.com/v1alpha1\netcdcluster.cluster.x-k8s.io/v1alpha3\netcdcluster.cluster.x-k8s.io/v1beta1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninfrastructure.cluster.x-k8s.io/v1alpha3\ninfrastructure.cluster.x-k8s.io/v1alpha4\ninfrastructure.cluster.x-k8s.io/v1beta1\nipam.cluster.x-k8s.io/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npackages.eks.amazonaws.com/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nruntime.cluster.x-k8s.io/v1alpha1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 17:49:04.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1891" for this suite. 02/06/23 17:49:04.413
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:49:04.421
Feb  6 17:49:04.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 17:49:04.422
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:49:04.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:49:04.441
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 02/06/23 17:49:04.443
Feb  6 17:49:04.452: INFO: Waiting up to 5m0s for pod "downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa" in namespace "downward-api-7459" to be "Succeeded or Failed"
Feb  6 17:49:04.459: INFO: Pod "downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.402233ms
Feb  6 17:49:06.465: INFO: Pod "downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013082081s
Feb  6 17:49:08.464: INFO: Pod "downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011808046s
Feb  6 17:49:10.463: INFO: Pod "downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011126229s
STEP: Saw pod success 02/06/23 17:49:10.463
Feb  6 17:49:10.464: INFO: Pod "downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa" satisfied condition "Succeeded or Failed"
Feb  6 17:49:10.467: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa container dapi-container: <nil>
STEP: delete the pod 02/06/23 17:49:10.476
Feb  6 17:49:10.498: INFO: Waiting for pod downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa to disappear
Feb  6 17:49:10.501: INFO: Pod downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb  6 17:49:10.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7459" for this suite. 02/06/23 17:49:10.506
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":108,"skipped":2082,"failed":0}
------------------------------
• [SLOW TEST] [6.092 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:49:04.421
    Feb  6 17:49:04.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 17:49:04.422
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:49:04.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:49:04.441
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 02/06/23 17:49:04.443
    Feb  6 17:49:04.452: INFO: Waiting up to 5m0s for pod "downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa" in namespace "downward-api-7459" to be "Succeeded or Failed"
    Feb  6 17:49:04.459: INFO: Pod "downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.402233ms
    Feb  6 17:49:06.465: INFO: Pod "downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013082081s
    Feb  6 17:49:08.464: INFO: Pod "downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011808046s
    Feb  6 17:49:10.463: INFO: Pod "downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011126229s
    STEP: Saw pod success 02/06/23 17:49:10.463
    Feb  6 17:49:10.464: INFO: Pod "downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa" satisfied condition "Succeeded or Failed"
    Feb  6 17:49:10.467: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa container dapi-container: <nil>
    STEP: delete the pod 02/06/23 17:49:10.476
    Feb  6 17:49:10.498: INFO: Waiting for pod downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa to disappear
    Feb  6 17:49:10.501: INFO: Pod downward-api-9ce98973-aba9-4b76-a3c0-f9d79be67bfa no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb  6 17:49:10.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7459" for this suite. 02/06/23 17:49:10.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:49:10.514
Feb  6 17:49:10.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 17:49:10.516
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:49:10.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:49:10.539
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-8147/secret-test-d2e129bb-990b-46b4-a22e-d4e0e55073aa 02/06/23 17:49:10.543
STEP: Creating a pod to test consume secrets 02/06/23 17:49:10.548
Feb  6 17:49:10.555: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5" in namespace "secrets-8147" to be "Succeeded or Failed"
Feb  6 17:49:10.559: INFO: Pod "pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.153571ms
Feb  6 17:49:12.565: INFO: Pod "pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00917018s
Feb  6 17:49:14.566: INFO: Pod "pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010262017s
Feb  6 17:49:16.563: INFO: Pod "pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007399382s
STEP: Saw pod success 02/06/23 17:49:16.563
Feb  6 17:49:16.563: INFO: Pod "pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5" satisfied condition "Succeeded or Failed"
Feb  6 17:49:16.566: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5 container env-test: <nil>
STEP: delete the pod 02/06/23 17:49:16.574
Feb  6 17:49:16.588: INFO: Waiting for pod pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5 to disappear
Feb  6 17:49:16.592: INFO: Pod pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb  6 17:49:16.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8147" for this suite. 02/06/23 17:49:16.612
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":109,"skipped":2111,"failed":0}
------------------------------
• [SLOW TEST] [6.113 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:49:10.514
    Feb  6 17:49:10.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 17:49:10.516
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:49:10.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:49:10.539
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-8147/secret-test-d2e129bb-990b-46b4-a22e-d4e0e55073aa 02/06/23 17:49:10.543
    STEP: Creating a pod to test consume secrets 02/06/23 17:49:10.548
    Feb  6 17:49:10.555: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5" in namespace "secrets-8147" to be "Succeeded or Failed"
    Feb  6 17:49:10.559: INFO: Pod "pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.153571ms
    Feb  6 17:49:12.565: INFO: Pod "pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00917018s
    Feb  6 17:49:14.566: INFO: Pod "pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010262017s
    Feb  6 17:49:16.563: INFO: Pod "pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007399382s
    STEP: Saw pod success 02/06/23 17:49:16.563
    Feb  6 17:49:16.563: INFO: Pod "pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5" satisfied condition "Succeeded or Failed"
    Feb  6 17:49:16.566: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5 container env-test: <nil>
    STEP: delete the pod 02/06/23 17:49:16.574
    Feb  6 17:49:16.588: INFO: Waiting for pod pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5 to disappear
    Feb  6 17:49:16.592: INFO: Pod pod-configmaps-4e0b5c28-4a96-43d3-8025-90a0aeb622f5 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 17:49:16.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8147" for this suite. 02/06/23 17:49:16.612
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:49:16.627
Feb  6 17:49:16.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 17:49:16.629
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:49:16.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:49:16.671
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 02/06/23 17:49:16.675
Feb  6 17:49:16.675: INFO: namespace kubectl-7844
Feb  6 17:49:16.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7844 create -f -'
Feb  6 17:49:18.658: INFO: stderr: ""
Feb  6 17:49:18.658: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/06/23 17:49:18.658
Feb  6 17:49:19.662: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 17:49:19.662: INFO: Found 0 / 1
Feb  6 17:49:20.665: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 17:49:20.665: INFO: Found 0 / 1
Feb  6 17:49:21.665: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 17:49:21.665: INFO: Found 0 / 1
Feb  6 17:49:22.663: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 17:49:22.663: INFO: Found 1 / 1
Feb  6 17:49:22.663: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  6 17:49:22.666: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 17:49:22.666: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  6 17:49:22.666: INFO: wait on agnhost-primary startup in kubectl-7844 
Feb  6 17:49:22.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7844 logs agnhost-primary-jgfbm agnhost-primary'
Feb  6 17:49:22.760: INFO: stderr: ""
Feb  6 17:49:22.760: INFO: stdout: "Paused\n"
STEP: exposing RC 02/06/23 17:49:22.76
Feb  6 17:49:22.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7844 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb  6 17:49:22.891: INFO: stderr: ""
Feb  6 17:49:22.891: INFO: stdout: "service/rm2 exposed\n"
Feb  6 17:49:22.897: INFO: Service rm2 in namespace kubectl-7844 found.
STEP: exposing service 02/06/23 17:49:24.906
Feb  6 17:49:24.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7844 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb  6 17:49:25.030: INFO: stderr: ""
Feb  6 17:49:25.030: INFO: stdout: "service/rm3 exposed\n"
Feb  6 17:49:25.039: INFO: Service rm3 in namespace kubectl-7844 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 17:49:27.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7844" for this suite. 02/06/23 17:49:27.05
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":110,"skipped":2111,"failed":0}
------------------------------
• [SLOW TEST] [10.429 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:49:16.627
    Feb  6 17:49:16.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 17:49:16.629
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:49:16.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:49:16.671
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 02/06/23 17:49:16.675
    Feb  6 17:49:16.675: INFO: namespace kubectl-7844
    Feb  6 17:49:16.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7844 create -f -'
    Feb  6 17:49:18.658: INFO: stderr: ""
    Feb  6 17:49:18.658: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/06/23 17:49:18.658
    Feb  6 17:49:19.662: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 17:49:19.662: INFO: Found 0 / 1
    Feb  6 17:49:20.665: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 17:49:20.665: INFO: Found 0 / 1
    Feb  6 17:49:21.665: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 17:49:21.665: INFO: Found 0 / 1
    Feb  6 17:49:22.663: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 17:49:22.663: INFO: Found 1 / 1
    Feb  6 17:49:22.663: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb  6 17:49:22.666: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 17:49:22.666: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb  6 17:49:22.666: INFO: wait on agnhost-primary startup in kubectl-7844 
    Feb  6 17:49:22.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7844 logs agnhost-primary-jgfbm agnhost-primary'
    Feb  6 17:49:22.760: INFO: stderr: ""
    Feb  6 17:49:22.760: INFO: stdout: "Paused\n"
    STEP: exposing RC 02/06/23 17:49:22.76
    Feb  6 17:49:22.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7844 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Feb  6 17:49:22.891: INFO: stderr: ""
    Feb  6 17:49:22.891: INFO: stdout: "service/rm2 exposed\n"
    Feb  6 17:49:22.897: INFO: Service rm2 in namespace kubectl-7844 found.
    STEP: exposing service 02/06/23 17:49:24.906
    Feb  6 17:49:24.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7844 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Feb  6 17:49:25.030: INFO: stderr: ""
    Feb  6 17:49:25.030: INFO: stdout: "service/rm3 exposed\n"
    Feb  6 17:49:25.039: INFO: Service rm3 in namespace kubectl-7844 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 17:49:27.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7844" for this suite. 02/06/23 17:49:27.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:49:27.058
Feb  6 17:49:27.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename statefulset 02/06/23 17:49:27.059
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:49:27.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:49:27.077
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9835 02/06/23 17:49:27.085
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-9835 02/06/23 17:49:27.093
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9835 02/06/23 17:49:27.101
Feb  6 17:49:27.107: INFO: Found 0 stateful pods, waiting for 1
Feb  6 17:49:37.111: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/06/23 17:49:37.111
Feb  6 17:49:37.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  6 17:49:37.306: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  6 17:49:37.306: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  6 17:49:37.306: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  6 17:49:37.311: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb  6 17:49:47.316: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  6 17:49:47.316: INFO: Waiting for statefulset status.replicas updated to 0
Feb  6 17:49:47.332: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Feb  6 17:49:47.332: INFO: ss-0  tneyla25-md-0-68cbcb4798-5xxmw  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:27 +0000 UTC  }]
Feb  6 17:49:47.332: INFO: 
Feb  6 17:49:47.332: INFO: StatefulSet ss has not reached scale 3, at 1
Feb  6 17:49:48.337: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995660916s
Feb  6 17:49:49.343: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990854055s
Feb  6 17:49:50.349: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983738806s
Feb  6 17:49:51.355: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978579611s
Feb  6 17:49:52.360: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973028663s
Feb  6 17:49:53.365: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967867695s
Feb  6 17:49:54.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.962901288s
Feb  6 17:49:55.375: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95738794s
Feb  6 17:49:56.385: INFO: Verifying statefulset ss doesn't scale past 3 for another 951.998574ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9835 02/06/23 17:49:57.386
Feb  6 17:49:57.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  6 17:49:57.566: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  6 17:49:57.566: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  6 17:49:57.567: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  6 17:49:57.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  6 17:49:57.727: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb  6 17:49:57.727: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  6 17:49:57.727: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  6 17:49:57.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  6 17:49:57.916: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb  6 17:49:57.916: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  6 17:49:57.916: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  6 17:49:57.919: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb  6 17:50:07.925: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 17:50:07.925: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 17:50:07.925: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 02/06/23 17:50:07.925
Feb  6 17:50:07.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  6 17:50:08.101: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  6 17:50:08.101: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  6 17:50:08.101: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  6 17:50:08.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  6 17:50:08.277: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  6 17:50:08.277: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  6 17:50:08.277: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  6 17:50:08.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  6 17:50:08.440: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  6 17:50:08.441: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  6 17:50:08.441: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  6 17:50:08.441: INFO: Waiting for statefulset status.replicas updated to 0
Feb  6 17:50:08.444: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb  6 17:50:18.455: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  6 17:50:18.455: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb  6 17:50:18.455: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb  6 17:50:18.472: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Feb  6 17:50:18.472: INFO: ss-0  tneyla25-md-0-68cbcb4798-5xxmw  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:27 +0000 UTC  }]
Feb  6 17:50:18.472: INFO: ss-1  tneyla25-md-0-68cbcb4798-lz542  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:47 +0000 UTC  }]
Feb  6 17:50:18.473: INFO: ss-2  tneyla25-md-0-68cbcb4798-6j94p  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:47 +0000 UTC  }]
Feb  6 17:50:18.473: INFO: 
Feb  6 17:50:18.473: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  6 17:50:19.477: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Feb  6 17:50:19.477: INFO: ss-2  tneyla25-md-0-68cbcb4798-6j94p  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:47 +0000 UTC  }]
Feb  6 17:50:19.477: INFO: 
Feb  6 17:50:19.477: INFO: StatefulSet ss has not reached scale 0, at 1
Feb  6 17:50:20.481: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990477325s
Feb  6 17:50:21.485: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986209437s
Feb  6 17:50:22.490: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.982024094s
Feb  6 17:50:23.494: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.977248046s
Feb  6 17:50:24.499: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.972464646s
Feb  6 17:50:25.503: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.968597329s
Feb  6 17:50:26.506: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.964671535s
Feb  6 17:50:27.511: INFO: Verifying statefulset ss doesn't scale past 0 for another 960.896417ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9835 02/06/23 17:50:28.511
Feb  6 17:50:28.517: INFO: Scaling statefulset ss to 0
Feb  6 17:50:28.528: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb  6 17:50:28.531: INFO: Deleting all statefulset in ns statefulset-9835
Feb  6 17:50:28.534: INFO: Scaling statefulset ss to 0
Feb  6 17:50:28.545: INFO: Waiting for statefulset status.replicas updated to 0
Feb  6 17:50:28.548: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb  6 17:50:28.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9835" for this suite. 02/06/23 17:50:28.59
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":111,"skipped":2131,"failed":0}
------------------------------
• [SLOW TEST] [61.540 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:49:27.058
    Feb  6 17:49:27.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename statefulset 02/06/23 17:49:27.059
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:49:27.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:49:27.077
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9835 02/06/23 17:49:27.085
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-9835 02/06/23 17:49:27.093
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9835 02/06/23 17:49:27.101
    Feb  6 17:49:27.107: INFO: Found 0 stateful pods, waiting for 1
    Feb  6 17:49:37.111: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/06/23 17:49:37.111
    Feb  6 17:49:37.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb  6 17:49:37.306: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb  6 17:49:37.306: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb  6 17:49:37.306: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb  6 17:49:37.311: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb  6 17:49:47.316: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb  6 17:49:47.316: INFO: Waiting for statefulset status.replicas updated to 0
    Feb  6 17:49:47.332: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
    Feb  6 17:49:47.332: INFO: ss-0  tneyla25-md-0-68cbcb4798-5xxmw  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:27 +0000 UTC  }]
    Feb  6 17:49:47.332: INFO: 
    Feb  6 17:49:47.332: INFO: StatefulSet ss has not reached scale 3, at 1
    Feb  6 17:49:48.337: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995660916s
    Feb  6 17:49:49.343: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990854055s
    Feb  6 17:49:50.349: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983738806s
    Feb  6 17:49:51.355: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978579611s
    Feb  6 17:49:52.360: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973028663s
    Feb  6 17:49:53.365: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967867695s
    Feb  6 17:49:54.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.962901288s
    Feb  6 17:49:55.375: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95738794s
    Feb  6 17:49:56.385: INFO: Verifying statefulset ss doesn't scale past 3 for another 951.998574ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9835 02/06/23 17:49:57.386
    Feb  6 17:49:57.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb  6 17:49:57.566: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb  6 17:49:57.566: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb  6 17:49:57.567: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb  6 17:49:57.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb  6 17:49:57.727: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb  6 17:49:57.727: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb  6 17:49:57.727: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb  6 17:49:57.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb  6 17:49:57.916: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb  6 17:49:57.916: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb  6 17:49:57.916: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb  6 17:49:57.919: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Feb  6 17:50:07.925: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 17:50:07.925: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 17:50:07.925: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 02/06/23 17:50:07.925
    Feb  6 17:50:07.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb  6 17:50:08.101: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb  6 17:50:08.101: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb  6 17:50:08.101: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb  6 17:50:08.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb  6 17:50:08.277: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb  6 17:50:08.277: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb  6 17:50:08.277: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb  6 17:50:08.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-9835 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb  6 17:50:08.440: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb  6 17:50:08.441: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb  6 17:50:08.441: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb  6 17:50:08.441: INFO: Waiting for statefulset status.replicas updated to 0
    Feb  6 17:50:08.444: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Feb  6 17:50:18.455: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb  6 17:50:18.455: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb  6 17:50:18.455: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb  6 17:50:18.472: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
    Feb  6 17:50:18.472: INFO: ss-0  tneyla25-md-0-68cbcb4798-5xxmw  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:27 +0000 UTC  }]
    Feb  6 17:50:18.472: INFO: ss-1  tneyla25-md-0-68cbcb4798-lz542  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:47 +0000 UTC  }]
    Feb  6 17:50:18.473: INFO: ss-2  tneyla25-md-0-68cbcb4798-6j94p  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:47 +0000 UTC  }]
    Feb  6 17:50:18.473: INFO: 
    Feb  6 17:50:18.473: INFO: StatefulSet ss has not reached scale 0, at 3
    Feb  6 17:50:19.477: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
    Feb  6 17:50:19.477: INFO: ss-2  tneyla25-md-0-68cbcb4798-6j94p  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:50:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:49:47 +0000 UTC  }]
    Feb  6 17:50:19.477: INFO: 
    Feb  6 17:50:19.477: INFO: StatefulSet ss has not reached scale 0, at 1
    Feb  6 17:50:20.481: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990477325s
    Feb  6 17:50:21.485: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986209437s
    Feb  6 17:50:22.490: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.982024094s
    Feb  6 17:50:23.494: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.977248046s
    Feb  6 17:50:24.499: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.972464646s
    Feb  6 17:50:25.503: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.968597329s
    Feb  6 17:50:26.506: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.964671535s
    Feb  6 17:50:27.511: INFO: Verifying statefulset ss doesn't scale past 0 for another 960.896417ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9835 02/06/23 17:50:28.511
    Feb  6 17:50:28.517: INFO: Scaling statefulset ss to 0
    Feb  6 17:50:28.528: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb  6 17:50:28.531: INFO: Deleting all statefulset in ns statefulset-9835
    Feb  6 17:50:28.534: INFO: Scaling statefulset ss to 0
    Feb  6 17:50:28.545: INFO: Waiting for statefulset status.replicas updated to 0
    Feb  6 17:50:28.548: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb  6 17:50:28.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9835" for this suite. 02/06/23 17:50:28.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:50:28.609
Feb  6 17:50:28.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubelet-test 02/06/23 17:50:28.61
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:50:28.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:50:28.627
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Feb  6 17:50:28.639: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479" in namespace "kubelet-test-271" to be "running and ready"
Feb  6 17:50:28.644: INFO: Pod "busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810547ms
Feb  6 17:50:28.644: INFO: The phase of Pod busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:50:30.648: INFO: Pod "busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009547649s
Feb  6 17:50:30.648: INFO: The phase of Pod busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:50:32.650: INFO: Pod "busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479": Phase="Running", Reason="", readiness=true. Elapsed: 4.01117507s
Feb  6 17:50:32.650: INFO: The phase of Pod busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479 is Running (Ready = true)
Feb  6 17:50:32.650: INFO: Pod "busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb  6 17:50:32.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-271" for this suite. 02/06/23 17:50:32.664
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":112,"skipped":2260,"failed":0}
------------------------------
• [4.062 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:50:28.609
    Feb  6 17:50:28.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubelet-test 02/06/23 17:50:28.61
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:50:28.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:50:28.627
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Feb  6 17:50:28.639: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479" in namespace "kubelet-test-271" to be "running and ready"
    Feb  6 17:50:28.644: INFO: Pod "busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810547ms
    Feb  6 17:50:28.644: INFO: The phase of Pod busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:50:30.648: INFO: Pod "busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009547649s
    Feb  6 17:50:30.648: INFO: The phase of Pod busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:50:32.650: INFO: Pod "busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479": Phase="Running", Reason="", readiness=true. Elapsed: 4.01117507s
    Feb  6 17:50:32.650: INFO: The phase of Pod busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479 is Running (Ready = true)
    Feb  6 17:50:32.650: INFO: Pod "busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb  6 17:50:32.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-271" for this suite. 02/06/23 17:50:32.664
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:50:32.672
Feb  6 17:50:32.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename daemonsets 02/06/23 17:50:32.673
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:50:32.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:50:32.693
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Feb  6 17:50:32.725: INFO: Create a RollingUpdate DaemonSet
Feb  6 17:50:32.730: INFO: Check that daemon pods launch on every node of the cluster
Feb  6 17:50:32.741: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:32.741: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:32.741: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:32.746: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:50:32.746: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:50:33.755: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:33.756: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:33.756: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:33.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:50:33.760: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:50:34.757: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:34.758: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:34.758: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:34.767: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:50:34.767: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:50:35.756: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:35.756: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:35.756: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:35.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:50:35.761: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:50:36.753: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:36.753: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:36.754: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:36.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb  6 17:50:36.757: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 17:50:37.752: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:37.752: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:37.752: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:37.756: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb  6 17:50:37.756: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Feb  6 17:50:37.756: INFO: Update the DaemonSet to trigger a rollout
Feb  6 17:50:37.768: INFO: Updating DaemonSet daemon-set
Feb  6 17:50:39.788: INFO: Roll back the DaemonSet before rollout is complete
Feb  6 17:50:39.800: INFO: Updating DaemonSet daemon-set
Feb  6 17:50:39.800: INFO: Make sure DaemonSet rollback is complete
Feb  6 17:50:39.806: INFO: Wrong image for pod: daemon-set-lxlqn. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Feb  6 17:50:39.806: INFO: Pod daemon-set-lxlqn is not available
Feb  6 17:50:39.812: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:39.813: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:39.813: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:40.822: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:40.822: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:40.822: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:41.823: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:41.823: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:41.823: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:42.823: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:42.823: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:42.823: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:43.821: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:43.821: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:43.821: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:44.818: INFO: Pod daemon-set-gnvp5 is not available
Feb  6 17:50:44.824: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:44.824: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 17:50:44.824: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/06/23 17:50:44.835
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-168, will wait for the garbage collector to delete the pods 02/06/23 17:50:44.836
Feb  6 17:50:44.898: INFO: Deleting DaemonSet.extensions daemon-set took: 9.170828ms
Feb  6 17:50:45.000: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.57391ms
Feb  6 17:50:48.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 17:50:48.305: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb  6 17:50:48.309: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31143"},"items":null}

Feb  6 17:50:48.313: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31143"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb  6 17:50:48.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-168" for this suite. 02/06/23 17:50:48.33
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":113,"skipped":2260,"failed":0}
------------------------------
• [SLOW TEST] [15.665 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:50:32.672
    Feb  6 17:50:32.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename daemonsets 02/06/23 17:50:32.673
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:50:32.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:50:32.693
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Feb  6 17:50:32.725: INFO: Create a RollingUpdate DaemonSet
    Feb  6 17:50:32.730: INFO: Check that daemon pods launch on every node of the cluster
    Feb  6 17:50:32.741: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:32.741: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:32.741: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:32.746: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:50:32.746: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:50:33.755: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:33.756: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:33.756: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:33.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:50:33.760: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:50:34.757: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:34.758: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:34.758: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:34.767: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:50:34.767: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:50:35.756: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:35.756: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:35.756: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:35.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:50:35.761: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:50:36.753: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:36.753: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:36.754: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:36.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb  6 17:50:36.757: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 17:50:37.752: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:37.752: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:37.752: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:37.756: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb  6 17:50:37.756: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Feb  6 17:50:37.756: INFO: Update the DaemonSet to trigger a rollout
    Feb  6 17:50:37.768: INFO: Updating DaemonSet daemon-set
    Feb  6 17:50:39.788: INFO: Roll back the DaemonSet before rollout is complete
    Feb  6 17:50:39.800: INFO: Updating DaemonSet daemon-set
    Feb  6 17:50:39.800: INFO: Make sure DaemonSet rollback is complete
    Feb  6 17:50:39.806: INFO: Wrong image for pod: daemon-set-lxlqn. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Feb  6 17:50:39.806: INFO: Pod daemon-set-lxlqn is not available
    Feb  6 17:50:39.812: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:39.813: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:39.813: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:40.822: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:40.822: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:40.822: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:41.823: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:41.823: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:41.823: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:42.823: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:42.823: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:42.823: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:43.821: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:43.821: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:43.821: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:44.818: INFO: Pod daemon-set-gnvp5 is not available
    Feb  6 17:50:44.824: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:44.824: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 17:50:44.824: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/06/23 17:50:44.835
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-168, will wait for the garbage collector to delete the pods 02/06/23 17:50:44.836
    Feb  6 17:50:44.898: INFO: Deleting DaemonSet.extensions daemon-set took: 9.170828ms
    Feb  6 17:50:45.000: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.57391ms
    Feb  6 17:50:48.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 17:50:48.305: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb  6 17:50:48.309: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31143"},"items":null}

    Feb  6 17:50:48.313: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31143"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 17:50:48.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-168" for this suite. 02/06/23 17:50:48.33
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:50:48.338
Feb  6 17:50:48.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename sched-pred 02/06/23 17:50:48.339
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:50:48.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:50:48.358
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb  6 17:50:48.361: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  6 17:50:48.372: INFO: Waiting for terminating namespaces to be deleted...
Feb  6 17:50:48.376: INFO: 
Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-5xxmw before test
Feb  6 17:50:48.387: INFO: cilium-9gdv4 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.387: INFO: 	Container cilium-agent ready: true, restart count 0
Feb  6 17:50:48.387: INFO: kube-proxy-mnvm8 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.387: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  6 17:50:48.387: INFO: vsphere-cloud-controller-manager-7dmv6 from kube-system started at 2023-02-06 17:31:18 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.387: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Feb  6 17:50:48.387: INFO: vsphere-csi-node-7l6qr from kube-system started at 2023-02-06 17:12:22 +0000 UTC (3 container statuses recorded)
Feb  6 17:50:48.387: INFO: 	Container liveness-probe ready: true, restart count 0
Feb  6 17:50:48.387: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  6 17:50:48.387: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Feb  6 17:50:48.387: INFO: busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479 from kubelet-test-271 started at 2023-02-06 17:50:28 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.387: INFO: 	Container busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479 ready: true, restart count 0
Feb  6 17:50:48.387: INFO: sonobuoy from sonobuoy started at 2023-02-06 17:18:54 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.387: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  6 17:50:48.387: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-cr8qs from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 17:50:48.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 17:50:48.388: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  6 17:50:48.388: INFO: 
Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-6j94p before test
Feb  6 17:50:48.399: INFO: capi-kubeadm-control-plane-controller-manager-5f686d9d47-lkld2 from capi-kubeadm-control-plane-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.399: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:50:48.399: INFO: capi-controller-manager-5b68b88d59-vcpp2 from capi-system started at 2023-02-06 17:15:34 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.399: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:50:48.399: INFO: cert-manager-7b9fcd6db9-rz78g from cert-manager started at 2023-02-06 17:15:16 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.400: INFO: 	Container cert-manager ready: true, restart count 0
Feb  6 17:50:48.400: INFO: cert-manager-cainjector-75b587b679-rjbzj from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.400: INFO: 	Container cert-manager ready: true, restart count 0
Feb  6 17:50:48.400: INFO: cert-manager-webhook-896b549c4-hj9c4 from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.400: INFO: 	Container cert-manager ready: true, restart count 0
Feb  6 17:50:48.400: INFO: eks-anywhere-packages-8589bd5d9c-4pc9v from eksa-packages started at 2023-02-06 17:17:15 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.400: INFO: 	Container controller ready: true, restart count 0
Feb  6 17:50:48.400: INFO: etcdadm-bootstrap-provider-controller-manager-84ff796964-d6tt8 from etcdadm-bootstrap-provider-system started at 2023-02-06 17:15:37 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.400: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:50:48.400: INFO: cilium-nb887 from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.400: INFO: 	Container cilium-agent ready: true, restart count 0
Feb  6 17:50:48.400: INFO: kube-proxy-ld4nv from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.401: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  6 17:50:48.401: INFO: vsphere-cloud-controller-manager-76l5k from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.401: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Feb  6 17:50:48.401: INFO: vsphere-csi-node-2tfxk from kube-system started at 2023-02-06 17:12:26 +0000 UTC (3 container statuses recorded)
Feb  6 17:50:48.401: INFO: 	Container liveness-probe ready: true, restart count 0
Feb  6 17:50:48.401: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  6 17:50:48.401: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Feb  6 17:50:48.401: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-hcvcv from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 17:50:48.401: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 17:50:48.401: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  6 17:50:48.401: INFO: 
Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-lz542 before test
Feb  6 17:50:48.413: INFO: capi-kubeadm-bootstrap-controller-manager-79d4bdf548-4xmjq from capi-kubeadm-bootstrap-system started at 2023-02-06 17:15:36 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.413: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:50:48.413: INFO: capv-controller-manager-598f9bf684-4qdws from capv-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.413: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:50:48.413: INFO: eksa-controller-manager-5588899966-wtcpc from eksa-system started at 2023-02-06 17:16:14 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.413: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:50:48.413: INFO: etcdadm-controller-controller-manager-767b7fb9b5-mt7lx from etcdadm-controller-system started at 2023-02-06 17:15:38 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.413: INFO: 	Container manager ready: true, restart count 0
Feb  6 17:50:48.413: INFO: cilium-ndf4k from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.413: INFO: 	Container cilium-agent ready: true, restart count 0
Feb  6 17:50:48.413: INFO: cilium-operator-7f966b595f-pzn9c from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.413: INFO: 	Container cilium-operator ready: true, restart count 0
Feb  6 17:50:48.413: INFO: kube-proxy-hfblf from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.413: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  6 17:50:48.413: INFO: vsphere-cloud-controller-manager-x8k8p from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 17:50:48.413: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Feb  6 17:50:48.413: INFO: vsphere-csi-node-v4zd6 from kube-system started at 2023-02-06 17:12:15 +0000 UTC (3 container statuses recorded)
Feb  6 17:50:48.413: INFO: 	Container liveness-probe ready: true, restart count 0
Feb  6 17:50:48.413: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  6 17:50:48.413: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Feb  6 17:50:48.413: INFO: sonobuoy-e2e-job-b0e9e31f5d894166 from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 17:50:48.413: INFO: 	Container e2e ready: true, restart count 0
Feb  6 17:50:48.413: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 17:50:48.413: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-fncmf from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 17:50:48.413: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 17:50:48.413: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node tneyla25-md-0-68cbcb4798-5xxmw 02/06/23 17:50:48.446
STEP: verifying the node has the label node tneyla25-md-0-68cbcb4798-6j94p 02/06/23 17:50:48.471
STEP: verifying the node has the label node tneyla25-md-0-68cbcb4798-lz542 02/06/23 17:50:48.501
Feb  6 17:50:48.547: INFO: Pod capi-kubeadm-bootstrap-controller-manager-79d4bdf548-4xmjq requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
Feb  6 17:50:48.547: INFO: Pod capi-kubeadm-control-plane-controller-manager-5f686d9d47-lkld2 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
Feb  6 17:50:48.548: INFO: Pod capi-controller-manager-5b68b88d59-vcpp2 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
Feb  6 17:50:48.548: INFO: Pod capv-controller-manager-598f9bf684-4qdws requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
Feb  6 17:50:48.548: INFO: Pod cert-manager-7b9fcd6db9-rz78g requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
Feb  6 17:50:48.549: INFO: Pod cert-manager-cainjector-75b587b679-rjbzj requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
Feb  6 17:50:48.556: INFO: Pod cert-manager-webhook-896b549c4-hj9c4 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
Feb  6 17:50:48.557: INFO: Pod eks-anywhere-packages-8589bd5d9c-4pc9v requesting resource cpu=100m on Node tneyla25-md-0-68cbcb4798-6j94p
Feb  6 17:50:48.557: INFO: Pod eksa-controller-manager-5588899966-wtcpc requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
Feb  6 17:50:48.558: INFO: Pod etcdadm-bootstrap-provider-controller-manager-84ff796964-d6tt8 requesting resource cpu=100m on Node tneyla25-md-0-68cbcb4798-6j94p
Feb  6 17:50:48.558: INFO: Pod etcdadm-controller-controller-manager-767b7fb9b5-mt7lx requesting resource cpu=100m on Node tneyla25-md-0-68cbcb4798-lz542
Feb  6 17:50:48.558: INFO: Pod cilium-9gdv4 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-5xxmw
Feb  6 17:50:48.558: INFO: Pod cilium-nb887 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
Feb  6 17:50:48.558: INFO: Pod cilium-ndf4k requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
Feb  6 17:50:48.559: INFO: Pod cilium-operator-7f966b595f-pzn9c requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
Feb  6 17:50:48.559: INFO: Pod kube-proxy-hfblf requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
Feb  6 17:50:48.559: INFO: Pod kube-proxy-ld4nv requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
Feb  6 17:50:48.560: INFO: Pod kube-proxy-mnvm8 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-5xxmw
Feb  6 17:50:48.560: INFO: Pod vsphere-cloud-controller-manager-76l5k requesting resource cpu=200m on Node tneyla25-md-0-68cbcb4798-6j94p
Feb  6 17:50:48.560: INFO: Pod vsphere-cloud-controller-manager-7dmv6 requesting resource cpu=200m on Node tneyla25-md-0-68cbcb4798-5xxmw
Feb  6 17:50:48.560: INFO: Pod vsphere-cloud-controller-manager-x8k8p requesting resource cpu=200m on Node tneyla25-md-0-68cbcb4798-lz542
Feb  6 17:50:48.561: INFO: Pod vsphere-csi-node-2tfxk requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
Feb  6 17:50:48.561: INFO: Pod vsphere-csi-node-7l6qr requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-5xxmw
Feb  6 17:50:48.561: INFO: Pod vsphere-csi-node-v4zd6 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
Feb  6 17:50:48.561: INFO: Pod busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-5xxmw
Feb  6 17:50:48.561: INFO: Pod sonobuoy requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-5xxmw
Feb  6 17:50:48.562: INFO: Pod sonobuoy-e2e-job-b0e9e31f5d894166 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
Feb  6 17:50:48.562: INFO: Pod sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-cr8qs requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-5xxmw
Feb  6 17:50:48.562: INFO: Pod sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-fncmf requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
Feb  6 17:50:48.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-hcvcv requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
STEP: Starting Pods to consume most of the cluster CPU. 02/06/23 17:50:48.563
Feb  6 17:50:48.564: INFO: Creating a pod which consumes cpu=1260m on Node tneyla25-md-0-68cbcb4798-5xxmw
Feb  6 17:50:48.573: INFO: Creating a pod which consumes cpu=1120m on Node tneyla25-md-0-68cbcb4798-6j94p
Feb  6 17:50:48.591: INFO: Creating a pod which consumes cpu=1190m on Node tneyla25-md-0-68cbcb4798-lz542
Feb  6 17:50:48.616: INFO: Waiting up to 5m0s for pod "filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c" in namespace "sched-pred-3449" to be "running"
Feb  6 17:50:48.637: INFO: Pod "filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.142822ms
Feb  6 17:50:50.642: INFO: Pod "filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02529846s
Feb  6 17:50:52.641: INFO: Pod "filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c": Phase="Running", Reason="", readiness=true. Elapsed: 4.024656168s
Feb  6 17:50:52.641: INFO: Pod "filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c" satisfied condition "running"
Feb  6 17:50:52.642: INFO: Waiting up to 5m0s for pod "filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c" in namespace "sched-pred-3449" to be "running"
Feb  6 17:50:52.644: INFO: Pod "filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c": Phase="Running", Reason="", readiness=true. Elapsed: 2.717538ms
Feb  6 17:50:52.644: INFO: Pod "filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c" satisfied condition "running"
Feb  6 17:50:52.644: INFO: Waiting up to 5m0s for pod "filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d" in namespace "sched-pred-3449" to be "running"
Feb  6 17:50:52.647: INFO: Pod "filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d": Phase="Running", Reason="", readiness=true. Elapsed: 2.705579ms
Feb  6 17:50:52.647: INFO: Pod "filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 02/06/23 17:50:52.647
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c.17414d974cfaac86], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3449/filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c to tneyla25-md-0-68cbcb4798-5xxmw] 02/06/23 17:50:52.651
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c.17414d97ebc0b43a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/06/23 17:50:52.651
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c.17414d97eed948fb], Reason = [Created], Message = [Created container filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c] 02/06/23 17:50:52.651
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c.17414d97f726fb54], Reason = [Started], Message = [Started container filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c] 02/06/23 17:50:52.651
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c.17414d974e752c7f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3449/filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c to tneyla25-md-0-68cbcb4798-6j94p] 02/06/23 17:50:52.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c.17414d97e770e6b7], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/06/23 17:50:52.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c.17414d97ea5c65cb], Reason = [Created], Message = [Created container filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c] 02/06/23 17:50:52.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c.17414d97f3766bd4], Reason = [Started], Message = [Started container filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c] 02/06/23 17:50:52.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d.17414d974fc1f412], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3449/filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d to tneyla25-md-0-68cbcb4798-lz542] 02/06/23 17:50:52.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d.17414d97ed52175a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/06/23 17:50:52.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d.17414d97f2f6d51e], Reason = [Created], Message = [Created container filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d] 02/06/23 17:50:52.652
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d.17414d97fc37560d], Reason = [Started], Message = [Started container filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d] 02/06/23 17:50:52.653
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17414d983fcff1f1], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 4 Insufficient cpu. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 02/06/23 17:50:52.666
STEP: removing the label node off the node tneyla25-md-0-68cbcb4798-5xxmw 02/06/23 17:50:53.664
STEP: verifying the node doesn't have the label node 02/06/23 17:50:53.685
STEP: removing the label node off the node tneyla25-md-0-68cbcb4798-6j94p 02/06/23 17:50:53.694
STEP: verifying the node doesn't have the label node 02/06/23 17:50:53.713
STEP: removing the label node off the node tneyla25-md-0-68cbcb4798-lz542 02/06/23 17:50:53.716
STEP: verifying the node doesn't have the label node 02/06/23 17:50:53.734
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb  6 17:50:53.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3449" for this suite. 02/06/23 17:50:53.755
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":114,"skipped":2261,"failed":0}
------------------------------
• [SLOW TEST] [5.428 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:50:48.338
    Feb  6 17:50:48.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename sched-pred 02/06/23 17:50:48.339
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:50:48.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:50:48.358
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb  6 17:50:48.361: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb  6 17:50:48.372: INFO: Waiting for terminating namespaces to be deleted...
    Feb  6 17:50:48.376: INFO: 
    Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-5xxmw before test
    Feb  6 17:50:48.387: INFO: cilium-9gdv4 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.387: INFO: 	Container cilium-agent ready: true, restart count 0
    Feb  6 17:50:48.387: INFO: kube-proxy-mnvm8 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.387: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb  6 17:50:48.387: INFO: vsphere-cloud-controller-manager-7dmv6 from kube-system started at 2023-02-06 17:31:18 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.387: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
    Feb  6 17:50:48.387: INFO: vsphere-csi-node-7l6qr from kube-system started at 2023-02-06 17:12:22 +0000 UTC (3 container statuses recorded)
    Feb  6 17:50:48.387: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb  6 17:50:48.387: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb  6 17:50:48.387: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Feb  6 17:50:48.387: INFO: busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479 from kubelet-test-271 started at 2023-02-06 17:50:28 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.387: INFO: 	Container busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479 ready: true, restart count 0
    Feb  6 17:50:48.387: INFO: sonobuoy from sonobuoy started at 2023-02-06 17:18:54 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.387: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb  6 17:50:48.387: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-cr8qs from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 17:50:48.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 17:50:48.388: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb  6 17:50:48.388: INFO: 
    Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-6j94p before test
    Feb  6 17:50:48.399: INFO: capi-kubeadm-control-plane-controller-manager-5f686d9d47-lkld2 from capi-kubeadm-control-plane-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.399: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:50:48.399: INFO: capi-controller-manager-5b68b88d59-vcpp2 from capi-system started at 2023-02-06 17:15:34 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.399: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:50:48.399: INFO: cert-manager-7b9fcd6db9-rz78g from cert-manager started at 2023-02-06 17:15:16 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.400: INFO: 	Container cert-manager ready: true, restart count 0
    Feb  6 17:50:48.400: INFO: cert-manager-cainjector-75b587b679-rjbzj from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.400: INFO: 	Container cert-manager ready: true, restart count 0
    Feb  6 17:50:48.400: INFO: cert-manager-webhook-896b549c4-hj9c4 from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.400: INFO: 	Container cert-manager ready: true, restart count 0
    Feb  6 17:50:48.400: INFO: eks-anywhere-packages-8589bd5d9c-4pc9v from eksa-packages started at 2023-02-06 17:17:15 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.400: INFO: 	Container controller ready: true, restart count 0
    Feb  6 17:50:48.400: INFO: etcdadm-bootstrap-provider-controller-manager-84ff796964-d6tt8 from etcdadm-bootstrap-provider-system started at 2023-02-06 17:15:37 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.400: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:50:48.400: INFO: cilium-nb887 from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.400: INFO: 	Container cilium-agent ready: true, restart count 0
    Feb  6 17:50:48.400: INFO: kube-proxy-ld4nv from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.401: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb  6 17:50:48.401: INFO: vsphere-cloud-controller-manager-76l5k from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.401: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
    Feb  6 17:50:48.401: INFO: vsphere-csi-node-2tfxk from kube-system started at 2023-02-06 17:12:26 +0000 UTC (3 container statuses recorded)
    Feb  6 17:50:48.401: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb  6 17:50:48.401: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb  6 17:50:48.401: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Feb  6 17:50:48.401: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-hcvcv from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 17:50:48.401: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 17:50:48.401: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb  6 17:50:48.401: INFO: 
    Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-lz542 before test
    Feb  6 17:50:48.413: INFO: capi-kubeadm-bootstrap-controller-manager-79d4bdf548-4xmjq from capi-kubeadm-bootstrap-system started at 2023-02-06 17:15:36 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.413: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: capv-controller-manager-598f9bf684-4qdws from capv-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.413: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: eksa-controller-manager-5588899966-wtcpc from eksa-system started at 2023-02-06 17:16:14 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.413: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: etcdadm-controller-controller-manager-767b7fb9b5-mt7lx from etcdadm-controller-system started at 2023-02-06 17:15:38 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.413: INFO: 	Container manager ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: cilium-ndf4k from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.413: INFO: 	Container cilium-agent ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: cilium-operator-7f966b595f-pzn9c from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.413: INFO: 	Container cilium-operator ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: kube-proxy-hfblf from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.413: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: vsphere-cloud-controller-manager-x8k8p from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 17:50:48.413: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: vsphere-csi-node-v4zd6 from kube-system started at 2023-02-06 17:12:15 +0000 UTC (3 container statuses recorded)
    Feb  6 17:50:48.413: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: sonobuoy-e2e-job-b0e9e31f5d894166 from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 17:50:48.413: INFO: 	Container e2e ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-fncmf from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 17:50:48.413: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 17:50:48.413: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node tneyla25-md-0-68cbcb4798-5xxmw 02/06/23 17:50:48.446
    STEP: verifying the node has the label node tneyla25-md-0-68cbcb4798-6j94p 02/06/23 17:50:48.471
    STEP: verifying the node has the label node tneyla25-md-0-68cbcb4798-lz542 02/06/23 17:50:48.501
    Feb  6 17:50:48.547: INFO: Pod capi-kubeadm-bootstrap-controller-manager-79d4bdf548-4xmjq requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
    Feb  6 17:50:48.547: INFO: Pod capi-kubeadm-control-plane-controller-manager-5f686d9d47-lkld2 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
    Feb  6 17:50:48.548: INFO: Pod capi-controller-manager-5b68b88d59-vcpp2 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
    Feb  6 17:50:48.548: INFO: Pod capv-controller-manager-598f9bf684-4qdws requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
    Feb  6 17:50:48.548: INFO: Pod cert-manager-7b9fcd6db9-rz78g requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
    Feb  6 17:50:48.549: INFO: Pod cert-manager-cainjector-75b587b679-rjbzj requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
    Feb  6 17:50:48.556: INFO: Pod cert-manager-webhook-896b549c4-hj9c4 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
    Feb  6 17:50:48.557: INFO: Pod eks-anywhere-packages-8589bd5d9c-4pc9v requesting resource cpu=100m on Node tneyla25-md-0-68cbcb4798-6j94p
    Feb  6 17:50:48.557: INFO: Pod eksa-controller-manager-5588899966-wtcpc requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
    Feb  6 17:50:48.558: INFO: Pod etcdadm-bootstrap-provider-controller-manager-84ff796964-d6tt8 requesting resource cpu=100m on Node tneyla25-md-0-68cbcb4798-6j94p
    Feb  6 17:50:48.558: INFO: Pod etcdadm-controller-controller-manager-767b7fb9b5-mt7lx requesting resource cpu=100m on Node tneyla25-md-0-68cbcb4798-lz542
    Feb  6 17:50:48.558: INFO: Pod cilium-9gdv4 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-5xxmw
    Feb  6 17:50:48.558: INFO: Pod cilium-nb887 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
    Feb  6 17:50:48.558: INFO: Pod cilium-ndf4k requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
    Feb  6 17:50:48.559: INFO: Pod cilium-operator-7f966b595f-pzn9c requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
    Feb  6 17:50:48.559: INFO: Pod kube-proxy-hfblf requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
    Feb  6 17:50:48.559: INFO: Pod kube-proxy-ld4nv requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
    Feb  6 17:50:48.560: INFO: Pod kube-proxy-mnvm8 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-5xxmw
    Feb  6 17:50:48.560: INFO: Pod vsphere-cloud-controller-manager-76l5k requesting resource cpu=200m on Node tneyla25-md-0-68cbcb4798-6j94p
    Feb  6 17:50:48.560: INFO: Pod vsphere-cloud-controller-manager-7dmv6 requesting resource cpu=200m on Node tneyla25-md-0-68cbcb4798-5xxmw
    Feb  6 17:50:48.560: INFO: Pod vsphere-cloud-controller-manager-x8k8p requesting resource cpu=200m on Node tneyla25-md-0-68cbcb4798-lz542
    Feb  6 17:50:48.561: INFO: Pod vsphere-csi-node-2tfxk requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
    Feb  6 17:50:48.561: INFO: Pod vsphere-csi-node-7l6qr requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-5xxmw
    Feb  6 17:50:48.561: INFO: Pod vsphere-csi-node-v4zd6 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
    Feb  6 17:50:48.561: INFO: Pod busybox-readonly-fsa6e31b72-c396-4ea7-bd9a-f1c6e15cb479 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-5xxmw
    Feb  6 17:50:48.561: INFO: Pod sonobuoy requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-5xxmw
    Feb  6 17:50:48.562: INFO: Pod sonobuoy-e2e-job-b0e9e31f5d894166 requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
    Feb  6 17:50:48.562: INFO: Pod sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-cr8qs requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-5xxmw
    Feb  6 17:50:48.562: INFO: Pod sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-fncmf requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-lz542
    Feb  6 17:50:48.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-hcvcv requesting resource cpu=0m on Node tneyla25-md-0-68cbcb4798-6j94p
    STEP: Starting Pods to consume most of the cluster CPU. 02/06/23 17:50:48.563
    Feb  6 17:50:48.564: INFO: Creating a pod which consumes cpu=1260m on Node tneyla25-md-0-68cbcb4798-5xxmw
    Feb  6 17:50:48.573: INFO: Creating a pod which consumes cpu=1120m on Node tneyla25-md-0-68cbcb4798-6j94p
    Feb  6 17:50:48.591: INFO: Creating a pod which consumes cpu=1190m on Node tneyla25-md-0-68cbcb4798-lz542
    Feb  6 17:50:48.616: INFO: Waiting up to 5m0s for pod "filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c" in namespace "sched-pred-3449" to be "running"
    Feb  6 17:50:48.637: INFO: Pod "filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.142822ms
    Feb  6 17:50:50.642: INFO: Pod "filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02529846s
    Feb  6 17:50:52.641: INFO: Pod "filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c": Phase="Running", Reason="", readiness=true. Elapsed: 4.024656168s
    Feb  6 17:50:52.641: INFO: Pod "filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c" satisfied condition "running"
    Feb  6 17:50:52.642: INFO: Waiting up to 5m0s for pod "filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c" in namespace "sched-pred-3449" to be "running"
    Feb  6 17:50:52.644: INFO: Pod "filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c": Phase="Running", Reason="", readiness=true. Elapsed: 2.717538ms
    Feb  6 17:50:52.644: INFO: Pod "filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c" satisfied condition "running"
    Feb  6 17:50:52.644: INFO: Waiting up to 5m0s for pod "filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d" in namespace "sched-pred-3449" to be "running"
    Feb  6 17:50:52.647: INFO: Pod "filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d": Phase="Running", Reason="", readiness=true. Elapsed: 2.705579ms
    Feb  6 17:50:52.647: INFO: Pod "filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 02/06/23 17:50:52.647
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c.17414d974cfaac86], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3449/filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c to tneyla25-md-0-68cbcb4798-5xxmw] 02/06/23 17:50:52.651
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c.17414d97ebc0b43a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/06/23 17:50:52.651
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c.17414d97eed948fb], Reason = [Created], Message = [Created container filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c] 02/06/23 17:50:52.651
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c.17414d97f726fb54], Reason = [Started], Message = [Started container filler-pod-28bcd5ca-9b6f-4b3a-97ae-84087744933c] 02/06/23 17:50:52.651
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c.17414d974e752c7f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3449/filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c to tneyla25-md-0-68cbcb4798-6j94p] 02/06/23 17:50:52.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c.17414d97e770e6b7], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/06/23 17:50:52.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c.17414d97ea5c65cb], Reason = [Created], Message = [Created container filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c] 02/06/23 17:50:52.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c.17414d97f3766bd4], Reason = [Started], Message = [Started container filler-pod-4ea96aa0-aef3-43e3-8b51-81f8397fac3c] 02/06/23 17:50:52.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d.17414d974fc1f412], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3449/filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d to tneyla25-md-0-68cbcb4798-lz542] 02/06/23 17:50:52.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d.17414d97ed52175a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/06/23 17:50:52.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d.17414d97f2f6d51e], Reason = [Created], Message = [Created container filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d] 02/06/23 17:50:52.652
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d.17414d97fc37560d], Reason = [Started], Message = [Started container filler-pod-56fc1d22-3616-4609-b31d-9e1cef7f986d] 02/06/23 17:50:52.653
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17414d983fcff1f1], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 4 Insufficient cpu. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 02/06/23 17:50:52.666
    STEP: removing the label node off the node tneyla25-md-0-68cbcb4798-5xxmw 02/06/23 17:50:53.664
    STEP: verifying the node doesn't have the label node 02/06/23 17:50:53.685
    STEP: removing the label node off the node tneyla25-md-0-68cbcb4798-6j94p 02/06/23 17:50:53.694
    STEP: verifying the node doesn't have the label node 02/06/23 17:50:53.713
    STEP: removing the label node off the node tneyla25-md-0-68cbcb4798-lz542 02/06/23 17:50:53.716
    STEP: verifying the node doesn't have the label node 02/06/23 17:50:53.734
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 17:50:53.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3449" for this suite. 02/06/23 17:50:53.755
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:50:53.796
Feb  6 17:50:53.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 17:50:53.797
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:50:53.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:50:53.817
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 02/06/23 17:50:53.82
Feb  6 17:50:53.833: INFO: Waiting up to 5m0s for pod "pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b" in namespace "emptydir-1249" to be "Succeeded or Failed"
Feb  6 17:50:53.838: INFO: Pod "pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.851379ms
Feb  6 17:50:55.842: INFO: Pod "pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008523936s
Feb  6 17:50:57.846: INFO: Pod "pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012465239s
Feb  6 17:50:59.845: INFO: Pod "pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011871097s
STEP: Saw pod success 02/06/23 17:50:59.845
Feb  6 17:50:59.846: INFO: Pod "pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b" satisfied condition "Succeeded or Failed"
Feb  6 17:50:59.849: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b container test-container: <nil>
STEP: delete the pod 02/06/23 17:50:59.862
Feb  6 17:50:59.880: INFO: Waiting for pod pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b to disappear
Feb  6 17:50:59.883: INFO: Pod pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 17:50:59.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1249" for this suite. 02/06/23 17:50:59.892
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":115,"skipped":2292,"failed":0}
------------------------------
• [SLOW TEST] [6.106 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:50:53.796
    Feb  6 17:50:53.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 17:50:53.797
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:50:53.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:50:53.817
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/06/23 17:50:53.82
    Feb  6 17:50:53.833: INFO: Waiting up to 5m0s for pod "pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b" in namespace "emptydir-1249" to be "Succeeded or Failed"
    Feb  6 17:50:53.838: INFO: Pod "pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.851379ms
    Feb  6 17:50:55.842: INFO: Pod "pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008523936s
    Feb  6 17:50:57.846: INFO: Pod "pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012465239s
    Feb  6 17:50:59.845: INFO: Pod "pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011871097s
    STEP: Saw pod success 02/06/23 17:50:59.845
    Feb  6 17:50:59.846: INFO: Pod "pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b" satisfied condition "Succeeded or Failed"
    Feb  6 17:50:59.849: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b container test-container: <nil>
    STEP: delete the pod 02/06/23 17:50:59.862
    Feb  6 17:50:59.880: INFO: Waiting for pod pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b to disappear
    Feb  6 17:50:59.883: INFO: Pod pod-b6d5b7cc-3a96-4b3d-9d0e-2cb51e53915b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 17:50:59.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1249" for this suite. 02/06/23 17:50:59.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:50:59.904
Feb  6 17:50:59.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 17:50:59.905
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:50:59.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:50:59.931
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-7773b0ab-a1b9-41a8-ae45-9369cb00fbc6 02/06/23 17:50:59.933
STEP: Creating a pod to test consume configMaps 02/06/23 17:50:59.941
Feb  6 17:50:59.952: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0" in namespace "projected-7897" to be "Succeeded or Failed"
Feb  6 17:50:59.962: INFO: Pod "pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.341147ms
Feb  6 17:51:01.968: INFO: Pod "pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015852701s
Feb  6 17:51:03.968: INFO: Pod "pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016144197s
Feb  6 17:51:05.968: INFO: Pod "pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015752185s
STEP: Saw pod success 02/06/23 17:51:05.968
Feb  6 17:51:05.968: INFO: Pod "pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0" satisfied condition "Succeeded or Failed"
Feb  6 17:51:05.971: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0 container agnhost-container: <nil>
STEP: delete the pod 02/06/23 17:51:05.977
Feb  6 17:51:05.995: INFO: Waiting for pod pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0 to disappear
Feb  6 17:51:05.998: INFO: Pod pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb  6 17:51:05.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7897" for this suite. 02/06/23 17:51:06.003
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":116,"skipped":2310,"failed":0}
------------------------------
• [SLOW TEST] [6.105 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:50:59.904
    Feb  6 17:50:59.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 17:50:59.905
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:50:59.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:50:59.931
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-7773b0ab-a1b9-41a8-ae45-9369cb00fbc6 02/06/23 17:50:59.933
    STEP: Creating a pod to test consume configMaps 02/06/23 17:50:59.941
    Feb  6 17:50:59.952: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0" in namespace "projected-7897" to be "Succeeded or Failed"
    Feb  6 17:50:59.962: INFO: Pod "pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.341147ms
    Feb  6 17:51:01.968: INFO: Pod "pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015852701s
    Feb  6 17:51:03.968: INFO: Pod "pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016144197s
    Feb  6 17:51:05.968: INFO: Pod "pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015752185s
    STEP: Saw pod success 02/06/23 17:51:05.968
    Feb  6 17:51:05.968: INFO: Pod "pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0" satisfied condition "Succeeded or Failed"
    Feb  6 17:51:05.971: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0 container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 17:51:05.977
    Feb  6 17:51:05.995: INFO: Waiting for pod pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0 to disappear
    Feb  6 17:51:05.998: INFO: Pod pod-projected-configmaps-4a135a6a-61f5-4c67-998e-6ff9824dbba0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb  6 17:51:05.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7897" for this suite. 02/06/23 17:51:06.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:51:06.013
Feb  6 17:51:06.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename security-context 02/06/23 17:51:06.014
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:51:06.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:51:06.029
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/06/23 17:51:06.032
Feb  6 17:51:06.040: INFO: Waiting up to 5m0s for pod "security-context-8844fe89-68e3-4481-944b-ccdf1f40839d" in namespace "security-context-3965" to be "Succeeded or Failed"
Feb  6 17:51:06.043: INFO: Pod "security-context-8844fe89-68e3-4481-944b-ccdf1f40839d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.986448ms
Feb  6 17:51:08.048: INFO: Pod "security-context-8844fe89-68e3-4481-944b-ccdf1f40839d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007790199s
Feb  6 17:51:10.047: INFO: Pod "security-context-8844fe89-68e3-4481-944b-ccdf1f40839d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00709569s
Feb  6 17:51:12.048: INFO: Pod "security-context-8844fe89-68e3-4481-944b-ccdf1f40839d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007558233s
STEP: Saw pod success 02/06/23 17:51:12.048
Feb  6 17:51:12.048: INFO: Pod "security-context-8844fe89-68e3-4481-944b-ccdf1f40839d" satisfied condition "Succeeded or Failed"
Feb  6 17:51:12.051: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod security-context-8844fe89-68e3-4481-944b-ccdf1f40839d container test-container: <nil>
STEP: delete the pod 02/06/23 17:51:12.056
Feb  6 17:51:12.068: INFO: Waiting for pod security-context-8844fe89-68e3-4481-944b-ccdf1f40839d to disappear
Feb  6 17:51:12.070: INFO: Pod security-context-8844fe89-68e3-4481-944b-ccdf1f40839d no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb  6 17:51:12.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3965" for this suite. 02/06/23 17:51:12.074
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":117,"skipped":2324,"failed":0}
------------------------------
• [SLOW TEST] [6.068 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:51:06.013
    Feb  6 17:51:06.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename security-context 02/06/23 17:51:06.014
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:51:06.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:51:06.029
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/06/23 17:51:06.032
    Feb  6 17:51:06.040: INFO: Waiting up to 5m0s for pod "security-context-8844fe89-68e3-4481-944b-ccdf1f40839d" in namespace "security-context-3965" to be "Succeeded or Failed"
    Feb  6 17:51:06.043: INFO: Pod "security-context-8844fe89-68e3-4481-944b-ccdf1f40839d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.986448ms
    Feb  6 17:51:08.048: INFO: Pod "security-context-8844fe89-68e3-4481-944b-ccdf1f40839d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007790199s
    Feb  6 17:51:10.047: INFO: Pod "security-context-8844fe89-68e3-4481-944b-ccdf1f40839d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00709569s
    Feb  6 17:51:12.048: INFO: Pod "security-context-8844fe89-68e3-4481-944b-ccdf1f40839d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007558233s
    STEP: Saw pod success 02/06/23 17:51:12.048
    Feb  6 17:51:12.048: INFO: Pod "security-context-8844fe89-68e3-4481-944b-ccdf1f40839d" satisfied condition "Succeeded or Failed"
    Feb  6 17:51:12.051: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod security-context-8844fe89-68e3-4481-944b-ccdf1f40839d container test-container: <nil>
    STEP: delete the pod 02/06/23 17:51:12.056
    Feb  6 17:51:12.068: INFO: Waiting for pod security-context-8844fe89-68e3-4481-944b-ccdf1f40839d to disappear
    Feb  6 17:51:12.070: INFO: Pod security-context-8844fe89-68e3-4481-944b-ccdf1f40839d no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb  6 17:51:12.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-3965" for this suite. 02/06/23 17:51:12.074
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:51:12.084
Feb  6 17:51:12.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename events 02/06/23 17:51:12.086
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:51:12.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:51:12.11
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 02/06/23 17:51:12.115
STEP: listing events in all namespaces 02/06/23 17:51:12.122
STEP: listing events in test namespace 02/06/23 17:51:12.136
STEP: listing events with field selection filtering on source 02/06/23 17:51:12.14
STEP: listing events with field selection filtering on reportingController 02/06/23 17:51:12.142
STEP: getting the test event 02/06/23 17:51:12.145
STEP: patching the test event 02/06/23 17:51:12.148
STEP: getting the test event 02/06/23 17:51:12.157
STEP: updating the test event 02/06/23 17:51:12.16
STEP: getting the test event 02/06/23 17:51:12.168
STEP: deleting the test event 02/06/23 17:51:12.171
STEP: listing events in all namespaces 02/06/23 17:51:12.178
STEP: listing events in test namespace 02/06/23 17:51:12.192
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Feb  6 17:51:12.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1614" for this suite. 02/06/23 17:51:12.2
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":118,"skipped":2328,"failed":0}
------------------------------
• [0.122 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:51:12.084
    Feb  6 17:51:12.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename events 02/06/23 17:51:12.086
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:51:12.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:51:12.11
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 02/06/23 17:51:12.115
    STEP: listing events in all namespaces 02/06/23 17:51:12.122
    STEP: listing events in test namespace 02/06/23 17:51:12.136
    STEP: listing events with field selection filtering on source 02/06/23 17:51:12.14
    STEP: listing events with field selection filtering on reportingController 02/06/23 17:51:12.142
    STEP: getting the test event 02/06/23 17:51:12.145
    STEP: patching the test event 02/06/23 17:51:12.148
    STEP: getting the test event 02/06/23 17:51:12.157
    STEP: updating the test event 02/06/23 17:51:12.16
    STEP: getting the test event 02/06/23 17:51:12.168
    STEP: deleting the test event 02/06/23 17:51:12.171
    STEP: listing events in all namespaces 02/06/23 17:51:12.178
    STEP: listing events in test namespace 02/06/23 17:51:12.192
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Feb  6 17:51:12.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1614" for this suite. 02/06/23 17:51:12.2
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:51:12.207
Feb  6 17:51:12.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-probe 02/06/23 17:51:12.208
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:51:12.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:51:12.226
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3 in namespace container-probe-1701 02/06/23 17:51:12.229
Feb  6 17:51:12.241: INFO: Waiting up to 5m0s for pod "busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3" in namespace "container-probe-1701" to be "not pending"
Feb  6 17:51:12.245: INFO: Pod "busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.741796ms
Feb  6 17:51:14.249: INFO: Pod "busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008383774s
Feb  6 17:51:16.249: INFO: Pod "busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3": Phase="Running", Reason="", readiness=true. Elapsed: 4.008103286s
Feb  6 17:51:16.249: INFO: Pod "busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3" satisfied condition "not pending"
Feb  6 17:51:16.249: INFO: Started pod busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3 in namespace container-probe-1701
STEP: checking the pod's current state and verifying that restartCount is present 02/06/23 17:51:16.249
Feb  6 17:51:16.252: INFO: Initial restart count of pod busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3 is 0
STEP: deleting the pod 02/06/23 17:55:16.847
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb  6 17:55:16.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1701" for this suite. 02/06/23 17:55:16.877
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":119,"skipped":2330,"failed":0}
------------------------------
• [SLOW TEST] [244.693 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:51:12.207
    Feb  6 17:51:12.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-probe 02/06/23 17:51:12.208
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:51:12.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:51:12.226
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3 in namespace container-probe-1701 02/06/23 17:51:12.229
    Feb  6 17:51:12.241: INFO: Waiting up to 5m0s for pod "busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3" in namespace "container-probe-1701" to be "not pending"
    Feb  6 17:51:12.245: INFO: Pod "busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.741796ms
    Feb  6 17:51:14.249: INFO: Pod "busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008383774s
    Feb  6 17:51:16.249: INFO: Pod "busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3": Phase="Running", Reason="", readiness=true. Elapsed: 4.008103286s
    Feb  6 17:51:16.249: INFO: Pod "busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3" satisfied condition "not pending"
    Feb  6 17:51:16.249: INFO: Started pod busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3 in namespace container-probe-1701
    STEP: checking the pod's current state and verifying that restartCount is present 02/06/23 17:51:16.249
    Feb  6 17:51:16.252: INFO: Initial restart count of pod busybox-3e6f913a-06a9-4cd0-acd0-686f6512fad3 is 0
    STEP: deleting the pod 02/06/23 17:55:16.847
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb  6 17:55:16.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1701" for this suite. 02/06/23 17:55:16.877
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:55:16.904
Feb  6 17:55:16.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 17:55:16.905
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:55:16.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:55:16.926
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4e76eb22-708d-4e6c-b123-99c8820899dd 02/06/23 17:55:16.934
STEP: Creating the pod 02/06/23 17:55:16.938
Feb  6 17:55:16.948: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff" in namespace "projected-6459" to be "running and ready"
Feb  6 17:55:16.954: INFO: Pod "pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.239813ms
Feb  6 17:55:16.954: INFO: The phase of Pod pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:55:18.958: INFO: Pod "pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009549779s
Feb  6 17:55:18.958: INFO: The phase of Pod pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff is Pending, waiting for it to be Running (with Ready = true)
Feb  6 17:55:20.960: INFO: Pod "pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff": Phase="Running", Reason="", readiness=true. Elapsed: 4.011184483s
Feb  6 17:55:20.960: INFO: The phase of Pod pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff is Running (Ready = true)
Feb  6 17:55:20.960: INFO: Pod "pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-4e76eb22-708d-4e6c-b123-99c8820899dd 02/06/23 17:55:20.98
STEP: waiting to observe update in volume 02/06/23 17:55:20.987
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb  6 17:55:23.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6459" for this suite. 02/06/23 17:55:23.01
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":120,"skipped":2331,"failed":0}
------------------------------
• [SLOW TEST] [6.114 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:55:16.904
    Feb  6 17:55:16.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 17:55:16.905
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:55:16.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:55:16.926
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-4e76eb22-708d-4e6c-b123-99c8820899dd 02/06/23 17:55:16.934
    STEP: Creating the pod 02/06/23 17:55:16.938
    Feb  6 17:55:16.948: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff" in namespace "projected-6459" to be "running and ready"
    Feb  6 17:55:16.954: INFO: Pod "pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.239813ms
    Feb  6 17:55:16.954: INFO: The phase of Pod pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:55:18.958: INFO: Pod "pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009549779s
    Feb  6 17:55:18.958: INFO: The phase of Pod pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 17:55:20.960: INFO: Pod "pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff": Phase="Running", Reason="", readiness=true. Elapsed: 4.011184483s
    Feb  6 17:55:20.960: INFO: The phase of Pod pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff is Running (Ready = true)
    Feb  6 17:55:20.960: INFO: Pod "pod-projected-configmaps-b693a79f-d0e2-41ce-85c6-ab95158f6bff" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-4e76eb22-708d-4e6c-b123-99c8820899dd 02/06/23 17:55:20.98
    STEP: waiting to observe update in volume 02/06/23 17:55:20.987
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb  6 17:55:23.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6459" for this suite. 02/06/23 17:55:23.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:55:23.027
Feb  6 17:55:23.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename crd-webhook 02/06/23 17:55:23.028
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:55:23.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:55:23.045
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/06/23 17:55:23.048
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/06/23 17:55:23.551
STEP: Deploying the custom resource conversion webhook pod 02/06/23 17:55:23.558
STEP: Wait for the deployment to be ready 02/06/23 17:55:23.572
Feb  6 17:55:23.593: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb  6 17:55:25.604: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 55, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 55, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 55, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 55, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 17:55:27.608
STEP: Verifying the service has paired with the endpoint 02/06/23 17:55:27.629
Feb  6 17:55:28.630: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Feb  6 17:55:28.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Creating a v1 custom resource 02/06/23 17:55:31.276
STEP: v2 custom resource should be converted 02/06/23 17:55:31.282
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:55:31.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3134" for this suite. 02/06/23 17:55:31.81
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":121,"skipped":2389,"failed":0}
------------------------------
• [SLOW TEST] [8.934 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:55:23.027
    Feb  6 17:55:23.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename crd-webhook 02/06/23 17:55:23.028
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:55:23.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:55:23.045
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/06/23 17:55:23.048
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/06/23 17:55:23.551
    STEP: Deploying the custom resource conversion webhook pod 02/06/23 17:55:23.558
    STEP: Wait for the deployment to be ready 02/06/23 17:55:23.572
    Feb  6 17:55:23.593: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Feb  6 17:55:25.604: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 55, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 55, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 55, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 55, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 17:55:27.608
    STEP: Verifying the service has paired with the endpoint 02/06/23 17:55:27.629
    Feb  6 17:55:28.630: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Feb  6 17:55:28.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Creating a v1 custom resource 02/06/23 17:55:31.276
    STEP: v2 custom resource should be converted 02/06/23 17:55:31.282
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:55:31.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3134" for this suite. 02/06/23 17:55:31.81
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:55:31.967
Feb  6 17:55:31.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename resourcequota 02/06/23 17:55:31.969
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:55:32.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:55:32.088
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 02/06/23 17:55:32.095
STEP: Ensuring ResourceQuota status is calculated 02/06/23 17:55:32.14
STEP: Creating a ResourceQuota with not terminating scope 02/06/23 17:55:34.146
STEP: Ensuring ResourceQuota status is calculated 02/06/23 17:55:34.154
STEP: Creating a long running pod 02/06/23 17:55:36.159
STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/06/23 17:55:36.172
STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/06/23 17:55:38.178
STEP: Deleting the pod 02/06/23 17:55:40.183
STEP: Ensuring resource quota status released the pod usage 02/06/23 17:55:40.202
STEP: Creating a terminating pod 02/06/23 17:55:42.206
STEP: Ensuring resource quota with terminating scope captures the pod usage 02/06/23 17:55:42.217
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/06/23 17:55:44.222
STEP: Deleting the pod 02/06/23 17:55:46.226
STEP: Ensuring resource quota status released the pod usage 02/06/23 17:55:46.241
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb  6 17:55:48.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5013" for this suite. 02/06/23 17:55:48.25
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":122,"skipped":2389,"failed":0}
------------------------------
• [SLOW TEST] [16.291 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:55:31.967
    Feb  6 17:55:31.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename resourcequota 02/06/23 17:55:31.969
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:55:32.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:55:32.088
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 02/06/23 17:55:32.095
    STEP: Ensuring ResourceQuota status is calculated 02/06/23 17:55:32.14
    STEP: Creating a ResourceQuota with not terminating scope 02/06/23 17:55:34.146
    STEP: Ensuring ResourceQuota status is calculated 02/06/23 17:55:34.154
    STEP: Creating a long running pod 02/06/23 17:55:36.159
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/06/23 17:55:36.172
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/06/23 17:55:38.178
    STEP: Deleting the pod 02/06/23 17:55:40.183
    STEP: Ensuring resource quota status released the pod usage 02/06/23 17:55:40.202
    STEP: Creating a terminating pod 02/06/23 17:55:42.206
    STEP: Ensuring resource quota with terminating scope captures the pod usage 02/06/23 17:55:42.217
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/06/23 17:55:44.222
    STEP: Deleting the pod 02/06/23 17:55:46.226
    STEP: Ensuring resource quota status released the pod usage 02/06/23 17:55:46.241
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb  6 17:55:48.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5013" for this suite. 02/06/23 17:55:48.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:55:48.263
Feb  6 17:55:48.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename limitrange 02/06/23 17:55:48.264
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:55:48.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:55:48.278
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 02/06/23 17:55:48.281
STEP: Setting up watch 02/06/23 17:55:48.281
STEP: Submitting a LimitRange 02/06/23 17:55:48.385
STEP: Verifying LimitRange creation was observed 02/06/23 17:55:48.392
STEP: Fetching the LimitRange to ensure it has proper values 02/06/23 17:55:48.392
Feb  6 17:55:48.395: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb  6 17:55:48.395: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 02/06/23 17:55:48.395
STEP: Ensuring Pod has resource requirements applied from LimitRange 02/06/23 17:55:48.402
Feb  6 17:55:48.411: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb  6 17:55:48.411: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 02/06/23 17:55:48.411
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/06/23 17:55:48.419
Feb  6 17:55:48.428: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb  6 17:55:48.428: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 02/06/23 17:55:48.428
STEP: Failing to create a Pod with more than max resources 02/06/23 17:55:48.43
STEP: Updating a LimitRange 02/06/23 17:55:48.432
STEP: Verifying LimitRange updating is effective 02/06/23 17:55:48.438
STEP: Creating a Pod with less than former min resources 02/06/23 17:55:50.443
STEP: Failing to create a Pod with more than max resources 02/06/23 17:55:50.449
STEP: Deleting a LimitRange 02/06/23 17:55:50.452
STEP: Verifying the LimitRange was deleted 02/06/23 17:55:50.463
Feb  6 17:55:55.470: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 02/06/23 17:55:55.47
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Feb  6 17:55:55.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-799" for this suite. 02/06/23 17:55:55.487
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":123,"skipped":2410,"failed":0}
------------------------------
• [SLOW TEST] [7.234 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:55:48.263
    Feb  6 17:55:48.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename limitrange 02/06/23 17:55:48.264
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:55:48.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:55:48.278
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 02/06/23 17:55:48.281
    STEP: Setting up watch 02/06/23 17:55:48.281
    STEP: Submitting a LimitRange 02/06/23 17:55:48.385
    STEP: Verifying LimitRange creation was observed 02/06/23 17:55:48.392
    STEP: Fetching the LimitRange to ensure it has proper values 02/06/23 17:55:48.392
    Feb  6 17:55:48.395: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb  6 17:55:48.395: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 02/06/23 17:55:48.395
    STEP: Ensuring Pod has resource requirements applied from LimitRange 02/06/23 17:55:48.402
    Feb  6 17:55:48.411: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb  6 17:55:48.411: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 02/06/23 17:55:48.411
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/06/23 17:55:48.419
    Feb  6 17:55:48.428: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Feb  6 17:55:48.428: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 02/06/23 17:55:48.428
    STEP: Failing to create a Pod with more than max resources 02/06/23 17:55:48.43
    STEP: Updating a LimitRange 02/06/23 17:55:48.432
    STEP: Verifying LimitRange updating is effective 02/06/23 17:55:48.438
    STEP: Creating a Pod with less than former min resources 02/06/23 17:55:50.443
    STEP: Failing to create a Pod with more than max resources 02/06/23 17:55:50.449
    STEP: Deleting a LimitRange 02/06/23 17:55:50.452
    STEP: Verifying the LimitRange was deleted 02/06/23 17:55:50.463
    Feb  6 17:55:55.470: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 02/06/23 17:55:55.47
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Feb  6 17:55:55.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-799" for this suite. 02/06/23 17:55:55.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:55:55.51
Feb  6 17:55:55.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename watch 02/06/23 17:55:55.511
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:55:55.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:55:55.532
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 02/06/23 17:55:55.534
STEP: creating a new configmap 02/06/23 17:55:55.536
STEP: modifying the configmap once 02/06/23 17:55:55.54
STEP: closing the watch once it receives two notifications 02/06/23 17:55:55.547
Feb  6 17:55:55.547: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7436  3b51e6b7-db32-4c20-b248-55ac3e0429f1 34546 0 2023-02-06 17:55:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-06 17:55:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 17:55:55.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7436  3b51e6b7-db32-4c20-b248-55ac3e0429f1 34547 0 2023-02-06 17:55:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-06 17:55:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 02/06/23 17:55:55.548
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/06/23 17:55:55.554
STEP: deleting the configmap 02/06/23 17:55:55.556
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/06/23 17:55:55.561
Feb  6 17:55:55.561: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7436  3b51e6b7-db32-4c20-b248-55ac3e0429f1 34548 0 2023-02-06 17:55:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-06 17:55:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 17:55:55.562: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7436  3b51e6b7-db32-4c20-b248-55ac3e0429f1 34549 0 2023-02-06 17:55:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-06 17:55:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb  6 17:55:55.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7436" for this suite. 02/06/23 17:55:55.567
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":124,"skipped":2475,"failed":0}
------------------------------
• [0.063 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:55:55.51
    Feb  6 17:55:55.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename watch 02/06/23 17:55:55.511
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:55:55.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:55:55.532
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 02/06/23 17:55:55.534
    STEP: creating a new configmap 02/06/23 17:55:55.536
    STEP: modifying the configmap once 02/06/23 17:55:55.54
    STEP: closing the watch once it receives two notifications 02/06/23 17:55:55.547
    Feb  6 17:55:55.547: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7436  3b51e6b7-db32-4c20-b248-55ac3e0429f1 34546 0 2023-02-06 17:55:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-06 17:55:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 17:55:55.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7436  3b51e6b7-db32-4c20-b248-55ac3e0429f1 34547 0 2023-02-06 17:55:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-06 17:55:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 02/06/23 17:55:55.548
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/06/23 17:55:55.554
    STEP: deleting the configmap 02/06/23 17:55:55.556
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/06/23 17:55:55.561
    Feb  6 17:55:55.561: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7436  3b51e6b7-db32-4c20-b248-55ac3e0429f1 34548 0 2023-02-06 17:55:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-06 17:55:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 17:55:55.562: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7436  3b51e6b7-db32-4c20-b248-55ac3e0429f1 34549 0 2023-02-06 17:55:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-06 17:55:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb  6 17:55:55.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7436" for this suite. 02/06/23 17:55:55.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:55:55.575
Feb  6 17:55:55.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pods 02/06/23 17:55:55.576
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:55:55.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:55:55.599
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 02/06/23 17:55:55.611
STEP: watching for Pod to be ready 02/06/23 17:55:55.62
Feb  6 17:55:55.623: INFO: observed Pod pod-test in namespace pods-9129 in phase Pending with labels: map[test-pod-static:true] & conditions []
Feb  6 17:55:55.625: INFO: observed Pod pod-test in namespace pods-9129 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC  }]
Feb  6 17:55:55.640: INFO: observed Pod pod-test in namespace pods-9129 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC  }]
Feb  6 17:55:59.173: INFO: Found Pod pod-test in namespace pods-9129 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 02/06/23 17:55:59.178
STEP: getting the Pod and ensuring that it's patched 02/06/23 17:55:59.189
STEP: replacing the Pod's status Ready condition to False 02/06/23 17:55:59.203
STEP: check the Pod again to ensure its Ready conditions are False 02/06/23 17:55:59.217
STEP: deleting the Pod via a Collection with a LabelSelector 02/06/23 17:55:59.217
STEP: watching for the Pod to be deleted 02/06/23 17:55:59.227
Feb  6 17:55:59.229: INFO: observed event type MODIFIED
Feb  6 17:56:00.225: INFO: observed event type MODIFIED
Feb  6 17:56:02.190: INFO: observed event type MODIFIED
Feb  6 17:56:02.201: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb  6 17:56:02.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9129" for this suite. 02/06/23 17:56:02.221
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":125,"skipped":2488,"failed":0}
------------------------------
• [SLOW TEST] [6.652 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:55:55.575
    Feb  6 17:55:55.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pods 02/06/23 17:55:55.576
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:55:55.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:55:55.599
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 02/06/23 17:55:55.611
    STEP: watching for Pod to be ready 02/06/23 17:55:55.62
    Feb  6 17:55:55.623: INFO: observed Pod pod-test in namespace pods-9129 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Feb  6 17:55:55.625: INFO: observed Pod pod-test in namespace pods-9129 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC  }]
    Feb  6 17:55:55.640: INFO: observed Pod pod-test in namespace pods-9129 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC  }]
    Feb  6 17:55:59.173: INFO: Found Pod pod-test in namespace pods-9129 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 17:55:55 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 02/06/23 17:55:59.178
    STEP: getting the Pod and ensuring that it's patched 02/06/23 17:55:59.189
    STEP: replacing the Pod's status Ready condition to False 02/06/23 17:55:59.203
    STEP: check the Pod again to ensure its Ready conditions are False 02/06/23 17:55:59.217
    STEP: deleting the Pod via a Collection with a LabelSelector 02/06/23 17:55:59.217
    STEP: watching for the Pod to be deleted 02/06/23 17:55:59.227
    Feb  6 17:55:59.229: INFO: observed event type MODIFIED
    Feb  6 17:56:00.225: INFO: observed event type MODIFIED
    Feb  6 17:56:02.190: INFO: observed event type MODIFIED
    Feb  6 17:56:02.201: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb  6 17:56:02.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9129" for this suite. 02/06/23 17:56:02.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:56:02.231
Feb  6 17:56:02.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename tables 02/06/23 17:56:02.232
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:02.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:02.253
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Feb  6 17:56:02.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7337" for this suite. 02/06/23 17:56:02.265
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":126,"skipped":2508,"failed":0}
------------------------------
• [0.039 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:56:02.231
    Feb  6 17:56:02.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename tables 02/06/23 17:56:02.232
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:02.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:02.253
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Feb  6 17:56:02.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-7337" for this suite. 02/06/23 17:56:02.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:56:02.272
Feb  6 17:56:02.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubelet-test 02/06/23 17:56:02.273
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:02.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:02.296
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb  6 17:56:02.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2133" for this suite. 02/06/23 17:56:02.352
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":127,"skipped":2520,"failed":0}
------------------------------
• [0.086 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:56:02.272
    Feb  6 17:56:02.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubelet-test 02/06/23 17:56:02.273
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:02.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:02.296
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb  6 17:56:02.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2133" for this suite. 02/06/23 17:56:02.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:56:02.361
Feb  6 17:56:02.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 17:56:02.362
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:02.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:02.381
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-b7c4f8ff-b553-440c-8399-9881da880ff5 02/06/23 17:56:02.384
STEP: Creating a pod to test consume secrets 02/06/23 17:56:02.392
Feb  6 17:56:02.414: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100" in namespace "projected-3077" to be "Succeeded or Failed"
Feb  6 17:56:02.422: INFO: Pod "pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100": Phase="Pending", Reason="", readiness=false. Elapsed: 7.418543ms
Feb  6 17:56:04.426: INFO: Pod "pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01168038s
Feb  6 17:56:06.425: INFO: Pod "pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011273143s
Feb  6 17:56:08.429: INFO: Pod "pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014323631s
STEP: Saw pod success 02/06/23 17:56:08.429
Feb  6 17:56:08.429: INFO: Pod "pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100" satisfied condition "Succeeded or Failed"
Feb  6 17:56:08.433: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/06/23 17:56:08.441
Feb  6 17:56:08.453: INFO: Waiting for pod pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100 to disappear
Feb  6 17:56:08.458: INFO: Pod pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb  6 17:56:08.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3077" for this suite. 02/06/23 17:56:08.466
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":128,"skipped":2537,"failed":0}
------------------------------
• [SLOW TEST] [6.111 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:56:02.361
    Feb  6 17:56:02.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 17:56:02.362
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:02.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:02.381
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-b7c4f8ff-b553-440c-8399-9881da880ff5 02/06/23 17:56:02.384
    STEP: Creating a pod to test consume secrets 02/06/23 17:56:02.392
    Feb  6 17:56:02.414: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100" in namespace "projected-3077" to be "Succeeded or Failed"
    Feb  6 17:56:02.422: INFO: Pod "pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100": Phase="Pending", Reason="", readiness=false. Elapsed: 7.418543ms
    Feb  6 17:56:04.426: INFO: Pod "pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01168038s
    Feb  6 17:56:06.425: INFO: Pod "pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011273143s
    Feb  6 17:56:08.429: INFO: Pod "pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014323631s
    STEP: Saw pod success 02/06/23 17:56:08.429
    Feb  6 17:56:08.429: INFO: Pod "pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100" satisfied condition "Succeeded or Failed"
    Feb  6 17:56:08.433: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 17:56:08.441
    Feb  6 17:56:08.453: INFO: Waiting for pod pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100 to disappear
    Feb  6 17:56:08.458: INFO: Pod pod-projected-secrets-cab036f5-a54f-4ad6-bf47-f9acd018c100 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb  6 17:56:08.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3077" for this suite. 02/06/23 17:56:08.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:56:08.477
Feb  6 17:56:08.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 17:56:08.479
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:08.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:08.497
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-1120 02/06/23 17:56:08.5
STEP: creating service affinity-nodeport-transition in namespace services-1120 02/06/23 17:56:08.5
STEP: creating replication controller affinity-nodeport-transition in namespace services-1120 02/06/23 17:56:08.52
I0206 17:56:08.537417      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1120, replica count: 3
I0206 17:56:11.595185      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 17:56:14.596415      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  6 17:56:14.608: INFO: Creating new exec pod
Feb  6 17:56:14.615: INFO: Waiting up to 5m0s for pod "execpod-affinity2hxxh" in namespace "services-1120" to be "running"
Feb  6 17:56:14.624: INFO: Pod "execpod-affinity2hxxh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.452764ms
Feb  6 17:56:16.629: INFO: Pod "execpod-affinity2hxxh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013414283s
Feb  6 17:56:18.629: INFO: Pod "execpod-affinity2hxxh": Phase="Running", Reason="", readiness=true. Elapsed: 4.013439962s
Feb  6 17:56:18.629: INFO: Pod "execpod-affinity2hxxh" satisfied condition "running"
Feb  6 17:56:19.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1120 exec execpod-affinity2hxxh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Feb  6 17:56:19.805: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb  6 17:56:19.805: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 17:56:19.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1120 exec execpod-affinity2hxxh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.37.232 80'
Feb  6 17:56:19.985: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.37.232 80\nConnection to 10.96.37.232 80 port [tcp/http] succeeded!\n"
Feb  6 17:56:19.985: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 17:56:19.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1120 exec execpod-affinity2hxxh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.4.133 30786'
Feb  6 17:56:20.143: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.4.133 30786\nConnection to 195.17.4.133 30786 port [tcp/*] succeeded!\n"
Feb  6 17:56:20.143: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 17:56:20.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1120 exec execpod-affinity2hxxh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.89 30786'
Feb  6 17:56:20.309: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.148.89 30786\nConnection to 195.17.148.89 30786 port [tcp/*] succeeded!\n"
Feb  6 17:56:20.309: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 17:56:20.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1120 exec execpod-affinity2hxxh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://195.17.4.133:30786/ ; done'
Feb  6 17:56:20.615: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n"
Feb  6 17:56:20.616: INFO: stdout: "\naffinity-nodeport-transition-sbsvs\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-sbsvs\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-jsttt"
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-sbsvs
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-sbsvs
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
Feb  6 17:56:20.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1120 exec execpod-affinity2hxxh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://195.17.4.133:30786/ ; done'
Feb  6 17:56:20.910: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n"
Feb  6 17:56:20.910: INFO: stdout: "\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q"
Feb  6 17:56:20.910: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
Feb  6 17:56:20.911: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1120, will wait for the garbage collector to delete the pods 02/06/23 17:56:20.927
Feb  6 17:56:20.992: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.830422ms
Feb  6 17:56:21.094: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.138353ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 17:56:23.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1120" for this suite. 02/06/23 17:56:23.463
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":129,"skipped":2561,"failed":0}
------------------------------
• [SLOW TEST] [14.994 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:56:08.477
    Feb  6 17:56:08.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 17:56:08.479
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:08.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:08.497
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-1120 02/06/23 17:56:08.5
    STEP: creating service affinity-nodeport-transition in namespace services-1120 02/06/23 17:56:08.5
    STEP: creating replication controller affinity-nodeport-transition in namespace services-1120 02/06/23 17:56:08.52
    I0206 17:56:08.537417      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1120, replica count: 3
    I0206 17:56:11.595185      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 17:56:14.596415      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb  6 17:56:14.608: INFO: Creating new exec pod
    Feb  6 17:56:14.615: INFO: Waiting up to 5m0s for pod "execpod-affinity2hxxh" in namespace "services-1120" to be "running"
    Feb  6 17:56:14.624: INFO: Pod "execpod-affinity2hxxh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.452764ms
    Feb  6 17:56:16.629: INFO: Pod "execpod-affinity2hxxh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013414283s
    Feb  6 17:56:18.629: INFO: Pod "execpod-affinity2hxxh": Phase="Running", Reason="", readiness=true. Elapsed: 4.013439962s
    Feb  6 17:56:18.629: INFO: Pod "execpod-affinity2hxxh" satisfied condition "running"
    Feb  6 17:56:19.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1120 exec execpod-affinity2hxxh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Feb  6 17:56:19.805: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Feb  6 17:56:19.805: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 17:56:19.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1120 exec execpod-affinity2hxxh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.37.232 80'
    Feb  6 17:56:19.985: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.37.232 80\nConnection to 10.96.37.232 80 port [tcp/http] succeeded!\n"
    Feb  6 17:56:19.985: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 17:56:19.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1120 exec execpod-affinity2hxxh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.4.133 30786'
    Feb  6 17:56:20.143: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.4.133 30786\nConnection to 195.17.4.133 30786 port [tcp/*] succeeded!\n"
    Feb  6 17:56:20.143: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 17:56:20.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1120 exec execpod-affinity2hxxh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.89 30786'
    Feb  6 17:56:20.309: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.148.89 30786\nConnection to 195.17.148.89 30786 port [tcp/*] succeeded!\n"
    Feb  6 17:56:20.309: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 17:56:20.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1120 exec execpod-affinity2hxxh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://195.17.4.133:30786/ ; done'
    Feb  6 17:56:20.615: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n"
    Feb  6 17:56:20.616: INFO: stdout: "\naffinity-nodeport-transition-sbsvs\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-sbsvs\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-jsttt\naffinity-nodeport-transition-jsttt"
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-sbsvs
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-sbsvs
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
    Feb  6 17:56:20.616: INFO: Received response from host: affinity-nodeport-transition-jsttt
    Feb  6 17:56:20.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1120 exec execpod-affinity2hxxh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://195.17.4.133:30786/ ; done'
    Feb  6 17:56:20.910: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:30786/\n"
    Feb  6 17:56:20.910: INFO: stdout: "\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q\naffinity-nodeport-transition-wvj9q"
    Feb  6 17:56:20.910: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Received response from host: affinity-nodeport-transition-wvj9q
    Feb  6 17:56:20.911: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1120, will wait for the garbage collector to delete the pods 02/06/23 17:56:20.927
    Feb  6 17:56:20.992: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.830422ms
    Feb  6 17:56:21.094: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.138353ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 17:56:23.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1120" for this suite. 02/06/23 17:56:23.463
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:56:23.473
Feb  6 17:56:23.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename var-expansion 02/06/23 17:56:23.474
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:23.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:23.493
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 02/06/23 17:56:23.497
Feb  6 17:56:23.506: INFO: Waiting up to 5m0s for pod "var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a" in namespace "var-expansion-5720" to be "Succeeded or Failed"
Feb  6 17:56:23.510: INFO: Pod "var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019005ms
Feb  6 17:56:25.515: INFO: Pod "var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008348997s
Feb  6 17:56:27.515: INFO: Pod "var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009138268s
Feb  6 17:56:29.515: INFO: Pod "var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00913593s
STEP: Saw pod success 02/06/23 17:56:29.515
Feb  6 17:56:29.515: INFO: Pod "var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a" satisfied condition "Succeeded or Failed"
Feb  6 17:56:29.518: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a container dapi-container: <nil>
STEP: delete the pod 02/06/23 17:56:29.525
Feb  6 17:56:29.538: INFO: Waiting for pod var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a to disappear
Feb  6 17:56:29.541: INFO: Pod var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb  6 17:56:29.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5720" for this suite. 02/06/23 17:56:29.546
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":130,"skipped":2568,"failed":0}
------------------------------
• [SLOW TEST] [6.079 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:56:23.473
    Feb  6 17:56:23.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename var-expansion 02/06/23 17:56:23.474
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:23.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:23.493
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 02/06/23 17:56:23.497
    Feb  6 17:56:23.506: INFO: Waiting up to 5m0s for pod "var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a" in namespace "var-expansion-5720" to be "Succeeded or Failed"
    Feb  6 17:56:23.510: INFO: Pod "var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019005ms
    Feb  6 17:56:25.515: INFO: Pod "var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008348997s
    Feb  6 17:56:27.515: INFO: Pod "var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009138268s
    Feb  6 17:56:29.515: INFO: Pod "var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00913593s
    STEP: Saw pod success 02/06/23 17:56:29.515
    Feb  6 17:56:29.515: INFO: Pod "var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a" satisfied condition "Succeeded or Failed"
    Feb  6 17:56:29.518: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a container dapi-container: <nil>
    STEP: delete the pod 02/06/23 17:56:29.525
    Feb  6 17:56:29.538: INFO: Waiting for pod var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a to disappear
    Feb  6 17:56:29.541: INFO: Pod var-expansion-e28cc6c8-f6f2-45a8-9b4a-5a7d091d231a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb  6 17:56:29.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5720" for this suite. 02/06/23 17:56:29.546
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:56:29.553
Feb  6 17:56:29.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename namespaces 02/06/23 17:56:29.554
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:29.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:29.573
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 02/06/23 17:56:29.576
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:29.588
STEP: Creating a pod in the namespace 02/06/23 17:56:29.591
STEP: Waiting for the pod to have running status 02/06/23 17:56:29.599
Feb  6 17:56:29.600: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6805" to be "running"
Feb  6 17:56:29.604: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.401994ms
Feb  6 17:56:31.610: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010608483s
Feb  6 17:56:33.609: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008964176s
Feb  6 17:56:33.609: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 02/06/23 17:56:33.609
STEP: Waiting for the namespace to be removed. 02/06/23 17:56:33.615
STEP: Recreating the namespace 02/06/23 17:56:44.619
STEP: Verifying there are no pods in the namespace 02/06/23 17:56:44.639
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb  6 17:56:44.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6180" for this suite. 02/06/23 17:56:44.65
STEP: Destroying namespace "nsdeletetest-6805" for this suite. 02/06/23 17:56:44.656
Feb  6 17:56:44.659: INFO: Namespace nsdeletetest-6805 was already deleted
STEP: Destroying namespace "nsdeletetest-5872" for this suite. 02/06/23 17:56:44.659
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":131,"skipped":2572,"failed":0}
------------------------------
• [SLOW TEST] [15.112 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:56:29.553
    Feb  6 17:56:29.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename namespaces 02/06/23 17:56:29.554
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:29.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:29.573
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 02/06/23 17:56:29.576
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:29.588
    STEP: Creating a pod in the namespace 02/06/23 17:56:29.591
    STEP: Waiting for the pod to have running status 02/06/23 17:56:29.599
    Feb  6 17:56:29.600: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6805" to be "running"
    Feb  6 17:56:29.604: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.401994ms
    Feb  6 17:56:31.610: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010608483s
    Feb  6 17:56:33.609: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008964176s
    Feb  6 17:56:33.609: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 02/06/23 17:56:33.609
    STEP: Waiting for the namespace to be removed. 02/06/23 17:56:33.615
    STEP: Recreating the namespace 02/06/23 17:56:44.619
    STEP: Verifying there are no pods in the namespace 02/06/23 17:56:44.639
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 17:56:44.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6180" for this suite. 02/06/23 17:56:44.65
    STEP: Destroying namespace "nsdeletetest-6805" for this suite. 02/06/23 17:56:44.656
    Feb  6 17:56:44.659: INFO: Namespace nsdeletetest-6805 was already deleted
    STEP: Destroying namespace "nsdeletetest-5872" for this suite. 02/06/23 17:56:44.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:56:44.668
Feb  6 17:56:44.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename resourcequota 02/06/23 17:56:44.669
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:44.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:44.688
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 02/06/23 17:56:44.69
STEP: Getting a ResourceQuota 02/06/23 17:56:44.695
STEP: Listing all ResourceQuotas with LabelSelector 02/06/23 17:56:44.7
STEP: Patching the ResourceQuota 02/06/23 17:56:44.703
STEP: Deleting a Collection of ResourceQuotas 02/06/23 17:56:44.709
STEP: Verifying the deleted ResourceQuota 02/06/23 17:56:44.721
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb  6 17:56:44.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1758" for this suite. 02/06/23 17:56:44.729
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":132,"skipped":2590,"failed":0}
------------------------------
• [0.066 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:56:44.668
    Feb  6 17:56:44.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename resourcequota 02/06/23 17:56:44.669
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:44.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:44.688
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 02/06/23 17:56:44.69
    STEP: Getting a ResourceQuota 02/06/23 17:56:44.695
    STEP: Listing all ResourceQuotas with LabelSelector 02/06/23 17:56:44.7
    STEP: Patching the ResourceQuota 02/06/23 17:56:44.703
    STEP: Deleting a Collection of ResourceQuotas 02/06/23 17:56:44.709
    STEP: Verifying the deleted ResourceQuota 02/06/23 17:56:44.721
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb  6 17:56:44.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1758" for this suite. 02/06/23 17:56:44.729
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:56:44.735
Feb  6 17:56:44.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 17:56:44.736
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:44.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:44.757
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/06/23 17:56:44.759
Feb  6 17:56:44.769: INFO: Waiting up to 5m0s for pod "pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53" in namespace "emptydir-346" to be "Succeeded or Failed"
Feb  6 17:56:44.774: INFO: Pod "pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.930656ms
Feb  6 17:56:46.780: INFO: Pod "pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010476802s
Feb  6 17:56:48.779: INFO: Pod "pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010096667s
Feb  6 17:56:50.778: INFO: Pod "pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008924483s
STEP: Saw pod success 02/06/23 17:56:50.778
Feb  6 17:56:50.778: INFO: Pod "pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53" satisfied condition "Succeeded or Failed"
Feb  6 17:56:50.781: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53 container test-container: <nil>
STEP: delete the pod 02/06/23 17:56:50.803
Feb  6 17:56:50.815: INFO: Waiting for pod pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53 to disappear
Feb  6 17:56:50.818: INFO: Pod pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 17:56:50.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-346" for this suite. 02/06/23 17:56:50.823
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":133,"skipped":2592,"failed":0}
------------------------------
• [SLOW TEST] [6.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:56:44.735
    Feb  6 17:56:44.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 17:56:44.736
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:44.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:44.757
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/06/23 17:56:44.759
    Feb  6 17:56:44.769: INFO: Waiting up to 5m0s for pod "pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53" in namespace "emptydir-346" to be "Succeeded or Failed"
    Feb  6 17:56:44.774: INFO: Pod "pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.930656ms
    Feb  6 17:56:46.780: INFO: Pod "pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010476802s
    Feb  6 17:56:48.779: INFO: Pod "pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010096667s
    Feb  6 17:56:50.778: INFO: Pod "pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008924483s
    STEP: Saw pod success 02/06/23 17:56:50.778
    Feb  6 17:56:50.778: INFO: Pod "pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53" satisfied condition "Succeeded or Failed"
    Feb  6 17:56:50.781: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53 container test-container: <nil>
    STEP: delete the pod 02/06/23 17:56:50.803
    Feb  6 17:56:50.815: INFO: Waiting for pod pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53 to disappear
    Feb  6 17:56:50.818: INFO: Pod pod-3fef62dc-2fdc-49e5-b9f4-d970012bec53 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 17:56:50.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-346" for this suite. 02/06/23 17:56:50.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:56:50.845
Feb  6 17:56:50.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename proxy 02/06/23 17:56:50.846
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:50.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:50.863
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Feb  6 17:56:50.866: INFO: Creating pod...
Feb  6 17:56:50.876: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2955" to be "running"
Feb  6 17:56:50.885: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.875569ms
Feb  6 17:56:52.888: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011766744s
Feb  6 17:56:54.890: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.0130274s
Feb  6 17:56:54.890: INFO: Pod "agnhost" satisfied condition "running"
Feb  6 17:56:54.890: INFO: Creating service...
Feb  6 17:56:54.904: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/DELETE
Feb  6 17:56:54.915: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb  6 17:56:54.915: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/GET
Feb  6 17:56:54.920: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb  6 17:56:54.920: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/HEAD
Feb  6 17:56:54.924: INFO: http.Client request:HEAD | StatusCode:200
Feb  6 17:56:54.925: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/OPTIONS
Feb  6 17:56:54.928: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb  6 17:56:54.928: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/PATCH
Feb  6 17:56:54.933: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb  6 17:56:54.934: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/POST
Feb  6 17:56:54.938: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb  6 17:56:54.939: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/PUT
Feb  6 17:56:54.943: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb  6 17:56:54.943: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/DELETE
Feb  6 17:56:54.949: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb  6 17:56:54.949: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/GET
Feb  6 17:56:54.954: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb  6 17:56:54.954: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/HEAD
Feb  6 17:56:54.959: INFO: http.Client request:HEAD | StatusCode:200
Feb  6 17:56:54.959: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/OPTIONS
Feb  6 17:56:54.964: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb  6 17:56:54.964: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/PATCH
Feb  6 17:56:54.970: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb  6 17:56:54.970: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/POST
Feb  6 17:56:54.975: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb  6 17:56:54.975: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/PUT
Feb  6 17:56:54.981: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb  6 17:56:54.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2955" for this suite. 02/06/23 17:56:54.985
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":134,"skipped":2657,"failed":0}
------------------------------
• [4.148 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:56:50.845
    Feb  6 17:56:50.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename proxy 02/06/23 17:56:50.846
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:50.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:50.863
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Feb  6 17:56:50.866: INFO: Creating pod...
    Feb  6 17:56:50.876: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2955" to be "running"
    Feb  6 17:56:50.885: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.875569ms
    Feb  6 17:56:52.888: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011766744s
    Feb  6 17:56:54.890: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.0130274s
    Feb  6 17:56:54.890: INFO: Pod "agnhost" satisfied condition "running"
    Feb  6 17:56:54.890: INFO: Creating service...
    Feb  6 17:56:54.904: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/DELETE
    Feb  6 17:56:54.915: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb  6 17:56:54.915: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/GET
    Feb  6 17:56:54.920: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb  6 17:56:54.920: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/HEAD
    Feb  6 17:56:54.924: INFO: http.Client request:HEAD | StatusCode:200
    Feb  6 17:56:54.925: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/OPTIONS
    Feb  6 17:56:54.928: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb  6 17:56:54.928: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/PATCH
    Feb  6 17:56:54.933: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb  6 17:56:54.934: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/POST
    Feb  6 17:56:54.938: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb  6 17:56:54.939: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/pods/agnhost/proxy/some/path/with/PUT
    Feb  6 17:56:54.943: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb  6 17:56:54.943: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/DELETE
    Feb  6 17:56:54.949: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb  6 17:56:54.949: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/GET
    Feb  6 17:56:54.954: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb  6 17:56:54.954: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/HEAD
    Feb  6 17:56:54.959: INFO: http.Client request:HEAD | StatusCode:200
    Feb  6 17:56:54.959: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/OPTIONS
    Feb  6 17:56:54.964: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb  6 17:56:54.964: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/PATCH
    Feb  6 17:56:54.970: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb  6 17:56:54.970: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/POST
    Feb  6 17:56:54.975: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb  6 17:56:54.975: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2955/services/test-service/proxy/some/path/with/PUT
    Feb  6 17:56:54.981: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb  6 17:56:54.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2955" for this suite. 02/06/23 17:56:54.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:56:54.995
Feb  6 17:56:54.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 17:56:54.996
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:55.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:55.018
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 17:56:55.043
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:56:55.704
STEP: Deploying the webhook pod 02/06/23 17:56:55.712
STEP: Wait for the deployment to be ready 02/06/23 17:56:55.726
Feb  6 17:56:55.740: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 17:56:57.750: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 56, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 56, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 56, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 56, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 17:56:59.757
STEP: Verifying the service has paired with the endpoint 02/06/23 17:56:59.774
Feb  6 17:57:00.774: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 02/06/23 17:57:00.78
STEP: create a pod that should be denied by the webhook 02/06/23 17:57:00.798
STEP: create a pod that causes the webhook to hang 02/06/23 17:57:00.809
STEP: create a configmap that should be denied by the webhook 02/06/23 17:57:10.816
STEP: create a configmap that should be admitted by the webhook 02/06/23 17:57:10.857
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/06/23 17:57:10.867
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/06/23 17:57:10.875
STEP: create a namespace that bypass the webhook 02/06/23 17:57:10.88
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/06/23 17:57:10.89
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:57:10.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7832" for this suite. 02/06/23 17:57:10.926
STEP: Destroying namespace "webhook-7832-markers" for this suite. 02/06/23 17:57:10.933
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":135,"skipped":2676,"failed":0}
------------------------------
• [SLOW TEST] [16.000 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:56:54.995
    Feb  6 17:56:54.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 17:56:54.996
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:56:55.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:56:55.018
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 17:56:55.043
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:56:55.704
    STEP: Deploying the webhook pod 02/06/23 17:56:55.712
    STEP: Wait for the deployment to be ready 02/06/23 17:56:55.726
    Feb  6 17:56:55.740: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 17:56:57.750: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 56, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 56, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 56, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 56, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 17:56:59.757
    STEP: Verifying the service has paired with the endpoint 02/06/23 17:56:59.774
    Feb  6 17:57:00.774: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 02/06/23 17:57:00.78
    STEP: create a pod that should be denied by the webhook 02/06/23 17:57:00.798
    STEP: create a pod that causes the webhook to hang 02/06/23 17:57:00.809
    STEP: create a configmap that should be denied by the webhook 02/06/23 17:57:10.816
    STEP: create a configmap that should be admitted by the webhook 02/06/23 17:57:10.857
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/06/23 17:57:10.867
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/06/23 17:57:10.875
    STEP: create a namespace that bypass the webhook 02/06/23 17:57:10.88
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/06/23 17:57:10.89
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:57:10.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7832" for this suite. 02/06/23 17:57:10.926
    STEP: Destroying namespace "webhook-7832-markers" for this suite. 02/06/23 17:57:10.933
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:57:10.995
Feb  6 17:57:10.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename disruption 02/06/23 17:57:10.998
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:57:11.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:57:11.027
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 02/06/23 17:57:11.039
STEP: Updating PodDisruptionBudget status 02/06/23 17:57:13.052
STEP: Waiting for all pods to be running 02/06/23 17:57:13.063
Feb  6 17:57:13.072: INFO: running pods: 0 < 1
Feb  6 17:57:15.076: INFO: running pods: 0 < 1
STEP: locating a running pod 02/06/23 17:57:17.077
STEP: Waiting for the pdb to be processed 02/06/23 17:57:17.088
STEP: Patching PodDisruptionBudget status 02/06/23 17:57:17.094
STEP: Waiting for the pdb to be processed 02/06/23 17:57:17.106
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb  6 17:57:17.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5298" for this suite. 02/06/23 17:57:17.116
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":136,"skipped":2677,"failed":0}
------------------------------
• [SLOW TEST] [6.127 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:57:10.995
    Feb  6 17:57:10.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename disruption 02/06/23 17:57:10.998
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:57:11.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:57:11.027
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 02/06/23 17:57:11.039
    STEP: Updating PodDisruptionBudget status 02/06/23 17:57:13.052
    STEP: Waiting for all pods to be running 02/06/23 17:57:13.063
    Feb  6 17:57:13.072: INFO: running pods: 0 < 1
    Feb  6 17:57:15.076: INFO: running pods: 0 < 1
    STEP: locating a running pod 02/06/23 17:57:17.077
    STEP: Waiting for the pdb to be processed 02/06/23 17:57:17.088
    STEP: Patching PodDisruptionBudget status 02/06/23 17:57:17.094
    STEP: Waiting for the pdb to be processed 02/06/23 17:57:17.106
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb  6 17:57:17.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5298" for this suite. 02/06/23 17:57:17.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:57:17.125
Feb  6 17:57:17.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename deployment 02/06/23 17:57:17.126
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:57:17.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:57:17.15
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Feb  6 17:57:17.154: INFO: Creating deployment "test-recreate-deployment"
Feb  6 17:57:17.159: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb  6 17:57:17.176: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb  6 17:57:19.184: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb  6 17:57:19.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 57, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 57, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 57, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 57, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 17:57:21.192: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb  6 17:57:21.203: INFO: Updating deployment test-recreate-deployment
Feb  6 17:57:21.203: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb  6 17:57:21.352: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1143  575f838c-08f4-4a6f-9171-c44854d72de1 35952 2 2023-02-06 17:57:17 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a8bfe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-06 17:57:21 +0000 UTC,LastTransitionTime:2023-02-06 17:57:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-02-06 17:57:21 +0000 UTC,LastTransitionTime:2023-02-06 17:57:17 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb  6 17:57:21.356: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-1143  54a4e981-99ec-4b9c-abad-13a4f6f0e72e 35950 1 2023-02-06 17:57:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 575f838c-08f4-4a6f-9171-c44854d72de1 0xc003ee4490 0xc003ee4491}] [] [{kube-controller-manager Update apps/v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"575f838c-08f4-4a6f-9171-c44854d72de1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ee4528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb  6 17:57:21.356: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb  6 17:57:21.356: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-1143  e059ab53-b3f3-448a-a510-b022fc16b1fb 35938 2 2023-02-06 17:57:17 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 575f838c-08f4-4a6f-9171-c44854d72de1 0xc003ee4377 0xc003ee4378}] [] [{kube-controller-manager Update apps/v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"575f838c-08f4-4a6f-9171-c44854d72de1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ee4428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb  6 17:57:21.360: INFO: Pod "test-recreate-deployment-9d58999df-tj26w" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-tj26w test-recreate-deployment-9d58999df- deployment-1143  7fada688-29ce-476b-a7b5-6c193796f5e8 35951 0 2023-02-06 17:57:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 54a4e981-99ec-4b9c-abad-13a4f6f0e72e 0xc003cdedc0 0xc003cdedc1}] [] [{kube-controller-manager Update v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54a4e981-99ec-4b9c-abad-13a4f6f0e72e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6g58w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6g58w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:57:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:57:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:57:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:57:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:57:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb  6 17:57:21.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1143" for this suite. 02/06/23 17:57:21.365
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":137,"skipped":2689,"failed":0}
------------------------------
• [4.247 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:57:17.125
    Feb  6 17:57:17.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename deployment 02/06/23 17:57:17.126
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:57:17.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:57:17.15
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Feb  6 17:57:17.154: INFO: Creating deployment "test-recreate-deployment"
    Feb  6 17:57:17.159: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Feb  6 17:57:17.176: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Feb  6 17:57:19.184: INFO: Waiting deployment "test-recreate-deployment" to complete
    Feb  6 17:57:19.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 57, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 57, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 57, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 57, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 17:57:21.192: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Feb  6 17:57:21.203: INFO: Updating deployment test-recreate-deployment
    Feb  6 17:57:21.203: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb  6 17:57:21.352: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1143  575f838c-08f4-4a6f-9171-c44854d72de1 35952 2 2023-02-06 17:57:17 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a8bfe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-06 17:57:21 +0000 UTC,LastTransitionTime:2023-02-06 17:57:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-02-06 17:57:21 +0000 UTC,LastTransitionTime:2023-02-06 17:57:17 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Feb  6 17:57:21.356: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-1143  54a4e981-99ec-4b9c-abad-13a4f6f0e72e 35950 1 2023-02-06 17:57:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 575f838c-08f4-4a6f-9171-c44854d72de1 0xc003ee4490 0xc003ee4491}] [] [{kube-controller-manager Update apps/v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"575f838c-08f4-4a6f-9171-c44854d72de1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ee4528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb  6 17:57:21.356: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Feb  6 17:57:21.356: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-1143  e059ab53-b3f3-448a-a510-b022fc16b1fb 35938 2 2023-02-06 17:57:17 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 575f838c-08f4-4a6f-9171-c44854d72de1 0xc003ee4377 0xc003ee4378}] [] [{kube-controller-manager Update apps/v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"575f838c-08f4-4a6f-9171-c44854d72de1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ee4428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb  6 17:57:21.360: INFO: Pod "test-recreate-deployment-9d58999df-tj26w" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-tj26w test-recreate-deployment-9d58999df- deployment-1143  7fada688-29ce-476b-a7b5-6c193796f5e8 35951 0 2023-02-06 17:57:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 54a4e981-99ec-4b9c-abad-13a4f6f0e72e 0xc003cdedc0 0xc003cdedc1}] [] [{kube-controller-manager Update v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54a4e981-99ec-4b9c-abad-13a4f6f0e72e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 17:57:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6g58w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6g58w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:57:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:57:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:57:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 17:57:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:,StartTime:2023-02-06 17:57:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb  6 17:57:21.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1143" for this suite. 02/06/23 17:57:21.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:57:21.39
Feb  6 17:57:21.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename statefulset 02/06/23 17:57:21.391
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:57:21.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:57:21.41
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3709 02/06/23 17:57:21.413
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-3709 02/06/23 17:57:21.419
Feb  6 17:57:21.435: INFO: Found 0 stateful pods, waiting for 1
Feb  6 17:57:31.440: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 02/06/23 17:57:31.446
STEP: updating a scale subresource 02/06/23 17:57:31.449
STEP: verifying the statefulset Spec.Replicas was modified 02/06/23 17:57:31.456
STEP: Patch a scale subresource 02/06/23 17:57:31.46
STEP: verifying the statefulset Spec.Replicas was modified 02/06/23 17:57:31.469
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb  6 17:57:31.487: INFO: Deleting all statefulset in ns statefulset-3709
Feb  6 17:57:31.492: INFO: Scaling statefulset ss to 0
Feb  6 17:57:41.530: INFO: Waiting for statefulset status.replicas updated to 0
Feb  6 17:57:41.533: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb  6 17:57:41.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3709" for this suite. 02/06/23 17:57:41.555
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":138,"skipped":2787,"failed":0}
------------------------------
• [SLOW TEST] [20.172 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:57:21.39
    Feb  6 17:57:21.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename statefulset 02/06/23 17:57:21.391
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:57:21.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:57:21.41
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3709 02/06/23 17:57:21.413
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-3709 02/06/23 17:57:21.419
    Feb  6 17:57:21.435: INFO: Found 0 stateful pods, waiting for 1
    Feb  6 17:57:31.440: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 02/06/23 17:57:31.446
    STEP: updating a scale subresource 02/06/23 17:57:31.449
    STEP: verifying the statefulset Spec.Replicas was modified 02/06/23 17:57:31.456
    STEP: Patch a scale subresource 02/06/23 17:57:31.46
    STEP: verifying the statefulset Spec.Replicas was modified 02/06/23 17:57:31.469
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb  6 17:57:31.487: INFO: Deleting all statefulset in ns statefulset-3709
    Feb  6 17:57:31.492: INFO: Scaling statefulset ss to 0
    Feb  6 17:57:41.530: INFO: Waiting for statefulset status.replicas updated to 0
    Feb  6 17:57:41.533: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb  6 17:57:41.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3709" for this suite. 02/06/23 17:57:41.555
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:57:41.564
Feb  6 17:57:41.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 17:57:41.566
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:57:41.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:57:41.584
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 02/06/23 17:57:41.599
STEP: watching for the Service to be added 02/06/23 17:57:41.621
Feb  6 17:57:41.623: INFO: Found Service test-service-xgvlc in namespace services-3001 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Feb  6 17:57:41.624: INFO: Service test-service-xgvlc created
STEP: Getting /status 02/06/23 17:57:41.624
Feb  6 17:57:41.628: INFO: Service test-service-xgvlc has LoadBalancer: {[]}
STEP: patching the ServiceStatus 02/06/23 17:57:41.628
STEP: watching for the Service to be patched 02/06/23 17:57:41.635
Feb  6 17:57:41.638: INFO: observed Service test-service-xgvlc in namespace services-3001 with annotations: map[] & LoadBalancer: {[]}
Feb  6 17:57:41.639: INFO: Found Service test-service-xgvlc in namespace services-3001 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Feb  6 17:57:41.639: INFO: Service test-service-xgvlc has service status patched
STEP: updating the ServiceStatus 02/06/23 17:57:41.639
Feb  6 17:57:41.652: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 02/06/23 17:57:41.652
Feb  6 17:57:41.653: INFO: Observed Service test-service-xgvlc in namespace services-3001 with annotations: map[] & Conditions: {[]}
Feb  6 17:57:41.654: INFO: Observed event: &Service{ObjectMeta:{test-service-xgvlc  services-3001  28a14333-1470-4747-86de-84b29bb51981 36269 0 2023-02-06 17:57:41 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-06 17:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-06 17:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.104.141.181,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.104.141.181],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Feb  6 17:57:41.654: INFO: Found Service test-service-xgvlc in namespace services-3001 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb  6 17:57:41.654: INFO: Service test-service-xgvlc has service status updated
STEP: patching the service 02/06/23 17:57:41.655
STEP: watching for the Service to be patched 02/06/23 17:57:41.675
Feb  6 17:57:41.677: INFO: observed Service test-service-xgvlc in namespace services-3001 with labels: map[test-service-static:true]
Feb  6 17:57:41.677: INFO: observed Service test-service-xgvlc in namespace services-3001 with labels: map[test-service-static:true]
Feb  6 17:57:41.678: INFO: observed Service test-service-xgvlc in namespace services-3001 with labels: map[test-service-static:true]
Feb  6 17:57:41.678: INFO: Found Service test-service-xgvlc in namespace services-3001 with labels: map[test-service:patched test-service-static:true]
Feb  6 17:57:41.678: INFO: Service test-service-xgvlc patched
STEP: deleting the service 02/06/23 17:57:41.678
STEP: watching for the Service to be deleted 02/06/23 17:57:41.698
Feb  6 17:57:41.700: INFO: Observed event: ADDED
Feb  6 17:57:41.700: INFO: Observed event: MODIFIED
Feb  6 17:57:41.701: INFO: Observed event: MODIFIED
Feb  6 17:57:41.701: INFO: Observed event: MODIFIED
Feb  6 17:57:41.701: INFO: Found Service test-service-xgvlc in namespace services-3001 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Feb  6 17:57:41.701: INFO: Service test-service-xgvlc deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 17:57:41.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3001" for this suite. 02/06/23 17:57:41.711
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":139,"skipped":2787,"failed":0}
------------------------------
• [0.153 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:57:41.564
    Feb  6 17:57:41.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 17:57:41.566
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:57:41.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:57:41.584
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 02/06/23 17:57:41.599
    STEP: watching for the Service to be added 02/06/23 17:57:41.621
    Feb  6 17:57:41.623: INFO: Found Service test-service-xgvlc in namespace services-3001 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Feb  6 17:57:41.624: INFO: Service test-service-xgvlc created
    STEP: Getting /status 02/06/23 17:57:41.624
    Feb  6 17:57:41.628: INFO: Service test-service-xgvlc has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 02/06/23 17:57:41.628
    STEP: watching for the Service to be patched 02/06/23 17:57:41.635
    Feb  6 17:57:41.638: INFO: observed Service test-service-xgvlc in namespace services-3001 with annotations: map[] & LoadBalancer: {[]}
    Feb  6 17:57:41.639: INFO: Found Service test-service-xgvlc in namespace services-3001 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Feb  6 17:57:41.639: INFO: Service test-service-xgvlc has service status patched
    STEP: updating the ServiceStatus 02/06/23 17:57:41.639
    Feb  6 17:57:41.652: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 02/06/23 17:57:41.652
    Feb  6 17:57:41.653: INFO: Observed Service test-service-xgvlc in namespace services-3001 with annotations: map[] & Conditions: {[]}
    Feb  6 17:57:41.654: INFO: Observed event: &Service{ObjectMeta:{test-service-xgvlc  services-3001  28a14333-1470-4747-86de-84b29bb51981 36269 0 2023-02-06 17:57:41 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-06 17:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-06 17:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.104.141.181,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.104.141.181],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Feb  6 17:57:41.654: INFO: Found Service test-service-xgvlc in namespace services-3001 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb  6 17:57:41.654: INFO: Service test-service-xgvlc has service status updated
    STEP: patching the service 02/06/23 17:57:41.655
    STEP: watching for the Service to be patched 02/06/23 17:57:41.675
    Feb  6 17:57:41.677: INFO: observed Service test-service-xgvlc in namespace services-3001 with labels: map[test-service-static:true]
    Feb  6 17:57:41.677: INFO: observed Service test-service-xgvlc in namespace services-3001 with labels: map[test-service-static:true]
    Feb  6 17:57:41.678: INFO: observed Service test-service-xgvlc in namespace services-3001 with labels: map[test-service-static:true]
    Feb  6 17:57:41.678: INFO: Found Service test-service-xgvlc in namespace services-3001 with labels: map[test-service:patched test-service-static:true]
    Feb  6 17:57:41.678: INFO: Service test-service-xgvlc patched
    STEP: deleting the service 02/06/23 17:57:41.678
    STEP: watching for the Service to be deleted 02/06/23 17:57:41.698
    Feb  6 17:57:41.700: INFO: Observed event: ADDED
    Feb  6 17:57:41.700: INFO: Observed event: MODIFIED
    Feb  6 17:57:41.701: INFO: Observed event: MODIFIED
    Feb  6 17:57:41.701: INFO: Observed event: MODIFIED
    Feb  6 17:57:41.701: INFO: Found Service test-service-xgvlc in namespace services-3001 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Feb  6 17:57:41.701: INFO: Service test-service-xgvlc deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 17:57:41.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3001" for this suite. 02/06/23 17:57:41.711
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:57:41.723
Feb  6 17:57:41.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename svcaccounts 02/06/23 17:57:41.725
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:57:41.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:57:41.746
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Feb  6 17:57:41.761: INFO: created pod
Feb  6 17:57:41.761: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6091" to be "Succeeded or Failed"
Feb  6 17:57:41.767: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.940974ms
Feb  6 17:57:43.773: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011800598s
Feb  6 17:57:45.772: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010099684s
Feb  6 17:57:47.771: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009967794s
STEP: Saw pod success 02/06/23 17:57:47.771
Feb  6 17:57:47.772: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Feb  6 17:58:17.773: INFO: polling logs
Feb  6 17:58:17.781: INFO: Pod logs: 
I0206 17:57:44.613768       1 log.go:195] OK: Got token
I0206 17:57:44.613829       1 log.go:195] validating with in-cluster discovery
I0206 17:57:44.614267       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0206 17:57:44.614404       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6091:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675706862, NotBefore:1675706262, IssuedAt:1675706262, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6091", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d03df0c2-0096-4782-ab95-035dae1336b8"}}}
I0206 17:57:44.625762       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0206 17:57:44.631891       1 log.go:195] OK: Validated signature on JWT
I0206 17:57:44.631981       1 log.go:195] OK: Got valid claims from token!
I0206 17:57:44.632008       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6091:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675706862, NotBefore:1675706262, IssuedAt:1675706262, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6091", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d03df0c2-0096-4782-ab95-035dae1336b8"}}}

Feb  6 17:58:17.781: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb  6 17:58:17.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6091" for this suite. 02/06/23 17:58:17.792
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":140,"skipped":2788,"failed":0}
------------------------------
• [SLOW TEST] [36.075 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:57:41.723
    Feb  6 17:57:41.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename svcaccounts 02/06/23 17:57:41.725
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:57:41.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:57:41.746
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Feb  6 17:57:41.761: INFO: created pod
    Feb  6 17:57:41.761: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6091" to be "Succeeded or Failed"
    Feb  6 17:57:41.767: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.940974ms
    Feb  6 17:57:43.773: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011800598s
    Feb  6 17:57:45.772: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010099684s
    Feb  6 17:57:47.771: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009967794s
    STEP: Saw pod success 02/06/23 17:57:47.771
    Feb  6 17:57:47.772: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Feb  6 17:58:17.773: INFO: polling logs
    Feb  6 17:58:17.781: INFO: Pod logs: 
    I0206 17:57:44.613768       1 log.go:195] OK: Got token
    I0206 17:57:44.613829       1 log.go:195] validating with in-cluster discovery
    I0206 17:57:44.614267       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0206 17:57:44.614404       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6091:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675706862, NotBefore:1675706262, IssuedAt:1675706262, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6091", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d03df0c2-0096-4782-ab95-035dae1336b8"}}}
    I0206 17:57:44.625762       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0206 17:57:44.631891       1 log.go:195] OK: Validated signature on JWT
    I0206 17:57:44.631981       1 log.go:195] OK: Got valid claims from token!
    I0206 17:57:44.632008       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6091:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675706862, NotBefore:1675706262, IssuedAt:1675706262, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6091", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d03df0c2-0096-4782-ab95-035dae1336b8"}}}

    Feb  6 17:58:17.781: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb  6 17:58:17.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6091" for this suite. 02/06/23 17:58:17.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:58:17.798
Feb  6 17:58:17.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 17:58:17.799
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:58:17.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:58:17.816
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 17:58:17.837
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:58:18.474
STEP: Deploying the webhook pod 02/06/23 17:58:18.484
STEP: Wait for the deployment to be ready 02/06/23 17:58:18.504
Feb  6 17:58:18.522: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 17:58:20.533: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 58, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 58, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 58, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 58, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 17:58:22.538
STEP: Verifying the service has paired with the endpoint 02/06/23 17:58:22.56
Feb  6 17:58:23.561: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Feb  6 17:58:23.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1690-crds.webhook.example.com via the AdmissionRegistration API 02/06/23 17:58:24.079
STEP: Creating a custom resource that should be mutated by the webhook 02/06/23 17:58:24.13
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:58:26.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3348" for this suite. 02/06/23 17:58:26.817
STEP: Destroying namespace "webhook-3348-markers" for this suite. 02/06/23 17:58:26.825
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":141,"skipped":2794,"failed":0}
------------------------------
• [SLOW TEST] [9.129 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:58:17.798
    Feb  6 17:58:17.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 17:58:17.799
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:58:17.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:58:17.816
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 17:58:17.837
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:58:18.474
    STEP: Deploying the webhook pod 02/06/23 17:58:18.484
    STEP: Wait for the deployment to be ready 02/06/23 17:58:18.504
    Feb  6 17:58:18.522: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 17:58:20.533: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 58, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 58, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 58, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 58, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 17:58:22.538
    STEP: Verifying the service has paired with the endpoint 02/06/23 17:58:22.56
    Feb  6 17:58:23.561: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Feb  6 17:58:23.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1690-crds.webhook.example.com via the AdmissionRegistration API 02/06/23 17:58:24.079
    STEP: Creating a custom resource that should be mutated by the webhook 02/06/23 17:58:24.13
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:58:26.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3348" for this suite. 02/06/23 17:58:26.817
    STEP: Destroying namespace "webhook-3348-markers" for this suite. 02/06/23 17:58:26.825
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:58:26.94
Feb  6 17:58:26.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 17:58:26.941
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:58:26.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:58:26.972
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 17:58:26.996
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:58:27.307
STEP: Deploying the webhook pod 02/06/23 17:58:27.314
STEP: Wait for the deployment to be ready 02/06/23 17:58:27.329
Feb  6 17:58:27.358: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 17:58:29.370: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 17:58:31.375
STEP: Verifying the service has paired with the endpoint 02/06/23 17:58:31.392
Feb  6 17:58:32.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Feb  6 17:58:32.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3419-crds.webhook.example.com via the AdmissionRegistration API 02/06/23 17:58:32.91
STEP: Creating a custom resource while v1 is storage version 02/06/23 17:58:32.934
STEP: Patching Custom Resource Definition to set v2 as storage 02/06/23 17:58:34.991
STEP: Patching the custom resource while v2 is storage version 02/06/23 17:58:35.016
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:58:35.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3128" for this suite. 02/06/23 17:58:35.636
STEP: Destroying namespace "webhook-3128-markers" for this suite. 02/06/23 17:58:35.645
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":142,"skipped":2818,"failed":0}
------------------------------
• [SLOW TEST] [8.779 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:58:26.94
    Feb  6 17:58:26.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 17:58:26.941
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:58:26.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:58:26.972
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 17:58:26.996
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:58:27.307
    STEP: Deploying the webhook pod 02/06/23 17:58:27.314
    STEP: Wait for the deployment to be ready 02/06/23 17:58:27.329
    Feb  6 17:58:27.358: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 17:58:29.370: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 17:58:31.375
    STEP: Verifying the service has paired with the endpoint 02/06/23 17:58:31.392
    Feb  6 17:58:32.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Feb  6 17:58:32.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3419-crds.webhook.example.com via the AdmissionRegistration API 02/06/23 17:58:32.91
    STEP: Creating a custom resource while v1 is storage version 02/06/23 17:58:32.934
    STEP: Patching Custom Resource Definition to set v2 as storage 02/06/23 17:58:34.991
    STEP: Patching the custom resource while v2 is storage version 02/06/23 17:58:35.016
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:58:35.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3128" for this suite. 02/06/23 17:58:35.636
    STEP: Destroying namespace "webhook-3128-markers" for this suite. 02/06/23 17:58:35.645
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:58:35.721
Feb  6 17:58:35.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename svcaccounts 02/06/23 17:58:35.722
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:58:35.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:58:35.767
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Feb  6 17:58:35.779: INFO: Got root ca configmap in namespace "svcaccounts-4120"
Feb  6 17:58:35.786: INFO: Deleted root ca configmap in namespace "svcaccounts-4120"
STEP: waiting for a new root ca configmap created 02/06/23 17:58:36.287
Feb  6 17:58:36.290: INFO: Recreated root ca configmap in namespace "svcaccounts-4120"
Feb  6 17:58:36.296: INFO: Updated root ca configmap in namespace "svcaccounts-4120"
STEP: waiting for the root ca configmap reconciled 02/06/23 17:58:36.796
Feb  6 17:58:36.800: INFO: Reconciled root ca configmap in namespace "svcaccounts-4120"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb  6 17:58:36.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4120" for this suite. 02/06/23 17:58:36.806
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":143,"skipped":2845,"failed":0}
------------------------------
• [1.095 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:58:35.721
    Feb  6 17:58:35.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename svcaccounts 02/06/23 17:58:35.722
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:58:35.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:58:35.767
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Feb  6 17:58:35.779: INFO: Got root ca configmap in namespace "svcaccounts-4120"
    Feb  6 17:58:35.786: INFO: Deleted root ca configmap in namespace "svcaccounts-4120"
    STEP: waiting for a new root ca configmap created 02/06/23 17:58:36.287
    Feb  6 17:58:36.290: INFO: Recreated root ca configmap in namespace "svcaccounts-4120"
    Feb  6 17:58:36.296: INFO: Updated root ca configmap in namespace "svcaccounts-4120"
    STEP: waiting for the root ca configmap reconciled 02/06/23 17:58:36.796
    Feb  6 17:58:36.800: INFO: Reconciled root ca configmap in namespace "svcaccounts-4120"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb  6 17:58:36.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4120" for this suite. 02/06/23 17:58:36.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:58:36.816
Feb  6 17:58:36.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 17:58:36.818
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:58:36.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:58:36.848
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 17:58:36.88
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:58:37.494
STEP: Deploying the webhook pod 02/06/23 17:58:37.504
STEP: Wait for the deployment to be ready 02/06/23 17:58:37.522
Feb  6 17:58:37.536: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 17:58:39.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 58, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 58, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 58, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 58, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 17:58:41.551
STEP: Verifying the service has paired with the endpoint 02/06/23 17:58:41.571
Feb  6 17:58:42.571: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/06/23 17:58:42.575
STEP: Registering slow webhook via the AdmissionRegistration API 02/06/23 17:58:42.576
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/06/23 17:58:42.591
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/06/23 17:58:43.601
STEP: Registering slow webhook via the AdmissionRegistration API 02/06/23 17:58:43.601
STEP: Having no error when timeout is longer than webhook latency 02/06/23 17:58:44.629
STEP: Registering slow webhook via the AdmissionRegistration API 02/06/23 17:58:44.629
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/06/23 17:58:49.666
STEP: Registering slow webhook via the AdmissionRegistration API 02/06/23 17:58:49.666
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:58:54.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2507" for this suite. 02/06/23 17:58:54.705
STEP: Destroying namespace "webhook-2507-markers" for this suite. 02/06/23 17:58:54.711
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":144,"skipped":2851,"failed":0}
------------------------------
• [SLOW TEST] [17.980 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:58:36.816
    Feb  6 17:58:36.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 17:58:36.818
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:58:36.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:58:36.848
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 17:58:36.88
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 17:58:37.494
    STEP: Deploying the webhook pod 02/06/23 17:58:37.504
    STEP: Wait for the deployment to be ready 02/06/23 17:58:37.522
    Feb  6 17:58:37.536: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 17:58:39.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 17, 58, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 58, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 17, 58, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 17, 58, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 17:58:41.551
    STEP: Verifying the service has paired with the endpoint 02/06/23 17:58:41.571
    Feb  6 17:58:42.571: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/06/23 17:58:42.575
    STEP: Registering slow webhook via the AdmissionRegistration API 02/06/23 17:58:42.576
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/06/23 17:58:42.591
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/06/23 17:58:43.601
    STEP: Registering slow webhook via the AdmissionRegistration API 02/06/23 17:58:43.601
    STEP: Having no error when timeout is longer than webhook latency 02/06/23 17:58:44.629
    STEP: Registering slow webhook via the AdmissionRegistration API 02/06/23 17:58:44.629
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/06/23 17:58:49.666
    STEP: Registering slow webhook via the AdmissionRegistration API 02/06/23 17:58:49.666
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:58:54.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2507" for this suite. 02/06/23 17:58:54.705
    STEP: Destroying namespace "webhook-2507-markers" for this suite. 02/06/23 17:58:54.711
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:58:54.813
Feb  6 17:58:54.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 17:58:54.815
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:58:54.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:58:54.844
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Feb  6 17:58:54.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/06/23 17:59:04.092
Feb  6 17:59:04.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-9540 --namespace=crd-publish-openapi-9540 create -f -'
Feb  6 17:59:05.554: INFO: stderr: ""
Feb  6 17:59:05.554: INFO: stdout: "e2e-test-crd-publish-openapi-1070-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb  6 17:59:05.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-9540 --namespace=crd-publish-openapi-9540 delete e2e-test-crd-publish-openapi-1070-crds test-cr'
Feb  6 17:59:05.681: INFO: stderr: ""
Feb  6 17:59:05.681: INFO: stdout: "e2e-test-crd-publish-openapi-1070-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb  6 17:59:05.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-9540 --namespace=crd-publish-openapi-9540 apply -f -'
Feb  6 17:59:07.136: INFO: stderr: ""
Feb  6 17:59:07.136: INFO: stdout: "e2e-test-crd-publish-openapi-1070-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb  6 17:59:07.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-9540 --namespace=crd-publish-openapi-9540 delete e2e-test-crd-publish-openapi-1070-crds test-cr'
Feb  6 17:59:07.236: INFO: stderr: ""
Feb  6 17:59:07.237: INFO: stdout: "e2e-test-crd-publish-openapi-1070-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/06/23 17:59:07.237
Feb  6 17:59:07.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-9540 explain e2e-test-crd-publish-openapi-1070-crds'
Feb  6 17:59:07.666: INFO: stderr: ""
Feb  6 17:59:07.666: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1070-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 17:59:15.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9540" for this suite. 02/06/23 17:59:15.668
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":145,"skipped":2875,"failed":0}
------------------------------
• [SLOW TEST] [20.864 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:58:54.813
    Feb  6 17:58:54.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 17:58:54.815
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:58:54.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:58:54.844
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Feb  6 17:58:54.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/06/23 17:59:04.092
    Feb  6 17:59:04.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-9540 --namespace=crd-publish-openapi-9540 create -f -'
    Feb  6 17:59:05.554: INFO: stderr: ""
    Feb  6 17:59:05.554: INFO: stdout: "e2e-test-crd-publish-openapi-1070-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb  6 17:59:05.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-9540 --namespace=crd-publish-openapi-9540 delete e2e-test-crd-publish-openapi-1070-crds test-cr'
    Feb  6 17:59:05.681: INFO: stderr: ""
    Feb  6 17:59:05.681: INFO: stdout: "e2e-test-crd-publish-openapi-1070-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Feb  6 17:59:05.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-9540 --namespace=crd-publish-openapi-9540 apply -f -'
    Feb  6 17:59:07.136: INFO: stderr: ""
    Feb  6 17:59:07.136: INFO: stdout: "e2e-test-crd-publish-openapi-1070-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb  6 17:59:07.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-9540 --namespace=crd-publish-openapi-9540 delete e2e-test-crd-publish-openapi-1070-crds test-cr'
    Feb  6 17:59:07.236: INFO: stderr: ""
    Feb  6 17:59:07.237: INFO: stdout: "e2e-test-crd-publish-openapi-1070-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/06/23 17:59:07.237
    Feb  6 17:59:07.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-9540 explain e2e-test-crd-publish-openapi-1070-crds'
    Feb  6 17:59:07.666: INFO: stderr: ""
    Feb  6 17:59:07.666: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1070-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 17:59:15.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9540" for this suite. 02/06/23 17:59:15.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:59:15.678
Feb  6 17:59:15.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename replication-controller 02/06/23 17:59:15.68
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:59:15.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:59:15.702
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5 02/06/23 17:59:15.705
Feb  6 17:59:15.715: INFO: Pod name my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5: Found 0 pods out of 1
Feb  6 17:59:20.725: INFO: Pod name my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5: Found 1 pods out of 1
Feb  6 17:59:20.725: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5" are running
Feb  6 17:59:20.725: INFO: Waiting up to 5m0s for pod "my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5-lhf2j" in namespace "replication-controller-7728" to be "running"
Feb  6 17:59:20.728: INFO: Pod "my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5-lhf2j": Phase="Running", Reason="", readiness=true. Elapsed: 3.76822ms
Feb  6 17:59:20.728: INFO: Pod "my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5-lhf2j" satisfied condition "running"
Feb  6 17:59:20.728: INFO: Pod "my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5-lhf2j" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:59:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:59:18 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:59:18 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:59:15 +0000 UTC Reason: Message:}])
Feb  6 17:59:20.729: INFO: Trying to dial the pod
Feb  6 17:59:25.744: INFO: Controller my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5: Got expected result from replica 1 [my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5-lhf2j]: "my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5-lhf2j", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb  6 17:59:25.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7728" for this suite. 02/06/23 17:59:25.751
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":146,"skipped":2905,"failed":0}
------------------------------
• [SLOW TEST] [10.080 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:59:15.678
    Feb  6 17:59:15.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename replication-controller 02/06/23 17:59:15.68
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:59:15.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:59:15.702
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5 02/06/23 17:59:15.705
    Feb  6 17:59:15.715: INFO: Pod name my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5: Found 0 pods out of 1
    Feb  6 17:59:20.725: INFO: Pod name my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5: Found 1 pods out of 1
    Feb  6 17:59:20.725: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5" are running
    Feb  6 17:59:20.725: INFO: Waiting up to 5m0s for pod "my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5-lhf2j" in namespace "replication-controller-7728" to be "running"
    Feb  6 17:59:20.728: INFO: Pod "my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5-lhf2j": Phase="Running", Reason="", readiness=true. Elapsed: 3.76822ms
    Feb  6 17:59:20.728: INFO: Pod "my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5-lhf2j" satisfied condition "running"
    Feb  6 17:59:20.728: INFO: Pod "my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5-lhf2j" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:59:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:59:18 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:59:18 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-06 17:59:15 +0000 UTC Reason: Message:}])
    Feb  6 17:59:20.729: INFO: Trying to dial the pod
    Feb  6 17:59:25.744: INFO: Controller my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5: Got expected result from replica 1 [my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5-lhf2j]: "my-hostname-basic-dc1b09b9-59d4-4b48-97e3-cdd9a7ff0ff5-lhf2j", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb  6 17:59:25.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7728" for this suite. 02/06/23 17:59:25.751
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:59:25.76
Feb  6 17:59:25.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 17:59:25.763
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:59:25.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:59:25.784
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 02/06/23 17:59:25.787
Feb  6 17:59:25.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 create -f -'
Feb  6 17:59:27.227: INFO: stderr: ""
Feb  6 17:59:27.227: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/06/23 17:59:27.227
Feb  6 17:59:27.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  6 17:59:27.337: INFO: stderr: ""
Feb  6 17:59:27.337: INFO: stdout: "update-demo-nautilus-5qw2k update-demo-nautilus-6fqmc "
Feb  6 17:59:27.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-5qw2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  6 17:59:27.428: INFO: stderr: ""
Feb  6 17:59:27.429: INFO: stdout: ""
Feb  6 17:59:27.429: INFO: update-demo-nautilus-5qw2k is created but not running
Feb  6 17:59:32.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  6 17:59:32.534: INFO: stderr: ""
Feb  6 17:59:32.534: INFO: stdout: "update-demo-nautilus-5qw2k update-demo-nautilus-6fqmc "
Feb  6 17:59:32.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-5qw2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  6 17:59:32.631: INFO: stderr: ""
Feb  6 17:59:32.631: INFO: stdout: ""
Feb  6 17:59:32.631: INFO: update-demo-nautilus-5qw2k is created but not running
Feb  6 17:59:37.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  6 17:59:37.719: INFO: stderr: ""
Feb  6 17:59:37.719: INFO: stdout: "update-demo-nautilus-5qw2k update-demo-nautilus-6fqmc "
Feb  6 17:59:37.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-5qw2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  6 17:59:37.807: INFO: stderr: ""
Feb  6 17:59:37.807: INFO: stdout: "true"
Feb  6 17:59:37.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-5qw2k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  6 17:59:37.888: INFO: stderr: ""
Feb  6 17:59:37.888: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb  6 17:59:37.888: INFO: validating pod update-demo-nautilus-5qw2k
Feb  6 17:59:37.895: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  6 17:59:37.895: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  6 17:59:37.895: INFO: update-demo-nautilus-5qw2k is verified up and running
Feb  6 17:59:37.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-6fqmc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  6 17:59:37.980: INFO: stderr: ""
Feb  6 17:59:37.980: INFO: stdout: "true"
Feb  6 17:59:37.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-6fqmc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  6 17:59:38.071: INFO: stderr: ""
Feb  6 17:59:38.071: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb  6 17:59:38.071: INFO: validating pod update-demo-nautilus-6fqmc
Feb  6 17:59:38.077: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  6 17:59:38.077: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  6 17:59:38.077: INFO: update-demo-nautilus-6fqmc is verified up and running
STEP: scaling down the replication controller 02/06/23 17:59:38.077
Feb  6 17:59:38.082: INFO: scanned /root for discovery docs: <nil>
Feb  6 17:59:38.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb  6 17:59:39.197: INFO: stderr: ""
Feb  6 17:59:39.197: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/06/23 17:59:39.197
Feb  6 17:59:39.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  6 17:59:39.290: INFO: stderr: ""
Feb  6 17:59:39.290: INFO: stdout: "update-demo-nautilus-6fqmc "
Feb  6 17:59:39.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-6fqmc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  6 17:59:39.384: INFO: stderr: ""
Feb  6 17:59:39.384: INFO: stdout: "true"
Feb  6 17:59:39.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-6fqmc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  6 17:59:39.470: INFO: stderr: ""
Feb  6 17:59:39.470: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb  6 17:59:39.470: INFO: validating pod update-demo-nautilus-6fqmc
Feb  6 17:59:39.476: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  6 17:59:39.476: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  6 17:59:39.476: INFO: update-demo-nautilus-6fqmc is verified up and running
STEP: scaling up the replication controller 02/06/23 17:59:39.476
Feb  6 17:59:39.485: INFO: scanned /root for discovery docs: <nil>
Feb  6 17:59:39.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb  6 17:59:40.603: INFO: stderr: ""
Feb  6 17:59:40.603: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/06/23 17:59:40.603
Feb  6 17:59:40.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  6 17:59:40.692: INFO: stderr: ""
Feb  6 17:59:40.692: INFO: stdout: "update-demo-nautilus-65xhw update-demo-nautilus-6fqmc "
Feb  6 17:59:40.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-65xhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  6 17:59:40.774: INFO: stderr: ""
Feb  6 17:59:40.774: INFO: stdout: ""
Feb  6 17:59:40.774: INFO: update-demo-nautilus-65xhw is created but not running
Feb  6 17:59:45.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  6 17:59:45.878: INFO: stderr: ""
Feb  6 17:59:45.878: INFO: stdout: "update-demo-nautilus-65xhw update-demo-nautilus-6fqmc "
Feb  6 17:59:45.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-65xhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  6 17:59:45.964: INFO: stderr: ""
Feb  6 17:59:45.964: INFO: stdout: "true"
Feb  6 17:59:45.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-65xhw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  6 17:59:46.053: INFO: stderr: ""
Feb  6 17:59:46.053: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb  6 17:59:46.053: INFO: validating pod update-demo-nautilus-65xhw
Feb  6 17:59:46.060: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  6 17:59:46.060: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  6 17:59:46.060: INFO: update-demo-nautilus-65xhw is verified up and running
Feb  6 17:59:46.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-6fqmc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  6 17:59:46.152: INFO: stderr: ""
Feb  6 17:59:46.152: INFO: stdout: "true"
Feb  6 17:59:46.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-6fqmc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  6 17:59:46.244: INFO: stderr: ""
Feb  6 17:59:46.244: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb  6 17:59:46.244: INFO: validating pod update-demo-nautilus-6fqmc
Feb  6 17:59:46.250: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  6 17:59:46.250: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  6 17:59:46.250: INFO: update-demo-nautilus-6fqmc is verified up and running
STEP: using delete to clean up resources 02/06/23 17:59:46.25
Feb  6 17:59:46.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 delete --grace-period=0 --force -f -'
Feb  6 17:59:46.344: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  6 17:59:46.344: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb  6 17:59:46.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get rc,svc -l name=update-demo --no-headers'
Feb  6 17:59:46.494: INFO: stderr: "No resources found in kubectl-1451 namespace.\n"
Feb  6 17:59:46.494: INFO: stdout: ""
Feb  6 17:59:46.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  6 17:59:46.605: INFO: stderr: ""
Feb  6 17:59:46.605: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 17:59:46.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1451" for this suite. 02/06/23 17:59:46.611
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":147,"skipped":2907,"failed":0}
------------------------------
• [SLOW TEST] [20.861 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:59:25.76
    Feb  6 17:59:25.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 17:59:25.763
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:59:25.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:59:25.784
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 02/06/23 17:59:25.787
    Feb  6 17:59:25.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 create -f -'
    Feb  6 17:59:27.227: INFO: stderr: ""
    Feb  6 17:59:27.227: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/06/23 17:59:27.227
    Feb  6 17:59:27.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb  6 17:59:27.337: INFO: stderr: ""
    Feb  6 17:59:27.337: INFO: stdout: "update-demo-nautilus-5qw2k update-demo-nautilus-6fqmc "
    Feb  6 17:59:27.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-5qw2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb  6 17:59:27.428: INFO: stderr: ""
    Feb  6 17:59:27.429: INFO: stdout: ""
    Feb  6 17:59:27.429: INFO: update-demo-nautilus-5qw2k is created but not running
    Feb  6 17:59:32.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb  6 17:59:32.534: INFO: stderr: ""
    Feb  6 17:59:32.534: INFO: stdout: "update-demo-nautilus-5qw2k update-demo-nautilus-6fqmc "
    Feb  6 17:59:32.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-5qw2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb  6 17:59:32.631: INFO: stderr: ""
    Feb  6 17:59:32.631: INFO: stdout: ""
    Feb  6 17:59:32.631: INFO: update-demo-nautilus-5qw2k is created but not running
    Feb  6 17:59:37.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb  6 17:59:37.719: INFO: stderr: ""
    Feb  6 17:59:37.719: INFO: stdout: "update-demo-nautilus-5qw2k update-demo-nautilus-6fqmc "
    Feb  6 17:59:37.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-5qw2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb  6 17:59:37.807: INFO: stderr: ""
    Feb  6 17:59:37.807: INFO: stdout: "true"
    Feb  6 17:59:37.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-5qw2k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb  6 17:59:37.888: INFO: stderr: ""
    Feb  6 17:59:37.888: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb  6 17:59:37.888: INFO: validating pod update-demo-nautilus-5qw2k
    Feb  6 17:59:37.895: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb  6 17:59:37.895: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb  6 17:59:37.895: INFO: update-demo-nautilus-5qw2k is verified up and running
    Feb  6 17:59:37.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-6fqmc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb  6 17:59:37.980: INFO: stderr: ""
    Feb  6 17:59:37.980: INFO: stdout: "true"
    Feb  6 17:59:37.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-6fqmc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb  6 17:59:38.071: INFO: stderr: ""
    Feb  6 17:59:38.071: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb  6 17:59:38.071: INFO: validating pod update-demo-nautilus-6fqmc
    Feb  6 17:59:38.077: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb  6 17:59:38.077: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb  6 17:59:38.077: INFO: update-demo-nautilus-6fqmc is verified up and running
    STEP: scaling down the replication controller 02/06/23 17:59:38.077
    Feb  6 17:59:38.082: INFO: scanned /root for discovery docs: <nil>
    Feb  6 17:59:38.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Feb  6 17:59:39.197: INFO: stderr: ""
    Feb  6 17:59:39.197: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/06/23 17:59:39.197
    Feb  6 17:59:39.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb  6 17:59:39.290: INFO: stderr: ""
    Feb  6 17:59:39.290: INFO: stdout: "update-demo-nautilus-6fqmc "
    Feb  6 17:59:39.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-6fqmc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb  6 17:59:39.384: INFO: stderr: ""
    Feb  6 17:59:39.384: INFO: stdout: "true"
    Feb  6 17:59:39.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-6fqmc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb  6 17:59:39.470: INFO: stderr: ""
    Feb  6 17:59:39.470: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb  6 17:59:39.470: INFO: validating pod update-demo-nautilus-6fqmc
    Feb  6 17:59:39.476: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb  6 17:59:39.476: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb  6 17:59:39.476: INFO: update-demo-nautilus-6fqmc is verified up and running
    STEP: scaling up the replication controller 02/06/23 17:59:39.476
    Feb  6 17:59:39.485: INFO: scanned /root for discovery docs: <nil>
    Feb  6 17:59:39.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Feb  6 17:59:40.603: INFO: stderr: ""
    Feb  6 17:59:40.603: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/06/23 17:59:40.603
    Feb  6 17:59:40.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb  6 17:59:40.692: INFO: stderr: ""
    Feb  6 17:59:40.692: INFO: stdout: "update-demo-nautilus-65xhw update-demo-nautilus-6fqmc "
    Feb  6 17:59:40.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-65xhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb  6 17:59:40.774: INFO: stderr: ""
    Feb  6 17:59:40.774: INFO: stdout: ""
    Feb  6 17:59:40.774: INFO: update-demo-nautilus-65xhw is created but not running
    Feb  6 17:59:45.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb  6 17:59:45.878: INFO: stderr: ""
    Feb  6 17:59:45.878: INFO: stdout: "update-demo-nautilus-65xhw update-demo-nautilus-6fqmc "
    Feb  6 17:59:45.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-65xhw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb  6 17:59:45.964: INFO: stderr: ""
    Feb  6 17:59:45.964: INFO: stdout: "true"
    Feb  6 17:59:45.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-65xhw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb  6 17:59:46.053: INFO: stderr: ""
    Feb  6 17:59:46.053: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb  6 17:59:46.053: INFO: validating pod update-demo-nautilus-65xhw
    Feb  6 17:59:46.060: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb  6 17:59:46.060: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb  6 17:59:46.060: INFO: update-demo-nautilus-65xhw is verified up and running
    Feb  6 17:59:46.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-6fqmc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb  6 17:59:46.152: INFO: stderr: ""
    Feb  6 17:59:46.152: INFO: stdout: "true"
    Feb  6 17:59:46.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods update-demo-nautilus-6fqmc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb  6 17:59:46.244: INFO: stderr: ""
    Feb  6 17:59:46.244: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb  6 17:59:46.244: INFO: validating pod update-demo-nautilus-6fqmc
    Feb  6 17:59:46.250: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb  6 17:59:46.250: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb  6 17:59:46.250: INFO: update-demo-nautilus-6fqmc is verified up and running
    STEP: using delete to clean up resources 02/06/23 17:59:46.25
    Feb  6 17:59:46.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 delete --grace-period=0 --force -f -'
    Feb  6 17:59:46.344: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb  6 17:59:46.344: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb  6 17:59:46.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get rc,svc -l name=update-demo --no-headers'
    Feb  6 17:59:46.494: INFO: stderr: "No resources found in kubectl-1451 namespace.\n"
    Feb  6 17:59:46.494: INFO: stdout: ""
    Feb  6 17:59:46.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1451 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb  6 17:59:46.605: INFO: stderr: ""
    Feb  6 17:59:46.605: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 17:59:46.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1451" for this suite. 02/06/23 17:59:46.611
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 17:59:46.621
Feb  6 17:59:46.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename sched-preemption 02/06/23 17:59:46.622
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:59:46.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:59:46.656
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb  6 17:59:46.675: INFO: Waiting up to 1m0s for all nodes to be ready
Feb  6 18:00:46.751: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 02/06/23 18:00:46.755
Feb  6 18:00:46.787: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb  6 18:00:46.799: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb  6 18:00:46.835: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb  6 18:00:46.848: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb  6 18:00:46.875: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb  6 18:00:46.887: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/06/23 18:00:46.887
Feb  6 18:00:46.887: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-920" to be "running"
Feb  6 18:00:46.894: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.791837ms
Feb  6 18:00:48.899: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012213856s
Feb  6 18:00:50.901: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013807357s
Feb  6 18:00:52.899: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012076071s
Feb  6 18:00:54.931: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.044168028s
Feb  6 18:00:54.931: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb  6 18:00:54.931: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-920" to be "running"
Feb  6 18:00:55.075: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 143.604215ms
Feb  6 18:00:55.091: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb  6 18:00:55.365: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-920" to be "running"
Feb  6 18:00:55.519: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 131.986071ms
Feb  6 18:00:57.565: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.177646255s
Feb  6 18:00:57.565: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb  6 18:00:57.565: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-920" to be "running"
Feb  6 18:00:57.572: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.074841ms
Feb  6 18:00:57.572: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb  6 18:00:57.572: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-920" to be "running"
Feb  6 18:00:57.581: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.655959ms
Feb  6 18:00:59.586: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.014388805s
Feb  6 18:00:59.586: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb  6 18:00:59.586: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-920" to be "running"
Feb  6 18:00:59.590: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.623523ms
Feb  6 18:00:59.590: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/06/23 18:00:59.59
Feb  6 18:00:59.598: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-920" to be "running"
Feb  6 18:00:59.605: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.34178ms
Feb  6 18:01:01.614: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015922311s
Feb  6 18:01:03.611: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013255642s
Feb  6 18:01:05.610: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.012643703s
Feb  6 18:01:05.611: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb  6 18:01:05.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-920" for this suite. 02/06/23 18:01:05.645
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":148,"skipped":2907,"failed":0}
------------------------------
• [SLOW TEST] [79.090 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 17:59:46.621
    Feb  6 17:59:46.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename sched-preemption 02/06/23 17:59:46.622
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 17:59:46.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 17:59:46.656
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb  6 17:59:46.675: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb  6 18:00:46.751: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 02/06/23 18:00:46.755
    Feb  6 18:00:46.787: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb  6 18:00:46.799: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb  6 18:00:46.835: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb  6 18:00:46.848: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb  6 18:00:46.875: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb  6 18:00:46.887: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/06/23 18:00:46.887
    Feb  6 18:00:46.887: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-920" to be "running"
    Feb  6 18:00:46.894: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.791837ms
    Feb  6 18:00:48.899: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012213856s
    Feb  6 18:00:50.901: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013807357s
    Feb  6 18:00:52.899: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012076071s
    Feb  6 18:00:54.931: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.044168028s
    Feb  6 18:00:54.931: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb  6 18:00:54.931: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-920" to be "running"
    Feb  6 18:00:55.075: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 143.604215ms
    Feb  6 18:00:55.091: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb  6 18:00:55.365: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-920" to be "running"
    Feb  6 18:00:55.519: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 131.986071ms
    Feb  6 18:00:57.565: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.177646255s
    Feb  6 18:00:57.565: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb  6 18:00:57.565: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-920" to be "running"
    Feb  6 18:00:57.572: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.074841ms
    Feb  6 18:00:57.572: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb  6 18:00:57.572: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-920" to be "running"
    Feb  6 18:00:57.581: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.655959ms
    Feb  6 18:00:59.586: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.014388805s
    Feb  6 18:00:59.586: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb  6 18:00:59.586: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-920" to be "running"
    Feb  6 18:00:59.590: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.623523ms
    Feb  6 18:00:59.590: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/06/23 18:00:59.59
    Feb  6 18:00:59.598: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-920" to be "running"
    Feb  6 18:00:59.605: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.34178ms
    Feb  6 18:01:01.614: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015922311s
    Feb  6 18:01:03.611: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013255642s
    Feb  6 18:01:05.610: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.012643703s
    Feb  6 18:01:05.611: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 18:01:05.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-920" for this suite. 02/06/23 18:01:05.645
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:01:05.721
Feb  6 18:01:05.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename gc 02/06/23 18:01:05.722
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:01:05.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:01:05.75
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 02/06/23 18:01:05.755
STEP: Wait for the Deployment to create new ReplicaSet 02/06/23 18:01:05.762
STEP: delete the deployment 02/06/23 18:01:06.278
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/06/23 18:01:06.311
STEP: Gathering metrics 02/06/23 18:01:06.87
Feb  6 18:01:06.903: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tneyla25-pnn8z" in namespace "kube-system" to be "running and ready"
Feb  6 18:01:06.907: INFO: Pod "kube-controller-manager-tneyla25-pnn8z": Phase="Running", Reason="", readiness=true. Elapsed: 3.812888ms
Feb  6 18:01:06.907: INFO: The phase of Pod kube-controller-manager-tneyla25-pnn8z is Running (Ready = true)
Feb  6 18:01:06.907: INFO: Pod "kube-controller-manager-tneyla25-pnn8z" satisfied condition "running and ready"
Feb  6 18:01:06.999: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb  6 18:01:07.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3770" for this suite. 02/06/23 18:01:07.006
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":149,"skipped":2948,"failed":0}
------------------------------
• [1.292 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:01:05.721
    Feb  6 18:01:05.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename gc 02/06/23 18:01:05.722
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:01:05.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:01:05.75
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 02/06/23 18:01:05.755
    STEP: Wait for the Deployment to create new ReplicaSet 02/06/23 18:01:05.762
    STEP: delete the deployment 02/06/23 18:01:06.278
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/06/23 18:01:06.311
    STEP: Gathering metrics 02/06/23 18:01:06.87
    Feb  6 18:01:06.903: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tneyla25-pnn8z" in namespace "kube-system" to be "running and ready"
    Feb  6 18:01:06.907: INFO: Pod "kube-controller-manager-tneyla25-pnn8z": Phase="Running", Reason="", readiness=true. Elapsed: 3.812888ms
    Feb  6 18:01:06.907: INFO: The phase of Pod kube-controller-manager-tneyla25-pnn8z is Running (Ready = true)
    Feb  6 18:01:06.907: INFO: Pod "kube-controller-manager-tneyla25-pnn8z" satisfied condition "running and ready"
    Feb  6 18:01:06.999: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb  6 18:01:07.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3770" for this suite. 02/06/23 18:01:07.006
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:01:07.013
Feb  6 18:01:07.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename dns 02/06/23 18:01:07.014
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:01:07.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:01:07.037
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/06/23 18:01:07.04
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/06/23 18:01:07.041
STEP: creating a pod to probe DNS 02/06/23 18:01:07.041
STEP: submitting the pod to kubernetes 02/06/23 18:01:07.041
Feb  6 18:01:07.055: INFO: Waiting up to 15m0s for pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c" in namespace "dns-7107" to be "running"
Feb  6 18:01:07.066: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.016871ms
Feb  6 18:01:09.073: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018123156s
Feb  6 18:01:11.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017126097s
Feb  6 18:01:13.073: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017628435s
Feb  6 18:01:15.071: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016343582s
Feb  6 18:01:17.074: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018988691s
Feb  6 18:01:19.073: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017744525s
Feb  6 18:01:21.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016795297s
Feb  6 18:01:23.071: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.016273308s
Feb  6 18:01:25.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016907551s
Feb  6 18:01:27.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.017178147s
Feb  6 18:01:29.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.016716139s
Feb  6 18:01:31.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 24.017279763s
Feb  6 18:01:33.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Running", Reason="", readiness=true. Elapsed: 26.017137906s
Feb  6 18:01:33.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c" satisfied condition "running"
STEP: retrieving the pod 02/06/23 18:01:33.072
STEP: looking for the results for each expected name from probers 02/06/23 18:01:33.076
Feb  6 18:01:33.096: INFO: DNS probes using dns-7107/dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c succeeded

STEP: deleting the pod 02/06/23 18:01:33.096
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb  6 18:01:33.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7107" for this suite. 02/06/23 18:01:33.122
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":150,"skipped":2948,"failed":0}
------------------------------
• [SLOW TEST] [26.115 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:01:07.013
    Feb  6 18:01:07.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename dns 02/06/23 18:01:07.014
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:01:07.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:01:07.037
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/06/23 18:01:07.04
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/06/23 18:01:07.041
    STEP: creating a pod to probe DNS 02/06/23 18:01:07.041
    STEP: submitting the pod to kubernetes 02/06/23 18:01:07.041
    Feb  6 18:01:07.055: INFO: Waiting up to 15m0s for pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c" in namespace "dns-7107" to be "running"
    Feb  6 18:01:07.066: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.016871ms
    Feb  6 18:01:09.073: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018123156s
    Feb  6 18:01:11.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017126097s
    Feb  6 18:01:13.073: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017628435s
    Feb  6 18:01:15.071: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016343582s
    Feb  6 18:01:17.074: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018988691s
    Feb  6 18:01:19.073: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017744525s
    Feb  6 18:01:21.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016795297s
    Feb  6 18:01:23.071: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.016273308s
    Feb  6 18:01:25.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016907551s
    Feb  6 18:01:27.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.017178147s
    Feb  6 18:01:29.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.016716139s
    Feb  6 18:01:31.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Pending", Reason="", readiness=false. Elapsed: 24.017279763s
    Feb  6 18:01:33.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c": Phase="Running", Reason="", readiness=true. Elapsed: 26.017137906s
    Feb  6 18:01:33.072: INFO: Pod "dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c" satisfied condition "running"
    STEP: retrieving the pod 02/06/23 18:01:33.072
    STEP: looking for the results for each expected name from probers 02/06/23 18:01:33.076
    Feb  6 18:01:33.096: INFO: DNS probes using dns-7107/dns-test-d194ac6d-3ada-4e3e-b831-fe4fb362467c succeeded

    STEP: deleting the pod 02/06/23 18:01:33.096
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb  6 18:01:33.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7107" for this suite. 02/06/23 18:01:33.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:01:33.13
Feb  6 18:01:33.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 18:01:33.131
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:01:33.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:01:33.173
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 02/06/23 18:01:33.176
Feb  6 18:01:33.188: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525" in namespace "downward-api-7087" to be "Succeeded or Failed"
Feb  6 18:01:33.194: INFO: Pod "downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525": Phase="Pending", Reason="", readiness=false. Elapsed: 5.421004ms
Feb  6 18:01:35.199: INFO: Pod "downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011365125s
Feb  6 18:01:37.199: INFO: Pod "downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010697881s
Feb  6 18:01:39.199: INFO: Pod "downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011216447s
STEP: Saw pod success 02/06/23 18:01:39.199
Feb  6 18:01:39.200: INFO: Pod "downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525" satisfied condition "Succeeded or Failed"
Feb  6 18:01:39.204: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525 container client-container: <nil>
STEP: delete the pod 02/06/23 18:01:39.22
Feb  6 18:01:39.234: INFO: Waiting for pod downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525 to disappear
Feb  6 18:01:39.237: INFO: Pod downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb  6 18:01:39.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7087" for this suite. 02/06/23 18:01:39.243
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":151,"skipped":2971,"failed":0}
------------------------------
• [SLOW TEST] [6.120 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:01:33.13
    Feb  6 18:01:33.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 18:01:33.131
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:01:33.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:01:33.173
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 02/06/23 18:01:33.176
    Feb  6 18:01:33.188: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525" in namespace "downward-api-7087" to be "Succeeded or Failed"
    Feb  6 18:01:33.194: INFO: Pod "downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525": Phase="Pending", Reason="", readiness=false. Elapsed: 5.421004ms
    Feb  6 18:01:35.199: INFO: Pod "downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011365125s
    Feb  6 18:01:37.199: INFO: Pod "downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010697881s
    Feb  6 18:01:39.199: INFO: Pod "downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011216447s
    STEP: Saw pod success 02/06/23 18:01:39.199
    Feb  6 18:01:39.200: INFO: Pod "downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525" satisfied condition "Succeeded or Failed"
    Feb  6 18:01:39.204: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525 container client-container: <nil>
    STEP: delete the pod 02/06/23 18:01:39.22
    Feb  6 18:01:39.234: INFO: Waiting for pod downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525 to disappear
    Feb  6 18:01:39.237: INFO: Pod downwardapi-volume-9b494377-ff76-4cf2-8f91-37822e40e525 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb  6 18:01:39.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7087" for this suite. 02/06/23 18:01:39.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:01:39.251
Feb  6 18:01:39.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-lifecycle-hook 02/06/23 18:01:39.253
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:01:39.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:01:39.285
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/06/23 18:01:39.294
Feb  6 18:01:39.308: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5686" to be "running and ready"
Feb  6 18:01:39.316: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.928932ms
Feb  6 18:01:39.316: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:01:41.321: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013276427s
Feb  6 18:01:41.321: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:01:43.321: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.013194504s
Feb  6 18:01:43.321: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb  6 18:01:43.321: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 02/06/23 18:01:43.325
Feb  6 18:01:43.334: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5686" to be "running and ready"
Feb  6 18:01:43.344: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.888335ms
Feb  6 18:01:43.344: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:01:45.351: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0166967s
Feb  6 18:01:45.351: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:01:47.348: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.014549762s
Feb  6 18:01:47.348: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Feb  6 18:01:47.348: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/06/23 18:01:47.352
STEP: delete the pod with lifecycle hook 02/06/23 18:01:47.36
Feb  6 18:01:47.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  6 18:01:47.372: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  6 18:01:49.372: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  6 18:01:49.377: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb  6 18:01:49.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5686" for this suite. 02/06/23 18:01:49.382
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":152,"skipped":2981,"failed":0}
------------------------------
• [SLOW TEST] [10.140 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:01:39.251
    Feb  6 18:01:39.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/06/23 18:01:39.253
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:01:39.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:01:39.285
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/06/23 18:01:39.294
    Feb  6 18:01:39.308: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5686" to be "running and ready"
    Feb  6 18:01:39.316: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.928932ms
    Feb  6 18:01:39.316: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:01:41.321: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013276427s
    Feb  6 18:01:41.321: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:01:43.321: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.013194504s
    Feb  6 18:01:43.321: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb  6 18:01:43.321: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 02/06/23 18:01:43.325
    Feb  6 18:01:43.334: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5686" to be "running and ready"
    Feb  6 18:01:43.344: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.888335ms
    Feb  6 18:01:43.344: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:01:45.351: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0166967s
    Feb  6 18:01:45.351: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:01:47.348: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.014549762s
    Feb  6 18:01:47.348: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Feb  6 18:01:47.348: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/06/23 18:01:47.352
    STEP: delete the pod with lifecycle hook 02/06/23 18:01:47.36
    Feb  6 18:01:47.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb  6 18:01:47.372: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb  6 18:01:49.372: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb  6 18:01:49.377: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb  6 18:01:49.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5686" for this suite. 02/06/23 18:01:49.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:01:49.393
Feb  6 18:01:49.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 18:01:49.394
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:01:49.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:01:49.415
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 02/06/23 18:01:49.418
Feb  6 18:01:49.419: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3074 proxy --unix-socket=/tmp/kubectl-proxy-unix1313233024/test'
STEP: retrieving proxy /api/ output 02/06/23 18:01:49.479
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 18:01:49.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3074" for this suite. 02/06/23 18:01:49.49
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":153,"skipped":3025,"failed":0}
------------------------------
• [0.111 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:01:49.393
    Feb  6 18:01:49.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 18:01:49.394
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:01:49.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:01:49.415
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 02/06/23 18:01:49.418
    Feb  6 18:01:49.419: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-3074 proxy --unix-socket=/tmp/kubectl-proxy-unix1313233024/test'
    STEP: retrieving proxy /api/ output 02/06/23 18:01:49.479
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 18:01:49.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3074" for this suite. 02/06/23 18:01:49.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:01:49.51
Feb  6 18:01:49.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pod-network-test 02/06/23 18:01:49.511
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:01:49.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:01:49.533
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6979 02/06/23 18:01:49.535
STEP: creating a selector 02/06/23 18:01:49.536
STEP: Creating the service pods in kubernetes 02/06/23 18:01:49.536
Feb  6 18:01:49.536: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb  6 18:01:49.585: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6979" to be "running and ready"
Feb  6 18:01:49.594: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.566927ms
Feb  6 18:01:49.594: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:01:51.600: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015099285s
Feb  6 18:01:51.600: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:01:53.599: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014500872s
Feb  6 18:01:53.599: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:01:55.601: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016182254s
Feb  6 18:01:55.601: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:01:57.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015220442s
Feb  6 18:01:57.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:01:59.603: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018392417s
Feb  6 18:01:59.603: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:02:01.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015294688s
Feb  6 18:02:01.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:02:03.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015632958s
Feb  6 18:02:03.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:02:05.601: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016382466s
Feb  6 18:02:05.601: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:02:07.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.0153246s
Feb  6 18:02:07.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:02:09.605: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.020216255s
Feb  6 18:02:09.605: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:02:11.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015067032s
Feb  6 18:02:11.600: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb  6 18:02:11.600: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb  6 18:02:11.603: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6979" to be "running and ready"
Feb  6 18:02:11.607: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.892132ms
Feb  6 18:02:11.607: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb  6 18:02:11.607: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb  6 18:02:11.611: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6979" to be "running and ready"
Feb  6 18:02:11.615: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.912305ms
Feb  6 18:02:11.615: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb  6 18:02:11.615: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/06/23 18:02:11.619
Feb  6 18:02:11.626: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6979" to be "running"
Feb  6 18:02:11.639: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.61158ms
Feb  6 18:02:13.644: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017965281s
Feb  6 18:02:15.645: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018912933s
Feb  6 18:02:15.645: INFO: Pod "test-container-pod" satisfied condition "running"
Feb  6 18:02:15.650: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb  6 18:02:15.650: INFO: Breadth first check of 192.168.2.126 on host 195.17.4.133...
Feb  6 18:02:15.654: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.136:9080/dial?request=hostname&protocol=udp&host=192.168.2.126&port=8081&tries=1'] Namespace:pod-network-test-6979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:02:15.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:02:15.654: INFO: ExecWithOptions: Clientset creation
Feb  6 18:02:15.654: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.2.126%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb  6 18:02:15.743: INFO: Waiting for responses: map[]
Feb  6 18:02:15.744: INFO: reached 192.168.2.126 after 0/1 tries
Feb  6 18:02:15.744: INFO: Breadth first check of 192.168.3.34 on host 195.17.148.89...
Feb  6 18:02:15.749: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.136:9080/dial?request=hostname&protocol=udp&host=192.168.3.34&port=8081&tries=1'] Namespace:pod-network-test-6979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:02:15.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:02:15.749: INFO: ExecWithOptions: Clientset creation
Feb  6 18:02:15.749: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.3.34%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb  6 18:02:15.832: INFO: Waiting for responses: map[]
Feb  6 18:02:15.832: INFO: reached 192.168.3.34 after 0/1 tries
Feb  6 18:02:15.832: INFO: Breadth first check of 192.168.1.30 on host 195.17.148.90...
Feb  6 18:02:15.836: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.136:9080/dial?request=hostname&protocol=udp&host=192.168.1.30&port=8081&tries=1'] Namespace:pod-network-test-6979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:02:15.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:02:15.837: INFO: ExecWithOptions: Clientset creation
Feb  6 18:02:15.837: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.1.30%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb  6 18:02:15.924: INFO: Waiting for responses: map[]
Feb  6 18:02:15.924: INFO: reached 192.168.1.30 after 0/1 tries
Feb  6 18:02:15.924: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb  6 18:02:15.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6979" for this suite. 02/06/23 18:02:15.93
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":154,"skipped":3045,"failed":0}
------------------------------
• [SLOW TEST] [26.432 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:01:49.51
    Feb  6 18:01:49.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pod-network-test 02/06/23 18:01:49.511
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:01:49.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:01:49.533
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6979 02/06/23 18:01:49.535
    STEP: creating a selector 02/06/23 18:01:49.536
    STEP: Creating the service pods in kubernetes 02/06/23 18:01:49.536
    Feb  6 18:01:49.536: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb  6 18:01:49.585: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6979" to be "running and ready"
    Feb  6 18:01:49.594: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.566927ms
    Feb  6 18:01:49.594: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:01:51.600: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015099285s
    Feb  6 18:01:51.600: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:01:53.599: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014500872s
    Feb  6 18:01:53.599: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:01:55.601: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016182254s
    Feb  6 18:01:55.601: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:01:57.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015220442s
    Feb  6 18:01:57.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:01:59.603: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018392417s
    Feb  6 18:01:59.603: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:02:01.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015294688s
    Feb  6 18:02:01.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:02:03.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015632958s
    Feb  6 18:02:03.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:02:05.601: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016382466s
    Feb  6 18:02:05.601: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:02:07.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.0153246s
    Feb  6 18:02:07.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:02:09.605: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.020216255s
    Feb  6 18:02:09.605: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:02:11.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015067032s
    Feb  6 18:02:11.600: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb  6 18:02:11.600: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb  6 18:02:11.603: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6979" to be "running and ready"
    Feb  6 18:02:11.607: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.892132ms
    Feb  6 18:02:11.607: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb  6 18:02:11.607: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb  6 18:02:11.611: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6979" to be "running and ready"
    Feb  6 18:02:11.615: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.912305ms
    Feb  6 18:02:11.615: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb  6 18:02:11.615: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/06/23 18:02:11.619
    Feb  6 18:02:11.626: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6979" to be "running"
    Feb  6 18:02:11.639: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.61158ms
    Feb  6 18:02:13.644: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017965281s
    Feb  6 18:02:15.645: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018912933s
    Feb  6 18:02:15.645: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb  6 18:02:15.650: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb  6 18:02:15.650: INFO: Breadth first check of 192.168.2.126 on host 195.17.4.133...
    Feb  6 18:02:15.654: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.136:9080/dial?request=hostname&protocol=udp&host=192.168.2.126&port=8081&tries=1'] Namespace:pod-network-test-6979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:02:15.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:02:15.654: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:02:15.654: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.2.126%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb  6 18:02:15.743: INFO: Waiting for responses: map[]
    Feb  6 18:02:15.744: INFO: reached 192.168.2.126 after 0/1 tries
    Feb  6 18:02:15.744: INFO: Breadth first check of 192.168.3.34 on host 195.17.148.89...
    Feb  6 18:02:15.749: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.136:9080/dial?request=hostname&protocol=udp&host=192.168.3.34&port=8081&tries=1'] Namespace:pod-network-test-6979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:02:15.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:02:15.749: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:02:15.749: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.3.34%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb  6 18:02:15.832: INFO: Waiting for responses: map[]
    Feb  6 18:02:15.832: INFO: reached 192.168.3.34 after 0/1 tries
    Feb  6 18:02:15.832: INFO: Breadth first check of 192.168.1.30 on host 195.17.148.90...
    Feb  6 18:02:15.836: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.136:9080/dial?request=hostname&protocol=udp&host=192.168.1.30&port=8081&tries=1'] Namespace:pod-network-test-6979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:02:15.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:02:15.837: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:02:15.837: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6979/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.2.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.1.30%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb  6 18:02:15.924: INFO: Waiting for responses: map[]
    Feb  6 18:02:15.924: INFO: reached 192.168.1.30 after 0/1 tries
    Feb  6 18:02:15.924: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb  6 18:02:15.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6979" for this suite. 02/06/23 18:02:15.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:02:15.95
Feb  6 18:02:15.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 18:02:15.951
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:15.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:15.974
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-5fd7f6b1-0009-4388-941b-cce506333583 02/06/23 18:02:15.977
STEP: Creating a pod to test consume configMaps 02/06/23 18:02:15.983
Feb  6 18:02:15.993: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49" in namespace "configmap-496" to be "Succeeded or Failed"
Feb  6 18:02:16.000: INFO: Pod "pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49": Phase="Pending", Reason="", readiness=false. Elapsed: 7.440829ms
Feb  6 18:02:18.007: INFO: Pod "pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014145282s
Feb  6 18:02:20.006: INFO: Pod "pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012827293s
Feb  6 18:02:22.006: INFO: Pod "pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013084576s
STEP: Saw pod success 02/06/23 18:02:22.006
Feb  6 18:02:22.006: INFO: Pod "pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49" satisfied condition "Succeeded or Failed"
Feb  6 18:02:22.010: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49 container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:02:22.017
Feb  6 18:02:22.031: INFO: Waiting for pod pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49 to disappear
Feb  6 18:02:22.034: INFO: Pod pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 18:02:22.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-496" for this suite. 02/06/23 18:02:22.04
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":155,"skipped":3066,"failed":0}
------------------------------
• [SLOW TEST] [6.096 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:02:15.95
    Feb  6 18:02:15.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 18:02:15.951
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:15.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:15.974
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-5fd7f6b1-0009-4388-941b-cce506333583 02/06/23 18:02:15.977
    STEP: Creating a pod to test consume configMaps 02/06/23 18:02:15.983
    Feb  6 18:02:15.993: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49" in namespace "configmap-496" to be "Succeeded or Failed"
    Feb  6 18:02:16.000: INFO: Pod "pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49": Phase="Pending", Reason="", readiness=false. Elapsed: 7.440829ms
    Feb  6 18:02:18.007: INFO: Pod "pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014145282s
    Feb  6 18:02:20.006: INFO: Pod "pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012827293s
    Feb  6 18:02:22.006: INFO: Pod "pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013084576s
    STEP: Saw pod success 02/06/23 18:02:22.006
    Feb  6 18:02:22.006: INFO: Pod "pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49" satisfied condition "Succeeded or Failed"
    Feb  6 18:02:22.010: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49 container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:02:22.017
    Feb  6 18:02:22.031: INFO: Waiting for pod pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49 to disappear
    Feb  6 18:02:22.034: INFO: Pod pod-configmaps-f3bd3c76-873b-4235-97e1-327ffe2cfe49 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 18:02:22.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-496" for this suite. 02/06/23 18:02:22.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:02:22.049
Feb  6 18:02:22.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:02:22.05
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:22.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:22.073
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-061d7b6c-e1c2-47bf-ad30-4c8fac6ba82a 02/06/23 18:02:22.082
STEP: Creating secret with name s-test-opt-upd-cb0f4b02-598b-4c78-aa7a-e9eb1c5a7ba8 02/06/23 18:02:22.087
STEP: Creating the pod 02/06/23 18:02:22.094
Feb  6 18:02:22.110: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f" in namespace "projected-9589" to be "running and ready"
Feb  6 18:02:22.116: INFO: Pod "pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.185052ms
Feb  6 18:02:22.116: INFO: The phase of Pod pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:02:24.123: INFO: Pod "pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013012672s
Feb  6 18:02:24.123: INFO: The phase of Pod pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:02:26.122: INFO: Pod "pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f": Phase="Running", Reason="", readiness=true. Elapsed: 4.011562178s
Feb  6 18:02:26.122: INFO: The phase of Pod pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f is Running (Ready = true)
Feb  6 18:02:26.122: INFO: Pod "pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-061d7b6c-e1c2-47bf-ad30-4c8fac6ba82a 02/06/23 18:02:26.15
STEP: Updating secret s-test-opt-upd-cb0f4b02-598b-4c78-aa7a-e9eb1c5a7ba8 02/06/23 18:02:26.158
STEP: Creating secret with name s-test-opt-create-cf85db23-fb49-4b96-ac9a-0e774db3892d 02/06/23 18:02:26.164
STEP: waiting to observe update in volume 02/06/23 18:02:26.169
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb  6 18:02:28.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9589" for this suite. 02/06/23 18:02:28.209
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":156,"skipped":3113,"failed":0}
------------------------------
• [SLOW TEST] [6.167 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:02:22.049
    Feb  6 18:02:22.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:02:22.05
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:22.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:22.073
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-061d7b6c-e1c2-47bf-ad30-4c8fac6ba82a 02/06/23 18:02:22.082
    STEP: Creating secret with name s-test-opt-upd-cb0f4b02-598b-4c78-aa7a-e9eb1c5a7ba8 02/06/23 18:02:22.087
    STEP: Creating the pod 02/06/23 18:02:22.094
    Feb  6 18:02:22.110: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f" in namespace "projected-9589" to be "running and ready"
    Feb  6 18:02:22.116: INFO: Pod "pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.185052ms
    Feb  6 18:02:22.116: INFO: The phase of Pod pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:02:24.123: INFO: Pod "pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013012672s
    Feb  6 18:02:24.123: INFO: The phase of Pod pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:02:26.122: INFO: Pod "pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f": Phase="Running", Reason="", readiness=true. Elapsed: 4.011562178s
    Feb  6 18:02:26.122: INFO: The phase of Pod pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f is Running (Ready = true)
    Feb  6 18:02:26.122: INFO: Pod "pod-projected-secrets-263fa129-67fa-4e56-8450-e9ffb4e4c72f" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-061d7b6c-e1c2-47bf-ad30-4c8fac6ba82a 02/06/23 18:02:26.15
    STEP: Updating secret s-test-opt-upd-cb0f4b02-598b-4c78-aa7a-e9eb1c5a7ba8 02/06/23 18:02:26.158
    STEP: Creating secret with name s-test-opt-create-cf85db23-fb49-4b96-ac9a-0e774db3892d 02/06/23 18:02:26.164
    STEP: waiting to observe update in volume 02/06/23 18:02:26.169
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb  6 18:02:28.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9589" for this suite. 02/06/23 18:02:28.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:02:28.22
Feb  6 18:02:28.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename daemonsets 02/06/23 18:02:28.221
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:28.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:28.244
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 02/06/23 18:02:28.267
STEP: Check that daemon pods launch on every node of the cluster. 02/06/23 18:02:28.275
Feb  6 18:02:28.280: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:28.280: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:28.280: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:28.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:02:28.285: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:02:29.291: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:29.291: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:29.292: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:29.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:02:29.296: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:02:30.293: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:30.294: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:30.294: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:30.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:02:30.299: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:02:31.292: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:31.292: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:31.292: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:31.297: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:02:31.297: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:02:32.292: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:32.292: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:32.292: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:32.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb  6 18:02:32.296: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 02/06/23 18:02:32.3
Feb  6 18:02:32.320: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:32.321: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:32.321: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:32.325: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb  6 18:02:32.325: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:02:33.332: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:33.332: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:33.333: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:33.338: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb  6 18:02:33.338: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:02:34.331: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:34.331: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:34.331: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:34.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb  6 18:02:34.335: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:02:35.332: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:35.332: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:35.332: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:35.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb  6 18:02:35.337: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:02:36.333: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:36.333: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:36.333: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:36.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb  6 18:02:36.337: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:02:37.335: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:37.335: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:37.335: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:37.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb  6 18:02:37.339: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:02:38.334: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:38.334: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:38.334: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:38.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb  6 18:02:38.339: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:02:39.331: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:39.332: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:39.332: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:02:39.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb  6 18:02:39.335: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/06/23 18:02:39.339
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7609, will wait for the garbage collector to delete the pods 02/06/23 18:02:39.339
Feb  6 18:02:39.400: INFO: Deleting DaemonSet.extensions daemon-set took: 7.83286ms
Feb  6 18:02:39.501: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.521933ms
Feb  6 18:02:41.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:02:41.505: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb  6 18:02:41.508: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40246"},"items":null}

Feb  6 18:02:41.511: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40246"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb  6 18:02:41.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7609" for this suite. 02/06/23 18:02:41.533
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":157,"skipped":3135,"failed":0}
------------------------------
• [SLOW TEST] [13.322 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:02:28.22
    Feb  6 18:02:28.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename daemonsets 02/06/23 18:02:28.221
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:28.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:28.244
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 02/06/23 18:02:28.267
    STEP: Check that daemon pods launch on every node of the cluster. 02/06/23 18:02:28.275
    Feb  6 18:02:28.280: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:28.280: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:28.280: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:28.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:02:28.285: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:02:29.291: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:29.291: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:29.292: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:29.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:02:29.296: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:02:30.293: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:30.294: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:30.294: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:30.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:02:30.299: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:02:31.292: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:31.292: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:31.292: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:31.297: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:02:31.297: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:02:32.292: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:32.292: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:32.292: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:32.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb  6 18:02:32.296: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 02/06/23 18:02:32.3
    Feb  6 18:02:32.320: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:32.321: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:32.321: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:32.325: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb  6 18:02:32.325: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:02:33.332: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:33.332: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:33.333: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:33.338: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb  6 18:02:33.338: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:02:34.331: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:34.331: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:34.331: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:34.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb  6 18:02:34.335: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:02:35.332: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:35.332: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:35.332: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:35.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb  6 18:02:35.337: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:02:36.333: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:36.333: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:36.333: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:36.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb  6 18:02:36.337: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:02:37.335: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:37.335: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:37.335: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:37.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb  6 18:02:37.339: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:02:38.334: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:38.334: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:38.334: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:38.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb  6 18:02:38.339: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:02:39.331: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:39.332: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:39.332: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:02:39.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb  6 18:02:39.335: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/06/23 18:02:39.339
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7609, will wait for the garbage collector to delete the pods 02/06/23 18:02:39.339
    Feb  6 18:02:39.400: INFO: Deleting DaemonSet.extensions daemon-set took: 7.83286ms
    Feb  6 18:02:39.501: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.521933ms
    Feb  6 18:02:41.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:02:41.505: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb  6 18:02:41.508: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40246"},"items":null}

    Feb  6 18:02:41.511: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40246"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 18:02:41.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7609" for this suite. 02/06/23 18:02:41.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:02:41.545
Feb  6 18:02:41.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename deployment 02/06/23 18:02:41.548
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:41.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:41.571
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 02/06/23 18:02:41.578
Feb  6 18:02:41.578: INFO: Creating simple deployment test-deployment-gww2h
Feb  6 18:02:41.594: INFO: deployment "test-deployment-gww2h" doesn't have the required revision set
Feb  6 18:02:43.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 2, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 2, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-gww2h-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 02/06/23 18:02:45.617
Feb  6 18:02:45.622: INFO: Deployment test-deployment-gww2h has Conditions: [{Available True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gww2h-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 02/06/23 18:02:45.622
Feb  6 18:02:45.638: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 2, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 2, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 2, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 2, 41, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-gww2h-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 02/06/23 18:02:45.638
Feb  6 18:02:45.640: INFO: Observed &Deployment event: ADDED
Feb  6 18:02:45.640: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gww2h-777898ffcc"}
Feb  6 18:02:45.641: INFO: Observed &Deployment event: MODIFIED
Feb  6 18:02:45.641: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gww2h-777898ffcc"}
Feb  6 18:02:45.641: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb  6 18:02:45.641: INFO: Observed &Deployment event: MODIFIED
Feb  6 18:02:45.641: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb  6 18:02:45.641: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gww2h-777898ffcc" is progressing.}
Feb  6 18:02:45.641: INFO: Observed &Deployment event: MODIFIED
Feb  6 18:02:45.641: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb  6 18:02:45.641: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gww2h-777898ffcc" has successfully progressed.}
Feb  6 18:02:45.642: INFO: Observed &Deployment event: MODIFIED
Feb  6 18:02:45.642: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb  6 18:02:45.642: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gww2h-777898ffcc" has successfully progressed.}
Feb  6 18:02:45.642: INFO: Found Deployment test-deployment-gww2h in namespace deployment-5613 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb  6 18:02:45.642: INFO: Deployment test-deployment-gww2h has an updated status
STEP: patching the Statefulset Status 02/06/23 18:02:45.642
Feb  6 18:02:45.642: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb  6 18:02:45.654: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 02/06/23 18:02:45.654
Feb  6 18:02:45.656: INFO: Observed &Deployment event: ADDED
Feb  6 18:02:45.656: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gww2h-777898ffcc"}
Feb  6 18:02:45.656: INFO: Observed &Deployment event: MODIFIED
Feb  6 18:02:45.656: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gww2h-777898ffcc"}
Feb  6 18:02:45.656: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb  6 18:02:45.657: INFO: Observed &Deployment event: MODIFIED
Feb  6 18:02:45.657: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb  6 18:02:45.657: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gww2h-777898ffcc" is progressing.}
Feb  6 18:02:45.657: INFO: Observed &Deployment event: MODIFIED
Feb  6 18:02:45.657: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb  6 18:02:45.657: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gww2h-777898ffcc" has successfully progressed.}
Feb  6 18:02:45.658: INFO: Observed &Deployment event: MODIFIED
Feb  6 18:02:45.658: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb  6 18:02:45.658: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gww2h-777898ffcc" has successfully progressed.}
Feb  6 18:02:45.658: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb  6 18:02:45.658: INFO: Observed &Deployment event: MODIFIED
Feb  6 18:02:45.658: INFO: Found deployment test-deployment-gww2h in namespace deployment-5613 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Feb  6 18:02:45.658: INFO: Deployment test-deployment-gww2h has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb  6 18:02:45.662: INFO: Deployment "test-deployment-gww2h":
&Deployment{ObjectMeta:{test-deployment-gww2h  deployment-5613  67de0f98-d17b-4ada-8d9d-22da6e9393f7 40321 1 2023-02-06 18:02:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-06 18:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-06 18:02:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-06 18:02:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048bca38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-gww2h-777898ffcc",LastUpdateTime:2023-02-06 18:02:45 +0000 UTC,LastTransitionTime:2023-02-06 18:02:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb  6 18:02:45.666: INFO: New ReplicaSet "test-deployment-gww2h-777898ffcc" of Deployment "test-deployment-gww2h":
&ReplicaSet{ObjectMeta:{test-deployment-gww2h-777898ffcc  deployment-5613  1a1ae244-89f7-4d5c-ad7c-0960f2e21a4b 40305 1 2023-02-06 18:02:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-gww2h 67de0f98-d17b-4ada-8d9d-22da6e9393f7 0xc002ff4ee0 0xc002ff4ee1}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67de0f98-d17b-4ada-8d9d-22da6e9393f7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:02:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ff4f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb  6 18:02:45.679: INFO: Pod "test-deployment-gww2h-777898ffcc-9njjd" is available:
&Pod{ObjectMeta:{test-deployment-gww2h-777898ffcc-9njjd test-deployment-gww2h-777898ffcc- deployment-5613  f8b74c66-5d19-407f-a52f-ea3a4a91bbc8 40304 0 2023-02-06 18:02:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-gww2h-777898ffcc 1a1ae244-89f7-4d5c-ad7c-0960f2e21a4b 0xc0048bce10 0xc0048bce11}] [] [{kube-controller-manager Update v1 2023-02-06 18:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a1ae244-89f7-4d5c-ad7c-0960f2e21a4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 18:02:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2kbkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2kbkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:02:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:02:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.234,StartTime:2023-02-06 18:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 18:02:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://94667e74d995bdd0db2528316414551e6adaaef26ef1540bc2ff17c7ea0750b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.234,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb  6 18:02:45.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5613" for this suite. 02/06/23 18:02:45.687
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":158,"skipped":3140,"failed":0}
------------------------------
• [4.149 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:02:41.545
    Feb  6 18:02:41.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename deployment 02/06/23 18:02:41.548
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:41.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:41.571
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 02/06/23 18:02:41.578
    Feb  6 18:02:41.578: INFO: Creating simple deployment test-deployment-gww2h
    Feb  6 18:02:41.594: INFO: deployment "test-deployment-gww2h" doesn't have the required revision set
    Feb  6 18:02:43.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 2, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 2, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-gww2h-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 02/06/23 18:02:45.617
    Feb  6 18:02:45.622: INFO: Deployment test-deployment-gww2h has Conditions: [{Available True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gww2h-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 02/06/23 18:02:45.622
    Feb  6 18:02:45.638: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 2, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 2, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 2, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 2, 41, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-gww2h-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 02/06/23 18:02:45.638
    Feb  6 18:02:45.640: INFO: Observed &Deployment event: ADDED
    Feb  6 18:02:45.640: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gww2h-777898ffcc"}
    Feb  6 18:02:45.641: INFO: Observed &Deployment event: MODIFIED
    Feb  6 18:02:45.641: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gww2h-777898ffcc"}
    Feb  6 18:02:45.641: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb  6 18:02:45.641: INFO: Observed &Deployment event: MODIFIED
    Feb  6 18:02:45.641: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb  6 18:02:45.641: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gww2h-777898ffcc" is progressing.}
    Feb  6 18:02:45.641: INFO: Observed &Deployment event: MODIFIED
    Feb  6 18:02:45.641: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb  6 18:02:45.641: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gww2h-777898ffcc" has successfully progressed.}
    Feb  6 18:02:45.642: INFO: Observed &Deployment event: MODIFIED
    Feb  6 18:02:45.642: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb  6 18:02:45.642: INFO: Observed Deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gww2h-777898ffcc" has successfully progressed.}
    Feb  6 18:02:45.642: INFO: Found Deployment test-deployment-gww2h in namespace deployment-5613 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb  6 18:02:45.642: INFO: Deployment test-deployment-gww2h has an updated status
    STEP: patching the Statefulset Status 02/06/23 18:02:45.642
    Feb  6 18:02:45.642: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb  6 18:02:45.654: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 02/06/23 18:02:45.654
    Feb  6 18:02:45.656: INFO: Observed &Deployment event: ADDED
    Feb  6 18:02:45.656: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gww2h-777898ffcc"}
    Feb  6 18:02:45.656: INFO: Observed &Deployment event: MODIFIED
    Feb  6 18:02:45.656: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gww2h-777898ffcc"}
    Feb  6 18:02:45.656: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb  6 18:02:45.657: INFO: Observed &Deployment event: MODIFIED
    Feb  6 18:02:45.657: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb  6 18:02:45.657: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:41 +0000 UTC 2023-02-06 18:02:41 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gww2h-777898ffcc" is progressing.}
    Feb  6 18:02:45.657: INFO: Observed &Deployment event: MODIFIED
    Feb  6 18:02:45.657: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb  6 18:02:45.657: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gww2h-777898ffcc" has successfully progressed.}
    Feb  6 18:02:45.658: INFO: Observed &Deployment event: MODIFIED
    Feb  6 18:02:45.658: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb  6 18:02:45.658: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-06 18:02:44 +0000 UTC 2023-02-06 18:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gww2h-777898ffcc" has successfully progressed.}
    Feb  6 18:02:45.658: INFO: Observed deployment test-deployment-gww2h in namespace deployment-5613 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb  6 18:02:45.658: INFO: Observed &Deployment event: MODIFIED
    Feb  6 18:02:45.658: INFO: Found deployment test-deployment-gww2h in namespace deployment-5613 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Feb  6 18:02:45.658: INFO: Deployment test-deployment-gww2h has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb  6 18:02:45.662: INFO: Deployment "test-deployment-gww2h":
    &Deployment{ObjectMeta:{test-deployment-gww2h  deployment-5613  67de0f98-d17b-4ada-8d9d-22da6e9393f7 40321 1 2023-02-06 18:02:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-06 18:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-06 18:02:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-06 18:02:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048bca38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-gww2h-777898ffcc",LastUpdateTime:2023-02-06 18:02:45 +0000 UTC,LastTransitionTime:2023-02-06 18:02:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb  6 18:02:45.666: INFO: New ReplicaSet "test-deployment-gww2h-777898ffcc" of Deployment "test-deployment-gww2h":
    &ReplicaSet{ObjectMeta:{test-deployment-gww2h-777898ffcc  deployment-5613  1a1ae244-89f7-4d5c-ad7c-0960f2e21a4b 40305 1 2023-02-06 18:02:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-gww2h 67de0f98-d17b-4ada-8d9d-22da6e9393f7 0xc002ff4ee0 0xc002ff4ee1}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67de0f98-d17b-4ada-8d9d-22da6e9393f7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:02:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ff4f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb  6 18:02:45.679: INFO: Pod "test-deployment-gww2h-777898ffcc-9njjd" is available:
    &Pod{ObjectMeta:{test-deployment-gww2h-777898ffcc-9njjd test-deployment-gww2h-777898ffcc- deployment-5613  f8b74c66-5d19-407f-a52f-ea3a4a91bbc8 40304 0 2023-02-06 18:02:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-gww2h-777898ffcc 1a1ae244-89f7-4d5c-ad7c-0960f2e21a4b 0xc0048bce10 0xc0048bce11}] [] [{kube-controller-manager Update v1 2023-02-06 18:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a1ae244-89f7-4d5c-ad7c-0960f2e21a4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 18:02:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2kbkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2kbkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:02:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:02:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.234,StartTime:2023-02-06 18:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 18:02:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://94667e74d995bdd0db2528316414551e6adaaef26ef1540bc2ff17c7ea0750b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.234,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb  6 18:02:45.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5613" for this suite. 02/06/23 18:02:45.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:02:45.696
Feb  6 18:02:45.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 18:02:45.697
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:45.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:45.726
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-eb1931be-305c-43a9-bce0-ce3dae65746d 02/06/23 18:02:45.729
STEP: Creating a pod to test consume secrets 02/06/23 18:02:45.735
Feb  6 18:02:45.750: INFO: Waiting up to 5m0s for pod "pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961" in namespace "secrets-8733" to be "Succeeded or Failed"
Feb  6 18:02:45.758: INFO: Pod "pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961": Phase="Pending", Reason="", readiness=false. Elapsed: 7.911537ms
Feb  6 18:02:47.764: INFO: Pod "pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013505936s
Feb  6 18:02:49.764: INFO: Pod "pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01379551s
Feb  6 18:02:51.765: INFO: Pod "pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014751075s
STEP: Saw pod success 02/06/23 18:02:51.766
Feb  6 18:02:51.766: INFO: Pod "pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961" satisfied condition "Succeeded or Failed"
Feb  6 18:02:51.770: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961 container secret-volume-test: <nil>
STEP: delete the pod 02/06/23 18:02:51.777
Feb  6 18:02:51.796: INFO: Waiting for pod pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961 to disappear
Feb  6 18:02:51.799: INFO: Pod pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb  6 18:02:51.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8733" for this suite. 02/06/23 18:02:51.805
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":159,"skipped":3173,"failed":0}
------------------------------
• [SLOW TEST] [6.117 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:02:45.696
    Feb  6 18:02:45.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 18:02:45.697
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:45.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:45.726
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-eb1931be-305c-43a9-bce0-ce3dae65746d 02/06/23 18:02:45.729
    STEP: Creating a pod to test consume secrets 02/06/23 18:02:45.735
    Feb  6 18:02:45.750: INFO: Waiting up to 5m0s for pod "pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961" in namespace "secrets-8733" to be "Succeeded or Failed"
    Feb  6 18:02:45.758: INFO: Pod "pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961": Phase="Pending", Reason="", readiness=false. Elapsed: 7.911537ms
    Feb  6 18:02:47.764: INFO: Pod "pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013505936s
    Feb  6 18:02:49.764: INFO: Pod "pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01379551s
    Feb  6 18:02:51.765: INFO: Pod "pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014751075s
    STEP: Saw pod success 02/06/23 18:02:51.766
    Feb  6 18:02:51.766: INFO: Pod "pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961" satisfied condition "Succeeded or Failed"
    Feb  6 18:02:51.770: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961 container secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 18:02:51.777
    Feb  6 18:02:51.796: INFO: Waiting for pod pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961 to disappear
    Feb  6 18:02:51.799: INFO: Pod pod-secrets-2d7a9cce-aa16-45ea-ac2e-b8843f0d9961 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 18:02:51.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8733" for this suite. 02/06/23 18:02:51.805
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:02:51.815
Feb  6 18:02:51.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename events 02/06/23 18:02:51.817
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:51.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:51.841
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 02/06/23 18:02:51.843
Feb  6 18:02:51.852: INFO: created test-event-1
Feb  6 18:02:51.860: INFO: created test-event-2
Feb  6 18:02:51.866: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 02/06/23 18:02:51.866
STEP: delete collection of events 02/06/23 18:02:51.87
Feb  6 18:02:51.870: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/06/23 18:02:51.895
Feb  6 18:02:51.895: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Feb  6 18:02:51.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7017" for this suite. 02/06/23 18:02:51.908
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":160,"skipped":3174,"failed":0}
------------------------------
• [0.100 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:02:51.815
    Feb  6 18:02:51.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename events 02/06/23 18:02:51.817
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:51.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:51.841
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 02/06/23 18:02:51.843
    Feb  6 18:02:51.852: INFO: created test-event-1
    Feb  6 18:02:51.860: INFO: created test-event-2
    Feb  6 18:02:51.866: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 02/06/23 18:02:51.866
    STEP: delete collection of events 02/06/23 18:02:51.87
    Feb  6 18:02:51.870: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/06/23 18:02:51.895
    Feb  6 18:02:51.895: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Feb  6 18:02:51.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-7017" for this suite. 02/06/23 18:02:51.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:02:51.919
Feb  6 18:02:51.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-probe 02/06/23 18:02:51.92
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:51.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:51.944
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Feb  6 18:02:51.957: INFO: Waiting up to 5m0s for pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9" in namespace "container-probe-3836" to be "running and ready"
Feb  6 18:02:51.962: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.076512ms
Feb  6 18:02:51.962: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:02:53.968: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010979825s
Feb  6 18:02:53.968: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:02:55.966: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 4.009507004s
Feb  6 18:02:55.966: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
Feb  6 18:02:57.966: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 6.009443271s
Feb  6 18:02:57.966: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
Feb  6 18:02:59.967: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 8.010569217s
Feb  6 18:02:59.967: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
Feb  6 18:03:01.968: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 10.011161394s
Feb  6 18:03:01.968: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
Feb  6 18:03:03.968: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 12.011045165s
Feb  6 18:03:03.968: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
Feb  6 18:03:05.968: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 14.011393664s
Feb  6 18:03:05.968: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
Feb  6 18:03:07.969: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 16.012279351s
Feb  6 18:03:07.969: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
Feb  6 18:03:09.966: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 18.009613488s
Feb  6 18:03:09.966: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
Feb  6 18:03:11.967: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 20.010652039s
Feb  6 18:03:11.967: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
Feb  6 18:03:13.966: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=true. Elapsed: 22.009800879s
Feb  6 18:03:13.967: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = true)
Feb  6 18:03:13.967: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9" satisfied condition "running and ready"
Feb  6 18:03:13.971: INFO: Container started at 2023-02-06 18:02:54 +0000 UTC, pod became ready at 2023-02-06 18:03:12 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb  6 18:03:13.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3836" for this suite. 02/06/23 18:03:13.977
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":161,"skipped":3192,"failed":0}
------------------------------
• [SLOW TEST] [22.067 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:02:51.919
    Feb  6 18:02:51.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-probe 02/06/23 18:02:51.92
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:02:51.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:02:51.944
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Feb  6 18:02:51.957: INFO: Waiting up to 5m0s for pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9" in namespace "container-probe-3836" to be "running and ready"
    Feb  6 18:02:51.962: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.076512ms
    Feb  6 18:02:51.962: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:02:53.968: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010979825s
    Feb  6 18:02:53.968: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:02:55.966: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 4.009507004s
    Feb  6 18:02:55.966: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
    Feb  6 18:02:57.966: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 6.009443271s
    Feb  6 18:02:57.966: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
    Feb  6 18:02:59.967: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 8.010569217s
    Feb  6 18:02:59.967: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
    Feb  6 18:03:01.968: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 10.011161394s
    Feb  6 18:03:01.968: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
    Feb  6 18:03:03.968: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 12.011045165s
    Feb  6 18:03:03.968: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
    Feb  6 18:03:05.968: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 14.011393664s
    Feb  6 18:03:05.968: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
    Feb  6 18:03:07.969: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 16.012279351s
    Feb  6 18:03:07.969: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
    Feb  6 18:03:09.966: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 18.009613488s
    Feb  6 18:03:09.966: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
    Feb  6 18:03:11.967: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=false. Elapsed: 20.010652039s
    Feb  6 18:03:11.967: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = false)
    Feb  6 18:03:13.966: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9": Phase="Running", Reason="", readiness=true. Elapsed: 22.009800879s
    Feb  6 18:03:13.967: INFO: The phase of Pod test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9 is Running (Ready = true)
    Feb  6 18:03:13.967: INFO: Pod "test-webserver-7ace2391-297c-42c6-8b8c-ef3bb31478c9" satisfied condition "running and ready"
    Feb  6 18:03:13.971: INFO: Container started at 2023-02-06 18:02:54 +0000 UTC, pod became ready at 2023-02-06 18:03:12 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb  6 18:03:13.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3836" for this suite. 02/06/23 18:03:13.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:03:13.989
Feb  6 18:03:13.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 18:03:13.99
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:14.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:14.013
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 02/06/23 18:03:14.016
Feb  6 18:03:14.028: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b" in namespace "downward-api-3182" to be "Succeeded or Failed"
Feb  6 18:03:14.036: INFO: Pod "downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.582252ms
Feb  6 18:03:16.041: INFO: Pod "downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012463014s
Feb  6 18:03:18.041: INFO: Pod "downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013214129s
Feb  6 18:03:20.041: INFO: Pod "downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013094415s
STEP: Saw pod success 02/06/23 18:03:20.042
Feb  6 18:03:20.042: INFO: Pod "downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b" satisfied condition "Succeeded or Failed"
Feb  6 18:03:20.045: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b container client-container: <nil>
STEP: delete the pod 02/06/23 18:03:20.053
Feb  6 18:03:20.066: INFO: Waiting for pod downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b to disappear
Feb  6 18:03:20.069: INFO: Pod downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb  6 18:03:20.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3182" for this suite. 02/06/23 18:03:20.075
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":162,"skipped":3202,"failed":0}
------------------------------
• [SLOW TEST] [6.092 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:03:13.989
    Feb  6 18:03:13.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 18:03:13.99
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:14.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:14.013
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 02/06/23 18:03:14.016
    Feb  6 18:03:14.028: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b" in namespace "downward-api-3182" to be "Succeeded or Failed"
    Feb  6 18:03:14.036: INFO: Pod "downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.582252ms
    Feb  6 18:03:16.041: INFO: Pod "downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012463014s
    Feb  6 18:03:18.041: INFO: Pod "downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013214129s
    Feb  6 18:03:20.041: INFO: Pod "downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013094415s
    STEP: Saw pod success 02/06/23 18:03:20.042
    Feb  6 18:03:20.042: INFO: Pod "downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b" satisfied condition "Succeeded or Failed"
    Feb  6 18:03:20.045: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b container client-container: <nil>
    STEP: delete the pod 02/06/23 18:03:20.053
    Feb  6 18:03:20.066: INFO: Waiting for pod downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b to disappear
    Feb  6 18:03:20.069: INFO: Pod downwardapi-volume-a8a92ae9-a192-41a6-bac1-b6f531c8017b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb  6 18:03:20.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3182" for this suite. 02/06/23 18:03:20.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:03:20.084
Feb  6 18:03:20.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 18:03:20.085
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:20.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:20.105
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 02/06/23 18:03:20.108
Feb  6 18:03:20.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2034 cluster-info'
Feb  6 18:03:20.196: INFO: stderr: ""
Feb  6 18:03:20.196: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 18:03:20.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2034" for this suite. 02/06/23 18:03:20.204
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":163,"skipped":3209,"failed":0}
------------------------------
• [0.129 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:03:20.084
    Feb  6 18:03:20.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 18:03:20.085
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:20.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:20.105
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 02/06/23 18:03:20.108
    Feb  6 18:03:20.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-2034 cluster-info'
    Feb  6 18:03:20.196: INFO: stderr: ""
    Feb  6 18:03:20.196: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 18:03:20.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2034" for this suite. 02/06/23 18:03:20.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:03:20.215
Feb  6 18:03:20.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 18:03:20.216
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:20.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:20.243
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-3037 02/06/23 18:03:20.246
STEP: creating service affinity-clusterip in namespace services-3037 02/06/23 18:03:20.246
STEP: creating replication controller affinity-clusterip in namespace services-3037 02/06/23 18:03:20.266
I0206 18:03:20.275686      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3037, replica count: 3
I0206 18:03:23.327841      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 18:03:26.328939      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  6 18:03:26.336: INFO: Creating new exec pod
Feb  6 18:03:26.348: INFO: Waiting up to 5m0s for pod "execpod-affinity959lq" in namespace "services-3037" to be "running"
Feb  6 18:03:26.353: INFO: Pod "execpod-affinity959lq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.436037ms
Feb  6 18:03:28.361: INFO: Pod "execpod-affinity959lq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012190211s
Feb  6 18:03:30.358: INFO: Pod "execpod-affinity959lq": Phase="Running", Reason="", readiness=true. Elapsed: 4.009415559s
Feb  6 18:03:30.358: INFO: Pod "execpod-affinity959lq" satisfied condition "running"
Feb  6 18:03:31.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3037 exec execpod-affinity959lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Feb  6 18:03:31.536: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb  6 18:03:31.536: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:03:31.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3037 exec execpod-affinity959lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.197.223 80'
Feb  6 18:03:31.721: INFO: stderr: "+ + echonc hostName\n -v -t -w 2 10.108.197.223 80\nConnection to 10.108.197.223 80 port [tcp/http] succeeded!\n"
Feb  6 18:03:31.721: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:03:31.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3037 exec execpod-affinity959lq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.197.223:80/ ; done'
Feb  6 18:03:31.992: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n"
Feb  6 18:03:31.992: INFO: stdout: "\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f"
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
Feb  6 18:03:31.992: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3037, will wait for the garbage collector to delete the pods 02/06/23 18:03:32.012
Feb  6 18:03:32.078: INFO: Deleting ReplicationController affinity-clusterip took: 10.516069ms
Feb  6 18:03:32.179: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.483097ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 18:03:34.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3037" for this suite. 02/06/23 18:03:34.424
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":164,"skipped":3218,"failed":0}
------------------------------
• [SLOW TEST] [14.219 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:03:20.215
    Feb  6 18:03:20.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 18:03:20.216
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:20.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:20.243
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-3037 02/06/23 18:03:20.246
    STEP: creating service affinity-clusterip in namespace services-3037 02/06/23 18:03:20.246
    STEP: creating replication controller affinity-clusterip in namespace services-3037 02/06/23 18:03:20.266
    I0206 18:03:20.275686      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3037, replica count: 3
    I0206 18:03:23.327841      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 18:03:26.328939      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb  6 18:03:26.336: INFO: Creating new exec pod
    Feb  6 18:03:26.348: INFO: Waiting up to 5m0s for pod "execpod-affinity959lq" in namespace "services-3037" to be "running"
    Feb  6 18:03:26.353: INFO: Pod "execpod-affinity959lq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.436037ms
    Feb  6 18:03:28.361: INFO: Pod "execpod-affinity959lq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012190211s
    Feb  6 18:03:30.358: INFO: Pod "execpod-affinity959lq": Phase="Running", Reason="", readiness=true. Elapsed: 4.009415559s
    Feb  6 18:03:30.358: INFO: Pod "execpod-affinity959lq" satisfied condition "running"
    Feb  6 18:03:31.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3037 exec execpod-affinity959lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Feb  6 18:03:31.536: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Feb  6 18:03:31.536: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:03:31.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3037 exec execpod-affinity959lq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.197.223 80'
    Feb  6 18:03:31.721: INFO: stderr: "+ + echonc hostName\n -v -t -w 2 10.108.197.223 80\nConnection to 10.108.197.223 80 port [tcp/http] succeeded!\n"
    Feb  6 18:03:31.721: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:03:31.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3037 exec execpod-affinity959lq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.197.223:80/ ; done'
    Feb  6 18:03:31.992: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.197.223:80/\n"
    Feb  6 18:03:31.992: INFO: stdout: "\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f\naffinity-clusterip-bk27f"
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Received response from host: affinity-clusterip-bk27f
    Feb  6 18:03:31.992: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3037, will wait for the garbage collector to delete the pods 02/06/23 18:03:32.012
    Feb  6 18:03:32.078: INFO: Deleting ReplicationController affinity-clusterip took: 10.516069ms
    Feb  6 18:03:32.179: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.483097ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 18:03:34.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3037" for this suite. 02/06/23 18:03:34.424
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:03:34.435
Feb  6 18:03:34.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 18:03:34.436
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:34.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:34.464
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 02/06/23 18:03:34.466
Feb  6 18:03:34.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 create -f -'
Feb  6 18:03:35.729: INFO: stderr: ""
Feb  6 18:03:35.729: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/06/23 18:03:35.729
Feb  6 18:03:35.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  6 18:03:35.829: INFO: stderr: ""
Feb  6 18:03:35.829: INFO: stdout: "update-demo-nautilus-khq8m update-demo-nautilus-wfz7r "
Feb  6 18:03:35.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods update-demo-nautilus-khq8m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  6 18:03:35.918: INFO: stderr: ""
Feb  6 18:03:35.918: INFO: stdout: ""
Feb  6 18:03:35.918: INFO: update-demo-nautilus-khq8m is created but not running
Feb  6 18:03:40.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  6 18:03:41.014: INFO: stderr: ""
Feb  6 18:03:41.014: INFO: stdout: "update-demo-nautilus-khq8m update-demo-nautilus-wfz7r "
Feb  6 18:03:41.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods update-demo-nautilus-khq8m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  6 18:03:41.098: INFO: stderr: ""
Feb  6 18:03:41.098: INFO: stdout: "true"
Feb  6 18:03:41.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods update-demo-nautilus-khq8m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  6 18:03:41.197: INFO: stderr: ""
Feb  6 18:03:41.197: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb  6 18:03:41.197: INFO: validating pod update-demo-nautilus-khq8m
Feb  6 18:03:41.207: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  6 18:03:41.207: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  6 18:03:41.207: INFO: update-demo-nautilus-khq8m is verified up and running
Feb  6 18:03:41.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods update-demo-nautilus-wfz7r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  6 18:03:41.338: INFO: stderr: ""
Feb  6 18:03:41.338: INFO: stdout: "true"
Feb  6 18:03:41.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods update-demo-nautilus-wfz7r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  6 18:03:41.420: INFO: stderr: ""
Feb  6 18:03:41.420: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb  6 18:03:41.420: INFO: validating pod update-demo-nautilus-wfz7r
Feb  6 18:03:41.427: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  6 18:03:41.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  6 18:03:41.427: INFO: update-demo-nautilus-wfz7r is verified up and running
STEP: using delete to clean up resources 02/06/23 18:03:41.427
Feb  6 18:03:41.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 delete --grace-period=0 --force -f -'
Feb  6 18:03:41.510: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  6 18:03:41.510: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb  6 18:03:41.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get rc,svc -l name=update-demo --no-headers'
Feb  6 18:03:41.664: INFO: stderr: "No resources found in kubectl-7898 namespace.\n"
Feb  6 18:03:41.664: INFO: stdout: ""
Feb  6 18:03:41.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  6 18:03:41.796: INFO: stderr: ""
Feb  6 18:03:41.796: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 18:03:41.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7898" for this suite. 02/06/23 18:03:41.807
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":165,"skipped":3227,"failed":0}
------------------------------
• [SLOW TEST] [7.380 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:03:34.435
    Feb  6 18:03:34.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 18:03:34.436
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:34.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:34.464
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 02/06/23 18:03:34.466
    Feb  6 18:03:34.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 create -f -'
    Feb  6 18:03:35.729: INFO: stderr: ""
    Feb  6 18:03:35.729: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/06/23 18:03:35.729
    Feb  6 18:03:35.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb  6 18:03:35.829: INFO: stderr: ""
    Feb  6 18:03:35.829: INFO: stdout: "update-demo-nautilus-khq8m update-demo-nautilus-wfz7r "
    Feb  6 18:03:35.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods update-demo-nautilus-khq8m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb  6 18:03:35.918: INFO: stderr: ""
    Feb  6 18:03:35.918: INFO: stdout: ""
    Feb  6 18:03:35.918: INFO: update-demo-nautilus-khq8m is created but not running
    Feb  6 18:03:40.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb  6 18:03:41.014: INFO: stderr: ""
    Feb  6 18:03:41.014: INFO: stdout: "update-demo-nautilus-khq8m update-demo-nautilus-wfz7r "
    Feb  6 18:03:41.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods update-demo-nautilus-khq8m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb  6 18:03:41.098: INFO: stderr: ""
    Feb  6 18:03:41.098: INFO: stdout: "true"
    Feb  6 18:03:41.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods update-demo-nautilus-khq8m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb  6 18:03:41.197: INFO: stderr: ""
    Feb  6 18:03:41.197: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb  6 18:03:41.197: INFO: validating pod update-demo-nautilus-khq8m
    Feb  6 18:03:41.207: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb  6 18:03:41.207: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb  6 18:03:41.207: INFO: update-demo-nautilus-khq8m is verified up and running
    Feb  6 18:03:41.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods update-demo-nautilus-wfz7r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb  6 18:03:41.338: INFO: stderr: ""
    Feb  6 18:03:41.338: INFO: stdout: "true"
    Feb  6 18:03:41.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods update-demo-nautilus-wfz7r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb  6 18:03:41.420: INFO: stderr: ""
    Feb  6 18:03:41.420: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb  6 18:03:41.420: INFO: validating pod update-demo-nautilus-wfz7r
    Feb  6 18:03:41.427: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb  6 18:03:41.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb  6 18:03:41.427: INFO: update-demo-nautilus-wfz7r is verified up and running
    STEP: using delete to clean up resources 02/06/23 18:03:41.427
    Feb  6 18:03:41.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 delete --grace-period=0 --force -f -'
    Feb  6 18:03:41.510: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb  6 18:03:41.510: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb  6 18:03:41.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get rc,svc -l name=update-demo --no-headers'
    Feb  6 18:03:41.664: INFO: stderr: "No resources found in kubectl-7898 namespace.\n"
    Feb  6 18:03:41.664: INFO: stdout: ""
    Feb  6 18:03:41.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7898 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb  6 18:03:41.796: INFO: stderr: ""
    Feb  6 18:03:41.796: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 18:03:41.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7898" for this suite. 02/06/23 18:03:41.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:03:41.817
Feb  6 18:03:41.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename deployment 02/06/23 18:03:41.818
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:41.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:41.845
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Feb  6 18:03:41.859: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb  6 18:03:46.865: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/06/23 18:03:46.865
Feb  6 18:03:46.865: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/06/23 18:03:46.879
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb  6 18:03:50.913: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1099  ccb548db-3ab7-4246-bb62-19789e230331 41348 1 2023-02-06 18:03:46 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-06 18:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:03:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b61c768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-06 18:03:46 +0000 UTC,LastTransitionTime:2023-02-06 18:03:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-02-06 18:03:50 +0000 UTC,LastTransitionTime:2023-02-06 18:03:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb  6 18:03:50.917: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-1099  5831fa8a-1ba6-47c6-b89d-5c3cb73899d6 41338 1 2023-02-06 18:03:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ccb548db-3ab7-4246-bb62-19789e230331 0xc0032ab937 0xc0032ab938}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccb548db-3ab7-4246-bb62-19789e230331\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:03:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032ab9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb  6 18:03:50.922: INFO: Pod "test-cleanup-deployment-69cb9c5497-j9gz9" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-j9gz9 test-cleanup-deployment-69cb9c5497- deployment-1099  ceae4aa9-1fb6-4818-88fe-a945d0daaf02 41337 0 2023-02-06 18:03:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 5831fa8a-1ba6-47c6-b89d-5c3cb73899d6 0xc0032abd77 0xc0032abd78}] [] [{kube-controller-manager Update v1 2023-02-06 18:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5831fa8a-1ba6-47c6-b89d-5c3cb73899d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 18:03:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55982,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55982,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:03:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:03:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:03:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.27,StartTime:2023-02-06 18:03:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 18:03:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://65584e357ba1c33f98b19e691a1ccc1b896f5f7e9f6010c9af3c14332eec9117,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb  6 18:03:50.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1099" for this suite. 02/06/23 18:03:50.928
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":166,"skipped":3256,"failed":0}
------------------------------
• [SLOW TEST] [9.120 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:03:41.817
    Feb  6 18:03:41.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename deployment 02/06/23 18:03:41.818
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:41.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:41.845
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Feb  6 18:03:41.859: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Feb  6 18:03:46.865: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/06/23 18:03:46.865
    Feb  6 18:03:46.865: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/06/23 18:03:46.879
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb  6 18:03:50.913: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1099  ccb548db-3ab7-4246-bb62-19789e230331 41348 1 2023-02-06 18:03:46 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-06 18:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:03:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b61c768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-06 18:03:46 +0000 UTC,LastTransitionTime:2023-02-06 18:03:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-02-06 18:03:50 +0000 UTC,LastTransitionTime:2023-02-06 18:03:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb  6 18:03:50.917: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-1099  5831fa8a-1ba6-47c6-b89d-5c3cb73899d6 41338 1 2023-02-06 18:03:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ccb548db-3ab7-4246-bb62-19789e230331 0xc0032ab937 0xc0032ab938}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccb548db-3ab7-4246-bb62-19789e230331\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:03:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032ab9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb  6 18:03:50.922: INFO: Pod "test-cleanup-deployment-69cb9c5497-j9gz9" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-j9gz9 test-cleanup-deployment-69cb9c5497- deployment-1099  ceae4aa9-1fb6-4818-88fe-a945d0daaf02 41337 0 2023-02-06 18:03:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 5831fa8a-1ba6-47c6-b89d-5c3cb73899d6 0xc0032abd77 0xc0032abd78}] [] [{kube-controller-manager Update v1 2023-02-06 18:03:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5831fa8a-1ba6-47c6-b89d-5c3cb73899d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 18:03:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55982,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55982,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:03:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:03:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:03:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:03:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.27,StartTime:2023-02-06 18:03:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 18:03:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://65584e357ba1c33f98b19e691a1ccc1b896f5f7e9f6010c9af3c14332eec9117,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb  6 18:03:50.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1099" for this suite. 02/06/23 18:03:50.928
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:03:50.939
Feb  6 18:03:50.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 18:03:50.94
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:50.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:50.964
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 02/06/23 18:03:50.967
Feb  6 18:03:50.976: INFO: Waiting up to 5m0s for pod "pod-0cb8f2b8-867a-4b67-ba21-406437bf1830" in namespace "emptydir-9492" to be "Succeeded or Failed"
Feb  6 18:03:50.982: INFO: Pod "pod-0cb8f2b8-867a-4b67-ba21-406437bf1830": Phase="Pending", Reason="", readiness=false. Elapsed: 6.42605ms
Feb  6 18:03:52.988: INFO: Pod "pod-0cb8f2b8-867a-4b67-ba21-406437bf1830": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012605189s
Feb  6 18:03:54.989: INFO: Pod "pod-0cb8f2b8-867a-4b67-ba21-406437bf1830": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013065066s
Feb  6 18:03:56.987: INFO: Pod "pod-0cb8f2b8-867a-4b67-ba21-406437bf1830": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011620354s
STEP: Saw pod success 02/06/23 18:03:56.987
Feb  6 18:03:56.987: INFO: Pod "pod-0cb8f2b8-867a-4b67-ba21-406437bf1830" satisfied condition "Succeeded or Failed"
Feb  6 18:03:56.992: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-0cb8f2b8-867a-4b67-ba21-406437bf1830 container test-container: <nil>
STEP: delete the pod 02/06/23 18:03:57
Feb  6 18:03:57.013: INFO: Waiting for pod pod-0cb8f2b8-867a-4b67-ba21-406437bf1830 to disappear
Feb  6 18:03:57.016: INFO: Pod pod-0cb8f2b8-867a-4b67-ba21-406437bf1830 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 18:03:57.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9492" for this suite. 02/06/23 18:03:57.022
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":167,"skipped":3260,"failed":0}
------------------------------
• [SLOW TEST] [6.090 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:03:50.939
    Feb  6 18:03:50.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 18:03:50.94
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:50.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:50.964
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 02/06/23 18:03:50.967
    Feb  6 18:03:50.976: INFO: Waiting up to 5m0s for pod "pod-0cb8f2b8-867a-4b67-ba21-406437bf1830" in namespace "emptydir-9492" to be "Succeeded or Failed"
    Feb  6 18:03:50.982: INFO: Pod "pod-0cb8f2b8-867a-4b67-ba21-406437bf1830": Phase="Pending", Reason="", readiness=false. Elapsed: 6.42605ms
    Feb  6 18:03:52.988: INFO: Pod "pod-0cb8f2b8-867a-4b67-ba21-406437bf1830": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012605189s
    Feb  6 18:03:54.989: INFO: Pod "pod-0cb8f2b8-867a-4b67-ba21-406437bf1830": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013065066s
    Feb  6 18:03:56.987: INFO: Pod "pod-0cb8f2b8-867a-4b67-ba21-406437bf1830": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011620354s
    STEP: Saw pod success 02/06/23 18:03:56.987
    Feb  6 18:03:56.987: INFO: Pod "pod-0cb8f2b8-867a-4b67-ba21-406437bf1830" satisfied condition "Succeeded or Failed"
    Feb  6 18:03:56.992: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-0cb8f2b8-867a-4b67-ba21-406437bf1830 container test-container: <nil>
    STEP: delete the pod 02/06/23 18:03:57
    Feb  6 18:03:57.013: INFO: Waiting for pod pod-0cb8f2b8-867a-4b67-ba21-406437bf1830 to disappear
    Feb  6 18:03:57.016: INFO: Pod pod-0cb8f2b8-867a-4b67-ba21-406437bf1830 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 18:03:57.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9492" for this suite. 02/06/23 18:03:57.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:03:57.03
Feb  6 18:03:57.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:03:57.031
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:57.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:57.053
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 02/06/23 18:03:57.057
Feb  6 18:03:57.066: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8" in namespace "projected-4786" to be "Succeeded or Failed"
Feb  6 18:03:57.073: INFO: Pod "downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.275044ms
Feb  6 18:03:59.079: INFO: Pod "downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013067816s
Feb  6 18:04:01.080: INFO: Pod "downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014380891s
Feb  6 18:04:03.079: INFO: Pod "downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012926543s
STEP: Saw pod success 02/06/23 18:04:03.079
Feb  6 18:04:03.079: INFO: Pod "downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8" satisfied condition "Succeeded or Failed"
Feb  6 18:04:03.082: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8 container client-container: <nil>
STEP: delete the pod 02/06/23 18:04:03.089
Feb  6 18:04:03.106: INFO: Waiting for pod downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8 to disappear
Feb  6 18:04:03.111: INFO: Pod downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb  6 18:04:03.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4786" for this suite. 02/06/23 18:04:03.116
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":168,"skipped":3272,"failed":0}
------------------------------
• [SLOW TEST] [6.097 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:03:57.03
    Feb  6 18:03:57.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:03:57.031
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:03:57.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:03:57.053
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 02/06/23 18:03:57.057
    Feb  6 18:03:57.066: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8" in namespace "projected-4786" to be "Succeeded or Failed"
    Feb  6 18:03:57.073: INFO: Pod "downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.275044ms
    Feb  6 18:03:59.079: INFO: Pod "downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013067816s
    Feb  6 18:04:01.080: INFO: Pod "downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014380891s
    Feb  6 18:04:03.079: INFO: Pod "downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012926543s
    STEP: Saw pod success 02/06/23 18:04:03.079
    Feb  6 18:04:03.079: INFO: Pod "downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8" satisfied condition "Succeeded or Failed"
    Feb  6 18:04:03.082: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8 container client-container: <nil>
    STEP: delete the pod 02/06/23 18:04:03.089
    Feb  6 18:04:03.106: INFO: Waiting for pod downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8 to disappear
    Feb  6 18:04:03.111: INFO: Pod downwardapi-volume-05887fbe-4096-4dc7-ae19-aec9606488a8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb  6 18:04:03.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4786" for this suite. 02/06/23 18:04:03.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:04:03.128
Feb  6 18:04:03.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 18:04:03.129
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:03.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:03.153
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 18:04:03.172
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:04:03.418
STEP: Deploying the webhook pod 02/06/23 18:04:03.429
STEP: Wait for the deployment to be ready 02/06/23 18:04:03.444
Feb  6 18:04:03.461: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 18:04:05.473: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 4, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 4, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 4, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 4, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 18:04:07.479
STEP: Verifying the service has paired with the endpoint 02/06/23 18:04:07.504
Feb  6 18:04:08.504: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/06/23 18:04:08.508
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/06/23 18:04:08.526
STEP: Creating a dummy validating-webhook-configuration object 02/06/23 18:04:08.542
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/06/23 18:04:08.554
STEP: Creating a dummy mutating-webhook-configuration object 02/06/23 18:04:08.564
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/06/23 18:04:08.574
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:04:08.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8617" for this suite. 02/06/23 18:04:08.603
STEP: Destroying namespace "webhook-8617-markers" for this suite. 02/06/23 18:04:08.613
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":169,"skipped":3285,"failed":0}
------------------------------
• [SLOW TEST] [5.560 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:04:03.128
    Feb  6 18:04:03.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 18:04:03.129
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:03.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:03.153
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 18:04:03.172
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:04:03.418
    STEP: Deploying the webhook pod 02/06/23 18:04:03.429
    STEP: Wait for the deployment to be ready 02/06/23 18:04:03.444
    Feb  6 18:04:03.461: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 18:04:05.473: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 4, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 4, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 4, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 4, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 18:04:07.479
    STEP: Verifying the service has paired with the endpoint 02/06/23 18:04:07.504
    Feb  6 18:04:08.504: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/06/23 18:04:08.508
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/06/23 18:04:08.526
    STEP: Creating a dummy validating-webhook-configuration object 02/06/23 18:04:08.542
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/06/23 18:04:08.554
    STEP: Creating a dummy mutating-webhook-configuration object 02/06/23 18:04:08.564
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/06/23 18:04:08.574
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:04:08.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8617" for this suite. 02/06/23 18:04:08.603
    STEP: Destroying namespace "webhook-8617-markers" for this suite. 02/06/23 18:04:08.613
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:04:08.689
Feb  6 18:04:08.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename daemonsets 02/06/23 18:04:08.69
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:08.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:08.722
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Feb  6 18:04:08.748: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 02/06/23 18:04:08.755
Feb  6 18:04:08.763: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:08.763: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:08.763: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:08.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:04:08.766: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:04:09.773: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:09.773: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:09.775: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:09.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:04:09.780: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:04:10.773: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:10.773: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:10.774: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:10.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:04:10.779: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:04:11.772: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:11.772: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:11.772: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:11.776: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb  6 18:04:11.777: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
Feb  6 18:04:12.773: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:12.774: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:12.774: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:12.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb  6 18:04:12.780: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 02/06/23 18:04:12.801
STEP: Check that daemon pods images are updated. 02/06/23 18:04:12.818
Feb  6 18:04:12.825: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:12.825: INFO: Wrong image for pod: daemon-set-c7v2l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:12.825: INFO: Wrong image for pod: daemon-set-dqqt6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:12.837: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:12.837: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:12.837: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:13.843: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:13.843: INFO: Wrong image for pod: daemon-set-c7v2l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:13.848: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:13.848: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:13.848: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:14.842: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:14.842: INFO: Wrong image for pod: daemon-set-c7v2l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:14.847: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:14.847: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:14.847: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:15.842: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:15.842: INFO: Wrong image for pod: daemon-set-c7v2l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:15.842: INFO: Pod daemon-set-pg59l is not available
Feb  6 18:04:15.848: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:15.849: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:15.849: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:16.843: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:16.844: INFO: Wrong image for pod: daemon-set-c7v2l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:16.844: INFO: Pod daemon-set-pg59l is not available
Feb  6 18:04:16.854: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:16.854: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:16.855: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:17.846: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:17.846: INFO: Wrong image for pod: daemon-set-c7v2l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:17.846: INFO: Pod daemon-set-pg59l is not available
Feb  6 18:04:17.854: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:17.854: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:17.854: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:18.860: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:18.867: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:18.867: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:18.867: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:19.842: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:19.842: INFO: Pod daemon-set-vhzfj is not available
Feb  6 18:04:19.847: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:19.847: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:19.847: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:20.843: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:20.843: INFO: Pod daemon-set-vhzfj is not available
Feb  6 18:04:20.850: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:20.850: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:20.850: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:21.846: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb  6 18:04:21.846: INFO: Pod daemon-set-vhzfj is not available
Feb  6 18:04:21.853: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:21.854: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:21.854: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:22.848: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:22.848: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:22.848: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:23.845: INFO: Pod daemon-set-69l6q is not available
Feb  6 18:04:23.851: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:23.851: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:23.851: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 02/06/23 18:04:23.851
Feb  6 18:04:23.856: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:23.856: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:23.856: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:23.860: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb  6 18:04:23.860: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:04:24.866: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:24.866: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:24.866: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:24.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb  6 18:04:24.870: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:04:25.867: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:25.867: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:25.867: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:25.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb  6 18:04:25.871: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:04:26.868: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:26.868: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:26.868: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:04:26.872: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb  6 18:04:26.872: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/06/23 18:04:26.895
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7177, will wait for the garbage collector to delete the pods 02/06/23 18:04:26.895
Feb  6 18:04:26.961: INFO: Deleting DaemonSet.extensions daemon-set took: 11.40003ms
Feb  6 18:04:27.062: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.886381ms
Feb  6 18:04:29.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:04:29.866: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb  6 18:04:29.869: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42010"},"items":null}

Feb  6 18:04:29.873: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42010"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb  6 18:04:29.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7177" for this suite. 02/06/23 18:04:29.893
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":170,"skipped":3298,"failed":0}
------------------------------
• [SLOW TEST] [21.216 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:04:08.689
    Feb  6 18:04:08.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename daemonsets 02/06/23 18:04:08.69
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:08.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:08.722
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Feb  6 18:04:08.748: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 02/06/23 18:04:08.755
    Feb  6 18:04:08.763: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:08.763: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:08.763: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:08.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:04:08.766: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:04:09.773: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:09.773: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:09.775: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:09.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:04:09.780: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:04:10.773: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:10.773: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:10.774: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:10.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:04:10.779: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:04:11.772: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:11.772: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:11.772: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:11.776: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb  6 18:04:11.777: INFO: Node tneyla25-md-0-68cbcb4798-6j94p is running 0 daemon pod, expected 1
    Feb  6 18:04:12.773: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:12.774: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:12.774: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:12.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb  6 18:04:12.780: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 02/06/23 18:04:12.801
    STEP: Check that daemon pods images are updated. 02/06/23 18:04:12.818
    Feb  6 18:04:12.825: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:12.825: INFO: Wrong image for pod: daemon-set-c7v2l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:12.825: INFO: Wrong image for pod: daemon-set-dqqt6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:12.837: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:12.837: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:12.837: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:13.843: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:13.843: INFO: Wrong image for pod: daemon-set-c7v2l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:13.848: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:13.848: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:13.848: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:14.842: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:14.842: INFO: Wrong image for pod: daemon-set-c7v2l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:14.847: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:14.847: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:14.847: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:15.842: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:15.842: INFO: Wrong image for pod: daemon-set-c7v2l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:15.842: INFO: Pod daemon-set-pg59l is not available
    Feb  6 18:04:15.848: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:15.849: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:15.849: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:16.843: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:16.844: INFO: Wrong image for pod: daemon-set-c7v2l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:16.844: INFO: Pod daemon-set-pg59l is not available
    Feb  6 18:04:16.854: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:16.854: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:16.855: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:17.846: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:17.846: INFO: Wrong image for pod: daemon-set-c7v2l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:17.846: INFO: Pod daemon-set-pg59l is not available
    Feb  6 18:04:17.854: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:17.854: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:17.854: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:18.860: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:18.867: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:18.867: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:18.867: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:19.842: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:19.842: INFO: Pod daemon-set-vhzfj is not available
    Feb  6 18:04:19.847: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:19.847: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:19.847: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:20.843: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:20.843: INFO: Pod daemon-set-vhzfj is not available
    Feb  6 18:04:20.850: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:20.850: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:20.850: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:21.846: INFO: Wrong image for pod: daemon-set-9wrh2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb  6 18:04:21.846: INFO: Pod daemon-set-vhzfj is not available
    Feb  6 18:04:21.853: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:21.854: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:21.854: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:22.848: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:22.848: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:22.848: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:23.845: INFO: Pod daemon-set-69l6q is not available
    Feb  6 18:04:23.851: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:23.851: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:23.851: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 02/06/23 18:04:23.851
    Feb  6 18:04:23.856: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:23.856: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:23.856: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:23.860: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb  6 18:04:23.860: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:04:24.866: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:24.866: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:24.866: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:24.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb  6 18:04:24.870: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:04:25.867: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:25.867: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:25.867: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:25.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb  6 18:04:25.871: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:04:26.868: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:26.868: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:26.868: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:04:26.872: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb  6 18:04:26.872: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/06/23 18:04:26.895
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7177, will wait for the garbage collector to delete the pods 02/06/23 18:04:26.895
    Feb  6 18:04:26.961: INFO: Deleting DaemonSet.extensions daemon-set took: 11.40003ms
    Feb  6 18:04:27.062: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.886381ms
    Feb  6 18:04:29.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:04:29.866: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb  6 18:04:29.869: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42010"},"items":null}

    Feb  6 18:04:29.873: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42010"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 18:04:29.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7177" for this suite. 02/06/23 18:04:29.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:04:29.91
Feb  6 18:04:29.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename crd-webhook 02/06/23 18:04:29.911
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:29.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:29.936
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/06/23 18:04:29.94
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/06/23 18:04:30.296
STEP: Deploying the custom resource conversion webhook pod 02/06/23 18:04:30.302
STEP: Wait for the deployment to be ready 02/06/23 18:04:30.32
Feb  6 18:04:30.344: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb  6 18:04:32.359: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 4, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 4, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 4, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 4, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 18:04:34.367
STEP: Verifying the service has paired with the endpoint 02/06/23 18:04:34.401
Feb  6 18:04:35.402: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Feb  6 18:04:35.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Creating a v1 custom resource 02/06/23 18:04:38.026
STEP: Create a v2 custom resource 02/06/23 18:04:38.07
STEP: List CRs in v1 02/06/23 18:04:38.139
STEP: List CRs in v2 02/06/23 18:04:38.148
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:04:38.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5165" for this suite. 02/06/23 18:04:38.703
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":171,"skipped":3324,"failed":0}
------------------------------
• [SLOW TEST] [8.874 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:04:29.91
    Feb  6 18:04:29.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename crd-webhook 02/06/23 18:04:29.911
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:29.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:29.936
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/06/23 18:04:29.94
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/06/23 18:04:30.296
    STEP: Deploying the custom resource conversion webhook pod 02/06/23 18:04:30.302
    STEP: Wait for the deployment to be ready 02/06/23 18:04:30.32
    Feb  6 18:04:30.344: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Feb  6 18:04:32.359: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 4, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 4, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 4, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 4, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 18:04:34.367
    STEP: Verifying the service has paired with the endpoint 02/06/23 18:04:34.401
    Feb  6 18:04:35.402: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Feb  6 18:04:35.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Creating a v1 custom resource 02/06/23 18:04:38.026
    STEP: Create a v2 custom resource 02/06/23 18:04:38.07
    STEP: List CRs in v1 02/06/23 18:04:38.139
    STEP: List CRs in v2 02/06/23 18:04:38.148
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:04:38.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-5165" for this suite. 02/06/23 18:04:38.703
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:04:38.785
Feb  6 18:04:38.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename containers 02/06/23 18:04:38.786
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:38.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:38.816
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 02/06/23 18:04:38.819
Feb  6 18:04:38.833: INFO: Waiting up to 5m0s for pod "client-containers-22ea1318-e893-4754-8631-6be5b02bda3d" in namespace "containers-8222" to be "Succeeded or Failed"
Feb  6 18:04:38.839: INFO: Pod "client-containers-22ea1318-e893-4754-8631-6be5b02bda3d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.174171ms
Feb  6 18:04:40.850: INFO: Pod "client-containers-22ea1318-e893-4754-8631-6be5b02bda3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016872167s
Feb  6 18:04:42.843: INFO: Pod "client-containers-22ea1318-e893-4754-8631-6be5b02bda3d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009660855s
Feb  6 18:04:44.844: INFO: Pod "client-containers-22ea1318-e893-4754-8631-6be5b02bda3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010494262s
STEP: Saw pod success 02/06/23 18:04:44.844
Feb  6 18:04:44.844: INFO: Pod "client-containers-22ea1318-e893-4754-8631-6be5b02bda3d" satisfied condition "Succeeded or Failed"
Feb  6 18:04:44.848: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod client-containers-22ea1318-e893-4754-8631-6be5b02bda3d container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:04:44.855
Feb  6 18:04:44.867: INFO: Waiting for pod client-containers-22ea1318-e893-4754-8631-6be5b02bda3d to disappear
Feb  6 18:04:44.871: INFO: Pod client-containers-22ea1318-e893-4754-8631-6be5b02bda3d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb  6 18:04:44.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8222" for this suite. 02/06/23 18:04:44.877
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":172,"skipped":3328,"failed":0}
------------------------------
• [SLOW TEST] [6.100 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:04:38.785
    Feb  6 18:04:38.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename containers 02/06/23 18:04:38.786
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:38.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:38.816
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 02/06/23 18:04:38.819
    Feb  6 18:04:38.833: INFO: Waiting up to 5m0s for pod "client-containers-22ea1318-e893-4754-8631-6be5b02bda3d" in namespace "containers-8222" to be "Succeeded or Failed"
    Feb  6 18:04:38.839: INFO: Pod "client-containers-22ea1318-e893-4754-8631-6be5b02bda3d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.174171ms
    Feb  6 18:04:40.850: INFO: Pod "client-containers-22ea1318-e893-4754-8631-6be5b02bda3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016872167s
    Feb  6 18:04:42.843: INFO: Pod "client-containers-22ea1318-e893-4754-8631-6be5b02bda3d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009660855s
    Feb  6 18:04:44.844: INFO: Pod "client-containers-22ea1318-e893-4754-8631-6be5b02bda3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010494262s
    STEP: Saw pod success 02/06/23 18:04:44.844
    Feb  6 18:04:44.844: INFO: Pod "client-containers-22ea1318-e893-4754-8631-6be5b02bda3d" satisfied condition "Succeeded or Failed"
    Feb  6 18:04:44.848: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod client-containers-22ea1318-e893-4754-8631-6be5b02bda3d container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:04:44.855
    Feb  6 18:04:44.867: INFO: Waiting for pod client-containers-22ea1318-e893-4754-8631-6be5b02bda3d to disappear
    Feb  6 18:04:44.871: INFO: Pod client-containers-22ea1318-e893-4754-8631-6be5b02bda3d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb  6 18:04:44.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8222" for this suite. 02/06/23 18:04:44.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:04:44.89
Feb  6 18:04:44.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename runtimeclass 02/06/23 18:04:44.891
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:44.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:44.912
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 02/06/23 18:04:44.915
STEP: getting /apis/node.k8s.io 02/06/23 18:04:44.917
STEP: getting /apis/node.k8s.io/v1 02/06/23 18:04:44.919
STEP: creating 02/06/23 18:04:44.92
STEP: watching 02/06/23 18:04:44.939
Feb  6 18:04:44.939: INFO: starting watch
STEP: getting 02/06/23 18:04:44.948
STEP: listing 02/06/23 18:04:44.953
STEP: patching 02/06/23 18:04:44.957
STEP: updating 02/06/23 18:04:44.963
Feb  6 18:04:44.969: INFO: waiting for watch events with expected annotations
STEP: deleting 02/06/23 18:04:44.969
STEP: deleting a collection 02/06/23 18:04:44.982
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb  6 18:04:45.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7907" for this suite. 02/06/23 18:04:45.005
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":173,"skipped":3346,"failed":0}
------------------------------
• [0.123 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:04:44.89
    Feb  6 18:04:44.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename runtimeclass 02/06/23 18:04:44.891
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:44.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:44.912
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 02/06/23 18:04:44.915
    STEP: getting /apis/node.k8s.io 02/06/23 18:04:44.917
    STEP: getting /apis/node.k8s.io/v1 02/06/23 18:04:44.919
    STEP: creating 02/06/23 18:04:44.92
    STEP: watching 02/06/23 18:04:44.939
    Feb  6 18:04:44.939: INFO: starting watch
    STEP: getting 02/06/23 18:04:44.948
    STEP: listing 02/06/23 18:04:44.953
    STEP: patching 02/06/23 18:04:44.957
    STEP: updating 02/06/23 18:04:44.963
    Feb  6 18:04:44.969: INFO: waiting for watch events with expected annotations
    STEP: deleting 02/06/23 18:04:44.969
    STEP: deleting a collection 02/06/23 18:04:44.982
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb  6 18:04:45.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7907" for this suite. 02/06/23 18:04:45.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:04:45.022
Feb  6 18:04:45.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 18:04:45.023
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:45.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:45.045
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 18:04:45.063
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:04:45.665
STEP: Deploying the webhook pod 02/06/23 18:04:45.675
STEP: Wait for the deployment to be ready 02/06/23 18:04:45.69
Feb  6 18:04:45.708: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 18:04:47.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 4, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 4, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 4, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 18:04:49.731
STEP: Verifying the service has paired with the endpoint 02/06/23 18:04:49.758
Feb  6 18:04:50.758: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Feb  6 18:04:50.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2444-crds.webhook.example.com via the AdmissionRegistration API 02/06/23 18:04:51.278
STEP: Creating a custom resource that should be mutated by the webhook 02/06/23 18:04:51.296
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:04:53.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6069" for this suite. 02/06/23 18:04:53.873
STEP: Destroying namespace "webhook-6069-markers" for this suite. 02/06/23 18:04:53.881
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":174,"skipped":3390,"failed":0}
------------------------------
• [SLOW TEST] [8.936 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:04:45.022
    Feb  6 18:04:45.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 18:04:45.023
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:45.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:45.045
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 18:04:45.063
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:04:45.665
    STEP: Deploying the webhook pod 02/06/23 18:04:45.675
    STEP: Wait for the deployment to be ready 02/06/23 18:04:45.69
    Feb  6 18:04:45.708: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 18:04:47.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 4, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 4, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 4, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 18:04:49.731
    STEP: Verifying the service has paired with the endpoint 02/06/23 18:04:49.758
    Feb  6 18:04:50.758: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Feb  6 18:04:50.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2444-crds.webhook.example.com via the AdmissionRegistration API 02/06/23 18:04:51.278
    STEP: Creating a custom resource that should be mutated by the webhook 02/06/23 18:04:51.296
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:04:53.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6069" for this suite. 02/06/23 18:04:53.873
    STEP: Destroying namespace "webhook-6069-markers" for this suite. 02/06/23 18:04:53.881
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:04:53.972
Feb  6 18:04:53.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename var-expansion 02/06/23 18:04:53.973
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:53.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:54.001
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 02/06/23 18:04:54.004
Feb  6 18:04:54.027: INFO: Waiting up to 5m0s for pod "var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462" in namespace "var-expansion-1541" to be "Succeeded or Failed"
Feb  6 18:04:54.033: INFO: Pod "var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462": Phase="Pending", Reason="", readiness=false. Elapsed: 6.248438ms
Feb  6 18:04:56.039: INFO: Pod "var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012114592s
Feb  6 18:04:58.039: INFO: Pod "var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011764261s
Feb  6 18:05:00.041: INFO: Pod "var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013516547s
STEP: Saw pod success 02/06/23 18:05:00.041
Feb  6 18:05:00.041: INFO: Pod "var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462" satisfied condition "Succeeded or Failed"
Feb  6 18:05:00.044: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462 container dapi-container: <nil>
STEP: delete the pod 02/06/23 18:05:00.056
Feb  6 18:05:00.072: INFO: Waiting for pod var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462 to disappear
Feb  6 18:05:00.075: INFO: Pod var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb  6 18:05:00.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1541" for this suite. 02/06/23 18:05:00.081
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":175,"skipped":3417,"failed":0}
------------------------------
• [SLOW TEST] [6.117 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:04:53.972
    Feb  6 18:04:53.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename var-expansion 02/06/23 18:04:53.973
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:04:53.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:04:54.001
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 02/06/23 18:04:54.004
    Feb  6 18:04:54.027: INFO: Waiting up to 5m0s for pod "var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462" in namespace "var-expansion-1541" to be "Succeeded or Failed"
    Feb  6 18:04:54.033: INFO: Pod "var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462": Phase="Pending", Reason="", readiness=false. Elapsed: 6.248438ms
    Feb  6 18:04:56.039: INFO: Pod "var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012114592s
    Feb  6 18:04:58.039: INFO: Pod "var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011764261s
    Feb  6 18:05:00.041: INFO: Pod "var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013516547s
    STEP: Saw pod success 02/06/23 18:05:00.041
    Feb  6 18:05:00.041: INFO: Pod "var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462" satisfied condition "Succeeded or Failed"
    Feb  6 18:05:00.044: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462 container dapi-container: <nil>
    STEP: delete the pod 02/06/23 18:05:00.056
    Feb  6 18:05:00.072: INFO: Waiting for pod var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462 to disappear
    Feb  6 18:05:00.075: INFO: Pod var-expansion-d546be47-4f4a-4266-8edf-37f6aa1c7462 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb  6 18:05:00.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1541" for this suite. 02/06/23 18:05:00.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:05:00.092
Feb  6 18:05:00.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename proxy 02/06/23 18:05:00.094
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:05:00.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:05:00.119
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Feb  6 18:05:00.122: INFO: Creating pod...
Feb  6 18:05:00.133: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8623" to be "running"
Feb  6 18:05:00.138: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.293132ms
Feb  6 18:05:02.144: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011026139s
Feb  6 18:05:04.145: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.012473665s
Feb  6 18:05:04.145: INFO: Pod "agnhost" satisfied condition "running"
Feb  6 18:05:04.145: INFO: Creating service...
Feb  6 18:05:04.182: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=DELETE
Feb  6 18:05:04.195: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb  6 18:05:04.195: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=OPTIONS
Feb  6 18:05:04.200: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb  6 18:05:04.200: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=PATCH
Feb  6 18:05:04.204: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb  6 18:05:04.204: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=POST
Feb  6 18:05:04.209: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb  6 18:05:04.209: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=PUT
Feb  6 18:05:04.216: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb  6 18:05:04.216: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=DELETE
Feb  6 18:05:04.223: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb  6 18:05:04.223: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=OPTIONS
Feb  6 18:05:04.231: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb  6 18:05:04.231: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=PATCH
Feb  6 18:05:04.238: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb  6 18:05:04.238: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=POST
Feb  6 18:05:04.246: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb  6 18:05:04.246: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=PUT
Feb  6 18:05:04.253: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb  6 18:05:04.253: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=GET
Feb  6 18:05:04.263: INFO: http.Client request:GET StatusCode:301
Feb  6 18:05:04.263: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=GET
Feb  6 18:05:04.269: INFO: http.Client request:GET StatusCode:301
Feb  6 18:05:04.269: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=HEAD
Feb  6 18:05:04.276: INFO: http.Client request:HEAD StatusCode:301
Feb  6 18:05:04.276: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=HEAD
Feb  6 18:05:04.281: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb  6 18:05:04.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8623" for this suite. 02/06/23 18:05:04.288
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":176,"skipped":3423,"failed":0}
------------------------------
• [4.204 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:05:00.092
    Feb  6 18:05:00.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename proxy 02/06/23 18:05:00.094
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:05:00.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:05:00.119
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Feb  6 18:05:00.122: INFO: Creating pod...
    Feb  6 18:05:00.133: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8623" to be "running"
    Feb  6 18:05:00.138: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.293132ms
    Feb  6 18:05:02.144: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011026139s
    Feb  6 18:05:04.145: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.012473665s
    Feb  6 18:05:04.145: INFO: Pod "agnhost" satisfied condition "running"
    Feb  6 18:05:04.145: INFO: Creating service...
    Feb  6 18:05:04.182: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=DELETE
    Feb  6 18:05:04.195: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb  6 18:05:04.195: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=OPTIONS
    Feb  6 18:05:04.200: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb  6 18:05:04.200: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=PATCH
    Feb  6 18:05:04.204: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb  6 18:05:04.204: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=POST
    Feb  6 18:05:04.209: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb  6 18:05:04.209: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=PUT
    Feb  6 18:05:04.216: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb  6 18:05:04.216: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=DELETE
    Feb  6 18:05:04.223: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb  6 18:05:04.223: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Feb  6 18:05:04.231: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb  6 18:05:04.231: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=PATCH
    Feb  6 18:05:04.238: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb  6 18:05:04.238: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=POST
    Feb  6 18:05:04.246: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb  6 18:05:04.246: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=PUT
    Feb  6 18:05:04.253: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb  6 18:05:04.253: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=GET
    Feb  6 18:05:04.263: INFO: http.Client request:GET StatusCode:301
    Feb  6 18:05:04.263: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=GET
    Feb  6 18:05:04.269: INFO: http.Client request:GET StatusCode:301
    Feb  6 18:05:04.269: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/pods/agnhost/proxy?method=HEAD
    Feb  6 18:05:04.276: INFO: http.Client request:HEAD StatusCode:301
    Feb  6 18:05:04.276: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8623/services/e2e-proxy-test-service/proxy?method=HEAD
    Feb  6 18:05:04.281: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb  6 18:05:04.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-8623" for this suite. 02/06/23 18:05:04.288
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:05:04.305
Feb  6 18:05:04.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename sched-pred 02/06/23 18:05:04.306
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:05:04.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:05:04.332
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb  6 18:05:04.337: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  6 18:05:04.351: INFO: Waiting for terminating namespaces to be deleted...
Feb  6 18:05:04.356: INFO: 
Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-5xxmw before test
Feb  6 18:05:04.369: INFO: cilium-9gdv4 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.369: INFO: 	Container cilium-agent ready: true, restart count 0
Feb  6 18:05:04.369: INFO: kube-proxy-mnvm8 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.369: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  6 18:05:04.369: INFO: vsphere-cloud-controller-manager-7dmv6 from kube-system started at 2023-02-06 17:31:18 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.369: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Feb  6 18:05:04.369: INFO: vsphere-csi-node-7l6qr from kube-system started at 2023-02-06 17:12:22 +0000 UTC (3 container statuses recorded)
Feb  6 18:05:04.369: INFO: 	Container liveness-probe ready: true, restart count 0
Feb  6 18:05:04.370: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  6 18:05:04.370: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Feb  6 18:05:04.370: INFO: agnhost from proxy-8623 started at 2023-02-06 18:05:00 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.370: INFO: 	Container agnhost ready: true, restart count 0
Feb  6 18:05:04.370: INFO: sonobuoy from sonobuoy started at 2023-02-06 17:18:54 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.371: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  6 18:05:04.371: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-cr8qs from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 18:05:04.371: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 18:05:04.371: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  6 18:05:04.371: INFO: 
Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-6j94p before test
Feb  6 18:05:04.387: INFO: capi-kubeadm-control-plane-controller-manager-5f686d9d47-lkld2 from capi-kubeadm-control-plane-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.387: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:05:04.387: INFO: capi-controller-manager-5b68b88d59-vcpp2 from capi-system started at 2023-02-06 17:15:34 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.387: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:05:04.387: INFO: cert-manager-7b9fcd6db9-rz78g from cert-manager started at 2023-02-06 17:15:16 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.387: INFO: 	Container cert-manager ready: true, restart count 0
Feb  6 18:05:04.387: INFO: cert-manager-cainjector-75b587b679-rjbzj from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.387: INFO: 	Container cert-manager ready: true, restart count 0
Feb  6 18:05:04.387: INFO: cert-manager-webhook-896b549c4-hj9c4 from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.387: INFO: 	Container cert-manager ready: true, restart count 0
Feb  6 18:05:04.387: INFO: eks-anywhere-packages-8589bd5d9c-4pc9v from eksa-packages started at 2023-02-06 17:17:15 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.387: INFO: 	Container controller ready: true, restart count 0
Feb  6 18:05:04.388: INFO: etcdadm-bootstrap-provider-controller-manager-84ff796964-d6tt8 from etcdadm-bootstrap-provider-system started at 2023-02-06 17:15:37 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.388: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:05:04.388: INFO: cilium-nb887 from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.388: INFO: 	Container cilium-agent ready: true, restart count 0
Feb  6 18:05:04.388: INFO: kube-proxy-ld4nv from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.388: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  6 18:05:04.388: INFO: vsphere-cloud-controller-manager-76l5k from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.388: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Feb  6 18:05:04.388: INFO: vsphere-csi-node-2tfxk from kube-system started at 2023-02-06 17:12:26 +0000 UTC (3 container statuses recorded)
Feb  6 18:05:04.388: INFO: 	Container liveness-probe ready: true, restart count 0
Feb  6 18:05:04.388: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  6 18:05:04.388: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Feb  6 18:05:04.388: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-hcvcv from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 18:05:04.388: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 18:05:04.388: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  6 18:05:04.388: INFO: 
Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-lz542 before test
Feb  6 18:05:04.399: INFO: capi-kubeadm-bootstrap-controller-manager-79d4bdf548-4xmjq from capi-kubeadm-bootstrap-system started at 2023-02-06 17:15:36 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.399: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:05:04.399: INFO: capv-controller-manager-598f9bf684-4qdws from capv-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.399: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:05:04.399: INFO: eksa-controller-manager-5588899966-wtcpc from eksa-system started at 2023-02-06 17:16:14 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.399: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:05:04.399: INFO: etcdadm-controller-controller-manager-767b7fb9b5-mt7lx from etcdadm-controller-system started at 2023-02-06 17:15:38 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.399: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:05:04.399: INFO: cilium-ndf4k from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.399: INFO: 	Container cilium-agent ready: true, restart count 0
Feb  6 18:05:04.399: INFO: cilium-operator-7f966b595f-pzn9c from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.399: INFO: 	Container cilium-operator ready: true, restart count 0
Feb  6 18:05:04.399: INFO: kube-proxy-hfblf from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.399: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  6 18:05:04.399: INFO: vsphere-cloud-controller-manager-x8k8p from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 18:05:04.399: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Feb  6 18:05:04.399: INFO: vsphere-csi-node-v4zd6 from kube-system started at 2023-02-06 17:12:15 +0000 UTC (3 container statuses recorded)
Feb  6 18:05:04.399: INFO: 	Container liveness-probe ready: true, restart count 0
Feb  6 18:05:04.399: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  6 18:05:04.399: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Feb  6 18:05:04.399: INFO: sonobuoy-e2e-job-b0e9e31f5d894166 from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 18:05:04.399: INFO: 	Container e2e ready: true, restart count 0
Feb  6 18:05:04.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 18:05:04.399: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-fncmf from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 18:05:04.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 18:05:04.399: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/06/23 18:05:04.399
Feb  6 18:05:04.410: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-36" to be "running"
Feb  6 18:05:04.417: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.752918ms
Feb  6 18:05:06.424: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014311785s
Feb  6 18:05:08.422: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.012178243s
Feb  6 18:05:08.422: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/06/23 18:05:08.426
STEP: Trying to apply a random label on the found node. 02/06/23 18:05:08.445
STEP: verifying the node has the label kubernetes.io/e2e-72cb9b8b-8584-46a9-bc3d-68d2462a964e 95 02/06/23 18:05:08.46
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/06/23 18:05:08.466
Feb  6 18:05:08.478: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-36" to be "not pending"
Feb  6 18:05:08.482: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095776ms
Feb  6 18:05:10.490: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012021648s
Feb  6 18:05:12.491: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.012920064s
Feb  6 18:05:12.491: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 195.17.4.133 on the node which pod4 resides and expect not scheduled 02/06/23 18:05:12.491
Feb  6 18:05:12.501: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-36" to be "not pending"
Feb  6 18:05:12.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.587035ms
Feb  6 18:05:14.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008656724s
Feb  6 18:05:16.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009057762s
Feb  6 18:05:18.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009151158s
Feb  6 18:05:20.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008866258s
Feb  6 18:05:22.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009818222s
Feb  6 18:05:24.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009163959s
Feb  6 18:05:26.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010613312s
Feb  6 18:05:28.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008493179s
Feb  6 18:05:30.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010696729s
Feb  6 18:05:32.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010660343s
Feb  6 18:05:34.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010818486s
Feb  6 18:05:36.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009604366s
Feb  6 18:05:38.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009044651s
Feb  6 18:05:40.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009923261s
Feb  6 18:05:42.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010801862s
Feb  6 18:05:44.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.009754904s
Feb  6 18:05:46.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009148011s
Feb  6 18:05:48.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008899979s
Feb  6 18:05:50.513: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012263158s
Feb  6 18:05:52.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008950713s
Feb  6 18:05:54.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009026065s
Feb  6 18:05:56.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.0107275s
Feb  6 18:05:58.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009705293s
Feb  6 18:06:00.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011155971s
Feb  6 18:06:02.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010252019s
Feb  6 18:06:04.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010471136s
Feb  6 18:06:06.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008681227s
Feb  6 18:06:08.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010276903s
Feb  6 18:06:10.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009513917s
Feb  6 18:06:12.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009742844s
Feb  6 18:06:14.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008488363s
Feb  6 18:06:16.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009837022s
Feb  6 18:06:18.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008869254s
Feb  6 18:06:20.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009386834s
Feb  6 18:06:22.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009911455s
Feb  6 18:06:24.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009267112s
Feb  6 18:06:26.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009325789s
Feb  6 18:06:28.513: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011906799s
Feb  6 18:06:30.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.010141037s
Feb  6 18:06:32.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.00937891s
Feb  6 18:06:34.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009671758s
Feb  6 18:06:36.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010476837s
Feb  6 18:06:38.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.0089185s
Feb  6 18:06:40.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009995592s
Feb  6 18:06:42.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010732127s
Feb  6 18:06:44.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008842752s
Feb  6 18:06:46.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010662149s
Feb  6 18:06:48.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.0085529s
Feb  6 18:06:50.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009791764s
Feb  6 18:06:52.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010397859s
Feb  6 18:06:54.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010762418s
Feb  6 18:06:56.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009811866s
Feb  6 18:06:58.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009254066s
Feb  6 18:07:00.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.0087473s
Feb  6 18:07:02.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009509698s
Feb  6 18:07:04.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009282101s
Feb  6 18:07:06.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009157091s
Feb  6 18:07:08.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010058246s
Feb  6 18:07:10.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009757199s
Feb  6 18:07:12.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009049364s
Feb  6 18:07:14.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.00867162s
Feb  6 18:07:16.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.00937392s
Feb  6 18:07:18.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009638647s
Feb  6 18:07:20.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.009785192s
Feb  6 18:07:22.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.010175347s
Feb  6 18:07:24.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009639148s
Feb  6 18:07:26.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009977033s
Feb  6 18:07:28.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009319812s
Feb  6 18:07:30.513: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.011754801s
Feb  6 18:07:32.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.009296276s
Feb  6 18:07:34.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008927447s
Feb  6 18:07:36.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009847679s
Feb  6 18:07:38.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.009830667s
Feb  6 18:07:40.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009594825s
Feb  6 18:07:42.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.010191017s
Feb  6 18:07:44.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.011296605s
Feb  6 18:07:46.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008770204s
Feb  6 18:07:48.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.009115206s
Feb  6 18:07:50.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.01101136s
Feb  6 18:07:52.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009325343s
Feb  6 18:07:54.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009366636s
Feb  6 18:07:56.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.011070764s
Feb  6 18:07:58.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009770655s
Feb  6 18:08:00.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009375211s
Feb  6 18:08:02.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.009905305s
Feb  6 18:08:04.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008909929s
Feb  6 18:08:06.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008864456s
Feb  6 18:08:08.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.00974751s
Feb  6 18:08:10.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.008676627s
Feb  6 18:08:12.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009250507s
Feb  6 18:08:14.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009038963s
Feb  6 18:08:16.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009914386s
Feb  6 18:08:18.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.010197865s
Feb  6 18:08:20.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009309739s
Feb  6 18:08:22.513: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.012621584s
Feb  6 18:08:24.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008637704s
Feb  6 18:08:26.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.00916966s
Feb  6 18:08:28.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009622981s
Feb  6 18:08:30.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.010000838s
Feb  6 18:08:32.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.009467251s
Feb  6 18:08:34.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.00872149s
Feb  6 18:08:36.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.009100099s
Feb  6 18:08:38.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.010019773s
Feb  6 18:08:40.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009895236s
Feb  6 18:08:42.514: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.012911215s
Feb  6 18:08:44.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009114583s
Feb  6 18:08:46.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.010168984s
Feb  6 18:08:48.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009154838s
Feb  6 18:08:50.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009148647s
Feb  6 18:08:52.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.010682064s
Feb  6 18:08:54.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008638016s
Feb  6 18:08:56.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.011336155s
Feb  6 18:08:58.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.009813645s
Feb  6 18:09:00.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.008761072s
Feb  6 18:09:02.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.009568165s
Feb  6 18:09:04.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.011584705s
Feb  6 18:09:06.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008919836s
Feb  6 18:09:08.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009653577s
Feb  6 18:09:10.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009175333s
Feb  6 18:09:12.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009070078s
Feb  6 18:09:14.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009184641s
Feb  6 18:09:16.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.00922789s
Feb  6 18:09:18.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009502669s
Feb  6 18:09:20.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.010822423s
Feb  6 18:09:22.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008963416s
Feb  6 18:09:24.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.011177718s
Feb  6 18:09:26.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.010976226s
Feb  6 18:09:28.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008954987s
Feb  6 18:09:30.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009263989s
Feb  6 18:09:32.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00966239s
Feb  6 18:09:34.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.008746302s
Feb  6 18:09:36.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.008948886s
Feb  6 18:09:38.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009106819s
Feb  6 18:09:40.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.009937052s
Feb  6 18:09:42.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.019918159s
Feb  6 18:09:44.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.00872511s
Feb  6 18:09:46.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.00873818s
Feb  6 18:09:48.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009495696s
Feb  6 18:09:50.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.010337952s
Feb  6 18:09:52.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.009695477s
Feb  6 18:09:54.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008886813s
Feb  6 18:09:56.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.01021767s
Feb  6 18:09:58.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009833587s
Feb  6 18:10:00.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010497969s
Feb  6 18:10:02.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.009223467s
Feb  6 18:10:04.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009673917s
Feb  6 18:10:06.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.009660305s
Feb  6 18:10:08.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009220831s
Feb  6 18:10:10.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.011173835s
Feb  6 18:10:12.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009391381s
Feb  6 18:10:12.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.018153308s
STEP: removing the label kubernetes.io/e2e-72cb9b8b-8584-46a9-bc3d-68d2462a964e off the node tneyla25-md-0-68cbcb4798-5xxmw 02/06/23 18:10:12.519
STEP: verifying the node doesn't have the label kubernetes.io/e2e-72cb9b8b-8584-46a9-bc3d-68d2462a964e 02/06/23 18:10:12.541
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb  6 18:10:12.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-36" for this suite. 02/06/23 18:10:12.552
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":177,"skipped":3427,"failed":0}
------------------------------
• [SLOW TEST] [308.260 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:05:04.305
    Feb  6 18:05:04.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename sched-pred 02/06/23 18:05:04.306
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:05:04.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:05:04.332
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb  6 18:05:04.337: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb  6 18:05:04.351: INFO: Waiting for terminating namespaces to be deleted...
    Feb  6 18:05:04.356: INFO: 
    Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-5xxmw before test
    Feb  6 18:05:04.369: INFO: cilium-9gdv4 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.369: INFO: 	Container cilium-agent ready: true, restart count 0
    Feb  6 18:05:04.369: INFO: kube-proxy-mnvm8 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.369: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb  6 18:05:04.369: INFO: vsphere-cloud-controller-manager-7dmv6 from kube-system started at 2023-02-06 17:31:18 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.369: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
    Feb  6 18:05:04.369: INFO: vsphere-csi-node-7l6qr from kube-system started at 2023-02-06 17:12:22 +0000 UTC (3 container statuses recorded)
    Feb  6 18:05:04.369: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb  6 18:05:04.370: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb  6 18:05:04.370: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Feb  6 18:05:04.370: INFO: agnhost from proxy-8623 started at 2023-02-06 18:05:00 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.370: INFO: 	Container agnhost ready: true, restart count 0
    Feb  6 18:05:04.370: INFO: sonobuoy from sonobuoy started at 2023-02-06 17:18:54 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.371: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb  6 18:05:04.371: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-cr8qs from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 18:05:04.371: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 18:05:04.371: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb  6 18:05:04.371: INFO: 
    Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-6j94p before test
    Feb  6 18:05:04.387: INFO: capi-kubeadm-control-plane-controller-manager-5f686d9d47-lkld2 from capi-kubeadm-control-plane-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.387: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:05:04.387: INFO: capi-controller-manager-5b68b88d59-vcpp2 from capi-system started at 2023-02-06 17:15:34 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.387: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:05:04.387: INFO: cert-manager-7b9fcd6db9-rz78g from cert-manager started at 2023-02-06 17:15:16 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.387: INFO: 	Container cert-manager ready: true, restart count 0
    Feb  6 18:05:04.387: INFO: cert-manager-cainjector-75b587b679-rjbzj from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.387: INFO: 	Container cert-manager ready: true, restart count 0
    Feb  6 18:05:04.387: INFO: cert-manager-webhook-896b549c4-hj9c4 from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.387: INFO: 	Container cert-manager ready: true, restart count 0
    Feb  6 18:05:04.387: INFO: eks-anywhere-packages-8589bd5d9c-4pc9v from eksa-packages started at 2023-02-06 17:17:15 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.387: INFO: 	Container controller ready: true, restart count 0
    Feb  6 18:05:04.388: INFO: etcdadm-bootstrap-provider-controller-manager-84ff796964-d6tt8 from etcdadm-bootstrap-provider-system started at 2023-02-06 17:15:37 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.388: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:05:04.388: INFO: cilium-nb887 from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.388: INFO: 	Container cilium-agent ready: true, restart count 0
    Feb  6 18:05:04.388: INFO: kube-proxy-ld4nv from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.388: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb  6 18:05:04.388: INFO: vsphere-cloud-controller-manager-76l5k from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.388: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
    Feb  6 18:05:04.388: INFO: vsphere-csi-node-2tfxk from kube-system started at 2023-02-06 17:12:26 +0000 UTC (3 container statuses recorded)
    Feb  6 18:05:04.388: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb  6 18:05:04.388: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb  6 18:05:04.388: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Feb  6 18:05:04.388: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-hcvcv from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 18:05:04.388: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 18:05:04.388: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb  6 18:05:04.388: INFO: 
    Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-lz542 before test
    Feb  6 18:05:04.399: INFO: capi-kubeadm-bootstrap-controller-manager-79d4bdf548-4xmjq from capi-kubeadm-bootstrap-system started at 2023-02-06 17:15:36 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.399: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: capv-controller-manager-598f9bf684-4qdws from capv-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.399: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: eksa-controller-manager-5588899966-wtcpc from eksa-system started at 2023-02-06 17:16:14 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.399: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: etcdadm-controller-controller-manager-767b7fb9b5-mt7lx from etcdadm-controller-system started at 2023-02-06 17:15:38 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.399: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: cilium-ndf4k from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.399: INFO: 	Container cilium-agent ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: cilium-operator-7f966b595f-pzn9c from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.399: INFO: 	Container cilium-operator ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: kube-proxy-hfblf from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.399: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: vsphere-cloud-controller-manager-x8k8p from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 18:05:04.399: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: vsphere-csi-node-v4zd6 from kube-system started at 2023-02-06 17:12:15 +0000 UTC (3 container statuses recorded)
    Feb  6 18:05:04.399: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: sonobuoy-e2e-job-b0e9e31f5d894166 from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 18:05:04.399: INFO: 	Container e2e ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-fncmf from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 18:05:04.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 18:05:04.399: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/06/23 18:05:04.399
    Feb  6 18:05:04.410: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-36" to be "running"
    Feb  6 18:05:04.417: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.752918ms
    Feb  6 18:05:06.424: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014311785s
    Feb  6 18:05:08.422: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.012178243s
    Feb  6 18:05:08.422: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/06/23 18:05:08.426
    STEP: Trying to apply a random label on the found node. 02/06/23 18:05:08.445
    STEP: verifying the node has the label kubernetes.io/e2e-72cb9b8b-8584-46a9-bc3d-68d2462a964e 95 02/06/23 18:05:08.46
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/06/23 18:05:08.466
    Feb  6 18:05:08.478: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-36" to be "not pending"
    Feb  6 18:05:08.482: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095776ms
    Feb  6 18:05:10.490: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012021648s
    Feb  6 18:05:12.491: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.012920064s
    Feb  6 18:05:12.491: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 195.17.4.133 on the node which pod4 resides and expect not scheduled 02/06/23 18:05:12.491
    Feb  6 18:05:12.501: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-36" to be "not pending"
    Feb  6 18:05:12.505: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.587035ms
    Feb  6 18:05:14.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008656724s
    Feb  6 18:05:16.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009057762s
    Feb  6 18:05:18.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009151158s
    Feb  6 18:05:20.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008866258s
    Feb  6 18:05:22.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009818222s
    Feb  6 18:05:24.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009163959s
    Feb  6 18:05:26.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010613312s
    Feb  6 18:05:28.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008493179s
    Feb  6 18:05:30.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010696729s
    Feb  6 18:05:32.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010660343s
    Feb  6 18:05:34.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010818486s
    Feb  6 18:05:36.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009604366s
    Feb  6 18:05:38.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009044651s
    Feb  6 18:05:40.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009923261s
    Feb  6 18:05:42.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010801862s
    Feb  6 18:05:44.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.009754904s
    Feb  6 18:05:46.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009148011s
    Feb  6 18:05:48.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008899979s
    Feb  6 18:05:50.513: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012263158s
    Feb  6 18:05:52.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008950713s
    Feb  6 18:05:54.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009026065s
    Feb  6 18:05:56.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.0107275s
    Feb  6 18:05:58.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009705293s
    Feb  6 18:06:00.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011155971s
    Feb  6 18:06:02.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010252019s
    Feb  6 18:06:04.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010471136s
    Feb  6 18:06:06.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008681227s
    Feb  6 18:06:08.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010276903s
    Feb  6 18:06:10.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009513917s
    Feb  6 18:06:12.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009742844s
    Feb  6 18:06:14.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008488363s
    Feb  6 18:06:16.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009837022s
    Feb  6 18:06:18.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008869254s
    Feb  6 18:06:20.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009386834s
    Feb  6 18:06:22.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009911455s
    Feb  6 18:06:24.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009267112s
    Feb  6 18:06:26.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009325789s
    Feb  6 18:06:28.513: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011906799s
    Feb  6 18:06:30.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.010141037s
    Feb  6 18:06:32.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.00937891s
    Feb  6 18:06:34.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009671758s
    Feb  6 18:06:36.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010476837s
    Feb  6 18:06:38.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.0089185s
    Feb  6 18:06:40.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009995592s
    Feb  6 18:06:42.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010732127s
    Feb  6 18:06:44.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008842752s
    Feb  6 18:06:46.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010662149s
    Feb  6 18:06:48.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.0085529s
    Feb  6 18:06:50.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009791764s
    Feb  6 18:06:52.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010397859s
    Feb  6 18:06:54.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010762418s
    Feb  6 18:06:56.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009811866s
    Feb  6 18:06:58.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009254066s
    Feb  6 18:07:00.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.0087473s
    Feb  6 18:07:02.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009509698s
    Feb  6 18:07:04.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009282101s
    Feb  6 18:07:06.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009157091s
    Feb  6 18:07:08.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010058246s
    Feb  6 18:07:10.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009757199s
    Feb  6 18:07:12.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009049364s
    Feb  6 18:07:14.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.00867162s
    Feb  6 18:07:16.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.00937392s
    Feb  6 18:07:18.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009638647s
    Feb  6 18:07:20.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.009785192s
    Feb  6 18:07:22.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.010175347s
    Feb  6 18:07:24.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009639148s
    Feb  6 18:07:26.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009977033s
    Feb  6 18:07:28.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009319812s
    Feb  6 18:07:30.513: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.011754801s
    Feb  6 18:07:32.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.009296276s
    Feb  6 18:07:34.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008927447s
    Feb  6 18:07:36.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009847679s
    Feb  6 18:07:38.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.009830667s
    Feb  6 18:07:40.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009594825s
    Feb  6 18:07:42.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.010191017s
    Feb  6 18:07:44.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.011296605s
    Feb  6 18:07:46.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008770204s
    Feb  6 18:07:48.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.009115206s
    Feb  6 18:07:50.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.01101136s
    Feb  6 18:07:52.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009325343s
    Feb  6 18:07:54.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009366636s
    Feb  6 18:07:56.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.011070764s
    Feb  6 18:07:58.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009770655s
    Feb  6 18:08:00.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009375211s
    Feb  6 18:08:02.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.009905305s
    Feb  6 18:08:04.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008909929s
    Feb  6 18:08:06.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008864456s
    Feb  6 18:08:08.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.00974751s
    Feb  6 18:08:10.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.008676627s
    Feb  6 18:08:12.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009250507s
    Feb  6 18:08:14.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009038963s
    Feb  6 18:08:16.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009914386s
    Feb  6 18:08:18.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.010197865s
    Feb  6 18:08:20.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009309739s
    Feb  6 18:08:22.513: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.012621584s
    Feb  6 18:08:24.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008637704s
    Feb  6 18:08:26.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.00916966s
    Feb  6 18:08:28.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009622981s
    Feb  6 18:08:30.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.010000838s
    Feb  6 18:08:32.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.009467251s
    Feb  6 18:08:34.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.00872149s
    Feb  6 18:08:36.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.009100099s
    Feb  6 18:08:38.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.010019773s
    Feb  6 18:08:40.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009895236s
    Feb  6 18:08:42.514: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.012911215s
    Feb  6 18:08:44.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009114583s
    Feb  6 18:08:46.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.010168984s
    Feb  6 18:08:48.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009154838s
    Feb  6 18:08:50.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009148647s
    Feb  6 18:08:52.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.010682064s
    Feb  6 18:08:54.509: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008638016s
    Feb  6 18:08:56.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.011336155s
    Feb  6 18:08:58.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.009813645s
    Feb  6 18:09:00.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.008761072s
    Feb  6 18:09:02.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.009568165s
    Feb  6 18:09:04.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.011584705s
    Feb  6 18:09:06.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008919836s
    Feb  6 18:09:08.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009653577s
    Feb  6 18:09:10.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009175333s
    Feb  6 18:09:12.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009070078s
    Feb  6 18:09:14.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009184641s
    Feb  6 18:09:16.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.00922789s
    Feb  6 18:09:18.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009502669s
    Feb  6 18:09:20.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.010822423s
    Feb  6 18:09:22.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008963416s
    Feb  6 18:09:24.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.011177718s
    Feb  6 18:09:26.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.010976226s
    Feb  6 18:09:28.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008954987s
    Feb  6 18:09:30.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009263989s
    Feb  6 18:09:32.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00966239s
    Feb  6 18:09:34.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.008746302s
    Feb  6 18:09:36.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.008948886s
    Feb  6 18:09:38.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009106819s
    Feb  6 18:09:40.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.009937052s
    Feb  6 18:09:42.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.019918159s
    Feb  6 18:09:44.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.00872511s
    Feb  6 18:09:46.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.00873818s
    Feb  6 18:09:48.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009495696s
    Feb  6 18:09:50.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.010337952s
    Feb  6 18:09:52.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.009695477s
    Feb  6 18:09:54.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008886813s
    Feb  6 18:09:56.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.01021767s
    Feb  6 18:09:58.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009833587s
    Feb  6 18:10:00.511: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010497969s
    Feb  6 18:10:02.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.009223467s
    Feb  6 18:10:04.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009673917s
    Feb  6 18:10:06.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.009660305s
    Feb  6 18:10:08.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009220831s
    Feb  6 18:10:10.512: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.011173835s
    Feb  6 18:10:12.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009391381s
    Feb  6 18:10:12.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.018153308s
    STEP: removing the label kubernetes.io/e2e-72cb9b8b-8584-46a9-bc3d-68d2462a964e off the node tneyla25-md-0-68cbcb4798-5xxmw 02/06/23 18:10:12.519
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-72cb9b8b-8584-46a9-bc3d-68d2462a964e 02/06/23 18:10:12.541
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 18:10:12.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-36" for this suite. 02/06/23 18:10:12.552
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:10:12.566
Feb  6 18:10:12.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename cronjob 02/06/23 18:10:12.567
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:10:12.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:10:12.595
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 02/06/23 18:10:12.598
STEP: Ensuring a job is scheduled 02/06/23 18:10:12.607
STEP: Ensuring exactly one is scheduled 02/06/23 18:11:00.615
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/06/23 18:11:00.619
STEP: Ensuring the job is replaced with a new one 02/06/23 18:11:00.623
STEP: Removing cronjob 02/06/23 18:12:00.629
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb  6 18:12:00.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9765" for this suite. 02/06/23 18:12:00.642
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":178,"skipped":3436,"failed":0}
------------------------------
• [SLOW TEST] [108.086 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:10:12.566
    Feb  6 18:10:12.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename cronjob 02/06/23 18:10:12.567
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:10:12.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:10:12.595
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 02/06/23 18:10:12.598
    STEP: Ensuring a job is scheduled 02/06/23 18:10:12.607
    STEP: Ensuring exactly one is scheduled 02/06/23 18:11:00.615
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/06/23 18:11:00.619
    STEP: Ensuring the job is replaced with a new one 02/06/23 18:11:00.623
    STEP: Removing cronjob 02/06/23 18:12:00.629
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb  6 18:12:00.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9765" for this suite. 02/06/23 18:12:00.642
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:12:00.653
Feb  6 18:12:00.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename replicaset 02/06/23 18:12:00.654
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:00.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:00.685
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 02/06/23 18:12:00.693
STEP: Verify that the required pods have come up. 02/06/23 18:12:00.701
Feb  6 18:12:00.705: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb  6 18:12:05.713: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/06/23 18:12:05.713
STEP: Getting /status 02/06/23 18:12:05.713
Feb  6 18:12:05.718: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 02/06/23 18:12:05.718
Feb  6 18:12:05.730: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 02/06/23 18:12:05.73
Feb  6 18:12:05.733: INFO: Observed &ReplicaSet event: ADDED
Feb  6 18:12:05.733: INFO: Observed &ReplicaSet event: MODIFIED
Feb  6 18:12:05.733: INFO: Observed &ReplicaSet event: MODIFIED
Feb  6 18:12:05.734: INFO: Observed &ReplicaSet event: MODIFIED
Feb  6 18:12:05.734: INFO: Found replicaset test-rs in namespace replicaset-6345 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb  6 18:12:05.734: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 02/06/23 18:12:05.734
Feb  6 18:12:05.734: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb  6 18:12:05.742: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 02/06/23 18:12:05.742
Feb  6 18:12:05.745: INFO: Observed &ReplicaSet event: ADDED
Feb  6 18:12:05.745: INFO: Observed &ReplicaSet event: MODIFIED
Feb  6 18:12:05.746: INFO: Observed &ReplicaSet event: MODIFIED
Feb  6 18:12:05.746: INFO: Observed &ReplicaSet event: MODIFIED
Feb  6 18:12:05.746: INFO: Observed replicaset test-rs in namespace replicaset-6345 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb  6 18:12:05.746: INFO: Observed &ReplicaSet event: MODIFIED
Feb  6 18:12:05.746: INFO: Found replicaset test-rs in namespace replicaset-6345 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Feb  6 18:12:05.746: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb  6 18:12:05.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6345" for this suite. 02/06/23 18:12:05.755
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":179,"skipped":3436,"failed":0}
------------------------------
• [SLOW TEST] [5.112 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:12:00.653
    Feb  6 18:12:00.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename replicaset 02/06/23 18:12:00.654
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:00.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:00.685
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 02/06/23 18:12:00.693
    STEP: Verify that the required pods have come up. 02/06/23 18:12:00.701
    Feb  6 18:12:00.705: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb  6 18:12:05.713: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/06/23 18:12:05.713
    STEP: Getting /status 02/06/23 18:12:05.713
    Feb  6 18:12:05.718: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 02/06/23 18:12:05.718
    Feb  6 18:12:05.730: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 02/06/23 18:12:05.73
    Feb  6 18:12:05.733: INFO: Observed &ReplicaSet event: ADDED
    Feb  6 18:12:05.733: INFO: Observed &ReplicaSet event: MODIFIED
    Feb  6 18:12:05.733: INFO: Observed &ReplicaSet event: MODIFIED
    Feb  6 18:12:05.734: INFO: Observed &ReplicaSet event: MODIFIED
    Feb  6 18:12:05.734: INFO: Found replicaset test-rs in namespace replicaset-6345 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb  6 18:12:05.734: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 02/06/23 18:12:05.734
    Feb  6 18:12:05.734: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb  6 18:12:05.742: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 02/06/23 18:12:05.742
    Feb  6 18:12:05.745: INFO: Observed &ReplicaSet event: ADDED
    Feb  6 18:12:05.745: INFO: Observed &ReplicaSet event: MODIFIED
    Feb  6 18:12:05.746: INFO: Observed &ReplicaSet event: MODIFIED
    Feb  6 18:12:05.746: INFO: Observed &ReplicaSet event: MODIFIED
    Feb  6 18:12:05.746: INFO: Observed replicaset test-rs in namespace replicaset-6345 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb  6 18:12:05.746: INFO: Observed &ReplicaSet event: MODIFIED
    Feb  6 18:12:05.746: INFO: Found replicaset test-rs in namespace replicaset-6345 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Feb  6 18:12:05.746: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb  6 18:12:05.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6345" for this suite. 02/06/23 18:12:05.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:12:05.768
Feb  6 18:12:05.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename deployment 02/06/23 18:12:05.769
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:05.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:05.792
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Feb  6 18:12:05.807: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb  6 18:12:10.812: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/06/23 18:12:10.812
Feb  6 18:12:10.812: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb  6 18:12:12.817: INFO: Creating deployment "test-rollover-deployment"
Feb  6 18:12:12.832: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb  6 18:12:14.845: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb  6 18:12:14.852: INFO: Ensure that both replica sets have 1 created replica
Feb  6 18:12:14.859: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb  6 18:12:14.871: INFO: Updating deployment test-rollover-deployment
Feb  6 18:12:14.871: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb  6 18:12:16.880: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb  6 18:12:16.887: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb  6 18:12:16.895: INFO: all replica sets need to contain the pod-template-hash label
Feb  6 18:12:16.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 18:12:18.905: INFO: all replica sets need to contain the pod-template-hash label
Feb  6 18:12:18.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 18:12:20.905: INFO: all replica sets need to contain the pod-template-hash label
Feb  6 18:12:20.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 18:12:22.903: INFO: all replica sets need to contain the pod-template-hash label
Feb  6 18:12:22.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 18:12:24.907: INFO: all replica sets need to contain the pod-template-hash label
Feb  6 18:12:24.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 18:12:26.905: INFO: all replica sets need to contain the pod-template-hash label
Feb  6 18:12:26.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 18:12:28.904: INFO: 
Feb  6 18:12:28.904: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb  6 18:12:28.914: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3849  1c28eee4-7c1a-4bf3-a03c-977336097d46 47279 2 2023-02-06 18:12:12 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-06 18:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:12:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049b8cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-06 18:12:12 +0000 UTC,LastTransitionTime:2023-02-06 18:12:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-02-06 18:12:27 +0000 UTC,LastTransitionTime:2023-02-06 18:12:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb  6 18:12:28.918: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3849  9dc93afe-69c4-4afe-a72f-0266cb3049f5 47266 2 2023-02-06 18:12:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1c28eee4-7c1a-4bf3-a03c-977336097d46 0xc00095f4b7 0xc00095f4b8}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28eee4-7c1a-4bf3-a03c-977336097d46\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:12:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00095f568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb  6 18:12:28.918: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb  6 18:12:28.918: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3849  61a09da5-2655-45c8-8d0b-1da91561cf55 47277 2 2023-02-06 18:12:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1c28eee4-7c1a-4bf3-a03c-977336097d46 0xc00095ef67 0xc00095ef68}] [] [{e2e.test Update apps/v1 2023-02-06 18:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:12:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28eee4-7c1a-4bf3-a03c-977336097d46\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:12:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00095f1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb  6 18:12:28.918: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3849  ab6881d8-05dc-4edc-bc95-a76d6a4caa8c 47120 2 2023-02-06 18:12:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1c28eee4-7c1a-4bf3-a03c-977336097d46 0xc00095f367 0xc00095f368}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28eee4-7c1a-4bf3-a03c-977336097d46\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:12:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00095f428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb  6 18:12:28.923: INFO: Pod "test-rollover-deployment-6d45fd857b-5mzl7" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-5mzl7 test-rollover-deployment-6d45fd857b- deployment-3849  4451dd72-e2f4-44cc-b3d8-7a4943b9eaf5 47167 0 2023-02-06 18:12:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 9dc93afe-69c4-4afe-a72f-0266cb3049f5 0xc0049b9267 0xc0049b9268}] [] [{kube-controller-manager Update v1 2023-02-06 18:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9dc93afe-69c4-4afe-a72f-0266cb3049f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 18:12:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qpll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qpll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:12:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:12:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:12:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:12:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.88,StartTime:2023-02-06 18:12:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 18:12:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://af3fa3769b5ff70bba3357b7896fa0f473f5e2dc90d9fc73858d61404837ae11,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb  6 18:12:28.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3849" for this suite. 02/06/23 18:12:28.928
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":180,"skipped":3443,"failed":0}
------------------------------
• [SLOW TEST] [23.170 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:12:05.768
    Feb  6 18:12:05.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename deployment 02/06/23 18:12:05.769
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:05.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:05.792
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Feb  6 18:12:05.807: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Feb  6 18:12:10.812: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/06/23 18:12:10.812
    Feb  6 18:12:10.812: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Feb  6 18:12:12.817: INFO: Creating deployment "test-rollover-deployment"
    Feb  6 18:12:12.832: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Feb  6 18:12:14.845: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Feb  6 18:12:14.852: INFO: Ensure that both replica sets have 1 created replica
    Feb  6 18:12:14.859: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Feb  6 18:12:14.871: INFO: Updating deployment test-rollover-deployment
    Feb  6 18:12:14.871: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Feb  6 18:12:16.880: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Feb  6 18:12:16.887: INFO: Make sure deployment "test-rollover-deployment" is complete
    Feb  6 18:12:16.895: INFO: all replica sets need to contain the pod-template-hash label
    Feb  6 18:12:16.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 18:12:18.905: INFO: all replica sets need to contain the pod-template-hash label
    Feb  6 18:12:18.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 18:12:20.905: INFO: all replica sets need to contain the pod-template-hash label
    Feb  6 18:12:20.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 18:12:22.903: INFO: all replica sets need to contain the pod-template-hash label
    Feb  6 18:12:22.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 18:12:24.907: INFO: all replica sets need to contain the pod-template-hash label
    Feb  6 18:12:24.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 18:12:26.905: INFO: all replica sets need to contain the pod-template-hash label
    Feb  6 18:12:26.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 18:12:28.904: INFO: 
    Feb  6 18:12:28.904: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb  6 18:12:28.914: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3849  1c28eee4-7c1a-4bf3-a03c-977336097d46 47279 2 2023-02-06 18:12:12 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-06 18:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:12:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049b8cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-06 18:12:12 +0000 UTC,LastTransitionTime:2023-02-06 18:12:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-02-06 18:12:27 +0000 UTC,LastTransitionTime:2023-02-06 18:12:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb  6 18:12:28.918: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3849  9dc93afe-69c4-4afe-a72f-0266cb3049f5 47266 2 2023-02-06 18:12:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1c28eee4-7c1a-4bf3-a03c-977336097d46 0xc00095f4b7 0xc00095f4b8}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28eee4-7c1a-4bf3-a03c-977336097d46\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:12:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00095f568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb  6 18:12:28.918: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Feb  6 18:12:28.918: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3849  61a09da5-2655-45c8-8d0b-1da91561cf55 47277 2 2023-02-06 18:12:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1c28eee4-7c1a-4bf3-a03c-977336097d46 0xc00095ef67 0xc00095ef68}] [] [{e2e.test Update apps/v1 2023-02-06 18:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:12:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28eee4-7c1a-4bf3-a03c-977336097d46\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:12:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00095f1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb  6 18:12:28.918: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3849  ab6881d8-05dc-4edc-bc95-a76d6a4caa8c 47120 2 2023-02-06 18:12:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1c28eee4-7c1a-4bf3-a03c-977336097d46 0xc00095f367 0xc00095f368}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28eee4-7c1a-4bf3-a03c-977336097d46\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:12:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00095f428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb  6 18:12:28.923: INFO: Pod "test-rollover-deployment-6d45fd857b-5mzl7" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-5mzl7 test-rollover-deployment-6d45fd857b- deployment-3849  4451dd72-e2f4-44cc-b3d8-7a4943b9eaf5 47167 0 2023-02-06 18:12:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 9dc93afe-69c4-4afe-a72f-0266cb3049f5 0xc0049b9267 0xc0049b9268}] [] [{kube-controller-manager Update v1 2023-02-06 18:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9dc93afe-69c4-4afe-a72f-0266cb3049f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 18:12:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qpll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qpll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:12:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:12:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:12:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:12:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.88,StartTime:2023-02-06 18:12:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 18:12:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://af3fa3769b5ff70bba3357b7896fa0f473f5e2dc90d9fc73858d61404837ae11,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb  6 18:12:28.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3849" for this suite. 02/06/23 18:12:28.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:12:28.939
Feb  6 18:12:28.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename containers 02/06/23 18:12:28.94
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:28.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:28.965
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 02/06/23 18:12:28.967
Feb  6 18:12:28.976: INFO: Waiting up to 5m0s for pod "client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8" in namespace "containers-6042" to be "Succeeded or Failed"
Feb  6 18:12:28.982: INFO: Pod "client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.283819ms
Feb  6 18:12:30.987: INFO: Pod "client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011056327s
Feb  6 18:12:32.988: INFO: Pod "client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012290072s
Feb  6 18:12:34.988: INFO: Pod "client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011424783s
STEP: Saw pod success 02/06/23 18:12:34.988
Feb  6 18:12:34.988: INFO: Pod "client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8" satisfied condition "Succeeded or Failed"
Feb  6 18:12:34.992: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8 container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:12:35.009
Feb  6 18:12:35.027: INFO: Waiting for pod client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8 to disappear
Feb  6 18:12:35.031: INFO: Pod client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb  6 18:12:35.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6042" for this suite. 02/06/23 18:12:35.038
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":181,"skipped":3453,"failed":0}
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:12:28.939
    Feb  6 18:12:28.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename containers 02/06/23 18:12:28.94
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:28.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:28.965
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 02/06/23 18:12:28.967
    Feb  6 18:12:28.976: INFO: Waiting up to 5m0s for pod "client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8" in namespace "containers-6042" to be "Succeeded or Failed"
    Feb  6 18:12:28.982: INFO: Pod "client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.283819ms
    Feb  6 18:12:30.987: INFO: Pod "client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011056327s
    Feb  6 18:12:32.988: INFO: Pod "client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012290072s
    Feb  6 18:12:34.988: INFO: Pod "client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011424783s
    STEP: Saw pod success 02/06/23 18:12:34.988
    Feb  6 18:12:34.988: INFO: Pod "client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8" satisfied condition "Succeeded or Failed"
    Feb  6 18:12:34.992: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8 container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:12:35.009
    Feb  6 18:12:35.027: INFO: Waiting for pod client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8 to disappear
    Feb  6 18:12:35.031: INFO: Pod client-containers-fece78b1-5758-4585-adbb-d96b435c5ad8 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb  6 18:12:35.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-6042" for this suite. 02/06/23 18:12:35.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:12:35.047
Feb  6 18:12:35.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename custom-resource-definition 02/06/23 18:12:35.048
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:35.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:35.071
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Feb  6 18:12:35.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:12:36.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5951" for this suite. 02/06/23 18:12:36.114
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":182,"skipped":3462,"failed":0}
------------------------------
• [1.078 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:12:35.047
    Feb  6 18:12:35.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename custom-resource-definition 02/06/23 18:12:35.048
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:35.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:35.071
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Feb  6 18:12:35.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:12:36.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-5951" for this suite. 02/06/23 18:12:36.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:12:36.132
Feb  6 18:12:36.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename custom-resource-definition 02/06/23 18:12:36.133
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:36.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:36.159
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Feb  6 18:12:36.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:12:43.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5663" for this suite. 02/06/23 18:12:43.147
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":183,"skipped":3495,"failed":0}
------------------------------
• [SLOW TEST] [7.027 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:12:36.132
    Feb  6 18:12:36.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename custom-resource-definition 02/06/23 18:12:36.133
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:36.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:36.159
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Feb  6 18:12:36.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:12:43.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-5663" for this suite. 02/06/23 18:12:43.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:12:43.159
Feb  6 18:12:43.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename replication-controller 02/06/23 18:12:43.16
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:43.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:43.192
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 02/06/23 18:12:43.195
STEP: When the matched label of one of its pods change 02/06/23 18:12:43.206
Feb  6 18:12:43.212: INFO: Pod name pod-release: Found 0 pods out of 1
Feb  6 18:12:48.218: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 02/06/23 18:12:48.237
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb  6 18:12:49.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9491" for this suite. 02/06/23 18:12:49.253
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":184,"skipped":3500,"failed":0}
------------------------------
• [SLOW TEST] [6.103 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:12:43.159
    Feb  6 18:12:43.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename replication-controller 02/06/23 18:12:43.16
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:43.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:43.192
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 02/06/23 18:12:43.195
    STEP: When the matched label of one of its pods change 02/06/23 18:12:43.206
    Feb  6 18:12:43.212: INFO: Pod name pod-release: Found 0 pods out of 1
    Feb  6 18:12:48.218: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/06/23 18:12:48.237
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb  6 18:12:49.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9491" for this suite. 02/06/23 18:12:49.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:12:49.268
Feb  6 18:12:49.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pods 02/06/23 18:12:49.269
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:49.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:49.292
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Feb  6 18:12:49.304: INFO: Waiting up to 5m0s for pod "server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69" in namespace "pods-8470" to be "running and ready"
Feb  6 18:12:49.312: INFO: Pod "server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821112ms
Feb  6 18:12:49.312: INFO: The phase of Pod server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:12:51.318: INFO: Pod "server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0140886s
Feb  6 18:12:51.318: INFO: The phase of Pod server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:12:53.323: INFO: Pod "server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69": Phase="Running", Reason="", readiness=true. Elapsed: 4.019372327s
Feb  6 18:12:53.323: INFO: The phase of Pod server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69 is Running (Ready = true)
Feb  6 18:12:53.323: INFO: Pod "server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69" satisfied condition "running and ready"
Feb  6 18:12:53.358: INFO: Waiting up to 5m0s for pod "client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b" in namespace "pods-8470" to be "Succeeded or Failed"
Feb  6 18:12:53.362: INFO: Pod "client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.465353ms
Feb  6 18:12:55.368: INFO: Pod "client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009295288s
Feb  6 18:12:57.367: INFO: Pod "client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009110618s
Feb  6 18:12:59.367: INFO: Pod "client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008920513s
STEP: Saw pod success 02/06/23 18:12:59.367
Feb  6 18:12:59.368: INFO: Pod "client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b" satisfied condition "Succeeded or Failed"
Feb  6 18:12:59.372: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b container env3cont: <nil>
STEP: delete the pod 02/06/23 18:12:59.38
Feb  6 18:12:59.397: INFO: Waiting for pod client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b to disappear
Feb  6 18:12:59.400: INFO: Pod client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb  6 18:12:59.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8470" for this suite. 02/06/23 18:12:59.406
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":185,"skipped":3534,"failed":0}
------------------------------
• [SLOW TEST] [10.145 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:12:49.268
    Feb  6 18:12:49.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pods 02/06/23 18:12:49.269
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:49.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:49.292
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Feb  6 18:12:49.304: INFO: Waiting up to 5m0s for pod "server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69" in namespace "pods-8470" to be "running and ready"
    Feb  6 18:12:49.312: INFO: Pod "server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821112ms
    Feb  6 18:12:49.312: INFO: The phase of Pod server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:12:51.318: INFO: Pod "server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0140886s
    Feb  6 18:12:51.318: INFO: The phase of Pod server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:12:53.323: INFO: Pod "server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69": Phase="Running", Reason="", readiness=true. Elapsed: 4.019372327s
    Feb  6 18:12:53.323: INFO: The phase of Pod server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69 is Running (Ready = true)
    Feb  6 18:12:53.323: INFO: Pod "server-envvars-3e6dc1da-13d2-4eb0-9d27-820c25efff69" satisfied condition "running and ready"
    Feb  6 18:12:53.358: INFO: Waiting up to 5m0s for pod "client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b" in namespace "pods-8470" to be "Succeeded or Failed"
    Feb  6 18:12:53.362: INFO: Pod "client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.465353ms
    Feb  6 18:12:55.368: INFO: Pod "client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009295288s
    Feb  6 18:12:57.367: INFO: Pod "client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009110618s
    Feb  6 18:12:59.367: INFO: Pod "client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008920513s
    STEP: Saw pod success 02/06/23 18:12:59.367
    Feb  6 18:12:59.368: INFO: Pod "client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b" satisfied condition "Succeeded or Failed"
    Feb  6 18:12:59.372: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b container env3cont: <nil>
    STEP: delete the pod 02/06/23 18:12:59.38
    Feb  6 18:12:59.397: INFO: Waiting for pod client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b to disappear
    Feb  6 18:12:59.400: INFO: Pod client-envvars-35cbde08-9594-4dbd-88f7-a2fb580a2e1b no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb  6 18:12:59.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8470" for this suite. 02/06/23 18:12:59.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:12:59.418
Feb  6 18:12:59.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 18:12:59.418
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:59.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:59.439
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 18:12:59.462
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:12:59.787
STEP: Deploying the webhook pod 02/06/23 18:12:59.797
STEP: Wait for the deployment to be ready 02/06/23 18:12:59.811
Feb  6 18:12:59.824: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 18:13:01.836: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 18:13:03.841
STEP: Verifying the service has paired with the endpoint 02/06/23 18:13:03.863
Feb  6 18:13:04.864: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 02/06/23 18:13:04.957
STEP: Creating a configMap that should be mutated 02/06/23 18:13:04.972
STEP: Deleting the collection of validation webhooks 02/06/23 18:13:05.006
STEP: Creating a configMap that should not be mutated 02/06/23 18:13:05.093
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:13:05.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8790" for this suite. 02/06/23 18:13:05.115
STEP: Destroying namespace "webhook-8790-markers" for this suite. 02/06/23 18:13:05.124
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":186,"skipped":3561,"failed":0}
------------------------------
• [SLOW TEST] [5.776 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:12:59.418
    Feb  6 18:12:59.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 18:12:59.418
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:12:59.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:12:59.439
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 18:12:59.462
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:12:59.787
    STEP: Deploying the webhook pod 02/06/23 18:12:59.797
    STEP: Wait for the deployment to be ready 02/06/23 18:12:59.811
    Feb  6 18:12:59.824: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 18:13:01.836: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 12, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 12, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 18:13:03.841
    STEP: Verifying the service has paired with the endpoint 02/06/23 18:13:03.863
    Feb  6 18:13:04.864: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 02/06/23 18:13:04.957
    STEP: Creating a configMap that should be mutated 02/06/23 18:13:04.972
    STEP: Deleting the collection of validation webhooks 02/06/23 18:13:05.006
    STEP: Creating a configMap that should not be mutated 02/06/23 18:13:05.093
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:13:05.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8790" for this suite. 02/06/23 18:13:05.115
    STEP: Destroying namespace "webhook-8790-markers" for this suite. 02/06/23 18:13:05.124
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:13:05.199
Feb  6 18:13:05.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 18:13:05.2
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:13:05.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:13:05.234
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 18:13:05.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8385" for this suite. 02/06/23 18:13:05.289
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":187,"skipped":3570,"failed":0}
------------------------------
• [0.099 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:13:05.199
    Feb  6 18:13:05.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 18:13:05.2
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:13:05.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:13:05.234
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 18:13:05.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8385" for this suite. 02/06/23 18:13:05.289
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:13:05.3
Feb  6 18:13:05.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename var-expansion 02/06/23 18:13:05.302
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:13:05.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:13:05.331
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Feb  6 18:13:05.347: INFO: Waiting up to 2m0s for pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139" in namespace "var-expansion-1472" to be "container 0 failed with reason CreateContainerConfigError"
Feb  6 18:13:05.355: INFO: Pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139": Phase="Pending", Reason="", readiness=false. Elapsed: 7.163423ms
Feb  6 18:13:07.359: INFO: Pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011695979s
Feb  6 18:13:09.359: INFO: Pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01172817s
Feb  6 18:13:09.359: INFO: Pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb  6 18:13:09.359: INFO: Deleting pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139" in namespace "var-expansion-1472"
Feb  6 18:13:09.372: INFO: Wait up to 5m0s for pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb  6 18:13:11.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1472" for this suite. 02/06/23 18:13:11.388
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":188,"skipped":3571,"failed":0}
------------------------------
• [SLOW TEST] [6.097 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:13:05.3
    Feb  6 18:13:05.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename var-expansion 02/06/23 18:13:05.302
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:13:05.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:13:05.331
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Feb  6 18:13:05.347: INFO: Waiting up to 2m0s for pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139" in namespace "var-expansion-1472" to be "container 0 failed with reason CreateContainerConfigError"
    Feb  6 18:13:05.355: INFO: Pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139": Phase="Pending", Reason="", readiness=false. Elapsed: 7.163423ms
    Feb  6 18:13:07.359: INFO: Pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011695979s
    Feb  6 18:13:09.359: INFO: Pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01172817s
    Feb  6 18:13:09.359: INFO: Pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb  6 18:13:09.359: INFO: Deleting pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139" in namespace "var-expansion-1472"
    Feb  6 18:13:09.372: INFO: Wait up to 5m0s for pod "var-expansion-eb582dff-2359-4d6e-af91-cfd44f7fb139" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb  6 18:13:11.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1472" for this suite. 02/06/23 18:13:11.388
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:13:11.398
Feb  6 18:13:11.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename subpath 02/06/23 18:13:11.4
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:13:11.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:13:11.42
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/06/23 18:13:11.426
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-ql5r 02/06/23 18:13:11.438
STEP: Creating a pod to test atomic-volume-subpath 02/06/23 18:13:11.439
Feb  6 18:13:11.449: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ql5r" in namespace "subpath-9066" to be "Succeeded or Failed"
Feb  6 18:13:11.456: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Pending", Reason="", readiness=false. Elapsed: 7.107759ms
Feb  6 18:13:13.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012757009s
Feb  6 18:13:15.461: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 4.012481736s
Feb  6 18:13:17.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 6.013026137s
Feb  6 18:13:19.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 8.01324633s
Feb  6 18:13:21.461: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 10.012235142s
Feb  6 18:13:23.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 12.012763049s
Feb  6 18:13:25.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 14.012584827s
Feb  6 18:13:27.460: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 16.011396826s
Feb  6 18:13:29.461: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 18.012508135s
Feb  6 18:13:31.461: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 20.012290009s
Feb  6 18:13:33.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 22.013404431s
Feb  6 18:13:35.463: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=false. Elapsed: 24.013571072s
Feb  6 18:13:37.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.012730314s
STEP: Saw pod success 02/06/23 18:13:37.462
Feb  6 18:13:37.462: INFO: Pod "pod-subpath-test-secret-ql5r" satisfied condition "Succeeded or Failed"
Feb  6 18:13:37.466: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-subpath-test-secret-ql5r container test-container-subpath-secret-ql5r: <nil>
STEP: delete the pod 02/06/23 18:13:37.473
Feb  6 18:13:37.486: INFO: Waiting for pod pod-subpath-test-secret-ql5r to disappear
Feb  6 18:13:37.493: INFO: Pod pod-subpath-test-secret-ql5r no longer exists
STEP: Deleting pod pod-subpath-test-secret-ql5r 02/06/23 18:13:37.493
Feb  6 18:13:37.493: INFO: Deleting pod "pod-subpath-test-secret-ql5r" in namespace "subpath-9066"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb  6 18:13:37.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9066" for this suite. 02/06/23 18:13:37.504
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":189,"skipped":3571,"failed":0}
------------------------------
• [SLOW TEST] [26.114 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:13:11.398
    Feb  6 18:13:11.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename subpath 02/06/23 18:13:11.4
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:13:11.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:13:11.42
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/06/23 18:13:11.426
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-ql5r 02/06/23 18:13:11.438
    STEP: Creating a pod to test atomic-volume-subpath 02/06/23 18:13:11.439
    Feb  6 18:13:11.449: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ql5r" in namespace "subpath-9066" to be "Succeeded or Failed"
    Feb  6 18:13:11.456: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Pending", Reason="", readiness=false. Elapsed: 7.107759ms
    Feb  6 18:13:13.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012757009s
    Feb  6 18:13:15.461: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 4.012481736s
    Feb  6 18:13:17.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 6.013026137s
    Feb  6 18:13:19.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 8.01324633s
    Feb  6 18:13:21.461: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 10.012235142s
    Feb  6 18:13:23.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 12.012763049s
    Feb  6 18:13:25.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 14.012584827s
    Feb  6 18:13:27.460: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 16.011396826s
    Feb  6 18:13:29.461: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 18.012508135s
    Feb  6 18:13:31.461: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 20.012290009s
    Feb  6 18:13:33.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=true. Elapsed: 22.013404431s
    Feb  6 18:13:35.463: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Running", Reason="", readiness=false. Elapsed: 24.013571072s
    Feb  6 18:13:37.462: INFO: Pod "pod-subpath-test-secret-ql5r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.012730314s
    STEP: Saw pod success 02/06/23 18:13:37.462
    Feb  6 18:13:37.462: INFO: Pod "pod-subpath-test-secret-ql5r" satisfied condition "Succeeded or Failed"
    Feb  6 18:13:37.466: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-subpath-test-secret-ql5r container test-container-subpath-secret-ql5r: <nil>
    STEP: delete the pod 02/06/23 18:13:37.473
    Feb  6 18:13:37.486: INFO: Waiting for pod pod-subpath-test-secret-ql5r to disappear
    Feb  6 18:13:37.493: INFO: Pod pod-subpath-test-secret-ql5r no longer exists
    STEP: Deleting pod pod-subpath-test-secret-ql5r 02/06/23 18:13:37.493
    Feb  6 18:13:37.493: INFO: Deleting pod "pod-subpath-test-secret-ql5r" in namespace "subpath-9066"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb  6 18:13:37.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9066" for this suite. 02/06/23 18:13:37.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:13:37.515
Feb  6 18:13:37.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename gc 02/06/23 18:13:37.516
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:13:37.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:13:37.538
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 02/06/23 18:13:37.543
STEP: Wait for the Deployment to create new ReplicaSet 02/06/23 18:13:37.549
STEP: delete the deployment 02/06/23 18:13:38.063
STEP: wait for all rs to be garbage collected 02/06/23 18:13:38.083
STEP: expected 0 rs, got 1 rs 02/06/23 18:13:38.095
STEP: expected 0 pods, got 2 pods 02/06/23 18:13:38.101
STEP: Gathering metrics 02/06/23 18:13:38.612
Feb  6 18:13:38.649: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tneyla25-pnn8z" in namespace "kube-system" to be "running and ready"
Feb  6 18:13:38.653: INFO: Pod "kube-controller-manager-tneyla25-pnn8z": Phase="Running", Reason="", readiness=true. Elapsed: 3.819893ms
Feb  6 18:13:38.653: INFO: The phase of Pod kube-controller-manager-tneyla25-pnn8z is Running (Ready = true)
Feb  6 18:13:38.654: INFO: Pod "kube-controller-manager-tneyla25-pnn8z" satisfied condition "running and ready"
Feb  6 18:13:38.776: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb  6 18:13:38.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8143" for this suite. 02/06/23 18:13:38.783
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":190,"skipped":3586,"failed":0}
------------------------------
• [1.276 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:13:37.515
    Feb  6 18:13:37.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename gc 02/06/23 18:13:37.516
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:13:37.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:13:37.538
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 02/06/23 18:13:37.543
    STEP: Wait for the Deployment to create new ReplicaSet 02/06/23 18:13:37.549
    STEP: delete the deployment 02/06/23 18:13:38.063
    STEP: wait for all rs to be garbage collected 02/06/23 18:13:38.083
    STEP: expected 0 rs, got 1 rs 02/06/23 18:13:38.095
    STEP: expected 0 pods, got 2 pods 02/06/23 18:13:38.101
    STEP: Gathering metrics 02/06/23 18:13:38.612
    Feb  6 18:13:38.649: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tneyla25-pnn8z" in namespace "kube-system" to be "running and ready"
    Feb  6 18:13:38.653: INFO: Pod "kube-controller-manager-tneyla25-pnn8z": Phase="Running", Reason="", readiness=true. Elapsed: 3.819893ms
    Feb  6 18:13:38.653: INFO: The phase of Pod kube-controller-manager-tneyla25-pnn8z is Running (Ready = true)
    Feb  6 18:13:38.654: INFO: Pod "kube-controller-manager-tneyla25-pnn8z" satisfied condition "running and ready"
    Feb  6 18:13:38.776: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb  6 18:13:38.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8143" for this suite. 02/06/23 18:13:38.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:13:38.792
Feb  6 18:13:38.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename hostport 02/06/23 18:13:38.793
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:13:38.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:13:38.817
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/06/23 18:13:38.825
Feb  6 18:13:38.841: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-7302" to be "running and ready"
Feb  6 18:13:38.853: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.802433ms
Feb  6 18:13:38.853: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:13:40.859: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017102583s
Feb  6 18:13:40.859: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:13:42.860: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.018280713s
Feb  6 18:13:42.860: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb  6 18:13:42.860: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 195.17.148.89 on the node which pod1 resides and expect scheduled 02/06/23 18:13:42.86
Feb  6 18:13:42.876: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-7302" to be "running and ready"
Feb  6 18:13:42.883: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.171069ms
Feb  6 18:13:42.883: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:13:44.889: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01249576s
Feb  6 18:13:44.889: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:13:46.889: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.012501543s
Feb  6 18:13:46.889: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb  6 18:13:46.889: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 195.17.148.89 but use UDP protocol on the node which pod2 resides 02/06/23 18:13:46.889
Feb  6 18:13:46.899: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-7302" to be "running and ready"
Feb  6 18:13:46.902: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.296557ms
Feb  6 18:13:46.902: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:13:48.908: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008868797s
Feb  6 18:13:48.908: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:13:50.908: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.009117234s
Feb  6 18:13:50.908: INFO: The phase of Pod pod3 is Running (Ready = true)
Feb  6 18:13:50.908: INFO: Pod "pod3" satisfied condition "running and ready"
Feb  6 18:13:50.916: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-7302" to be "running and ready"
Feb  6 18:13:50.919: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.284207ms
Feb  6 18:13:50.919: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:13:52.926: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009651897s
Feb  6 18:13:52.926: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Feb  6 18:13:52.926: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/06/23 18:13:52.929
Feb  6 18:13:52.930: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 195.17.148.89 http://127.0.0.1:54323/hostname] Namespace:hostport-7302 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:13:52.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:13:52.930: INFO: ExecWithOptions: Clientset creation
Feb  6 18:13:52.931: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7302/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+195.17.148.89+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 195.17.148.89, port: 54323 02/06/23 18:13:53.021
Feb  6 18:13:53.021: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://195.17.148.89:54323/hostname] Namespace:hostport-7302 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:13:53.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:13:53.022: INFO: ExecWithOptions: Clientset creation
Feb  6 18:13:53.022: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7302/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F195.17.148.89%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 195.17.148.89, port: 54323 UDP 02/06/23 18:13:53.106
Feb  6 18:13:53.106: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 195.17.148.89 54323] Namespace:hostport-7302 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:13:53.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:13:53.108: INFO: ExecWithOptions: Clientset creation
Feb  6 18:13:53.108: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7302/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+195.17.148.89+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Feb  6 18:13:58.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-7302" for this suite. 02/06/23 18:13:58.199
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":191,"skipped":3608,"failed":0}
------------------------------
• [SLOW TEST] [19.416 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:13:38.792
    Feb  6 18:13:38.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename hostport 02/06/23 18:13:38.793
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:13:38.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:13:38.817
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/06/23 18:13:38.825
    Feb  6 18:13:38.841: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-7302" to be "running and ready"
    Feb  6 18:13:38.853: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.802433ms
    Feb  6 18:13:38.853: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:13:40.859: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017102583s
    Feb  6 18:13:40.859: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:13:42.860: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.018280713s
    Feb  6 18:13:42.860: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb  6 18:13:42.860: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 195.17.148.89 on the node which pod1 resides and expect scheduled 02/06/23 18:13:42.86
    Feb  6 18:13:42.876: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-7302" to be "running and ready"
    Feb  6 18:13:42.883: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.171069ms
    Feb  6 18:13:42.883: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:13:44.889: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01249576s
    Feb  6 18:13:44.889: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:13:46.889: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.012501543s
    Feb  6 18:13:46.889: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb  6 18:13:46.889: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 195.17.148.89 but use UDP protocol on the node which pod2 resides 02/06/23 18:13:46.889
    Feb  6 18:13:46.899: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-7302" to be "running and ready"
    Feb  6 18:13:46.902: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.296557ms
    Feb  6 18:13:46.902: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:13:48.908: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008868797s
    Feb  6 18:13:48.908: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:13:50.908: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.009117234s
    Feb  6 18:13:50.908: INFO: The phase of Pod pod3 is Running (Ready = true)
    Feb  6 18:13:50.908: INFO: Pod "pod3" satisfied condition "running and ready"
    Feb  6 18:13:50.916: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-7302" to be "running and ready"
    Feb  6 18:13:50.919: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.284207ms
    Feb  6 18:13:50.919: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:13:52.926: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009651897s
    Feb  6 18:13:52.926: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Feb  6 18:13:52.926: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/06/23 18:13:52.929
    Feb  6 18:13:52.930: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 195.17.148.89 http://127.0.0.1:54323/hostname] Namespace:hostport-7302 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:13:52.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:13:52.930: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:13:52.931: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7302/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+195.17.148.89+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 195.17.148.89, port: 54323 02/06/23 18:13:53.021
    Feb  6 18:13:53.021: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://195.17.148.89:54323/hostname] Namespace:hostport-7302 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:13:53.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:13:53.022: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:13:53.022: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7302/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F195.17.148.89%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 195.17.148.89, port: 54323 UDP 02/06/23 18:13:53.106
    Feb  6 18:13:53.106: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 195.17.148.89 54323] Namespace:hostport-7302 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:13:53.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:13:53.108: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:13:53.108: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7302/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+195.17.148.89+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Feb  6 18:13:58.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-7302" for this suite. 02/06/23 18:13:58.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:13:58.211
Feb  6 18:13:58.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 18:13:58.212
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:13:58.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:13:58.231
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 02/06/23 18:13:58.234
Feb  6 18:13:58.246: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be" in namespace "downward-api-670" to be "Succeeded or Failed"
Feb  6 18:13:58.253: INFO: Pod "downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be": Phase="Pending", Reason="", readiness=false. Elapsed: 6.915424ms
Feb  6 18:14:00.258: INFO: Pod "downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012487459s
Feb  6 18:14:02.258: INFO: Pod "downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be": Phase="Running", Reason="", readiness=false. Elapsed: 4.012315414s
Feb  6 18:14:04.258: INFO: Pod "downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012254242s
STEP: Saw pod success 02/06/23 18:14:04.258
Feb  6 18:14:04.259: INFO: Pod "downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be" satisfied condition "Succeeded or Failed"
Feb  6 18:14:04.262: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be container client-container: <nil>
STEP: delete the pod 02/06/23 18:14:04.27
Feb  6 18:14:04.288: INFO: Waiting for pod downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be to disappear
Feb  6 18:14:04.291: INFO: Pod downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb  6 18:14:04.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-670" for this suite. 02/06/23 18:14:04.297
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":192,"skipped":3623,"failed":0}
------------------------------
• [SLOW TEST] [6.094 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:13:58.211
    Feb  6 18:13:58.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 18:13:58.212
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:13:58.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:13:58.231
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 02/06/23 18:13:58.234
    Feb  6 18:13:58.246: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be" in namespace "downward-api-670" to be "Succeeded or Failed"
    Feb  6 18:13:58.253: INFO: Pod "downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be": Phase="Pending", Reason="", readiness=false. Elapsed: 6.915424ms
    Feb  6 18:14:00.258: INFO: Pod "downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012487459s
    Feb  6 18:14:02.258: INFO: Pod "downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be": Phase="Running", Reason="", readiness=false. Elapsed: 4.012315414s
    Feb  6 18:14:04.258: INFO: Pod "downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012254242s
    STEP: Saw pod success 02/06/23 18:14:04.258
    Feb  6 18:14:04.259: INFO: Pod "downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be" satisfied condition "Succeeded or Failed"
    Feb  6 18:14:04.262: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be container client-container: <nil>
    STEP: delete the pod 02/06/23 18:14:04.27
    Feb  6 18:14:04.288: INFO: Waiting for pod downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be to disappear
    Feb  6 18:14:04.291: INFO: Pod downwardapi-volume-eadd9f6c-6803-44a5-a70d-9c916116a1be no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb  6 18:14:04.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-670" for this suite. 02/06/23 18:14:04.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:14:04.31
Feb  6 18:14:04.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 18:14:04.311
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:14:04.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:14:04.335
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/06/23 18:14:04.338
Feb  6 18:14:04.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/06/23 18:14:33.3
Feb  6 18:14:33.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:14:41.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:15:11.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-176" for this suite. 02/06/23 18:15:11.289
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":193,"skipped":3640,"failed":0}
------------------------------
• [SLOW TEST] [66.986 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:14:04.31
    Feb  6 18:14:04.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 18:14:04.311
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:14:04.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:14:04.335
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/06/23 18:14:04.338
    Feb  6 18:14:04.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/06/23 18:14:33.3
    Feb  6 18:14:33.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:14:41.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:15:11.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-176" for this suite. 02/06/23 18:15:11.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:15:11.305
Feb  6 18:15:11.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename certificates 02/06/23 18:15:11.306
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:15:11.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:15:11.329
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 02/06/23 18:15:11.871
STEP: getting /apis/certificates.k8s.io 02/06/23 18:15:11.874
STEP: getting /apis/certificates.k8s.io/v1 02/06/23 18:15:11.876
STEP: creating 02/06/23 18:15:11.877
STEP: getting 02/06/23 18:15:11.893
STEP: listing 02/06/23 18:15:11.896
STEP: watching 02/06/23 18:15:11.9
Feb  6 18:15:11.900: INFO: starting watch
STEP: patching 02/06/23 18:15:11.901
STEP: updating 02/06/23 18:15:11.908
Feb  6 18:15:11.914: INFO: waiting for watch events with expected annotations
Feb  6 18:15:11.914: INFO: saw patched and updated annotations
STEP: getting /approval 02/06/23 18:15:11.914
STEP: patching /approval 02/06/23 18:15:11.917
STEP: updating /approval 02/06/23 18:15:11.925
STEP: getting /status 02/06/23 18:15:11.932
STEP: patching /status 02/06/23 18:15:11.937
STEP: updating /status 02/06/23 18:15:11.946
STEP: deleting 02/06/23 18:15:11.955
STEP: deleting a collection 02/06/23 18:15:11.967
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:15:11.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1687" for this suite. 02/06/23 18:15:11.988
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":194,"skipped":3679,"failed":0}
------------------------------
• [0.688 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:15:11.305
    Feb  6 18:15:11.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename certificates 02/06/23 18:15:11.306
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:15:11.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:15:11.329
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 02/06/23 18:15:11.871
    STEP: getting /apis/certificates.k8s.io 02/06/23 18:15:11.874
    STEP: getting /apis/certificates.k8s.io/v1 02/06/23 18:15:11.876
    STEP: creating 02/06/23 18:15:11.877
    STEP: getting 02/06/23 18:15:11.893
    STEP: listing 02/06/23 18:15:11.896
    STEP: watching 02/06/23 18:15:11.9
    Feb  6 18:15:11.900: INFO: starting watch
    STEP: patching 02/06/23 18:15:11.901
    STEP: updating 02/06/23 18:15:11.908
    Feb  6 18:15:11.914: INFO: waiting for watch events with expected annotations
    Feb  6 18:15:11.914: INFO: saw patched and updated annotations
    STEP: getting /approval 02/06/23 18:15:11.914
    STEP: patching /approval 02/06/23 18:15:11.917
    STEP: updating /approval 02/06/23 18:15:11.925
    STEP: getting /status 02/06/23 18:15:11.932
    STEP: patching /status 02/06/23 18:15:11.937
    STEP: updating /status 02/06/23 18:15:11.946
    STEP: deleting 02/06/23 18:15:11.955
    STEP: deleting a collection 02/06/23 18:15:11.967
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:15:11.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-1687" for this suite. 02/06/23 18:15:11.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:15:11.995
Feb  6 18:15:11.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename watch 02/06/23 18:15:11.997
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:15:12.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:15:12.018
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 02/06/23 18:15:12.021
STEP: starting a background goroutine to produce watch events 02/06/23 18:15:12.024
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/06/23 18:15:12.024
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb  6 18:15:14.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7942" for this suite. 02/06/23 18:15:14.855
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":195,"skipped":3700,"failed":0}
------------------------------
• [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:15:11.995
    Feb  6 18:15:11.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename watch 02/06/23 18:15:11.997
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:15:12.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:15:12.018
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 02/06/23 18:15:12.021
    STEP: starting a background goroutine to produce watch events 02/06/23 18:15:12.024
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/06/23 18:15:12.024
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb  6 18:15:14.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7942" for this suite. 02/06/23 18:15:14.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:15:14.912
Feb  6 18:15:14.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:15:14.913
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:15:14.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:15:14.933
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 02/06/23 18:15:14.936
Feb  6 18:15:14.944: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b" in namespace "projected-8030" to be "Succeeded or Failed"
Feb  6 18:15:14.948: INFO: Pod "downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.9562ms
Feb  6 18:15:16.953: INFO: Pod "downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008520452s
Feb  6 18:15:18.951: INFO: Pod "downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007002699s
Feb  6 18:15:20.953: INFO: Pod "downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009013886s
STEP: Saw pod success 02/06/23 18:15:20.953
Feb  6 18:15:20.954: INFO: Pod "downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b" satisfied condition "Succeeded or Failed"
Feb  6 18:15:20.957: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b container client-container: <nil>
STEP: delete the pod 02/06/23 18:15:20.969
Feb  6 18:15:20.979: INFO: Waiting for pod downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b to disappear
Feb  6 18:15:20.982: INFO: Pod downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb  6 18:15:20.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8030" for this suite. 02/06/23 18:15:20.987
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":196,"skipped":3743,"failed":0}
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:15:14.912
    Feb  6 18:15:14.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:15:14.913
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:15:14.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:15:14.933
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 02/06/23 18:15:14.936
    Feb  6 18:15:14.944: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b" in namespace "projected-8030" to be "Succeeded or Failed"
    Feb  6 18:15:14.948: INFO: Pod "downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.9562ms
    Feb  6 18:15:16.953: INFO: Pod "downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008520452s
    Feb  6 18:15:18.951: INFO: Pod "downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007002699s
    Feb  6 18:15:20.953: INFO: Pod "downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009013886s
    STEP: Saw pod success 02/06/23 18:15:20.953
    Feb  6 18:15:20.954: INFO: Pod "downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b" satisfied condition "Succeeded or Failed"
    Feb  6 18:15:20.957: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b container client-container: <nil>
    STEP: delete the pod 02/06/23 18:15:20.969
    Feb  6 18:15:20.979: INFO: Waiting for pod downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b to disappear
    Feb  6 18:15:20.982: INFO: Pod downwardapi-volume-f3e59810-a689-4b3f-8e84-83d9e2d58d6b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb  6 18:15:20.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8030" for this suite. 02/06/23 18:15:20.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:15:20.995
Feb  6 18:15:20.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename endpointslice 02/06/23 18:15:20.996
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:15:21.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:15:21.015
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb  6 18:15:23.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2574" for this suite. 02/06/23 18:15:23.181
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":197,"skipped":3760,"failed":0}
------------------------------
• [2.219 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:15:20.995
    Feb  6 18:15:20.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename endpointslice 02/06/23 18:15:20.996
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:15:21.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:15:21.015
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb  6 18:15:23.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2574" for this suite. 02/06/23 18:15:23.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:15:23.241
Feb  6 18:15:23.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 18:15:23.242
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:15:23.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:15:23.296
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3718 02/06/23 18:15:23.299
STEP: changing the ExternalName service to type=NodePort 02/06/23 18:15:23.308
STEP: creating replication controller externalname-service in namespace services-3718 02/06/23 18:15:23.393
I0206 18:15:23.401444      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3718, replica count: 2
I0206 18:15:26.452695      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 18:15:29.453051      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  6 18:15:29.453: INFO: Creating new exec pod
Feb  6 18:15:29.460: INFO: Waiting up to 5m0s for pod "execpodngllv" in namespace "services-3718" to be "running"
Feb  6 18:15:29.466: INFO: Pod "execpodngllv": Phase="Pending", Reason="", readiness=false. Elapsed: 5.136861ms
Feb  6 18:15:31.470: INFO: Pod "execpodngllv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009842416s
Feb  6 18:15:33.470: INFO: Pod "execpodngllv": Phase="Running", Reason="", readiness=true. Elapsed: 4.009830109s
Feb  6 18:15:33.471: INFO: Pod "execpodngllv" satisfied condition "running"
Feb  6 18:15:34.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3718 exec execpodngllv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb  6 18:15:34.658: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb  6 18:15:34.658: INFO: stdout: "externalname-service-7mglr"
Feb  6 18:15:34.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3718 exec execpodngllv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.162.39 80'
Feb  6 18:15:34.834: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.162.39 80\nConnection to 10.98.162.39 80 port [tcp/http] succeeded!\n"
Feb  6 18:15:34.834: INFO: stdout: "externalname-service-7mglr"
Feb  6 18:15:34.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3718 exec execpodngllv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.89 31294'
Feb  6 18:15:35.002: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.148.89 31294\nConnection to 195.17.148.89 31294 port [tcp/*] succeeded!\n"
Feb  6 18:15:35.002: INFO: stdout: "externalname-service-7mglr"
Feb  6 18:15:35.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3718 exec execpodngllv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.90 31294'
Feb  6 18:15:35.183: INFO: stderr: "+ nc -v -t -w 2 195.17.148.90 31294\n+ echo hostName\nConnection to 195.17.148.90 31294 port [tcp/*] succeeded!\n"
Feb  6 18:15:35.183: INFO: stdout: "externalname-service-7mglr"
Feb  6 18:15:35.183: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 18:15:35.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3718" for this suite. 02/06/23 18:15:35.237
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":198,"skipped":3800,"failed":0}
------------------------------
• [SLOW TEST] [12.008 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:15:23.241
    Feb  6 18:15:23.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 18:15:23.242
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:15:23.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:15:23.296
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-3718 02/06/23 18:15:23.299
    STEP: changing the ExternalName service to type=NodePort 02/06/23 18:15:23.308
    STEP: creating replication controller externalname-service in namespace services-3718 02/06/23 18:15:23.393
    I0206 18:15:23.401444      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3718, replica count: 2
    I0206 18:15:26.452695      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 18:15:29.453051      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb  6 18:15:29.453: INFO: Creating new exec pod
    Feb  6 18:15:29.460: INFO: Waiting up to 5m0s for pod "execpodngllv" in namespace "services-3718" to be "running"
    Feb  6 18:15:29.466: INFO: Pod "execpodngllv": Phase="Pending", Reason="", readiness=false. Elapsed: 5.136861ms
    Feb  6 18:15:31.470: INFO: Pod "execpodngllv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009842416s
    Feb  6 18:15:33.470: INFO: Pod "execpodngllv": Phase="Running", Reason="", readiness=true. Elapsed: 4.009830109s
    Feb  6 18:15:33.471: INFO: Pod "execpodngllv" satisfied condition "running"
    Feb  6 18:15:34.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3718 exec execpodngllv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb  6 18:15:34.658: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb  6 18:15:34.658: INFO: stdout: "externalname-service-7mglr"
    Feb  6 18:15:34.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3718 exec execpodngllv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.162.39 80'
    Feb  6 18:15:34.834: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.162.39 80\nConnection to 10.98.162.39 80 port [tcp/http] succeeded!\n"
    Feb  6 18:15:34.834: INFO: stdout: "externalname-service-7mglr"
    Feb  6 18:15:34.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3718 exec execpodngllv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.89 31294'
    Feb  6 18:15:35.002: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.148.89 31294\nConnection to 195.17.148.89 31294 port [tcp/*] succeeded!\n"
    Feb  6 18:15:35.002: INFO: stdout: "externalname-service-7mglr"
    Feb  6 18:15:35.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-3718 exec execpodngllv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.90 31294'
    Feb  6 18:15:35.183: INFO: stderr: "+ nc -v -t -w 2 195.17.148.90 31294\n+ echo hostName\nConnection to 195.17.148.90 31294 port [tcp/*] succeeded!\n"
    Feb  6 18:15:35.183: INFO: stdout: "externalname-service-7mglr"
    Feb  6 18:15:35.183: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 18:15:35.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3718" for this suite. 02/06/23 18:15:35.237
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:15:35.249
Feb  6 18:15:35.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 18:15:35.25
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:15:35.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:15:35.275
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 02/06/23 18:15:35.278
Feb  6 18:15:35.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: mark a version not serverd 02/06/23 18:15:52.516
STEP: check the unserved version gets removed 02/06/23 18:15:52.538
STEP: check the other version is not changed 02/06/23 18:16:00.08
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:16:14.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2057" for this suite. 02/06/23 18:16:14.116
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":199,"skipped":3806,"failed":0}
------------------------------
• [SLOW TEST] [38.875 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:15:35.249
    Feb  6 18:15:35.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 18:15:35.25
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:15:35.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:15:35.275
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 02/06/23 18:15:35.278
    Feb  6 18:15:35.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: mark a version not serverd 02/06/23 18:15:52.516
    STEP: check the unserved version gets removed 02/06/23 18:15:52.538
    STEP: check the other version is not changed 02/06/23 18:16:00.08
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:16:14.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2057" for this suite. 02/06/23 18:16:14.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:16:14.124
Feb  6 18:16:14.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename dns 02/06/23 18:16:14.125
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:16:14.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:16:14.148
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 02/06/23 18:16:14.149
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5613.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local; sleep 1; done
 02/06/23 18:16:14.155
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5613.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local; sleep 1; done
 02/06/23 18:16:14.156
STEP: creating a pod to probe DNS 02/06/23 18:16:14.156
STEP: submitting the pod to kubernetes 02/06/23 18:16:14.156
Feb  6 18:16:14.167: INFO: Waiting up to 15m0s for pod "dns-test-940ff551-2407-480c-880a-1ef888e90b02" in namespace "dns-5613" to be "running"
Feb  6 18:16:14.172: INFO: Pod "dns-test-940ff551-2407-480c-880a-1ef888e90b02": Phase="Pending", Reason="", readiness=false. Elapsed: 5.32955ms
Feb  6 18:16:16.179: INFO: Pod "dns-test-940ff551-2407-480c-880a-1ef888e90b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01174692s
Feb  6 18:16:18.177: INFO: Pod "dns-test-940ff551-2407-480c-880a-1ef888e90b02": Phase="Running", Reason="", readiness=true. Elapsed: 4.009586329s
Feb  6 18:16:18.177: INFO: Pod "dns-test-940ff551-2407-480c-880a-1ef888e90b02" satisfied condition "running"
STEP: retrieving the pod 02/06/23 18:16:18.177
STEP: looking for the results for each expected name from probers 02/06/23 18:16:18.181
Feb  6 18:16:18.191: INFO: DNS probes using dns-test-940ff551-2407-480c-880a-1ef888e90b02 succeeded

STEP: deleting the pod 02/06/23 18:16:18.191
STEP: changing the externalName to bar.example.com 02/06/23 18:16:18.206
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5613.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local; sleep 1; done
 02/06/23 18:16:18.225
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5613.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local; sleep 1; done
 02/06/23 18:16:18.225
STEP: creating a second pod to probe DNS 02/06/23 18:16:18.225
STEP: submitting the pod to kubernetes 02/06/23 18:16:18.225
Feb  6 18:16:18.234: INFO: Waiting up to 15m0s for pod "dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099" in namespace "dns-5613" to be "running"
Feb  6 18:16:18.242: INFO: Pod "dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026454ms
Feb  6 18:16:20.248: INFO: Pod "dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012937326s
Feb  6 18:16:22.248: INFO: Pod "dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099": Phase="Running", Reason="", readiness=true. Elapsed: 4.012902529s
Feb  6 18:16:22.248: INFO: Pod "dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099" satisfied condition "running"
STEP: retrieving the pod 02/06/23 18:16:22.248
STEP: looking for the results for each expected name from probers 02/06/23 18:16:22.252
Feb  6 18:16:22.258: INFO: File wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  6 18:16:22.262: INFO: File jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  6 18:16:22.262: INFO: Lookups using dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 failed for: [wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local]

Feb  6 18:16:27.272: INFO: File wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  6 18:16:27.277: INFO: File jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  6 18:16:27.278: INFO: Lookups using dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 failed for: [wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local]

Feb  6 18:16:32.269: INFO: File wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  6 18:16:32.274: INFO: File jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  6 18:16:32.274: INFO: Lookups using dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 failed for: [wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local]

Feb  6 18:16:37.268: INFO: File wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  6 18:16:37.272: INFO: File jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  6 18:16:37.272: INFO: Lookups using dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 failed for: [wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local]

Feb  6 18:16:42.268: INFO: File wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  6 18:16:42.273: INFO: File jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  6 18:16:42.273: INFO: Lookups using dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 failed for: [wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local]

Feb  6 18:16:47.268: INFO: File wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  6 18:16:47.273: INFO: File jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  6 18:16:47.273: INFO: Lookups using dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 failed for: [wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local]

Feb  6 18:16:52.275: INFO: DNS probes using dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 succeeded

STEP: deleting the pod 02/06/23 18:16:52.275
STEP: changing the service to type=ClusterIP 02/06/23 18:16:52.296
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5613.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local; sleep 1; done
 02/06/23 18:16:52.345
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5613.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local; sleep 1; done
 02/06/23 18:16:52.345
STEP: creating a third pod to probe DNS 02/06/23 18:16:52.345
STEP: submitting the pod to kubernetes 02/06/23 18:16:52.351
Feb  6 18:16:52.364: INFO: Waiting up to 15m0s for pod "dns-test-e73613b7-a9bf-4af6-a66e-941084e00200" in namespace "dns-5613" to be "running"
Feb  6 18:16:52.372: INFO: Pod "dns-test-e73613b7-a9bf-4af6-a66e-941084e00200": Phase="Pending", Reason="", readiness=false. Elapsed: 7.379762ms
Feb  6 18:16:54.378: INFO: Pod "dns-test-e73613b7-a9bf-4af6-a66e-941084e00200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013529882s
Feb  6 18:16:56.381: INFO: Pod "dns-test-e73613b7-a9bf-4af6-a66e-941084e00200": Phase="Running", Reason="", readiness=true. Elapsed: 4.016118518s
Feb  6 18:16:56.381: INFO: Pod "dns-test-e73613b7-a9bf-4af6-a66e-941084e00200" satisfied condition "running"
STEP: retrieving the pod 02/06/23 18:16:56.381
STEP: looking for the results for each expected name from probers 02/06/23 18:16:56.385
Feb  6 18:16:56.395: INFO: DNS probes using dns-test-e73613b7-a9bf-4af6-a66e-941084e00200 succeeded

STEP: deleting the pod 02/06/23 18:16:56.395
STEP: deleting the test externalName service 02/06/23 18:16:56.412
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb  6 18:16:56.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5613" for this suite. 02/06/23 18:16:56.474
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":200,"skipped":3811,"failed":0}
------------------------------
• [SLOW TEST] [42.360 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:16:14.124
    Feb  6 18:16:14.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename dns 02/06/23 18:16:14.125
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:16:14.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:16:14.148
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 02/06/23 18:16:14.149
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5613.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local; sleep 1; done
     02/06/23 18:16:14.155
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5613.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local; sleep 1; done
     02/06/23 18:16:14.156
    STEP: creating a pod to probe DNS 02/06/23 18:16:14.156
    STEP: submitting the pod to kubernetes 02/06/23 18:16:14.156
    Feb  6 18:16:14.167: INFO: Waiting up to 15m0s for pod "dns-test-940ff551-2407-480c-880a-1ef888e90b02" in namespace "dns-5613" to be "running"
    Feb  6 18:16:14.172: INFO: Pod "dns-test-940ff551-2407-480c-880a-1ef888e90b02": Phase="Pending", Reason="", readiness=false. Elapsed: 5.32955ms
    Feb  6 18:16:16.179: INFO: Pod "dns-test-940ff551-2407-480c-880a-1ef888e90b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01174692s
    Feb  6 18:16:18.177: INFO: Pod "dns-test-940ff551-2407-480c-880a-1ef888e90b02": Phase="Running", Reason="", readiness=true. Elapsed: 4.009586329s
    Feb  6 18:16:18.177: INFO: Pod "dns-test-940ff551-2407-480c-880a-1ef888e90b02" satisfied condition "running"
    STEP: retrieving the pod 02/06/23 18:16:18.177
    STEP: looking for the results for each expected name from probers 02/06/23 18:16:18.181
    Feb  6 18:16:18.191: INFO: DNS probes using dns-test-940ff551-2407-480c-880a-1ef888e90b02 succeeded

    STEP: deleting the pod 02/06/23 18:16:18.191
    STEP: changing the externalName to bar.example.com 02/06/23 18:16:18.206
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5613.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local; sleep 1; done
     02/06/23 18:16:18.225
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5613.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local; sleep 1; done
     02/06/23 18:16:18.225
    STEP: creating a second pod to probe DNS 02/06/23 18:16:18.225
    STEP: submitting the pod to kubernetes 02/06/23 18:16:18.225
    Feb  6 18:16:18.234: INFO: Waiting up to 15m0s for pod "dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099" in namespace "dns-5613" to be "running"
    Feb  6 18:16:18.242: INFO: Pod "dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026454ms
    Feb  6 18:16:20.248: INFO: Pod "dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012937326s
    Feb  6 18:16:22.248: INFO: Pod "dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099": Phase="Running", Reason="", readiness=true. Elapsed: 4.012902529s
    Feb  6 18:16:22.248: INFO: Pod "dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099" satisfied condition "running"
    STEP: retrieving the pod 02/06/23 18:16:22.248
    STEP: looking for the results for each expected name from probers 02/06/23 18:16:22.252
    Feb  6 18:16:22.258: INFO: File wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb  6 18:16:22.262: INFO: File jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb  6 18:16:22.262: INFO: Lookups using dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 failed for: [wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local]

    Feb  6 18:16:27.272: INFO: File wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb  6 18:16:27.277: INFO: File jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb  6 18:16:27.278: INFO: Lookups using dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 failed for: [wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local]

    Feb  6 18:16:32.269: INFO: File wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb  6 18:16:32.274: INFO: File jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb  6 18:16:32.274: INFO: Lookups using dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 failed for: [wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local]

    Feb  6 18:16:37.268: INFO: File wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb  6 18:16:37.272: INFO: File jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb  6 18:16:37.272: INFO: Lookups using dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 failed for: [wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local]

    Feb  6 18:16:42.268: INFO: File wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb  6 18:16:42.273: INFO: File jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb  6 18:16:42.273: INFO: Lookups using dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 failed for: [wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local]

    Feb  6 18:16:47.268: INFO: File wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb  6 18:16:47.273: INFO: File jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local from pod  dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb  6 18:16:47.273: INFO: Lookups using dns-5613/dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 failed for: [wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local]

    Feb  6 18:16:52.275: INFO: DNS probes using dns-test-beacc6b6-11f3-4007-9ba5-67ea8edf3099 succeeded

    STEP: deleting the pod 02/06/23 18:16:52.275
    STEP: changing the service to type=ClusterIP 02/06/23 18:16:52.296
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5613.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5613.svc.cluster.local; sleep 1; done
     02/06/23 18:16:52.345
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5613.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5613.svc.cluster.local; sleep 1; done
     02/06/23 18:16:52.345
    STEP: creating a third pod to probe DNS 02/06/23 18:16:52.345
    STEP: submitting the pod to kubernetes 02/06/23 18:16:52.351
    Feb  6 18:16:52.364: INFO: Waiting up to 15m0s for pod "dns-test-e73613b7-a9bf-4af6-a66e-941084e00200" in namespace "dns-5613" to be "running"
    Feb  6 18:16:52.372: INFO: Pod "dns-test-e73613b7-a9bf-4af6-a66e-941084e00200": Phase="Pending", Reason="", readiness=false. Elapsed: 7.379762ms
    Feb  6 18:16:54.378: INFO: Pod "dns-test-e73613b7-a9bf-4af6-a66e-941084e00200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013529882s
    Feb  6 18:16:56.381: INFO: Pod "dns-test-e73613b7-a9bf-4af6-a66e-941084e00200": Phase="Running", Reason="", readiness=true. Elapsed: 4.016118518s
    Feb  6 18:16:56.381: INFO: Pod "dns-test-e73613b7-a9bf-4af6-a66e-941084e00200" satisfied condition "running"
    STEP: retrieving the pod 02/06/23 18:16:56.381
    STEP: looking for the results for each expected name from probers 02/06/23 18:16:56.385
    Feb  6 18:16:56.395: INFO: DNS probes using dns-test-e73613b7-a9bf-4af6-a66e-941084e00200 succeeded

    STEP: deleting the pod 02/06/23 18:16:56.395
    STEP: deleting the test externalName service 02/06/23 18:16:56.412
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb  6 18:16:56.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5613" for this suite. 02/06/23 18:16:56.474
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:16:56.493
Feb  6 18:16:56.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 18:16:56.494
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:16:56.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:16:56.519
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-a2c87d3c-9736-43e4-9bb4-83243422828c 02/06/23 18:16:56.521
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 18:16:56.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2401" for this suite. 02/06/23 18:16:56.529
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":201,"skipped":3811,"failed":0}
------------------------------
• [0.047 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:16:56.493
    Feb  6 18:16:56.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 18:16:56.494
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:16:56.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:16:56.519
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-a2c87d3c-9736-43e4-9bb4-83243422828c 02/06/23 18:16:56.521
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 18:16:56.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2401" for this suite. 02/06/23 18:16:56.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:16:56.541
Feb  6 18:16:56.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 18:16:56.543
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:16:56.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:16:56.567
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 02/06/23 18:16:56.57
STEP: listing secrets in all namespaces to ensure that there are more than zero 02/06/23 18:16:56.576
STEP: patching the secret 02/06/23 18:16:56.583
STEP: deleting the secret using a LabelSelector 02/06/23 18:16:56.593
STEP: listing secrets in all namespaces, searching for label name and value in patch 02/06/23 18:16:56.601
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb  6 18:16:56.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7359" for this suite. 02/06/23 18:16:56.614
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":202,"skipped":3818,"failed":0}
------------------------------
• [0.081 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:16:56.541
    Feb  6 18:16:56.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 18:16:56.543
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:16:56.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:16:56.567
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 02/06/23 18:16:56.57
    STEP: listing secrets in all namespaces to ensure that there are more than zero 02/06/23 18:16:56.576
    STEP: patching the secret 02/06/23 18:16:56.583
    STEP: deleting the secret using a LabelSelector 02/06/23 18:16:56.593
    STEP: listing secrets in all namespaces, searching for label name and value in patch 02/06/23 18:16:56.601
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 18:16:56.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7359" for this suite. 02/06/23 18:16:56.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:16:56.625
Feb  6 18:16:56.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 18:16:56.626
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:16:56.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:16:56.651
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-e3d4394c-118f-4dba-b218-5fad561777cf 02/06/23 18:16:56.654
STEP: Creating a pod to test consume secrets 02/06/23 18:16:56.66
Feb  6 18:16:56.671: INFO: Waiting up to 5m0s for pod "pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4" in namespace "secrets-3781" to be "Succeeded or Failed"
Feb  6 18:16:56.677: INFO: Pod "pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574125ms
Feb  6 18:16:58.685: INFO: Pod "pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0132352s
Feb  6 18:17:00.683: INFO: Pod "pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01142525s
Feb  6 18:17:02.683: INFO: Pod "pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011981303s
STEP: Saw pod success 02/06/23 18:17:02.683
Feb  6 18:17:02.684: INFO: Pod "pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4" satisfied condition "Succeeded or Failed"
Feb  6 18:17:02.688: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4 container secret-env-test: <nil>
STEP: delete the pod 02/06/23 18:17:02.714
Feb  6 18:17:02.738: INFO: Waiting for pod pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4 to disappear
Feb  6 18:17:02.744: INFO: Pod pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb  6 18:17:02.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3781" for this suite. 02/06/23 18:17:02.751
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":203,"skipped":3864,"failed":0}
------------------------------
• [SLOW TEST] [6.135 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:16:56.625
    Feb  6 18:16:56.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 18:16:56.626
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:16:56.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:16:56.651
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-e3d4394c-118f-4dba-b218-5fad561777cf 02/06/23 18:16:56.654
    STEP: Creating a pod to test consume secrets 02/06/23 18:16:56.66
    Feb  6 18:16:56.671: INFO: Waiting up to 5m0s for pod "pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4" in namespace "secrets-3781" to be "Succeeded or Failed"
    Feb  6 18:16:56.677: INFO: Pod "pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574125ms
    Feb  6 18:16:58.685: INFO: Pod "pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0132352s
    Feb  6 18:17:00.683: INFO: Pod "pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01142525s
    Feb  6 18:17:02.683: INFO: Pod "pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011981303s
    STEP: Saw pod success 02/06/23 18:17:02.683
    Feb  6 18:17:02.684: INFO: Pod "pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4" satisfied condition "Succeeded or Failed"
    Feb  6 18:17:02.688: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4 container secret-env-test: <nil>
    STEP: delete the pod 02/06/23 18:17:02.714
    Feb  6 18:17:02.738: INFO: Waiting for pod pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4 to disappear
    Feb  6 18:17:02.744: INFO: Pod pod-secrets-b8328770-8717-4df7-bc44-f183245ce1e4 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 18:17:02.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3781" for this suite. 02/06/23 18:17:02.751
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:17:02.76
Feb  6 18:17:02.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:17:02.761
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:17:02.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:17:02.793
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-09c4acff-6504-4762-9ce5-3bc89ca33bb7 02/06/23 18:17:02.796
STEP: Creating a pod to test consume configMaps 02/06/23 18:17:02.802
Feb  6 18:17:02.813: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f" in namespace "projected-7768" to be "Succeeded or Failed"
Feb  6 18:17:02.821: INFO: Pod "pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.56011ms
Feb  6 18:17:04.826: INFO: Pod "pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01327189s
Feb  6 18:17:06.826: INFO: Pod "pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f": Phase="Running", Reason="", readiness=false. Elapsed: 4.013590325s
Feb  6 18:17:08.826: INFO: Pod "pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013167331s
STEP: Saw pod success 02/06/23 18:17:08.826
Feb  6 18:17:08.826: INFO: Pod "pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f" satisfied condition "Succeeded or Failed"
Feb  6 18:17:08.829: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:17:08.84
Feb  6 18:17:08.853: INFO: Waiting for pod pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f to disappear
Feb  6 18:17:08.857: INFO: Pod pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb  6 18:17:08.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7768" for this suite. 02/06/23 18:17:08.862
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":204,"skipped":3866,"failed":0}
------------------------------
• [SLOW TEST] [6.110 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:17:02.76
    Feb  6 18:17:02.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:17:02.761
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:17:02.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:17:02.793
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-09c4acff-6504-4762-9ce5-3bc89ca33bb7 02/06/23 18:17:02.796
    STEP: Creating a pod to test consume configMaps 02/06/23 18:17:02.802
    Feb  6 18:17:02.813: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f" in namespace "projected-7768" to be "Succeeded or Failed"
    Feb  6 18:17:02.821: INFO: Pod "pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.56011ms
    Feb  6 18:17:04.826: INFO: Pod "pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01327189s
    Feb  6 18:17:06.826: INFO: Pod "pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f": Phase="Running", Reason="", readiness=false. Elapsed: 4.013590325s
    Feb  6 18:17:08.826: INFO: Pod "pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013167331s
    STEP: Saw pod success 02/06/23 18:17:08.826
    Feb  6 18:17:08.826: INFO: Pod "pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f" satisfied condition "Succeeded or Failed"
    Feb  6 18:17:08.829: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:17:08.84
    Feb  6 18:17:08.853: INFO: Waiting for pod pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f to disappear
    Feb  6 18:17:08.857: INFO: Pod pod-projected-configmaps-14e17503-5382-4676-9073-e5216076fc5f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb  6 18:17:08.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7768" for this suite. 02/06/23 18:17:08.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:17:08.872
Feb  6 18:17:08.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename svcaccounts 02/06/23 18:17:08.873
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:17:08.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:17:08.895
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Feb  6 18:17:08.916: INFO: Waiting up to 5m0s for pod "pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd" in namespace "svcaccounts-7283" to be "running"
Feb  6 18:17:08.923: INFO: Pod "pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.698068ms
Feb  6 18:17:10.928: INFO: Pod "pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012061799s
Feb  6 18:17:12.928: INFO: Pod "pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd": Phase="Running", Reason="", readiness=true. Elapsed: 4.011853785s
Feb  6 18:17:12.928: INFO: Pod "pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd" satisfied condition "running"
STEP: reading a file in the container 02/06/23 18:17:12.928
Feb  6 18:17:12.928: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7283 pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 02/06/23 18:17:13.096
Feb  6 18:17:13.096: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7283 pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 02/06/23 18:17:13.26
Feb  6 18:17:13.261: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7283 pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Feb  6 18:17:13.428: INFO: Got root ca configmap in namespace "svcaccounts-7283"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb  6 18:17:13.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7283" for this suite. 02/06/23 18:17:13.435
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":205,"skipped":3880,"failed":0}
------------------------------
• [4.571 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:17:08.872
    Feb  6 18:17:08.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename svcaccounts 02/06/23 18:17:08.873
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:17:08.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:17:08.895
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Feb  6 18:17:08.916: INFO: Waiting up to 5m0s for pod "pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd" in namespace "svcaccounts-7283" to be "running"
    Feb  6 18:17:08.923: INFO: Pod "pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.698068ms
    Feb  6 18:17:10.928: INFO: Pod "pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012061799s
    Feb  6 18:17:12.928: INFO: Pod "pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd": Phase="Running", Reason="", readiness=true. Elapsed: 4.011853785s
    Feb  6 18:17:12.928: INFO: Pod "pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd" satisfied condition "running"
    STEP: reading a file in the container 02/06/23 18:17:12.928
    Feb  6 18:17:12.928: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7283 pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 02/06/23 18:17:13.096
    Feb  6 18:17:13.096: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7283 pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 02/06/23 18:17:13.26
    Feb  6 18:17:13.261: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7283 pod-service-account-f8124761-cde4-4ed7-942c-fdc67a6018fd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Feb  6 18:17:13.428: INFO: Got root ca configmap in namespace "svcaccounts-7283"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb  6 18:17:13.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7283" for this suite. 02/06/23 18:17:13.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:17:13.446
Feb  6 18:17:13.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:17:13.447
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:17:13.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:17:13.469
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 02/06/23 18:17:13.472
Feb  6 18:17:13.481: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8" in namespace "projected-9977" to be "Succeeded or Failed"
Feb  6 18:17:13.488: INFO: Pod "downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.421754ms
Feb  6 18:17:15.494: INFO: Pod "downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013387136s
Feb  6 18:17:17.495: INFO: Pod "downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013624674s
Feb  6 18:17:19.495: INFO: Pod "downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014212569s
STEP: Saw pod success 02/06/23 18:17:19.495
Feb  6 18:17:19.495: INFO: Pod "downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8" satisfied condition "Succeeded or Failed"
Feb  6 18:17:19.498: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8 container client-container: <nil>
STEP: delete the pod 02/06/23 18:17:19.506
Feb  6 18:17:19.523: INFO: Waiting for pod downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8 to disappear
Feb  6 18:17:19.527: INFO: Pod downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb  6 18:17:19.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9977" for this suite. 02/06/23 18:17:19.531
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":206,"skipped":3901,"failed":0}
------------------------------
• [SLOW TEST] [6.093 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:17:13.446
    Feb  6 18:17:13.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:17:13.447
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:17:13.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:17:13.469
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 02/06/23 18:17:13.472
    Feb  6 18:17:13.481: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8" in namespace "projected-9977" to be "Succeeded or Failed"
    Feb  6 18:17:13.488: INFO: Pod "downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.421754ms
    Feb  6 18:17:15.494: INFO: Pod "downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013387136s
    Feb  6 18:17:17.495: INFO: Pod "downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013624674s
    Feb  6 18:17:19.495: INFO: Pod "downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014212569s
    STEP: Saw pod success 02/06/23 18:17:19.495
    Feb  6 18:17:19.495: INFO: Pod "downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8" satisfied condition "Succeeded or Failed"
    Feb  6 18:17:19.498: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8 container client-container: <nil>
    STEP: delete the pod 02/06/23 18:17:19.506
    Feb  6 18:17:19.523: INFO: Waiting for pod downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8 to disappear
    Feb  6 18:17:19.527: INFO: Pod downwardapi-volume-a2a70b99-43f9-4d61-a885-0a666e8016a8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb  6 18:17:19.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9977" for this suite. 02/06/23 18:17:19.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:17:19.542
Feb  6 18:17:19.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename dns 02/06/23 18:17:19.543
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:17:19.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:17:19.563
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 02/06/23 18:17:19.566
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3491.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3491.svc.cluster.local;sleep 1; done
 02/06/23 18:17:19.573
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3491.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3491.svc.cluster.local;sleep 1; done
 02/06/23 18:17:19.573
STEP: creating a pod to probe DNS 02/06/23 18:17:19.573
STEP: submitting the pod to kubernetes 02/06/23 18:17:19.573
Feb  6 18:17:19.589: INFO: Waiting up to 15m0s for pod "dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b" in namespace "dns-3491" to be "running"
Feb  6 18:17:19.598: INFO: Pod "dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.059899ms
Feb  6 18:17:21.604: INFO: Pod "dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014885644s
Feb  6 18:17:23.605: INFO: Pod "dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015950803s
Feb  6 18:17:25.605: INFO: Pod "dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b": Phase="Running", Reason="", readiness=true. Elapsed: 6.015786816s
Feb  6 18:17:25.605: INFO: Pod "dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b" satisfied condition "running"
STEP: retrieving the pod 02/06/23 18:17:25.605
STEP: looking for the results for each expected name from probers 02/06/23 18:17:25.609
Feb  6 18:17:25.616: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:25.621: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:25.630: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:25.634: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:25.638: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:25.647: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:25.647: INFO: Lookups using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3491.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3491.svc.cluster.local]

Feb  6 18:17:30.653: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:30.657: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:30.670: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:30.673: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:30.681: INFO: Lookups using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local]

Feb  6 18:17:35.655: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:35.659: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:35.671: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:35.675: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:35.685: INFO: Lookups using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local]

Feb  6 18:17:40.654: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:40.658: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:40.671: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:40.676: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:40.684: INFO: Lookups using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local]

Feb  6 18:17:45.655: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:45.660: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:45.672: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:45.677: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:45.684: INFO: Lookups using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local]

Feb  6 18:17:50.657: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:50.662: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:50.677: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:50.681: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
Feb  6 18:17:50.689: INFO: Lookups using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local]

Feb  6 18:17:55.685: INFO: DNS probes using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b succeeded

STEP: deleting the pod 02/06/23 18:17:55.685
STEP: deleting the test headless service 02/06/23 18:17:55.702
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb  6 18:17:55.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3491" for this suite. 02/06/23 18:17:55.738
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":207,"skipped":3934,"failed":0}
------------------------------
• [SLOW TEST] [36.205 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:17:19.542
    Feb  6 18:17:19.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename dns 02/06/23 18:17:19.543
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:17:19.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:17:19.563
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 02/06/23 18:17:19.566
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3491.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3491.svc.cluster.local;sleep 1; done
     02/06/23 18:17:19.573
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3491.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3491.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3491.svc.cluster.local;sleep 1; done
     02/06/23 18:17:19.573
    STEP: creating a pod to probe DNS 02/06/23 18:17:19.573
    STEP: submitting the pod to kubernetes 02/06/23 18:17:19.573
    Feb  6 18:17:19.589: INFO: Waiting up to 15m0s for pod "dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b" in namespace "dns-3491" to be "running"
    Feb  6 18:17:19.598: INFO: Pod "dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.059899ms
    Feb  6 18:17:21.604: INFO: Pod "dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014885644s
    Feb  6 18:17:23.605: INFO: Pod "dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015950803s
    Feb  6 18:17:25.605: INFO: Pod "dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b": Phase="Running", Reason="", readiness=true. Elapsed: 6.015786816s
    Feb  6 18:17:25.605: INFO: Pod "dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b" satisfied condition "running"
    STEP: retrieving the pod 02/06/23 18:17:25.605
    STEP: looking for the results for each expected name from probers 02/06/23 18:17:25.609
    Feb  6 18:17:25.616: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:25.621: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:25.630: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:25.634: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:25.638: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:25.647: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:25.647: INFO: Lookups using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3491.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3491.svc.cluster.local]

    Feb  6 18:17:30.653: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:30.657: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:30.670: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:30.673: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:30.681: INFO: Lookups using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local]

    Feb  6 18:17:35.655: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:35.659: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:35.671: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:35.675: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:35.685: INFO: Lookups using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local]

    Feb  6 18:17:40.654: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:40.658: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:40.671: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:40.676: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:40.684: INFO: Lookups using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local]

    Feb  6 18:17:45.655: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:45.660: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:45.672: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:45.677: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:45.684: INFO: Lookups using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local]

    Feb  6 18:17:50.657: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:50.662: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:50.677: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:50.681: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local from pod dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b: the server could not find the requested resource (get pods dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b)
    Feb  6 18:17:50.689: INFO: Lookups using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3491.svc.cluster.local]

    Feb  6 18:17:55.685: INFO: DNS probes using dns-3491/dns-test-b37ef0a6-669b-4e3d-a57c-af00b0ae009b succeeded

    STEP: deleting the pod 02/06/23 18:17:55.685
    STEP: deleting the test headless service 02/06/23 18:17:55.702
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb  6 18:17:55.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3491" for this suite. 02/06/23 18:17:55.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:17:55.75
Feb  6 18:17:55.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-probe 02/06/23 18:17:55.751
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:17:55.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:17:55.782
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-79212238-cadb-4014-93f0-08b6aafe72ca in namespace container-probe-2785 02/06/23 18:17:55.785
Feb  6 18:17:55.794: INFO: Waiting up to 5m0s for pod "liveness-79212238-cadb-4014-93f0-08b6aafe72ca" in namespace "container-probe-2785" to be "not pending"
Feb  6 18:17:55.798: INFO: Pod "liveness-79212238-cadb-4014-93f0-08b6aafe72ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.307006ms
Feb  6 18:17:57.803: INFO: Pod "liveness-79212238-cadb-4014-93f0-08b6aafe72ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008328386s
Feb  6 18:17:59.804: INFO: Pod "liveness-79212238-cadb-4014-93f0-08b6aafe72ca": Phase="Running", Reason="", readiness=true. Elapsed: 4.009196109s
Feb  6 18:17:59.804: INFO: Pod "liveness-79212238-cadb-4014-93f0-08b6aafe72ca" satisfied condition "not pending"
Feb  6 18:17:59.804: INFO: Started pod liveness-79212238-cadb-4014-93f0-08b6aafe72ca in namespace container-probe-2785
STEP: checking the pod's current state and verifying that restartCount is present 02/06/23 18:17:59.804
Feb  6 18:17:59.808: INFO: Initial restart count of pod liveness-79212238-cadb-4014-93f0-08b6aafe72ca is 0
Feb  6 18:18:17.864: INFO: Restart count of pod container-probe-2785/liveness-79212238-cadb-4014-93f0-08b6aafe72ca is now 1 (18.056130819s elapsed)
STEP: deleting the pod 02/06/23 18:18:17.864
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb  6 18:18:17.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2785" for this suite. 02/06/23 18:18:17.89
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":208,"skipped":3966,"failed":0}
------------------------------
• [SLOW TEST] [22.152 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:17:55.75
    Feb  6 18:17:55.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-probe 02/06/23 18:17:55.751
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:17:55.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:17:55.782
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-79212238-cadb-4014-93f0-08b6aafe72ca in namespace container-probe-2785 02/06/23 18:17:55.785
    Feb  6 18:17:55.794: INFO: Waiting up to 5m0s for pod "liveness-79212238-cadb-4014-93f0-08b6aafe72ca" in namespace "container-probe-2785" to be "not pending"
    Feb  6 18:17:55.798: INFO: Pod "liveness-79212238-cadb-4014-93f0-08b6aafe72ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.307006ms
    Feb  6 18:17:57.803: INFO: Pod "liveness-79212238-cadb-4014-93f0-08b6aafe72ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008328386s
    Feb  6 18:17:59.804: INFO: Pod "liveness-79212238-cadb-4014-93f0-08b6aafe72ca": Phase="Running", Reason="", readiness=true. Elapsed: 4.009196109s
    Feb  6 18:17:59.804: INFO: Pod "liveness-79212238-cadb-4014-93f0-08b6aafe72ca" satisfied condition "not pending"
    Feb  6 18:17:59.804: INFO: Started pod liveness-79212238-cadb-4014-93f0-08b6aafe72ca in namespace container-probe-2785
    STEP: checking the pod's current state and verifying that restartCount is present 02/06/23 18:17:59.804
    Feb  6 18:17:59.808: INFO: Initial restart count of pod liveness-79212238-cadb-4014-93f0-08b6aafe72ca is 0
    Feb  6 18:18:17.864: INFO: Restart count of pod container-probe-2785/liveness-79212238-cadb-4014-93f0-08b6aafe72ca is now 1 (18.056130819s elapsed)
    STEP: deleting the pod 02/06/23 18:18:17.864
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb  6 18:18:17.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2785" for this suite. 02/06/23 18:18:17.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:18:17.906
Feb  6 18:18:17.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 18:18:17.907
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:18:17.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:18:17.938
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/06/23 18:18:17.94
Feb  6 18:18:17.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:18:26.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:18:56.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3735" for this suite. 02/06/23 18:18:56.19
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":209,"skipped":3992,"failed":0}
------------------------------
• [SLOW TEST] [38.292 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:18:17.906
    Feb  6 18:18:17.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 18:18:17.907
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:18:17.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:18:17.938
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/06/23 18:18:17.94
    Feb  6 18:18:17.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:18:26.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:18:56.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3735" for this suite. 02/06/23 18:18:56.19
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:18:56.199
Feb  6 18:18:56.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename init-container 02/06/23 18:18:56.2
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:18:56.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:18:56.223
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 02/06/23 18:18:56.226
Feb  6 18:18:56.226: INFO: PodSpec: initContainers in spec.initContainers
Feb  6 18:19:48.055: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-12b49448-c997-4c59-9a91-396d32ddbb3e", GenerateName:"", Namespace:"init-container-4722", SelfLink:"", UID:"af8cab4a-9dbb-4f7e-b216-f4eb3a24f24d", ResourceVersion:"52866", Generation:0, CreationTimestamp:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"226954121"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004237e78), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 6, 18, 19, 48, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004237ea8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-whfqm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00aae47c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-whfqm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-whfqm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-whfqm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc009624dd0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"tneyla25-md-0-68cbcb4798-5xxmw", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00351a070), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc009624eb0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc009624ef0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc009624ef8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc009624efc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0065d1140), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"195.17.4.133", PodIP:"192.168.2.191", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.2.191"}}, StartTime:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00351a150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00351a1c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://62a0c2226ed48e95ad635c9a05fcb71870ce90662bb488f98bbabc84bfc89c7f", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00aae4840), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00aae4820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc009624f8f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb  6 18:19:48.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4722" for this suite. 02/06/23 18:19:48.062
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":210,"skipped":3992,"failed":0}
------------------------------
• [SLOW TEST] [51.871 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:18:56.199
    Feb  6 18:18:56.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename init-container 02/06/23 18:18:56.2
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:18:56.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:18:56.223
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 02/06/23 18:18:56.226
    Feb  6 18:18:56.226: INFO: PodSpec: initContainers in spec.initContainers
    Feb  6 18:19:48.055: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-12b49448-c997-4c59-9a91-396d32ddbb3e", GenerateName:"", Namespace:"init-container-4722", SelfLink:"", UID:"af8cab4a-9dbb-4f7e-b216-f4eb3a24f24d", ResourceVersion:"52866", Generation:0, CreationTimestamp:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"226954121"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004237e78), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 6, 18, 19, 48, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004237ea8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-whfqm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00aae47c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-whfqm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-whfqm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-whfqm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc009624dd0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"tneyla25-md-0-68cbcb4798-5xxmw", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00351a070), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc009624eb0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc009624ef0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc009624ef8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc009624efc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0065d1140), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"195.17.4.133", PodIP:"192.168.2.191", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.2.191"}}, StartTime:time.Date(2023, time.February, 6, 18, 18, 56, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00351a150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00351a1c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://62a0c2226ed48e95ad635c9a05fcb71870ce90662bb488f98bbabc84bfc89c7f", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00aae4840), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00aae4820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc009624f8f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb  6 18:19:48.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4722" for this suite. 02/06/23 18:19:48.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:19:48.07
Feb  6 18:19:48.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename containers 02/06/23 18:19:48.071
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:19:48.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:19:48.093
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Feb  6 18:19:48.104: INFO: Waiting up to 5m0s for pod "client-containers-f4089045-8cfe-4c3f-84e0-09ea36fd86c1" in namespace "containers-3840" to be "running"
Feb  6 18:19:48.111: INFO: Pod "client-containers-f4089045-8cfe-4c3f-84e0-09ea36fd86c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.782037ms
Feb  6 18:19:50.118: INFO: Pod "client-containers-f4089045-8cfe-4c3f-84e0-09ea36fd86c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013186322s
Feb  6 18:19:52.116: INFO: Pod "client-containers-f4089045-8cfe-4c3f-84e0-09ea36fd86c1": Phase="Running", Reason="", readiness=true. Elapsed: 4.011262975s
Feb  6 18:19:52.116: INFO: Pod "client-containers-f4089045-8cfe-4c3f-84e0-09ea36fd86c1" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb  6 18:19:52.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3840" for this suite. 02/06/23 18:19:52.137
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":211,"skipped":4000,"failed":0}
------------------------------
• [4.074 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:19:48.07
    Feb  6 18:19:48.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename containers 02/06/23 18:19:48.071
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:19:48.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:19:48.093
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Feb  6 18:19:48.104: INFO: Waiting up to 5m0s for pod "client-containers-f4089045-8cfe-4c3f-84e0-09ea36fd86c1" in namespace "containers-3840" to be "running"
    Feb  6 18:19:48.111: INFO: Pod "client-containers-f4089045-8cfe-4c3f-84e0-09ea36fd86c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.782037ms
    Feb  6 18:19:50.118: INFO: Pod "client-containers-f4089045-8cfe-4c3f-84e0-09ea36fd86c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013186322s
    Feb  6 18:19:52.116: INFO: Pod "client-containers-f4089045-8cfe-4c3f-84e0-09ea36fd86c1": Phase="Running", Reason="", readiness=true. Elapsed: 4.011262975s
    Feb  6 18:19:52.116: INFO: Pod "client-containers-f4089045-8cfe-4c3f-84e0-09ea36fd86c1" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb  6 18:19:52.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3840" for this suite. 02/06/23 18:19:52.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:19:52.154
Feb  6 18:19:52.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename cronjob 02/06/23 18:19:52.155
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:19:52.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:19:52.179
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 02/06/23 18:19:52.181
STEP: Ensuring no jobs are scheduled 02/06/23 18:19:52.189
STEP: Ensuring no job exists by listing jobs explicitly 02/06/23 18:24:52.199
STEP: Removing cronjob 02/06/23 18:24:52.202
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb  6 18:24:52.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4340" for this suite. 02/06/23 18:24:52.215
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":212,"skipped":4038,"failed":0}
------------------------------
• [SLOW TEST] [300.069 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:19:52.154
    Feb  6 18:19:52.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename cronjob 02/06/23 18:19:52.155
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:19:52.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:19:52.179
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 02/06/23 18:19:52.181
    STEP: Ensuring no jobs are scheduled 02/06/23 18:19:52.189
    STEP: Ensuring no job exists by listing jobs explicitly 02/06/23 18:24:52.199
    STEP: Removing cronjob 02/06/23 18:24:52.202
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb  6 18:24:52.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4340" for this suite. 02/06/23 18:24:52.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:24:52.227
Feb  6 18:24:52.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename endpointslice 02/06/23 18:24:52.228
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:24:52.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:24:52.25
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 02/06/23 18:24:52.253
STEP: getting /apis/discovery.k8s.io 02/06/23 18:24:52.255
STEP: getting /apis/discovery.k8s.iov1 02/06/23 18:24:52.256
STEP: creating 02/06/23 18:24:52.258
STEP: getting 02/06/23 18:24:52.274
STEP: listing 02/06/23 18:24:52.277
STEP: watching 02/06/23 18:24:52.28
Feb  6 18:24:52.280: INFO: starting watch
STEP: cluster-wide listing 02/06/23 18:24:52.282
STEP: cluster-wide watching 02/06/23 18:24:52.286
Feb  6 18:24:52.286: INFO: starting watch
STEP: patching 02/06/23 18:24:52.287
STEP: updating 02/06/23 18:24:52.292
Feb  6 18:24:52.302: INFO: waiting for watch events with expected annotations
Feb  6 18:24:52.302: INFO: saw patched and updated annotations
STEP: deleting 02/06/23 18:24:52.303
STEP: deleting a collection 02/06/23 18:24:52.315
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb  6 18:24:52.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3260" for this suite. 02/06/23 18:24:52.337
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":213,"skipped":4051,"failed":0}
------------------------------
• [0.117 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:24:52.227
    Feb  6 18:24:52.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename endpointslice 02/06/23 18:24:52.228
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:24:52.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:24:52.25
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 02/06/23 18:24:52.253
    STEP: getting /apis/discovery.k8s.io 02/06/23 18:24:52.255
    STEP: getting /apis/discovery.k8s.iov1 02/06/23 18:24:52.256
    STEP: creating 02/06/23 18:24:52.258
    STEP: getting 02/06/23 18:24:52.274
    STEP: listing 02/06/23 18:24:52.277
    STEP: watching 02/06/23 18:24:52.28
    Feb  6 18:24:52.280: INFO: starting watch
    STEP: cluster-wide listing 02/06/23 18:24:52.282
    STEP: cluster-wide watching 02/06/23 18:24:52.286
    Feb  6 18:24:52.286: INFO: starting watch
    STEP: patching 02/06/23 18:24:52.287
    STEP: updating 02/06/23 18:24:52.292
    Feb  6 18:24:52.302: INFO: waiting for watch events with expected annotations
    Feb  6 18:24:52.302: INFO: saw patched and updated annotations
    STEP: deleting 02/06/23 18:24:52.303
    STEP: deleting a collection 02/06/23 18:24:52.315
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb  6 18:24:52.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3260" for this suite. 02/06/23 18:24:52.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:24:52.348
Feb  6 18:24:52.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 18:24:52.349
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:24:52.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:24:52.371
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 02/06/23 18:24:52.377
STEP: waiting for available Endpoint 02/06/23 18:24:52.383
STEP: listing all Endpoints 02/06/23 18:24:52.384
STEP: updating the Endpoint 02/06/23 18:24:52.387
STEP: fetching the Endpoint 02/06/23 18:24:52.393
STEP: patching the Endpoint 02/06/23 18:24:52.396
STEP: fetching the Endpoint 02/06/23 18:24:52.403
STEP: deleting the Endpoint by Collection 02/06/23 18:24:52.405
STEP: waiting for Endpoint deletion 02/06/23 18:24:52.413
STEP: fetching the Endpoint 02/06/23 18:24:52.415
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 18:24:52.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1795" for this suite. 02/06/23 18:24:52.425
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":214,"skipped":4070,"failed":0}
------------------------------
• [0.085 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:24:52.348
    Feb  6 18:24:52.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 18:24:52.349
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:24:52.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:24:52.371
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 02/06/23 18:24:52.377
    STEP: waiting for available Endpoint 02/06/23 18:24:52.383
    STEP: listing all Endpoints 02/06/23 18:24:52.384
    STEP: updating the Endpoint 02/06/23 18:24:52.387
    STEP: fetching the Endpoint 02/06/23 18:24:52.393
    STEP: patching the Endpoint 02/06/23 18:24:52.396
    STEP: fetching the Endpoint 02/06/23 18:24:52.403
    STEP: deleting the Endpoint by Collection 02/06/23 18:24:52.405
    STEP: waiting for Endpoint deletion 02/06/23 18:24:52.413
    STEP: fetching the Endpoint 02/06/23 18:24:52.415
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 18:24:52.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1795" for this suite. 02/06/23 18:24:52.425
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:24:52.445
Feb  6 18:24:52.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:24:52.447
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:24:52.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:24:52.472
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-989971eb-d0ac-4541-9ec6-2e96d89148fd 02/06/23 18:24:52.474
STEP: Creating a pod to test consume configMaps 02/06/23 18:24:52.48
Feb  6 18:24:52.489: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf" in namespace "projected-3942" to be "Succeeded or Failed"
Feb  6 18:24:52.492: INFO: Pod "pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.3131ms
Feb  6 18:24:54.504: INFO: Pod "pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014694805s
Feb  6 18:24:56.497: INFO: Pod "pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008204245s
Feb  6 18:24:58.504: INFO: Pod "pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014549934s
STEP: Saw pod success 02/06/23 18:24:58.504
Feb  6 18:24:58.504: INFO: Pod "pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf" satisfied condition "Succeeded or Failed"
Feb  6 18:24:58.508: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf container projected-configmap-volume-test: <nil>
STEP: delete the pod 02/06/23 18:24:58.524
Feb  6 18:24:58.536: INFO: Waiting for pod pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf to disappear
Feb  6 18:24:58.539: INFO: Pod pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb  6 18:24:58.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3942" for this suite. 02/06/23 18:24:58.545
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":215,"skipped":4105,"failed":0}
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:24:52.445
    Feb  6 18:24:52.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:24:52.447
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:24:52.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:24:52.472
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-989971eb-d0ac-4541-9ec6-2e96d89148fd 02/06/23 18:24:52.474
    STEP: Creating a pod to test consume configMaps 02/06/23 18:24:52.48
    Feb  6 18:24:52.489: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf" in namespace "projected-3942" to be "Succeeded or Failed"
    Feb  6 18:24:52.492: INFO: Pod "pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.3131ms
    Feb  6 18:24:54.504: INFO: Pod "pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014694805s
    Feb  6 18:24:56.497: INFO: Pod "pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008204245s
    Feb  6 18:24:58.504: INFO: Pod "pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014549934s
    STEP: Saw pod success 02/06/23 18:24:58.504
    Feb  6 18:24:58.504: INFO: Pod "pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf" satisfied condition "Succeeded or Failed"
    Feb  6 18:24:58.508: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf container projected-configmap-volume-test: <nil>
    STEP: delete the pod 02/06/23 18:24:58.524
    Feb  6 18:24:58.536: INFO: Waiting for pod pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf to disappear
    Feb  6 18:24:58.539: INFO: Pod pod-projected-configmaps-617805b8-6abb-40ca-8936-5245a125c0cf no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb  6 18:24:58.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3942" for this suite. 02/06/23 18:24:58.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:24:58.555
Feb  6 18:24:58.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 18:24:58.556
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:24:58.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:24:58.581
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-5dfc5160-3fd5-41dc-a364-6f388037bd9e 02/06/23 18:24:58.583
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb  6 18:24:58.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-291" for this suite. 02/06/23 18:24:58.591
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":216,"skipped":4116,"failed":0}
------------------------------
• [0.042 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:24:58.555
    Feb  6 18:24:58.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 18:24:58.556
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:24:58.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:24:58.581
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-5dfc5160-3fd5-41dc-a364-6f388037bd9e 02/06/23 18:24:58.583
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 18:24:58.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-291" for this suite. 02/06/23 18:24:58.591
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:24:58.599
Feb  6 18:24:58.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename ingress 02/06/23 18:24:58.601
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:24:58.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:24:58.62
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 02/06/23 18:24:58.623
STEP: getting /apis/networking.k8s.io 02/06/23 18:24:58.625
STEP: getting /apis/networking.k8s.iov1 02/06/23 18:24:58.626
STEP: creating 02/06/23 18:24:58.627
STEP: getting 02/06/23 18:24:58.647
STEP: listing 02/06/23 18:24:58.651
STEP: watching 02/06/23 18:24:58.654
Feb  6 18:24:58.654: INFO: starting watch
STEP: cluster-wide listing 02/06/23 18:24:58.655
STEP: cluster-wide watching 02/06/23 18:24:58.659
Feb  6 18:24:58.659: INFO: starting watch
STEP: patching 02/06/23 18:24:58.66
STEP: updating 02/06/23 18:24:58.667
Feb  6 18:24:58.675: INFO: waiting for watch events with expected annotations
Feb  6 18:24:58.675: INFO: saw patched and updated annotations
STEP: patching /status 02/06/23 18:24:58.676
STEP: updating /status 02/06/23 18:24:58.684
STEP: get /status 02/06/23 18:24:58.693
STEP: deleting 02/06/23 18:24:58.696
STEP: deleting a collection 02/06/23 18:24:58.712
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Feb  6 18:24:58.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4167" for this suite. 02/06/23 18:24:58.735
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":217,"skipped":4118,"failed":0}
------------------------------
• [0.143 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:24:58.599
    Feb  6 18:24:58.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename ingress 02/06/23 18:24:58.601
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:24:58.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:24:58.62
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 02/06/23 18:24:58.623
    STEP: getting /apis/networking.k8s.io 02/06/23 18:24:58.625
    STEP: getting /apis/networking.k8s.iov1 02/06/23 18:24:58.626
    STEP: creating 02/06/23 18:24:58.627
    STEP: getting 02/06/23 18:24:58.647
    STEP: listing 02/06/23 18:24:58.651
    STEP: watching 02/06/23 18:24:58.654
    Feb  6 18:24:58.654: INFO: starting watch
    STEP: cluster-wide listing 02/06/23 18:24:58.655
    STEP: cluster-wide watching 02/06/23 18:24:58.659
    Feb  6 18:24:58.659: INFO: starting watch
    STEP: patching 02/06/23 18:24:58.66
    STEP: updating 02/06/23 18:24:58.667
    Feb  6 18:24:58.675: INFO: waiting for watch events with expected annotations
    Feb  6 18:24:58.675: INFO: saw patched and updated annotations
    STEP: patching /status 02/06/23 18:24:58.676
    STEP: updating /status 02/06/23 18:24:58.684
    STEP: get /status 02/06/23 18:24:58.693
    STEP: deleting 02/06/23 18:24:58.696
    STEP: deleting a collection 02/06/23 18:24:58.712
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Feb  6 18:24:58.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-4167" for this suite. 02/06/23 18:24:58.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:24:58.749
Feb  6 18:24:58.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename subpath 02/06/23 18:24:58.751
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:24:58.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:24:58.773
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/06/23 18:24:58.776
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-v57n 02/06/23 18:24:58.786
STEP: Creating a pod to test atomic-volume-subpath 02/06/23 18:24:58.786
Feb  6 18:24:58.797: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-v57n" in namespace "subpath-8642" to be "Succeeded or Failed"
Feb  6 18:24:58.809: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Pending", Reason="", readiness=false. Elapsed: 11.336759ms
Feb  6 18:25:00.815: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017233089s
Feb  6 18:25:02.814: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 4.016917535s
Feb  6 18:25:04.815: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 6.017150293s
Feb  6 18:25:06.816: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 8.018579272s
Feb  6 18:25:08.814: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 10.016685661s
Feb  6 18:25:10.814: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 12.016120155s
Feb  6 18:25:12.815: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 14.017187638s
Feb  6 18:25:14.817: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 16.019261476s
Feb  6 18:25:16.816: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 18.018012376s
Feb  6 18:25:18.814: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 20.0160832s
Feb  6 18:25:20.815: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 22.017337246s
Feb  6 18:25:22.815: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=false. Elapsed: 24.017173185s
Feb  6 18:25:24.814: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.016172788s
STEP: Saw pod success 02/06/23 18:25:24.814
Feb  6 18:25:24.814: INFO: Pod "pod-subpath-test-projected-v57n" satisfied condition "Succeeded or Failed"
Feb  6 18:25:24.817: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-subpath-test-projected-v57n container test-container-subpath-projected-v57n: <nil>
STEP: delete the pod 02/06/23 18:25:24.825
Feb  6 18:25:24.844: INFO: Waiting for pod pod-subpath-test-projected-v57n to disappear
Feb  6 18:25:24.849: INFO: Pod pod-subpath-test-projected-v57n no longer exists
STEP: Deleting pod pod-subpath-test-projected-v57n 02/06/23 18:25:24.849
Feb  6 18:25:24.849: INFO: Deleting pod "pod-subpath-test-projected-v57n" in namespace "subpath-8642"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb  6 18:25:24.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8642" for this suite. 02/06/23 18:25:24.858
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":218,"skipped":4199,"failed":0}
------------------------------
• [SLOW TEST] [26.116 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:24:58.749
    Feb  6 18:24:58.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename subpath 02/06/23 18:24:58.751
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:24:58.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:24:58.773
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/06/23 18:24:58.776
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-v57n 02/06/23 18:24:58.786
    STEP: Creating a pod to test atomic-volume-subpath 02/06/23 18:24:58.786
    Feb  6 18:24:58.797: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-v57n" in namespace "subpath-8642" to be "Succeeded or Failed"
    Feb  6 18:24:58.809: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Pending", Reason="", readiness=false. Elapsed: 11.336759ms
    Feb  6 18:25:00.815: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017233089s
    Feb  6 18:25:02.814: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 4.016917535s
    Feb  6 18:25:04.815: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 6.017150293s
    Feb  6 18:25:06.816: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 8.018579272s
    Feb  6 18:25:08.814: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 10.016685661s
    Feb  6 18:25:10.814: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 12.016120155s
    Feb  6 18:25:12.815: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 14.017187638s
    Feb  6 18:25:14.817: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 16.019261476s
    Feb  6 18:25:16.816: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 18.018012376s
    Feb  6 18:25:18.814: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 20.0160832s
    Feb  6 18:25:20.815: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=true. Elapsed: 22.017337246s
    Feb  6 18:25:22.815: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Running", Reason="", readiness=false. Elapsed: 24.017173185s
    Feb  6 18:25:24.814: INFO: Pod "pod-subpath-test-projected-v57n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.016172788s
    STEP: Saw pod success 02/06/23 18:25:24.814
    Feb  6 18:25:24.814: INFO: Pod "pod-subpath-test-projected-v57n" satisfied condition "Succeeded or Failed"
    Feb  6 18:25:24.817: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-subpath-test-projected-v57n container test-container-subpath-projected-v57n: <nil>
    STEP: delete the pod 02/06/23 18:25:24.825
    Feb  6 18:25:24.844: INFO: Waiting for pod pod-subpath-test-projected-v57n to disappear
    Feb  6 18:25:24.849: INFO: Pod pod-subpath-test-projected-v57n no longer exists
    STEP: Deleting pod pod-subpath-test-projected-v57n 02/06/23 18:25:24.849
    Feb  6 18:25:24.849: INFO: Deleting pod "pod-subpath-test-projected-v57n" in namespace "subpath-8642"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb  6 18:25:24.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8642" for this suite. 02/06/23 18:25:24.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:25:24.868
Feb  6 18:25:24.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 18:25:24.869
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:25:24.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:25:24.897
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 02/06/23 18:25:24.9
Feb  6 18:25:24.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1143 create -f -'
Feb  6 18:25:26.532: INFO: stderr: ""
Feb  6 18:25:26.532: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/06/23 18:25:26.532
Feb  6 18:25:27.538: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 18:25:27.538: INFO: Found 0 / 1
Feb  6 18:25:28.538: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 18:25:28.538: INFO: Found 0 / 1
Feb  6 18:25:29.538: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 18:25:29.538: INFO: Found 0 / 1
Feb  6 18:25:30.537: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 18:25:30.537: INFO: Found 1 / 1
Feb  6 18:25:30.538: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 02/06/23 18:25:30.538
Feb  6 18:25:30.541: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 18:25:30.541: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  6 18:25:30.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1143 patch pod agnhost-primary-fvsrs -p {"metadata":{"annotations":{"x":"y"}}}'
Feb  6 18:25:30.641: INFO: stderr: ""
Feb  6 18:25:30.641: INFO: stdout: "pod/agnhost-primary-fvsrs patched\n"
STEP: checking annotations 02/06/23 18:25:30.641
Feb  6 18:25:30.645: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  6 18:25:30.645: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 18:25:30.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1143" for this suite. 02/06/23 18:25:30.651
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":219,"skipped":4209,"failed":0}
------------------------------
• [SLOW TEST] [5.789 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:25:24.868
    Feb  6 18:25:24.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 18:25:24.869
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:25:24.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:25:24.897
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 02/06/23 18:25:24.9
    Feb  6 18:25:24.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1143 create -f -'
    Feb  6 18:25:26.532: INFO: stderr: ""
    Feb  6 18:25:26.532: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/06/23 18:25:26.532
    Feb  6 18:25:27.538: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 18:25:27.538: INFO: Found 0 / 1
    Feb  6 18:25:28.538: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 18:25:28.538: INFO: Found 0 / 1
    Feb  6 18:25:29.538: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 18:25:29.538: INFO: Found 0 / 1
    Feb  6 18:25:30.537: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 18:25:30.537: INFO: Found 1 / 1
    Feb  6 18:25:30.538: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 02/06/23 18:25:30.538
    Feb  6 18:25:30.541: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 18:25:30.541: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb  6 18:25:30.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-1143 patch pod agnhost-primary-fvsrs -p {"metadata":{"annotations":{"x":"y"}}}'
    Feb  6 18:25:30.641: INFO: stderr: ""
    Feb  6 18:25:30.641: INFO: stdout: "pod/agnhost-primary-fvsrs patched\n"
    STEP: checking annotations 02/06/23 18:25:30.641
    Feb  6 18:25:30.645: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb  6 18:25:30.645: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 18:25:30.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1143" for this suite. 02/06/23 18:25:30.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:25:30.659
Feb  6 18:25:30.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 18:25:30.661
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:25:30.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:25:30.683
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Feb  6 18:25:30.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/06/23 18:25:38.874
Feb  6 18:25:38.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 create -f -'
Feb  6 18:25:40.572: INFO: stderr: ""
Feb  6 18:25:40.572: INFO: stdout: "e2e-test-crd-publish-openapi-915-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb  6 18:25:40.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 delete e2e-test-crd-publish-openapi-915-crds test-foo'
Feb  6 18:25:40.707: INFO: stderr: ""
Feb  6 18:25:40.707: INFO: stdout: "e2e-test-crd-publish-openapi-915-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb  6 18:25:40.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 apply -f -'
Feb  6 18:25:41.131: INFO: stderr: ""
Feb  6 18:25:41.131: INFO: stdout: "e2e-test-crd-publish-openapi-915-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb  6 18:25:41.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 delete e2e-test-crd-publish-openapi-915-crds test-foo'
Feb  6 18:25:41.282: INFO: stderr: ""
Feb  6 18:25:41.282: INFO: stdout: "e2e-test-crd-publish-openapi-915-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/06/23 18:25:41.282
Feb  6 18:25:41.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 create -f -'
Feb  6 18:25:42.675: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/06/23 18:25:42.675
Feb  6 18:25:42.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 create -f -'
Feb  6 18:25:44.126: INFO: rc: 1
Feb  6 18:25:44.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 apply -f -'
Feb  6 18:25:44.545: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/06/23 18:25:44.545
Feb  6 18:25:44.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 create -f -'
Feb  6 18:25:44.946: INFO: rc: 1
Feb  6 18:25:44.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 apply -f -'
Feb  6 18:25:45.365: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 02/06/23 18:25:45.366
Feb  6 18:25:45.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 explain e2e-test-crd-publish-openapi-915-crds'
Feb  6 18:25:45.798: INFO: stderr: ""
Feb  6 18:25:45.798: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-915-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 02/06/23 18:25:45.798
Feb  6 18:25:45.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 explain e2e-test-crd-publish-openapi-915-crds.metadata'
Feb  6 18:25:46.225: INFO: stderr: ""
Feb  6 18:25:46.225: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-915-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb  6 18:25:46.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 explain e2e-test-crd-publish-openapi-915-crds.spec'
Feb  6 18:25:46.643: INFO: stderr: ""
Feb  6 18:25:46.643: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-915-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb  6 18:25:46.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 explain e2e-test-crd-publish-openapi-915-crds.spec.bars'
Feb  6 18:25:47.030: INFO: stderr: ""
Feb  6 18:25:47.030: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-915-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/06/23 18:25:47.03
Feb  6 18:25:47.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 explain e2e-test-crd-publish-openapi-915-crds.spec.bars2'
Feb  6 18:25:47.435: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:25:55.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7061" for this suite. 02/06/23 18:25:55.578
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":220,"skipped":4214,"failed":0}
------------------------------
• [SLOW TEST] [24.935 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:25:30.659
    Feb  6 18:25:30.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 18:25:30.661
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:25:30.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:25:30.683
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Feb  6 18:25:30.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/06/23 18:25:38.874
    Feb  6 18:25:38.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 create -f -'
    Feb  6 18:25:40.572: INFO: stderr: ""
    Feb  6 18:25:40.572: INFO: stdout: "e2e-test-crd-publish-openapi-915-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb  6 18:25:40.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 delete e2e-test-crd-publish-openapi-915-crds test-foo'
    Feb  6 18:25:40.707: INFO: stderr: ""
    Feb  6 18:25:40.707: INFO: stdout: "e2e-test-crd-publish-openapi-915-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Feb  6 18:25:40.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 apply -f -'
    Feb  6 18:25:41.131: INFO: stderr: ""
    Feb  6 18:25:41.131: INFO: stdout: "e2e-test-crd-publish-openapi-915-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb  6 18:25:41.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 delete e2e-test-crd-publish-openapi-915-crds test-foo'
    Feb  6 18:25:41.282: INFO: stderr: ""
    Feb  6 18:25:41.282: INFO: stdout: "e2e-test-crd-publish-openapi-915-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/06/23 18:25:41.282
    Feb  6 18:25:41.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 create -f -'
    Feb  6 18:25:42.675: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/06/23 18:25:42.675
    Feb  6 18:25:42.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 create -f -'
    Feb  6 18:25:44.126: INFO: rc: 1
    Feb  6 18:25:44.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 apply -f -'
    Feb  6 18:25:44.545: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/06/23 18:25:44.545
    Feb  6 18:25:44.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 create -f -'
    Feb  6 18:25:44.946: INFO: rc: 1
    Feb  6 18:25:44.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 --namespace=crd-publish-openapi-7061 apply -f -'
    Feb  6 18:25:45.365: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 02/06/23 18:25:45.366
    Feb  6 18:25:45.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 explain e2e-test-crd-publish-openapi-915-crds'
    Feb  6 18:25:45.798: INFO: stderr: ""
    Feb  6 18:25:45.798: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-915-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 02/06/23 18:25:45.798
    Feb  6 18:25:45.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 explain e2e-test-crd-publish-openapi-915-crds.metadata'
    Feb  6 18:25:46.225: INFO: stderr: ""
    Feb  6 18:25:46.225: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-915-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Feb  6 18:25:46.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 explain e2e-test-crd-publish-openapi-915-crds.spec'
    Feb  6 18:25:46.643: INFO: stderr: ""
    Feb  6 18:25:46.643: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-915-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Feb  6 18:25:46.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 explain e2e-test-crd-publish-openapi-915-crds.spec.bars'
    Feb  6 18:25:47.030: INFO: stderr: ""
    Feb  6 18:25:47.030: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-915-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/06/23 18:25:47.03
    Feb  6 18:25:47.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-7061 explain e2e-test-crd-publish-openapi-915-crds.spec.bars2'
    Feb  6 18:25:47.435: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:25:55.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7061" for this suite. 02/06/23 18:25:55.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:25:55.596
Feb  6 18:25:55.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 18:25:55.597
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:25:55.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:25:55.638
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 02/06/23 18:25:55.641
Feb  6 18:25:55.651: INFO: Waiting up to 5m0s for pod "downward-api-950daf89-1fb5-4781-919e-2ada2c030205" in namespace "downward-api-2840" to be "Succeeded or Failed"
Feb  6 18:25:55.659: INFO: Pod "downward-api-950daf89-1fb5-4781-919e-2ada2c030205": Phase="Pending", Reason="", readiness=false. Elapsed: 7.470789ms
Feb  6 18:25:57.664: INFO: Pod "downward-api-950daf89-1fb5-4781-919e-2ada2c030205": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012443416s
Feb  6 18:25:59.662: INFO: Pod "downward-api-950daf89-1fb5-4781-919e-2ada2c030205": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011063876s
Feb  6 18:26:01.664: INFO: Pod "downward-api-950daf89-1fb5-4781-919e-2ada2c030205": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012239216s
STEP: Saw pod success 02/06/23 18:26:01.664
Feb  6 18:26:01.664: INFO: Pod "downward-api-950daf89-1fb5-4781-919e-2ada2c030205" satisfied condition "Succeeded or Failed"
Feb  6 18:26:01.667: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downward-api-950daf89-1fb5-4781-919e-2ada2c030205 container dapi-container: <nil>
STEP: delete the pod 02/06/23 18:26:01.682
Feb  6 18:26:01.694: INFO: Waiting for pod downward-api-950daf89-1fb5-4781-919e-2ada2c030205 to disappear
Feb  6 18:26:01.699: INFO: Pod downward-api-950daf89-1fb5-4781-919e-2ada2c030205 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb  6 18:26:01.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2840" for this suite. 02/06/23 18:26:01.706
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":221,"skipped":4227,"failed":0}
------------------------------
• [SLOW TEST] [6.116 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:25:55.596
    Feb  6 18:25:55.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 18:25:55.597
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:25:55.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:25:55.638
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 02/06/23 18:25:55.641
    Feb  6 18:25:55.651: INFO: Waiting up to 5m0s for pod "downward-api-950daf89-1fb5-4781-919e-2ada2c030205" in namespace "downward-api-2840" to be "Succeeded or Failed"
    Feb  6 18:25:55.659: INFO: Pod "downward-api-950daf89-1fb5-4781-919e-2ada2c030205": Phase="Pending", Reason="", readiness=false. Elapsed: 7.470789ms
    Feb  6 18:25:57.664: INFO: Pod "downward-api-950daf89-1fb5-4781-919e-2ada2c030205": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012443416s
    Feb  6 18:25:59.662: INFO: Pod "downward-api-950daf89-1fb5-4781-919e-2ada2c030205": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011063876s
    Feb  6 18:26:01.664: INFO: Pod "downward-api-950daf89-1fb5-4781-919e-2ada2c030205": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012239216s
    STEP: Saw pod success 02/06/23 18:26:01.664
    Feb  6 18:26:01.664: INFO: Pod "downward-api-950daf89-1fb5-4781-919e-2ada2c030205" satisfied condition "Succeeded or Failed"
    Feb  6 18:26:01.667: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downward-api-950daf89-1fb5-4781-919e-2ada2c030205 container dapi-container: <nil>
    STEP: delete the pod 02/06/23 18:26:01.682
    Feb  6 18:26:01.694: INFO: Waiting for pod downward-api-950daf89-1fb5-4781-919e-2ada2c030205 to disappear
    Feb  6 18:26:01.699: INFO: Pod downward-api-950daf89-1fb5-4781-919e-2ada2c030205 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb  6 18:26:01.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2840" for this suite. 02/06/23 18:26:01.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:26:01.724
Feb  6 18:26:01.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename security-context-test 02/06/23 18:26:01.725
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:26:01.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:26:01.742
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Feb  6 18:26:01.752: INFO: Waiting up to 5m0s for pod "busybox-user-65534-815d0284-377c-4fdb-a264-b28fe0a0df11" in namespace "security-context-test-6434" to be "Succeeded or Failed"
Feb  6 18:26:01.756: INFO: Pod "busybox-user-65534-815d0284-377c-4fdb-a264-b28fe0a0df11": Phase="Pending", Reason="", readiness=false. Elapsed: 3.460255ms
Feb  6 18:26:03.761: INFO: Pod "busybox-user-65534-815d0284-377c-4fdb-a264-b28fe0a0df11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008621477s
Feb  6 18:26:05.762: INFO: Pod "busybox-user-65534-815d0284-377c-4fdb-a264-b28fe0a0df11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010163993s
Feb  6 18:26:07.762: INFO: Pod "busybox-user-65534-815d0284-377c-4fdb-a264-b28fe0a0df11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010227717s
Feb  6 18:26:07.762: INFO: Pod "busybox-user-65534-815d0284-377c-4fdb-a264-b28fe0a0df11" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb  6 18:26:07.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6434" for this suite. 02/06/23 18:26:07.769
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":222,"skipped":4289,"failed":0}
------------------------------
• [SLOW TEST] [6.052 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:26:01.724
    Feb  6 18:26:01.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename security-context-test 02/06/23 18:26:01.725
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:26:01.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:26:01.742
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Feb  6 18:26:01.752: INFO: Waiting up to 5m0s for pod "busybox-user-65534-815d0284-377c-4fdb-a264-b28fe0a0df11" in namespace "security-context-test-6434" to be "Succeeded or Failed"
    Feb  6 18:26:01.756: INFO: Pod "busybox-user-65534-815d0284-377c-4fdb-a264-b28fe0a0df11": Phase="Pending", Reason="", readiness=false. Elapsed: 3.460255ms
    Feb  6 18:26:03.761: INFO: Pod "busybox-user-65534-815d0284-377c-4fdb-a264-b28fe0a0df11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008621477s
    Feb  6 18:26:05.762: INFO: Pod "busybox-user-65534-815d0284-377c-4fdb-a264-b28fe0a0df11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010163993s
    Feb  6 18:26:07.762: INFO: Pod "busybox-user-65534-815d0284-377c-4fdb-a264-b28fe0a0df11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010227717s
    Feb  6 18:26:07.762: INFO: Pod "busybox-user-65534-815d0284-377c-4fdb-a264-b28fe0a0df11" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb  6 18:26:07.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-6434" for this suite. 02/06/23 18:26:07.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:26:07.78
Feb  6 18:26:07.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:26:07.781
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:26:07.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:26:07.798
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-bc9ca4aa-ddd0-4814-a5bd-54830b05a27c 02/06/23 18:26:07.805
STEP: Creating configMap with name cm-test-opt-upd-f583b874-54b4-4ddd-a6db-afff01ac0e25 02/06/23 18:26:07.81
STEP: Creating the pod 02/06/23 18:26:07.814
Feb  6 18:26:07.829: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba" in namespace "projected-5718" to be "running and ready"
Feb  6 18:26:07.835: INFO: Pod "pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.866779ms
Feb  6 18:26:07.835: INFO: The phase of Pod pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:26:09.839: INFO: Pod "pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009790162s
Feb  6 18:26:09.839: INFO: The phase of Pod pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:26:11.839: INFO: Pod "pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009727558s
Feb  6 18:26:11.839: INFO: The phase of Pod pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:26:13.839: INFO: Pod "pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba": Phase="Running", Reason="", readiness=true. Elapsed: 6.009700919s
Feb  6 18:26:13.839: INFO: The phase of Pod pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba is Running (Ready = true)
Feb  6 18:26:13.839: INFO: Pod "pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-bc9ca4aa-ddd0-4814-a5bd-54830b05a27c 02/06/23 18:26:13.862
STEP: Updating configmap cm-test-opt-upd-f583b874-54b4-4ddd-a6db-afff01ac0e25 02/06/23 18:26:13.868
STEP: Creating configMap with name cm-test-opt-create-ba9ff335-e161-409e-8400-416746731337 02/06/23 18:26:13.875
STEP: waiting to observe update in volume 02/06/23 18:26:13.88
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb  6 18:27:38.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5718" for this suite. 02/06/23 18:27:38.278
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":223,"skipped":4294,"failed":0}
------------------------------
• [SLOW TEST] [90.505 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:26:07.78
    Feb  6 18:26:07.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:26:07.781
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:26:07.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:26:07.798
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-bc9ca4aa-ddd0-4814-a5bd-54830b05a27c 02/06/23 18:26:07.805
    STEP: Creating configMap with name cm-test-opt-upd-f583b874-54b4-4ddd-a6db-afff01ac0e25 02/06/23 18:26:07.81
    STEP: Creating the pod 02/06/23 18:26:07.814
    Feb  6 18:26:07.829: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba" in namespace "projected-5718" to be "running and ready"
    Feb  6 18:26:07.835: INFO: Pod "pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.866779ms
    Feb  6 18:26:07.835: INFO: The phase of Pod pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:26:09.839: INFO: Pod "pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009790162s
    Feb  6 18:26:09.839: INFO: The phase of Pod pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:26:11.839: INFO: Pod "pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009727558s
    Feb  6 18:26:11.839: INFO: The phase of Pod pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:26:13.839: INFO: Pod "pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba": Phase="Running", Reason="", readiness=true. Elapsed: 6.009700919s
    Feb  6 18:26:13.839: INFO: The phase of Pod pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba is Running (Ready = true)
    Feb  6 18:26:13.839: INFO: Pod "pod-projected-configmaps-cc15587f-8857-4802-8b95-8ab15028b8ba" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-bc9ca4aa-ddd0-4814-a5bd-54830b05a27c 02/06/23 18:26:13.862
    STEP: Updating configmap cm-test-opt-upd-f583b874-54b4-4ddd-a6db-afff01ac0e25 02/06/23 18:26:13.868
    STEP: Creating configMap with name cm-test-opt-create-ba9ff335-e161-409e-8400-416746731337 02/06/23 18:26:13.875
    STEP: waiting to observe update in volume 02/06/23 18:26:13.88
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb  6 18:27:38.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5718" for this suite. 02/06/23 18:27:38.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:27:38.285
Feb  6 18:27:38.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename endpointslicemirroring 02/06/23 18:27:38.286
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:27:38.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:27:38.304
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 02/06/23 18:27:38.325
Feb  6 18:27:38.336: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 02/06/23 18:27:40.345
Feb  6 18:27:40.360: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 02/06/23 18:27:42.365
Feb  6 18:27:42.380: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Feb  6 18:27:44.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-8125" for this suite. 02/06/23 18:27:44.396
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":224,"skipped":4299,"failed":0}
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:27:38.285
    Feb  6 18:27:38.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename endpointslicemirroring 02/06/23 18:27:38.286
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:27:38.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:27:38.304
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 02/06/23 18:27:38.325
    Feb  6 18:27:38.336: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 02/06/23 18:27:40.345
    Feb  6 18:27:40.360: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 02/06/23 18:27:42.365
    Feb  6 18:27:42.380: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Feb  6 18:27:44.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-8125" for this suite. 02/06/23 18:27:44.396
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:27:44.403
Feb  6 18:27:44.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pod-network-test 02/06/23 18:27:44.404
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:27:44.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:27:44.421
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-9666 02/06/23 18:27:44.424
STEP: creating a selector 02/06/23 18:27:44.424
STEP: Creating the service pods in kubernetes 02/06/23 18:27:44.425
Feb  6 18:27:44.425: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb  6 18:27:44.457: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9666" to be "running and ready"
Feb  6 18:27:44.468: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.557627ms
Feb  6 18:27:44.468: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:27:46.473: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015599877s
Feb  6 18:27:46.473: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:27:48.472: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014599017s
Feb  6 18:27:48.472: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:27:50.473: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015350162s
Feb  6 18:27:50.473: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:27:52.473: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015986783s
Feb  6 18:27:52.474: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:27:54.473: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015832454s
Feb  6 18:27:54.473: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:27:56.473: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015368864s
Feb  6 18:27:56.473: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:27:58.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017382032s
Feb  6 18:27:58.475: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:28:00.474: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016856278s
Feb  6 18:28:00.474: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:28:02.472: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014790386s
Feb  6 18:28:02.472: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:28:04.474: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016480372s
Feb  6 18:28:04.474: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:28:06.472: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015102157s
Feb  6 18:28:06.472: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb  6 18:28:06.472: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb  6 18:28:06.476: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9666" to be "running and ready"
Feb  6 18:28:06.480: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.401471ms
Feb  6 18:28:06.480: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb  6 18:28:06.480: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb  6 18:28:06.483: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9666" to be "running and ready"
Feb  6 18:28:06.495: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.317806ms
Feb  6 18:28:06.495: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb  6 18:28:06.495: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/06/23 18:28:06.498
Feb  6 18:28:06.515: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9666" to be "running"
Feb  6 18:28:06.519: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428452ms
Feb  6 18:28:08.526: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010714199s
Feb  6 18:28:10.527: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012350562s
Feb  6 18:28:10.528: INFO: Pod "test-container-pod" satisfied condition "running"
Feb  6 18:28:10.533: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9666" to be "running"
Feb  6 18:28:10.538: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.796862ms
Feb  6 18:28:10.538: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb  6 18:28:10.541: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb  6 18:28:10.541: INFO: Going to poll 192.168.2.186 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb  6 18:28:10.548: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.2.186 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9666 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:28:10.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:28:10.549: INFO: ExecWithOptions: Clientset creation
Feb  6 18:28:10.549: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9666/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.2.186+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb  6 18:28:11.629: INFO: Found all 1 expected endpoints: [netserver-0]
Feb  6 18:28:11.629: INFO: Going to poll 192.168.3.38 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb  6 18:28:11.634: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.3.38 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9666 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:28:11.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:28:11.635: INFO: ExecWithOptions: Clientset creation
Feb  6 18:28:11.635: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9666/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.3.38+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb  6 18:28:12.717: INFO: Found all 1 expected endpoints: [netserver-1]
Feb  6 18:28:12.717: INFO: Going to poll 192.168.1.179 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb  6 18:28:12.721: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.1.179 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9666 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:28:12.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:28:12.721: INFO: ExecWithOptions: Clientset creation
Feb  6 18:28:12.722: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9666/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.1.179+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb  6 18:28:13.793: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb  6 18:28:13.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9666" for this suite. 02/06/23 18:28:13.799
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":225,"skipped":4301,"failed":0}
------------------------------
• [SLOW TEST] [29.402 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:27:44.403
    Feb  6 18:27:44.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pod-network-test 02/06/23 18:27:44.404
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:27:44.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:27:44.421
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-9666 02/06/23 18:27:44.424
    STEP: creating a selector 02/06/23 18:27:44.424
    STEP: Creating the service pods in kubernetes 02/06/23 18:27:44.425
    Feb  6 18:27:44.425: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb  6 18:27:44.457: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9666" to be "running and ready"
    Feb  6 18:27:44.468: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.557627ms
    Feb  6 18:27:44.468: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:27:46.473: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015599877s
    Feb  6 18:27:46.473: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:27:48.472: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014599017s
    Feb  6 18:27:48.472: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:27:50.473: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015350162s
    Feb  6 18:27:50.473: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:27:52.473: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015986783s
    Feb  6 18:27:52.474: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:27:54.473: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015832454s
    Feb  6 18:27:54.473: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:27:56.473: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015368864s
    Feb  6 18:27:56.473: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:27:58.475: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017382032s
    Feb  6 18:27:58.475: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:28:00.474: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016856278s
    Feb  6 18:28:00.474: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:28:02.472: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014790386s
    Feb  6 18:28:02.472: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:28:04.474: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016480372s
    Feb  6 18:28:04.474: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:28:06.472: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015102157s
    Feb  6 18:28:06.472: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb  6 18:28:06.472: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb  6 18:28:06.476: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9666" to be "running and ready"
    Feb  6 18:28:06.480: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.401471ms
    Feb  6 18:28:06.480: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb  6 18:28:06.480: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb  6 18:28:06.483: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9666" to be "running and ready"
    Feb  6 18:28:06.495: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.317806ms
    Feb  6 18:28:06.495: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb  6 18:28:06.495: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/06/23 18:28:06.498
    Feb  6 18:28:06.515: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9666" to be "running"
    Feb  6 18:28:06.519: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428452ms
    Feb  6 18:28:08.526: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010714199s
    Feb  6 18:28:10.527: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012350562s
    Feb  6 18:28:10.528: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb  6 18:28:10.533: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9666" to be "running"
    Feb  6 18:28:10.538: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.796862ms
    Feb  6 18:28:10.538: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb  6 18:28:10.541: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb  6 18:28:10.541: INFO: Going to poll 192.168.2.186 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb  6 18:28:10.548: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.2.186 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9666 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:28:10.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:28:10.549: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:28:10.549: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9666/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.2.186+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb  6 18:28:11.629: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb  6 18:28:11.629: INFO: Going to poll 192.168.3.38 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb  6 18:28:11.634: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.3.38 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9666 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:28:11.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:28:11.635: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:28:11.635: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9666/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.3.38+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb  6 18:28:12.717: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb  6 18:28:12.717: INFO: Going to poll 192.168.1.179 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb  6 18:28:12.721: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.1.179 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9666 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:28:12.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:28:12.721: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:28:12.722: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9666/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.1.179+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb  6 18:28:13.793: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb  6 18:28:13.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9666" for this suite. 02/06/23 18:28:13.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:28:13.807
Feb  6 18:28:13.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename containers 02/06/23 18:28:13.808
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:13.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:13.86
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 02/06/23 18:28:13.863
Feb  6 18:28:13.871: INFO: Waiting up to 5m0s for pod "client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0" in namespace "containers-6269" to be "Succeeded or Failed"
Feb  6 18:28:13.877: INFO: Pod "client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.208763ms
Feb  6 18:28:15.881: INFO: Pod "client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010363536s
Feb  6 18:28:17.882: INFO: Pod "client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011207432s
Feb  6 18:28:19.883: INFO: Pod "client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012625202s
STEP: Saw pod success 02/06/23 18:28:19.884
Feb  6 18:28:19.884: INFO: Pod "client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0" satisfied condition "Succeeded or Failed"
Feb  6 18:28:19.888: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0 container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:28:19.894
Feb  6 18:28:19.908: INFO: Waiting for pod client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0 to disappear
Feb  6 18:28:19.913: INFO: Pod client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb  6 18:28:19.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6269" for this suite. 02/06/23 18:28:19.918
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":226,"skipped":4314,"failed":0}
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:28:13.807
    Feb  6 18:28:13.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename containers 02/06/23 18:28:13.808
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:13.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:13.86
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 02/06/23 18:28:13.863
    Feb  6 18:28:13.871: INFO: Waiting up to 5m0s for pod "client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0" in namespace "containers-6269" to be "Succeeded or Failed"
    Feb  6 18:28:13.877: INFO: Pod "client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.208763ms
    Feb  6 18:28:15.881: INFO: Pod "client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010363536s
    Feb  6 18:28:17.882: INFO: Pod "client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011207432s
    Feb  6 18:28:19.883: INFO: Pod "client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012625202s
    STEP: Saw pod success 02/06/23 18:28:19.884
    Feb  6 18:28:19.884: INFO: Pod "client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0" satisfied condition "Succeeded or Failed"
    Feb  6 18:28:19.888: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0 container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:28:19.894
    Feb  6 18:28:19.908: INFO: Waiting for pod client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0 to disappear
    Feb  6 18:28:19.913: INFO: Pod client-containers-82363700-64c9-4d5d-87bd-ba4a008ec3c0 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb  6 18:28:19.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-6269" for this suite. 02/06/23 18:28:19.918
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:28:19.925
Feb  6 18:28:19.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 18:28:19.927
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:19.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:19.946
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 18:28:19.966
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:28:20.273
STEP: Deploying the webhook pod 02/06/23 18:28:20.28
STEP: Wait for the deployment to be ready 02/06/23 18:28:20.294
Feb  6 18:28:20.303: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 18:28:22.313: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 28, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 28, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 28, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 28, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 18:28:24.318
STEP: Verifying the service has paired with the endpoint 02/06/23 18:28:24.338
Feb  6 18:28:25.338: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/06/23 18:28:25.342
STEP: create a configmap that should be updated by the webhook 02/06/23 18:28:25.36
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:28:25.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5205" for this suite. 02/06/23 18:28:25.388
STEP: Destroying namespace "webhook-5205-markers" for this suite. 02/06/23 18:28:25.396
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":227,"skipped":4316,"failed":0}
------------------------------
• [SLOW TEST] [5.585 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:28:19.925
    Feb  6 18:28:19.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 18:28:19.927
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:19.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:19.946
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 18:28:19.966
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:28:20.273
    STEP: Deploying the webhook pod 02/06/23 18:28:20.28
    STEP: Wait for the deployment to be ready 02/06/23 18:28:20.294
    Feb  6 18:28:20.303: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 18:28:22.313: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 28, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 28, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 28, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 28, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 18:28:24.318
    STEP: Verifying the service has paired with the endpoint 02/06/23 18:28:24.338
    Feb  6 18:28:25.338: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/06/23 18:28:25.342
    STEP: create a configmap that should be updated by the webhook 02/06/23 18:28:25.36
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:28:25.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5205" for this suite. 02/06/23 18:28:25.388
    STEP: Destroying namespace "webhook-5205-markers" for this suite. 02/06/23 18:28:25.396
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:28:25.514
Feb  6 18:28:25.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename disruption 02/06/23 18:28:25.515
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:25.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:25.552
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 02/06/23 18:28:25.56
STEP: Waiting for the pdb to be processed 02/06/23 18:28:25.569
STEP: updating the pdb 02/06/23 18:28:27.587
STEP: Waiting for the pdb to be processed 02/06/23 18:28:27.596
STEP: patching the pdb 02/06/23 18:28:29.611
STEP: Waiting for the pdb to be processed 02/06/23 18:28:29.623
STEP: Waiting for the pdb to be deleted 02/06/23 18:28:31.641
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb  6 18:28:31.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7437" for this suite. 02/06/23 18:28:31.648
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":228,"skipped":4342,"failed":0}
------------------------------
• [SLOW TEST] [6.140 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:28:25.514
    Feb  6 18:28:25.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename disruption 02/06/23 18:28:25.515
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:25.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:25.552
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 02/06/23 18:28:25.56
    STEP: Waiting for the pdb to be processed 02/06/23 18:28:25.569
    STEP: updating the pdb 02/06/23 18:28:27.587
    STEP: Waiting for the pdb to be processed 02/06/23 18:28:27.596
    STEP: patching the pdb 02/06/23 18:28:29.611
    STEP: Waiting for the pdb to be processed 02/06/23 18:28:29.623
    STEP: Waiting for the pdb to be deleted 02/06/23 18:28:31.641
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb  6 18:28:31.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7437" for this suite. 02/06/23 18:28:31.648
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:28:31.655
Feb  6 18:28:31.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:28:31.656
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:31.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:31.679
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-fd823cfd-d6b0-4581-9caa-5ccfea451adf 02/06/23 18:28:31.682
STEP: Creating a pod to test consume secrets 02/06/23 18:28:31.686
Feb  6 18:28:31.698: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b" in namespace "projected-4242" to be "Succeeded or Failed"
Feb  6 18:28:31.702: INFO: Pod "pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.323209ms
Feb  6 18:28:33.707: INFO: Pod "pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008738034s
Feb  6 18:28:35.706: INFO: Pod "pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007616596s
Feb  6 18:28:37.706: INFO: Pod "pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00784124s
STEP: Saw pod success 02/06/23 18:28:37.707
Feb  6 18:28:37.707: INFO: Pod "pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b" satisfied condition "Succeeded or Failed"
Feb  6 18:28:37.711: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b container projected-secret-volume-test: <nil>
STEP: delete the pod 02/06/23 18:28:37.717
Feb  6 18:28:37.735: INFO: Waiting for pod pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b to disappear
Feb  6 18:28:37.738: INFO: Pod pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb  6 18:28:37.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4242" for this suite. 02/06/23 18:28:37.743
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":229,"skipped":4345,"failed":0}
------------------------------
• [SLOW TEST] [6.094 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:28:31.655
    Feb  6 18:28:31.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:28:31.656
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:31.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:31.679
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-fd823cfd-d6b0-4581-9caa-5ccfea451adf 02/06/23 18:28:31.682
    STEP: Creating a pod to test consume secrets 02/06/23 18:28:31.686
    Feb  6 18:28:31.698: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b" in namespace "projected-4242" to be "Succeeded or Failed"
    Feb  6 18:28:31.702: INFO: Pod "pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.323209ms
    Feb  6 18:28:33.707: INFO: Pod "pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008738034s
    Feb  6 18:28:35.706: INFO: Pod "pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007616596s
    Feb  6 18:28:37.706: INFO: Pod "pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00784124s
    STEP: Saw pod success 02/06/23 18:28:37.707
    Feb  6 18:28:37.707: INFO: Pod "pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b" satisfied condition "Succeeded or Failed"
    Feb  6 18:28:37.711: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 18:28:37.717
    Feb  6 18:28:37.735: INFO: Waiting for pod pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b to disappear
    Feb  6 18:28:37.738: INFO: Pod pod-projected-secrets-7d023fd3-cad6-4bab-a7c6-491cfa76858b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb  6 18:28:37.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4242" for this suite. 02/06/23 18:28:37.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:28:37.75
Feb  6 18:28:37.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename sched-pred 02/06/23 18:28:37.751
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:37.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:37.772
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb  6 18:28:37.774: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  6 18:28:37.790: INFO: Waiting for terminating namespaces to be deleted...
Feb  6 18:28:37.798: INFO: 
Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-5xxmw before test
Feb  6 18:28:37.808: INFO: cilium-9gdv4 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.808: INFO: 	Container cilium-agent ready: true, restart count 0
Feb  6 18:28:37.808: INFO: kube-proxy-mnvm8 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.808: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  6 18:28:37.808: INFO: vsphere-cloud-controller-manager-7dmv6 from kube-system started at 2023-02-06 17:31:18 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.808: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Feb  6 18:28:37.808: INFO: vsphere-csi-node-7l6qr from kube-system started at 2023-02-06 17:12:22 +0000 UTC (3 container statuses recorded)
Feb  6 18:28:37.808: INFO: 	Container liveness-probe ready: true, restart count 0
Feb  6 18:28:37.808: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  6 18:28:37.808: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Feb  6 18:28:37.808: INFO: sonobuoy from sonobuoy started at 2023-02-06 17:18:54 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.808: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  6 18:28:37.808: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-cr8qs from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 18:28:37.808: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 18:28:37.808: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  6 18:28:37.808: INFO: 
Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-6j94p before test
Feb  6 18:28:37.819: INFO: capi-kubeadm-control-plane-controller-manager-5f686d9d47-lkld2 from capi-kubeadm-control-plane-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.820: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:28:37.820: INFO: capi-controller-manager-5b68b88d59-vcpp2 from capi-system started at 2023-02-06 17:15:34 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.820: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:28:37.820: INFO: cert-manager-7b9fcd6db9-rz78g from cert-manager started at 2023-02-06 17:15:16 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.820: INFO: 	Container cert-manager ready: true, restart count 0
Feb  6 18:28:37.820: INFO: cert-manager-cainjector-75b587b679-rjbzj from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.820: INFO: 	Container cert-manager ready: true, restart count 0
Feb  6 18:28:37.820: INFO: cert-manager-webhook-896b549c4-hj9c4 from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.820: INFO: 	Container cert-manager ready: true, restart count 0
Feb  6 18:28:37.820: INFO: eks-anywhere-packages-8589bd5d9c-4pc9v from eksa-packages started at 2023-02-06 17:17:15 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.820: INFO: 	Container controller ready: true, restart count 0
Feb  6 18:28:37.820: INFO: etcdadm-bootstrap-provider-controller-manager-84ff796964-d6tt8 from etcdadm-bootstrap-provider-system started at 2023-02-06 17:15:37 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.820: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:28:37.820: INFO: cilium-nb887 from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.820: INFO: 	Container cilium-agent ready: true, restart count 0
Feb  6 18:28:37.820: INFO: kube-proxy-ld4nv from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.820: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  6 18:28:37.820: INFO: vsphere-cloud-controller-manager-76l5k from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.820: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Feb  6 18:28:37.820: INFO: vsphere-csi-node-2tfxk from kube-system started at 2023-02-06 17:12:26 +0000 UTC (3 container statuses recorded)
Feb  6 18:28:37.820: INFO: 	Container liveness-probe ready: true, restart count 0
Feb  6 18:28:37.820: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  6 18:28:37.820: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Feb  6 18:28:37.820: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-hcvcv from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 18:28:37.820: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 18:28:37.820: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  6 18:28:37.820: INFO: 
Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-lz542 before test
Feb  6 18:28:37.835: INFO: capi-kubeadm-bootstrap-controller-manager-79d4bdf548-4xmjq from capi-kubeadm-bootstrap-system started at 2023-02-06 17:15:36 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.835: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:28:37.835: INFO: capv-controller-manager-598f9bf684-4qdws from capv-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.835: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:28:37.835: INFO: eksa-controller-manager-5588899966-wtcpc from eksa-system started at 2023-02-06 17:16:14 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.835: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:28:37.835: INFO: etcdadm-controller-controller-manager-767b7fb9b5-mt7lx from etcdadm-controller-system started at 2023-02-06 17:15:38 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.835: INFO: 	Container manager ready: true, restart count 0
Feb  6 18:28:37.835: INFO: cilium-ndf4k from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.835: INFO: 	Container cilium-agent ready: true, restart count 0
Feb  6 18:28:37.835: INFO: cilium-operator-7f966b595f-pzn9c from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.835: INFO: 	Container cilium-operator ready: true, restart count 0
Feb  6 18:28:37.835: INFO: kube-proxy-hfblf from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.835: INFO: 	Container kube-proxy ready: true, restart count 0
Feb  6 18:28:37.835: INFO: vsphere-cloud-controller-manager-x8k8p from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
Feb  6 18:28:37.835: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Feb  6 18:28:37.835: INFO: vsphere-csi-node-v4zd6 from kube-system started at 2023-02-06 17:12:15 +0000 UTC (3 container statuses recorded)
Feb  6 18:28:37.835: INFO: 	Container liveness-probe ready: true, restart count 0
Feb  6 18:28:37.835: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  6 18:28:37.835: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Feb  6 18:28:37.835: INFO: sonobuoy-e2e-job-b0e9e31f5d894166 from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 18:28:37.835: INFO: 	Container e2e ready: true, restart count 0
Feb  6 18:28:37.835: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 18:28:37.835: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-fncmf from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
Feb  6 18:28:37.835: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  6 18:28:37.835: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 02/06/23 18:28:37.836
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17414fa7a8e5bbf4], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 6 node(s) didn't match Pod's node affinity/selector. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 02/06/23 18:28:37.877
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb  6 18:28:38.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-959" for this suite. 02/06/23 18:28:38.879
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":230,"skipped":4366,"failed":0}
------------------------------
• [1.135 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:28:37.75
    Feb  6 18:28:37.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename sched-pred 02/06/23 18:28:37.751
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:37.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:37.772
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb  6 18:28:37.774: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb  6 18:28:37.790: INFO: Waiting for terminating namespaces to be deleted...
    Feb  6 18:28:37.798: INFO: 
    Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-5xxmw before test
    Feb  6 18:28:37.808: INFO: cilium-9gdv4 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.808: INFO: 	Container cilium-agent ready: true, restart count 0
    Feb  6 18:28:37.808: INFO: kube-proxy-mnvm8 from kube-system started at 2023-02-06 17:12:22 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.808: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb  6 18:28:37.808: INFO: vsphere-cloud-controller-manager-7dmv6 from kube-system started at 2023-02-06 17:31:18 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.808: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
    Feb  6 18:28:37.808: INFO: vsphere-csi-node-7l6qr from kube-system started at 2023-02-06 17:12:22 +0000 UTC (3 container statuses recorded)
    Feb  6 18:28:37.808: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb  6 18:28:37.808: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb  6 18:28:37.808: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Feb  6 18:28:37.808: INFO: sonobuoy from sonobuoy started at 2023-02-06 17:18:54 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.808: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb  6 18:28:37.808: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-cr8qs from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 18:28:37.808: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 18:28:37.808: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb  6 18:28:37.808: INFO: 
    Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-6j94p before test
    Feb  6 18:28:37.819: INFO: capi-kubeadm-control-plane-controller-manager-5f686d9d47-lkld2 from capi-kubeadm-control-plane-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.820: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: capi-controller-manager-5b68b88d59-vcpp2 from capi-system started at 2023-02-06 17:15:34 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.820: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: cert-manager-7b9fcd6db9-rz78g from cert-manager started at 2023-02-06 17:15:16 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.820: INFO: 	Container cert-manager ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: cert-manager-cainjector-75b587b679-rjbzj from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.820: INFO: 	Container cert-manager ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: cert-manager-webhook-896b549c4-hj9c4 from cert-manager started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.820: INFO: 	Container cert-manager ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: eks-anywhere-packages-8589bd5d9c-4pc9v from eksa-packages started at 2023-02-06 17:17:15 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.820: INFO: 	Container controller ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: etcdadm-bootstrap-provider-controller-manager-84ff796964-d6tt8 from etcdadm-bootstrap-provider-system started at 2023-02-06 17:15:37 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.820: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: cilium-nb887 from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.820: INFO: 	Container cilium-agent ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: kube-proxy-ld4nv from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.820: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: vsphere-cloud-controller-manager-76l5k from kube-system started at 2023-02-06 17:12:26 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.820: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: vsphere-csi-node-2tfxk from kube-system started at 2023-02-06 17:12:26 +0000 UTC (3 container statuses recorded)
    Feb  6 18:28:37.820: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-hcvcv from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 18:28:37.820: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb  6 18:28:37.820: INFO: 
    Logging pods the apiserver thinks is on node tneyla25-md-0-68cbcb4798-lz542 before test
    Feb  6 18:28:37.835: INFO: capi-kubeadm-bootstrap-controller-manager-79d4bdf548-4xmjq from capi-kubeadm-bootstrap-system started at 2023-02-06 17:15:36 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.835: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: capv-controller-manager-598f9bf684-4qdws from capv-system started at 2023-02-06 17:30:52 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.835: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: eksa-controller-manager-5588899966-wtcpc from eksa-system started at 2023-02-06 17:16:14 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.835: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: etcdadm-controller-controller-manager-767b7fb9b5-mt7lx from etcdadm-controller-system started at 2023-02-06 17:15:38 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.835: INFO: 	Container manager ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: cilium-ndf4k from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.835: INFO: 	Container cilium-agent ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: cilium-operator-7f966b595f-pzn9c from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.835: INFO: 	Container cilium-operator ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: kube-proxy-hfblf from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.835: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: vsphere-cloud-controller-manager-x8k8p from kube-system started at 2023-02-06 17:12:15 +0000 UTC (1 container statuses recorded)
    Feb  6 18:28:37.835: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: vsphere-csi-node-v4zd6 from kube-system started at 2023-02-06 17:12:15 +0000 UTC (3 container statuses recorded)
    Feb  6 18:28:37.835: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: sonobuoy-e2e-job-b0e9e31f5d894166 from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 18:28:37.835: INFO: 	Container e2e ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: sonobuoy-systemd-logs-daemon-set-0b96cc05ea5549c0-fncmf from sonobuoy started at 2023-02-06 17:19:02 +0000 UTC (2 container statuses recorded)
    Feb  6 18:28:37.835: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb  6 18:28:37.835: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 02/06/23 18:28:37.836
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17414fa7a8e5bbf4], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 6 node(s) didn't match Pod's node affinity/selector. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 02/06/23 18:28:37.877
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 18:28:38.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-959" for this suite. 02/06/23 18:28:38.879
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:28:38.888
Feb  6 18:28:38.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename gc 02/06/23 18:28:38.889
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:38.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:38.907
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Feb  6 18:28:38.945: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e04df169-975d-4216-b4dc-7ed6261602b6", Controller:(*bool)(0xc0031fb30a), BlockOwnerDeletion:(*bool)(0xc0031fb30b)}}
Feb  6 18:28:38.955: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"fc659694-a792-4c63-b88a-056b8ac25f14", Controller:(*bool)(0xc0044d8036), BlockOwnerDeletion:(*bool)(0xc0044d8037)}}
Feb  6 18:28:38.965: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2fbddc1e-c86d-47f9-95fd-79405966da97", Controller:(*bool)(0xc0044d825e), BlockOwnerDeletion:(*bool)(0xc0044d825f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb  6 18:28:43.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1802" for this suite. 02/06/23 18:28:43.985
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":231,"skipped":4389,"failed":0}
------------------------------
• [SLOW TEST] [5.105 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:28:38.888
    Feb  6 18:28:38.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename gc 02/06/23 18:28:38.889
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:38.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:38.907
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Feb  6 18:28:38.945: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e04df169-975d-4216-b4dc-7ed6261602b6", Controller:(*bool)(0xc0031fb30a), BlockOwnerDeletion:(*bool)(0xc0031fb30b)}}
    Feb  6 18:28:38.955: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"fc659694-a792-4c63-b88a-056b8ac25f14", Controller:(*bool)(0xc0044d8036), BlockOwnerDeletion:(*bool)(0xc0044d8037)}}
    Feb  6 18:28:38.965: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2fbddc1e-c86d-47f9-95fd-79405966da97", Controller:(*bool)(0xc0044d825e), BlockOwnerDeletion:(*bool)(0xc0044d825f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb  6 18:28:43.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1802" for this suite. 02/06/23 18:28:43.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:28:43.996
Feb  6 18:28:43.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:28:43.997
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:44.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:44.029
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-d809b1cf-132e-4559-a131-804057658ab6 02/06/23 18:28:44.036
STEP: Creating a pod to test consume configMaps 02/06/23 18:28:44.043
Feb  6 18:28:44.056: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5" in namespace "projected-7983" to be "Succeeded or Failed"
Feb  6 18:28:44.064: INFO: Pod "pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.774505ms
Feb  6 18:28:46.068: INFO: Pod "pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012388673s
Feb  6 18:28:48.069: INFO: Pod "pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012810824s
Feb  6 18:28:50.069: INFO: Pod "pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013071133s
STEP: Saw pod success 02/06/23 18:28:50.069
Feb  6 18:28:50.069: INFO: Pod "pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5" satisfied condition "Succeeded or Failed"
Feb  6 18:28:50.072: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5 container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:28:50.078
Feb  6 18:28:50.090: INFO: Waiting for pod pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5 to disappear
Feb  6 18:28:50.093: INFO: Pod pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb  6 18:28:50.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7983" for this suite. 02/06/23 18:28:50.098
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":232,"skipped":4396,"failed":0}
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:28:43.996
    Feb  6 18:28:43.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:28:43.997
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:44.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:44.029
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-d809b1cf-132e-4559-a131-804057658ab6 02/06/23 18:28:44.036
    STEP: Creating a pod to test consume configMaps 02/06/23 18:28:44.043
    Feb  6 18:28:44.056: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5" in namespace "projected-7983" to be "Succeeded or Failed"
    Feb  6 18:28:44.064: INFO: Pod "pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.774505ms
    Feb  6 18:28:46.068: INFO: Pod "pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012388673s
    Feb  6 18:28:48.069: INFO: Pod "pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012810824s
    Feb  6 18:28:50.069: INFO: Pod "pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013071133s
    STEP: Saw pod success 02/06/23 18:28:50.069
    Feb  6 18:28:50.069: INFO: Pod "pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5" satisfied condition "Succeeded or Failed"
    Feb  6 18:28:50.072: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5 container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:28:50.078
    Feb  6 18:28:50.090: INFO: Waiting for pod pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5 to disappear
    Feb  6 18:28:50.093: INFO: Pod pod-projected-configmaps-fd9af1de-e9f8-4b1a-81f3-2d1fc61018d5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb  6 18:28:50.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7983" for this suite. 02/06/23 18:28:50.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:28:50.111
Feb  6 18:28:50.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 18:28:50.112
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:50.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:50.13
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/06/23 18:28:50.135
Feb  6 18:28:50.143: INFO: Waiting up to 5m0s for pod "pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0" in namespace "emptydir-9549" to be "Succeeded or Failed"
Feb  6 18:28:50.147: INFO: Pod "pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.245289ms
Feb  6 18:28:52.152: INFO: Pod "pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008555391s
Feb  6 18:28:54.155: INFO: Pod "pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012264885s
Feb  6 18:28:56.154: INFO: Pod "pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010649729s
STEP: Saw pod success 02/06/23 18:28:56.154
Feb  6 18:28:56.154: INFO: Pod "pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0" satisfied condition "Succeeded or Failed"
Feb  6 18:28:56.157: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0 container test-container: <nil>
STEP: delete the pod 02/06/23 18:28:56.166
Feb  6 18:28:56.182: INFO: Waiting for pod pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0 to disappear
Feb  6 18:28:56.186: INFO: Pod pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 18:28:56.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9549" for this suite. 02/06/23 18:28:56.191
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":233,"skipped":4428,"failed":0}
------------------------------
• [SLOW TEST] [6.087 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:28:50.111
    Feb  6 18:28:50.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 18:28:50.112
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:50.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:50.13
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/06/23 18:28:50.135
    Feb  6 18:28:50.143: INFO: Waiting up to 5m0s for pod "pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0" in namespace "emptydir-9549" to be "Succeeded or Failed"
    Feb  6 18:28:50.147: INFO: Pod "pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.245289ms
    Feb  6 18:28:52.152: INFO: Pod "pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008555391s
    Feb  6 18:28:54.155: INFO: Pod "pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012264885s
    Feb  6 18:28:56.154: INFO: Pod "pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010649729s
    STEP: Saw pod success 02/06/23 18:28:56.154
    Feb  6 18:28:56.154: INFO: Pod "pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0" satisfied condition "Succeeded or Failed"
    Feb  6 18:28:56.157: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0 container test-container: <nil>
    STEP: delete the pod 02/06/23 18:28:56.166
    Feb  6 18:28:56.182: INFO: Waiting for pod pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0 to disappear
    Feb  6 18:28:56.186: INFO: Pod pod-5f756e8c-c85b-42ca-93c2-19e7bdaee0c0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 18:28:56.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9549" for this suite. 02/06/23 18:28:56.191
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:28:56.201
Feb  6 18:28:56.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename crd-watch 02/06/23 18:28:56.202
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:56.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:56.223
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Feb  6 18:28:56.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Creating first CR  02/06/23 18:28:58.794
Feb  6 18:28:58.803: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-06T18:28:58Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-06T18:28:58Z]] name:name1 resourceVersion:59074 uid:c756aa99-678e-4d03-a53e-63b50b192132] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 02/06/23 18:29:08.805
Feb  6 18:29:08.812: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-06T18:29:08Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-06T18:29:08Z]] name:name2 resourceVersion:59180 uid:5ede23d5-963a-4843-88eb-3b2cfc8c1da7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 02/06/23 18:29:18.814
Feb  6 18:29:18.822: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-06T18:28:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-06T18:29:18Z]] name:name1 resourceVersion:59279 uid:c756aa99-678e-4d03-a53e-63b50b192132] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 02/06/23 18:29:28.824
Feb  6 18:29:28.833: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-06T18:29:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-06T18:29:28Z]] name:name2 resourceVersion:59379 uid:5ede23d5-963a-4843-88eb-3b2cfc8c1da7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 02/06/23 18:29:38.835
Feb  6 18:29:38.842: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-06T18:28:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-06T18:29:18Z]] name:name1 resourceVersion:59478 uid:c756aa99-678e-4d03-a53e-63b50b192132] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 02/06/23 18:29:48.843
Feb  6 18:29:48.851: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-06T18:29:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-06T18:29:28Z]] name:name2 resourceVersion:59577 uid:5ede23d5-963a-4843-88eb-3b2cfc8c1da7] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:29:59.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2283" for this suite. 02/06/23 18:29:59.37
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":234,"skipped":4429,"failed":0}
------------------------------
• [SLOW TEST] [63.177 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:28:56.201
    Feb  6 18:28:56.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename crd-watch 02/06/23 18:28:56.202
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:28:56.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:28:56.223
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Feb  6 18:28:56.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Creating first CR  02/06/23 18:28:58.794
    Feb  6 18:28:58.803: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-06T18:28:58Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-06T18:28:58Z]] name:name1 resourceVersion:59074 uid:c756aa99-678e-4d03-a53e-63b50b192132] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 02/06/23 18:29:08.805
    Feb  6 18:29:08.812: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-06T18:29:08Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-06T18:29:08Z]] name:name2 resourceVersion:59180 uid:5ede23d5-963a-4843-88eb-3b2cfc8c1da7] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 02/06/23 18:29:18.814
    Feb  6 18:29:18.822: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-06T18:28:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-06T18:29:18Z]] name:name1 resourceVersion:59279 uid:c756aa99-678e-4d03-a53e-63b50b192132] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 02/06/23 18:29:28.824
    Feb  6 18:29:28.833: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-06T18:29:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-06T18:29:28Z]] name:name2 resourceVersion:59379 uid:5ede23d5-963a-4843-88eb-3b2cfc8c1da7] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 02/06/23 18:29:38.835
    Feb  6 18:29:38.842: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-06T18:28:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-06T18:29:18Z]] name:name1 resourceVersion:59478 uid:c756aa99-678e-4d03-a53e-63b50b192132] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 02/06/23 18:29:48.843
    Feb  6 18:29:48.851: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-06T18:29:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-06T18:29:28Z]] name:name2 resourceVersion:59577 uid:5ede23d5-963a-4843-88eb-3b2cfc8c1da7] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:29:59.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-2283" for this suite. 02/06/23 18:29:59.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:29:59.38
Feb  6 18:29:59.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 18:29:59.381
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:29:59.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:29:59.407
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 02/06/23 18:29:59.414
Feb  6 18:29:59.424: INFO: Waiting up to 5m0s for pod "pod-62912443-2331-4ded-9823-4056bca9064d" in namespace "emptydir-3647" to be "Succeeded or Failed"
Feb  6 18:29:59.429: INFO: Pod "pod-62912443-2331-4ded-9823-4056bca9064d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.469728ms
Feb  6 18:30:01.433: INFO: Pod "pod-62912443-2331-4ded-9823-4056bca9064d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00976243s
Feb  6 18:30:03.437: INFO: Pod "pod-62912443-2331-4ded-9823-4056bca9064d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013679243s
Feb  6 18:30:05.434: INFO: Pod "pod-62912443-2331-4ded-9823-4056bca9064d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010667606s
STEP: Saw pod success 02/06/23 18:30:05.434
Feb  6 18:30:05.435: INFO: Pod "pod-62912443-2331-4ded-9823-4056bca9064d" satisfied condition "Succeeded or Failed"
Feb  6 18:30:05.438: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-62912443-2331-4ded-9823-4056bca9064d container test-container: <nil>
STEP: delete the pod 02/06/23 18:30:05.449
Feb  6 18:30:05.463: INFO: Waiting for pod pod-62912443-2331-4ded-9823-4056bca9064d to disappear
Feb  6 18:30:05.466: INFO: Pod pod-62912443-2331-4ded-9823-4056bca9064d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 18:30:05.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3647" for this suite. 02/06/23 18:30:05.471
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":235,"skipped":4470,"failed":0}
------------------------------
• [SLOW TEST] [6.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:29:59.38
    Feb  6 18:29:59.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 18:29:59.381
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:29:59.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:29:59.407
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/06/23 18:29:59.414
    Feb  6 18:29:59.424: INFO: Waiting up to 5m0s for pod "pod-62912443-2331-4ded-9823-4056bca9064d" in namespace "emptydir-3647" to be "Succeeded or Failed"
    Feb  6 18:29:59.429: INFO: Pod "pod-62912443-2331-4ded-9823-4056bca9064d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.469728ms
    Feb  6 18:30:01.433: INFO: Pod "pod-62912443-2331-4ded-9823-4056bca9064d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00976243s
    Feb  6 18:30:03.437: INFO: Pod "pod-62912443-2331-4ded-9823-4056bca9064d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013679243s
    Feb  6 18:30:05.434: INFO: Pod "pod-62912443-2331-4ded-9823-4056bca9064d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010667606s
    STEP: Saw pod success 02/06/23 18:30:05.434
    Feb  6 18:30:05.435: INFO: Pod "pod-62912443-2331-4ded-9823-4056bca9064d" satisfied condition "Succeeded or Failed"
    Feb  6 18:30:05.438: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-62912443-2331-4ded-9823-4056bca9064d container test-container: <nil>
    STEP: delete the pod 02/06/23 18:30:05.449
    Feb  6 18:30:05.463: INFO: Waiting for pod pod-62912443-2331-4ded-9823-4056bca9064d to disappear
    Feb  6 18:30:05.466: INFO: Pod pod-62912443-2331-4ded-9823-4056bca9064d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 18:30:05.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3647" for this suite. 02/06/23 18:30:05.471
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:30:05.48
Feb  6 18:30:05.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 18:30:05.481
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:30:05.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:30:05.499
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 18:30:05.517
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:30:05.875
STEP: Deploying the webhook pod 02/06/23 18:30:05.884
STEP: Wait for the deployment to be ready 02/06/23 18:30:05.899
Feb  6 18:30:05.919: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 18:30:07.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 30, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 30, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 30, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 30, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 18:30:09.939
STEP: Verifying the service has paired with the endpoint 02/06/23 18:30:09.959
Feb  6 18:30:10.960: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 02/06/23 18:30:10.965
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/06/23 18:30:10.966
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/06/23 18:30:10.966
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/06/23 18:30:10.967
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/06/23 18:30:10.969
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/06/23 18:30:10.969
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/06/23 18:30:10.97
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:30:10.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-420" for this suite. 02/06/23 18:30:10.975
STEP: Destroying namespace "webhook-420-markers" for this suite. 02/06/23 18:30:10.985
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":236,"skipped":4470,"failed":0}
------------------------------
• [SLOW TEST] [5.596 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:30:05.48
    Feb  6 18:30:05.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 18:30:05.481
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:30:05.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:30:05.499
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 18:30:05.517
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:30:05.875
    STEP: Deploying the webhook pod 02/06/23 18:30:05.884
    STEP: Wait for the deployment to be ready 02/06/23 18:30:05.899
    Feb  6 18:30:05.919: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 18:30:07.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 30, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 30, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 30, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 30, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 18:30:09.939
    STEP: Verifying the service has paired with the endpoint 02/06/23 18:30:09.959
    Feb  6 18:30:10.960: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 02/06/23 18:30:10.965
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/06/23 18:30:10.966
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/06/23 18:30:10.966
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/06/23 18:30:10.967
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/06/23 18:30:10.969
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/06/23 18:30:10.969
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/06/23 18:30:10.97
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:30:10.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-420" for this suite. 02/06/23 18:30:10.975
    STEP: Destroying namespace "webhook-420-markers" for this suite. 02/06/23 18:30:10.985
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:30:11.083
Feb  6 18:30:11.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-lifecycle-hook 02/06/23 18:30:11.085
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:30:11.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:30:11.114
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/06/23 18:30:11.125
Feb  6 18:30:11.137: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3522" to be "running and ready"
Feb  6 18:30:11.144: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.634948ms
Feb  6 18:30:11.144: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:30:13.164: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026835538s
Feb  6 18:30:13.164: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:30:15.149: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.012162544s
Feb  6 18:30:15.149: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb  6 18:30:15.149: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 02/06/23 18:30:15.152
Feb  6 18:30:15.160: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3522" to be "running and ready"
Feb  6 18:30:15.168: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.972163ms
Feb  6 18:30:15.168: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:30:17.173: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013699835s
Feb  6 18:30:17.174: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:30:19.173: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.013430912s
Feb  6 18:30:19.173: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Feb  6 18:30:19.173: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/06/23 18:30:19.178
Feb  6 18:30:19.187: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  6 18:30:19.191: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  6 18:30:21.192: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  6 18:30:21.197: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  6 18:30:23.192: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  6 18:30:23.196: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 02/06/23 18:30:23.196
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb  6 18:30:23.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3522" for this suite. 02/06/23 18:30:23.253
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":237,"skipped":4478,"failed":0}
------------------------------
• [SLOW TEST] [12.196 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:30:11.083
    Feb  6 18:30:11.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/06/23 18:30:11.085
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:30:11.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:30:11.114
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/06/23 18:30:11.125
    Feb  6 18:30:11.137: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3522" to be "running and ready"
    Feb  6 18:30:11.144: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.634948ms
    Feb  6 18:30:11.144: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:30:13.164: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026835538s
    Feb  6 18:30:13.164: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:30:15.149: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.012162544s
    Feb  6 18:30:15.149: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb  6 18:30:15.149: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 02/06/23 18:30:15.152
    Feb  6 18:30:15.160: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3522" to be "running and ready"
    Feb  6 18:30:15.168: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.972163ms
    Feb  6 18:30:15.168: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:30:17.173: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013699835s
    Feb  6 18:30:17.174: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:30:19.173: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.013430912s
    Feb  6 18:30:19.173: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Feb  6 18:30:19.173: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/06/23 18:30:19.178
    Feb  6 18:30:19.187: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb  6 18:30:19.191: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb  6 18:30:21.192: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb  6 18:30:21.197: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb  6 18:30:23.192: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb  6 18:30:23.196: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 02/06/23 18:30:23.196
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb  6 18:30:23.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-3522" for this suite. 02/06/23 18:30:23.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:30:23.291
Feb  6 18:30:23.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename gc 02/06/23 18:30:23.293
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:30:23.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:30:23.352
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 02/06/23 18:30:23.365
STEP: create the rc2 02/06/23 18:30:23.388
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/06/23 18:30:28.75
STEP: delete the rc simpletest-rc-to-be-deleted 02/06/23 18:30:34.268
STEP: wait for the rc to be deleted 02/06/23 18:30:34.28
Feb  6 18:30:39.302: INFO: 80 pods remaining
Feb  6 18:30:39.302: INFO: 71 pods has nil DeletionTimestamp
Feb  6 18:30:39.302: INFO: 
Feb  6 18:30:44.318: INFO: 51 pods remaining
Feb  6 18:30:44.318: INFO: 50 pods has nil DeletionTimestamp
Feb  6 18:30:44.318: INFO: 
STEP: Gathering metrics 02/06/23 18:30:49.291
Feb  6 18:30:49.319: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tneyla25-pnn8z" in namespace "kube-system" to be "running and ready"
Feb  6 18:30:49.323: INFO: Pod "kube-controller-manager-tneyla25-pnn8z": Phase="Running", Reason="", readiness=true. Elapsed: 3.497444ms
Feb  6 18:30:49.323: INFO: The phase of Pod kube-controller-manager-tneyla25-pnn8z is Running (Ready = true)
Feb  6 18:30:49.323: INFO: Pod "kube-controller-manager-tneyla25-pnn8z" satisfied condition "running and ready"
Feb  6 18:30:49.404: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb  6 18:30:49.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j6bv" in namespace "gc-40"
Feb  6 18:30:49.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rq27" in namespace "gc-40"
Feb  6 18:30:49.431: INFO: Deleting pod "simpletest-rc-to-be-deleted-45cjx" in namespace "gc-40"
Feb  6 18:30:49.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-46xqs" in namespace "gc-40"
Feb  6 18:30:49.460: INFO: Deleting pod "simpletest-rc-to-be-deleted-485jh" in namespace "gc-40"
Feb  6 18:30:49.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-4llrc" in namespace "gc-40"
Feb  6 18:30:49.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rk9f" in namespace "gc-40"
Feb  6 18:30:49.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fr6c" in namespace "gc-40"
Feb  6 18:30:49.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-5sr4r" in namespace "gc-40"
Feb  6 18:30:49.545: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ssfk" in namespace "gc-40"
Feb  6 18:30:49.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-62qgw" in namespace "gc-40"
Feb  6 18:30:49.583: INFO: Deleting pod "simpletest-rc-to-be-deleted-68w5t" in namespace "gc-40"
Feb  6 18:30:49.594: INFO: Deleting pod "simpletest-rc-to-be-deleted-6j9ch" in namespace "gc-40"
Feb  6 18:30:49.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ksjw" in namespace "gc-40"
Feb  6 18:30:49.620: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rt2r" in namespace "gc-40"
Feb  6 18:30:49.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wx2v" in namespace "gc-40"
Feb  6 18:30:49.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-72cqt" in namespace "gc-40"
Feb  6 18:30:49.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-7654g" in namespace "gc-40"
Feb  6 18:30:49.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-766jl" in namespace "gc-40"
Feb  6 18:30:49.704: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tdpd" in namespace "gc-40"
Feb  6 18:30:49.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v78h" in namespace "gc-40"
Feb  6 18:30:49.739: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rhvz" in namespace "gc-40"
Feb  6 18:30:49.763: INFO: Deleting pod "simpletest-rc-to-be-deleted-99s4r" in namespace "gc-40"
Feb  6 18:30:49.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-99x6q" in namespace "gc-40"
Feb  6 18:30:49.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wx7n" in namespace "gc-40"
Feb  6 18:30:49.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5lnj" in namespace "gc-40"
Feb  6 18:30:49.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9xdq" in namespace "gc-40"
Feb  6 18:30:49.865: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbtzz" in namespace "gc-40"
Feb  6 18:30:49.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-bc6w9" in namespace "gc-40"
Feb  6 18:30:49.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfw7h" in namespace "gc-40"
Feb  6 18:30:49.942: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhvgd" in namespace "gc-40"
Feb  6 18:30:49.963: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv4wp" in namespace "gc-40"
Feb  6 18:30:49.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-c24gr" in namespace "gc-40"
Feb  6 18:30:49.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4wn5" in namespace "gc-40"
Feb  6 18:30:50.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgwpw" in namespace "gc-40"
Feb  6 18:30:50.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmxh7" in namespace "gc-40"
Feb  6 18:30:50.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnp8v" in namespace "gc-40"
Feb  6 18:30:50.062: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct9dw" in namespace "gc-40"
Feb  6 18:30:50.074: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxnqj" in namespace "gc-40"
Feb  6 18:30:50.095: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8bhw" in namespace "gc-40"
Feb  6 18:30:50.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzn4p" in namespace "gc-40"
Feb  6 18:30:50.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5fk9" in namespace "gc-40"
Feb  6 18:30:50.167: INFO: Deleting pod "simpletest-rc-to-be-deleted-gpzms" in namespace "gc-40"
Feb  6 18:30:50.191: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxsr5" in namespace "gc-40"
Feb  6 18:30:50.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6rpv" in namespace "gc-40"
Feb  6 18:30:50.246: INFO: Deleting pod "simpletest-rc-to-be-deleted-jbnlf" in namespace "gc-40"
Feb  6 18:30:50.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-jclvg" in namespace "gc-40"
Feb  6 18:30:50.285: INFO: Deleting pod "simpletest-rc-to-be-deleted-jlkr4" in namespace "gc-40"
Feb  6 18:30:50.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-k449d" in namespace "gc-40"
Feb  6 18:30:50.345: INFO: Deleting pod "simpletest-rc-to-be-deleted-kj6vj" in namespace "gc-40"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb  6 18:30:50.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-40" for this suite. 02/06/23 18:30:50.37
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":238,"skipped":4529,"failed":0}
------------------------------
• [SLOW TEST] [27.088 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:30:23.291
    Feb  6 18:30:23.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename gc 02/06/23 18:30:23.293
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:30:23.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:30:23.352
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 02/06/23 18:30:23.365
    STEP: create the rc2 02/06/23 18:30:23.388
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/06/23 18:30:28.75
    STEP: delete the rc simpletest-rc-to-be-deleted 02/06/23 18:30:34.268
    STEP: wait for the rc to be deleted 02/06/23 18:30:34.28
    Feb  6 18:30:39.302: INFO: 80 pods remaining
    Feb  6 18:30:39.302: INFO: 71 pods has nil DeletionTimestamp
    Feb  6 18:30:39.302: INFO: 
    Feb  6 18:30:44.318: INFO: 51 pods remaining
    Feb  6 18:30:44.318: INFO: 50 pods has nil DeletionTimestamp
    Feb  6 18:30:44.318: INFO: 
    STEP: Gathering metrics 02/06/23 18:30:49.291
    Feb  6 18:30:49.319: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tneyla25-pnn8z" in namespace "kube-system" to be "running and ready"
    Feb  6 18:30:49.323: INFO: Pod "kube-controller-manager-tneyla25-pnn8z": Phase="Running", Reason="", readiness=true. Elapsed: 3.497444ms
    Feb  6 18:30:49.323: INFO: The phase of Pod kube-controller-manager-tneyla25-pnn8z is Running (Ready = true)
    Feb  6 18:30:49.323: INFO: Pod "kube-controller-manager-tneyla25-pnn8z" satisfied condition "running and ready"
    Feb  6 18:30:49.404: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb  6 18:30:49.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j6bv" in namespace "gc-40"
    Feb  6 18:30:49.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rq27" in namespace "gc-40"
    Feb  6 18:30:49.431: INFO: Deleting pod "simpletest-rc-to-be-deleted-45cjx" in namespace "gc-40"
    Feb  6 18:30:49.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-46xqs" in namespace "gc-40"
    Feb  6 18:30:49.460: INFO: Deleting pod "simpletest-rc-to-be-deleted-485jh" in namespace "gc-40"
    Feb  6 18:30:49.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-4llrc" in namespace "gc-40"
    Feb  6 18:30:49.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rk9f" in namespace "gc-40"
    Feb  6 18:30:49.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fr6c" in namespace "gc-40"
    Feb  6 18:30:49.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-5sr4r" in namespace "gc-40"
    Feb  6 18:30:49.545: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ssfk" in namespace "gc-40"
    Feb  6 18:30:49.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-62qgw" in namespace "gc-40"
    Feb  6 18:30:49.583: INFO: Deleting pod "simpletest-rc-to-be-deleted-68w5t" in namespace "gc-40"
    Feb  6 18:30:49.594: INFO: Deleting pod "simpletest-rc-to-be-deleted-6j9ch" in namespace "gc-40"
    Feb  6 18:30:49.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ksjw" in namespace "gc-40"
    Feb  6 18:30:49.620: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rt2r" in namespace "gc-40"
    Feb  6 18:30:49.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wx2v" in namespace "gc-40"
    Feb  6 18:30:49.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-72cqt" in namespace "gc-40"
    Feb  6 18:30:49.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-7654g" in namespace "gc-40"
    Feb  6 18:30:49.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-766jl" in namespace "gc-40"
    Feb  6 18:30:49.704: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tdpd" in namespace "gc-40"
    Feb  6 18:30:49.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v78h" in namespace "gc-40"
    Feb  6 18:30:49.739: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rhvz" in namespace "gc-40"
    Feb  6 18:30:49.763: INFO: Deleting pod "simpletest-rc-to-be-deleted-99s4r" in namespace "gc-40"
    Feb  6 18:30:49.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-99x6q" in namespace "gc-40"
    Feb  6 18:30:49.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wx7n" in namespace "gc-40"
    Feb  6 18:30:49.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5lnj" in namespace "gc-40"
    Feb  6 18:30:49.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9xdq" in namespace "gc-40"
    Feb  6 18:30:49.865: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbtzz" in namespace "gc-40"
    Feb  6 18:30:49.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-bc6w9" in namespace "gc-40"
    Feb  6 18:30:49.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfw7h" in namespace "gc-40"
    Feb  6 18:30:49.942: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhvgd" in namespace "gc-40"
    Feb  6 18:30:49.963: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv4wp" in namespace "gc-40"
    Feb  6 18:30:49.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-c24gr" in namespace "gc-40"
    Feb  6 18:30:49.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4wn5" in namespace "gc-40"
    Feb  6 18:30:50.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgwpw" in namespace "gc-40"
    Feb  6 18:30:50.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmxh7" in namespace "gc-40"
    Feb  6 18:30:50.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnp8v" in namespace "gc-40"
    Feb  6 18:30:50.062: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct9dw" in namespace "gc-40"
    Feb  6 18:30:50.074: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxnqj" in namespace "gc-40"
    Feb  6 18:30:50.095: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8bhw" in namespace "gc-40"
    Feb  6 18:30:50.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzn4p" in namespace "gc-40"
    Feb  6 18:30:50.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5fk9" in namespace "gc-40"
    Feb  6 18:30:50.167: INFO: Deleting pod "simpletest-rc-to-be-deleted-gpzms" in namespace "gc-40"
    Feb  6 18:30:50.191: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxsr5" in namespace "gc-40"
    Feb  6 18:30:50.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6rpv" in namespace "gc-40"
    Feb  6 18:30:50.246: INFO: Deleting pod "simpletest-rc-to-be-deleted-jbnlf" in namespace "gc-40"
    Feb  6 18:30:50.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-jclvg" in namespace "gc-40"
    Feb  6 18:30:50.285: INFO: Deleting pod "simpletest-rc-to-be-deleted-jlkr4" in namespace "gc-40"
    Feb  6 18:30:50.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-k449d" in namespace "gc-40"
    Feb  6 18:30:50.345: INFO: Deleting pod "simpletest-rc-to-be-deleted-kj6vj" in namespace "gc-40"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb  6 18:30:50.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-40" for this suite. 02/06/23 18:30:50.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:30:50.384
Feb  6 18:30:50.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:30:50.385
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:30:50.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:30:50.408
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 02/06/23 18:30:50.412
Feb  6 18:30:50.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13" in namespace "projected-9207" to be "Succeeded or Failed"
Feb  6 18:30:50.428: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13": Phase="Pending", Reason="", readiness=false. Elapsed: 5.19478ms
Feb  6 18:30:52.432: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009058507s
Feb  6 18:30:54.434: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011307891s
Feb  6 18:30:56.435: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012526435s
Feb  6 18:30:58.433: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010451346s
Feb  6 18:31:00.433: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.010396229s
STEP: Saw pod success 02/06/23 18:31:00.433
Feb  6 18:31:00.434: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13" satisfied condition "Succeeded or Failed"
Feb  6 18:31:00.438: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13 container client-container: <nil>
STEP: delete the pod 02/06/23 18:31:00.446
Feb  6 18:31:00.460: INFO: Waiting for pod downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13 to disappear
Feb  6 18:31:00.466: INFO: Pod downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb  6 18:31:00.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9207" for this suite. 02/06/23 18:31:00.471
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":239,"skipped":4549,"failed":0}
------------------------------
• [SLOW TEST] [10.094 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:30:50.384
    Feb  6 18:30:50.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:30:50.385
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:30:50.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:30:50.408
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 02/06/23 18:30:50.412
    Feb  6 18:30:50.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13" in namespace "projected-9207" to be "Succeeded or Failed"
    Feb  6 18:30:50.428: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13": Phase="Pending", Reason="", readiness=false. Elapsed: 5.19478ms
    Feb  6 18:30:52.432: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009058507s
    Feb  6 18:30:54.434: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011307891s
    Feb  6 18:30:56.435: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012526435s
    Feb  6 18:30:58.433: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010451346s
    Feb  6 18:31:00.433: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.010396229s
    STEP: Saw pod success 02/06/23 18:31:00.433
    Feb  6 18:31:00.434: INFO: Pod "downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13" satisfied condition "Succeeded or Failed"
    Feb  6 18:31:00.438: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13 container client-container: <nil>
    STEP: delete the pod 02/06/23 18:31:00.446
    Feb  6 18:31:00.460: INFO: Waiting for pod downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13 to disappear
    Feb  6 18:31:00.466: INFO: Pod downwardapi-volume-77469997-eadb-4cc5-b46a-48bb1bd54a13 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb  6 18:31:00.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9207" for this suite. 02/06/23 18:31:00.471
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:31:00.478
Feb  6 18:31:00.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 18:31:00.479
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:31:00.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:31:00.5
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-4bf248e9-8318-466c-acca-3e0f1f6de552 02/06/23 18:31:00.503
STEP: Creating a pod to test consume configMaps 02/06/23 18:31:00.509
Feb  6 18:31:00.517: INFO: Waiting up to 5m0s for pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8" in namespace "configmap-98" to be "Succeeded or Failed"
Feb  6 18:31:00.520: INFO: Pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612051ms
Feb  6 18:31:02.524: INFO: Pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006994277s
Feb  6 18:31:04.525: INFO: Pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007352871s
Feb  6 18:31:06.525: INFO: Pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007169335s
Feb  6 18:31:08.524: INFO: Pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.006732842s
STEP: Saw pod success 02/06/23 18:31:08.524
Feb  6 18:31:08.524: INFO: Pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8" satisfied condition "Succeeded or Failed"
Feb  6 18:31:08.527: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8 container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:31:08.534
Feb  6 18:31:08.548: INFO: Waiting for pod pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8 to disappear
Feb  6 18:31:08.551: INFO: Pod pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 18:31:08.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-98" for this suite. 02/06/23 18:31:08.557
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":240,"skipped":4551,"failed":0}
------------------------------
• [SLOW TEST] [8.089 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:31:00.478
    Feb  6 18:31:00.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 18:31:00.479
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:31:00.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:31:00.5
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-4bf248e9-8318-466c-acca-3e0f1f6de552 02/06/23 18:31:00.503
    STEP: Creating a pod to test consume configMaps 02/06/23 18:31:00.509
    Feb  6 18:31:00.517: INFO: Waiting up to 5m0s for pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8" in namespace "configmap-98" to be "Succeeded or Failed"
    Feb  6 18:31:00.520: INFO: Pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612051ms
    Feb  6 18:31:02.524: INFO: Pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006994277s
    Feb  6 18:31:04.525: INFO: Pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007352871s
    Feb  6 18:31:06.525: INFO: Pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007169335s
    Feb  6 18:31:08.524: INFO: Pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.006732842s
    STEP: Saw pod success 02/06/23 18:31:08.524
    Feb  6 18:31:08.524: INFO: Pod "pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8" satisfied condition "Succeeded or Failed"
    Feb  6 18:31:08.527: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8 container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:31:08.534
    Feb  6 18:31:08.548: INFO: Waiting for pod pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8 to disappear
    Feb  6 18:31:08.551: INFO: Pod pod-configmaps-9cd13042-c5b9-484d-b11f-b77da57b0ea8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 18:31:08.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-98" for this suite. 02/06/23 18:31:08.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:31:08.568
Feb  6 18:31:08.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename statefulset 02/06/23 18:31:08.569
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:31:08.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:31:08.588
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8959 02/06/23 18:31:08.591
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 02/06/23 18:31:08.595
STEP: Creating stateful set ss in namespace statefulset-8959 02/06/23 18:31:08.606
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8959 02/06/23 18:31:08.613
Feb  6 18:31:08.618: INFO: Found 0 stateful pods, waiting for 1
Feb  6 18:31:18.623: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/06/23 18:31:18.623
Feb  6 18:31:18.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  6 18:31:18.797: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  6 18:31:18.797: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  6 18:31:18.797: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  6 18:31:18.801: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb  6 18:31:28.806: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  6 18:31:28.806: INFO: Waiting for statefulset status.replicas updated to 0
Feb  6 18:31:28.824: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999637s
Feb  6 18:31:29.833: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.9959131s
Feb  6 18:31:30.836: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987390975s
Feb  6 18:31:31.840: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983515513s
Feb  6 18:31:32.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979784959s
Feb  6 18:31:33.848: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.975606087s
Feb  6 18:31:34.852: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971599175s
Feb  6 18:31:35.859: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967429398s
Feb  6 18:31:36.863: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.961092104s
Feb  6 18:31:37.869: INFO: Verifying statefulset ss doesn't scale past 1 for another 955.993437ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8959 02/06/23 18:31:38.87
Feb  6 18:31:38.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  6 18:31:39.030: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  6 18:31:39.030: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  6 18:31:39.030: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  6 18:31:39.033: INFO: Found 1 stateful pods, waiting for 3
Feb  6 18:31:49.039: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 18:31:49.039: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 18:31:49.039: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 02/06/23 18:31:49.039
STEP: Scale down will halt with unhealthy stateful pod 02/06/23 18:31:49.04
Feb  6 18:31:49.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  6 18:31:49.204: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  6 18:31:49.204: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  6 18:31:49.204: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  6 18:31:49.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  6 18:31:49.391: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  6 18:31:49.391: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  6 18:31:49.391: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  6 18:31:49.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  6 18:31:49.544: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  6 18:31:49.544: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  6 18:31:49.544: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  6 18:31:49.544: INFO: Waiting for statefulset status.replicas updated to 0
Feb  6 18:31:49.548: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb  6 18:31:59.556: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  6 18:31:59.556: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb  6 18:31:59.557: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb  6 18:31:59.573: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999974s
Feb  6 18:32:00.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992195703s
Feb  6 18:32:01.584: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986965091s
Feb  6 18:32:02.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981093343s
Feb  6 18:32:03.594: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976293719s
Feb  6 18:32:04.599: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970972217s
Feb  6 18:32:05.606: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966088303s
Feb  6 18:32:06.612: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959439437s
Feb  6 18:32:07.617: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952961057s
Feb  6 18:32:08.622: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.032007ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8959 02/06/23 18:32:09.623
Feb  6 18:32:09.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  6 18:32:09.789: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  6 18:32:09.789: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  6 18:32:09.789: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  6 18:32:09.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  6 18:32:09.946: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  6 18:32:09.946: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  6 18:32:09.946: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  6 18:32:09.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  6 18:32:10.125: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  6 18:32:10.125: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  6 18:32:10.125: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  6 18:32:10.125: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 02/06/23 18:32:20.143
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb  6 18:32:20.144: INFO: Deleting all statefulset in ns statefulset-8959
Feb  6 18:32:20.147: INFO: Scaling statefulset ss to 0
Feb  6 18:32:20.160: INFO: Waiting for statefulset status.replicas updated to 0
Feb  6 18:32:20.162: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb  6 18:32:20.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8959" for this suite. 02/06/23 18:32:20.194
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":241,"skipped":4561,"failed":0}
------------------------------
• [SLOW TEST] [71.634 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:31:08.568
    Feb  6 18:31:08.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename statefulset 02/06/23 18:31:08.569
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:31:08.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:31:08.588
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8959 02/06/23 18:31:08.591
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 02/06/23 18:31:08.595
    STEP: Creating stateful set ss in namespace statefulset-8959 02/06/23 18:31:08.606
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8959 02/06/23 18:31:08.613
    Feb  6 18:31:08.618: INFO: Found 0 stateful pods, waiting for 1
    Feb  6 18:31:18.623: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/06/23 18:31:18.623
    Feb  6 18:31:18.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb  6 18:31:18.797: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb  6 18:31:18.797: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb  6 18:31:18.797: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb  6 18:31:18.801: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb  6 18:31:28.806: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb  6 18:31:28.806: INFO: Waiting for statefulset status.replicas updated to 0
    Feb  6 18:31:28.824: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999637s
    Feb  6 18:31:29.833: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.9959131s
    Feb  6 18:31:30.836: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987390975s
    Feb  6 18:31:31.840: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983515513s
    Feb  6 18:31:32.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979784959s
    Feb  6 18:31:33.848: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.975606087s
    Feb  6 18:31:34.852: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971599175s
    Feb  6 18:31:35.859: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967429398s
    Feb  6 18:31:36.863: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.961092104s
    Feb  6 18:31:37.869: INFO: Verifying statefulset ss doesn't scale past 1 for another 955.993437ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8959 02/06/23 18:31:38.87
    Feb  6 18:31:38.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb  6 18:31:39.030: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb  6 18:31:39.030: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb  6 18:31:39.030: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb  6 18:31:39.033: INFO: Found 1 stateful pods, waiting for 3
    Feb  6 18:31:49.039: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 18:31:49.039: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 18:31:49.039: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 02/06/23 18:31:49.039
    STEP: Scale down will halt with unhealthy stateful pod 02/06/23 18:31:49.04
    Feb  6 18:31:49.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb  6 18:31:49.204: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb  6 18:31:49.204: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb  6 18:31:49.204: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb  6 18:31:49.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb  6 18:31:49.391: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb  6 18:31:49.391: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb  6 18:31:49.391: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb  6 18:31:49.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb  6 18:31:49.544: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb  6 18:31:49.544: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb  6 18:31:49.544: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb  6 18:31:49.544: INFO: Waiting for statefulset status.replicas updated to 0
    Feb  6 18:31:49.548: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Feb  6 18:31:59.556: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb  6 18:31:59.556: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb  6 18:31:59.557: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb  6 18:31:59.573: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999974s
    Feb  6 18:32:00.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992195703s
    Feb  6 18:32:01.584: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986965091s
    Feb  6 18:32:02.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981093343s
    Feb  6 18:32:03.594: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976293719s
    Feb  6 18:32:04.599: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970972217s
    Feb  6 18:32:05.606: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966088303s
    Feb  6 18:32:06.612: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959439437s
    Feb  6 18:32:07.617: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952961057s
    Feb  6 18:32:08.622: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.032007ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8959 02/06/23 18:32:09.623
    Feb  6 18:32:09.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb  6 18:32:09.789: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb  6 18:32:09.789: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb  6 18:32:09.789: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb  6 18:32:09.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb  6 18:32:09.946: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb  6 18:32:09.946: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb  6 18:32:09.946: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb  6 18:32:09.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=statefulset-8959 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb  6 18:32:10.125: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb  6 18:32:10.125: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb  6 18:32:10.125: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb  6 18:32:10.125: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 02/06/23 18:32:20.143
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb  6 18:32:20.144: INFO: Deleting all statefulset in ns statefulset-8959
    Feb  6 18:32:20.147: INFO: Scaling statefulset ss to 0
    Feb  6 18:32:20.160: INFO: Waiting for statefulset status.replicas updated to 0
    Feb  6 18:32:20.162: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb  6 18:32:20.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8959" for this suite. 02/06/23 18:32:20.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:32:20.209
Feb  6 18:32:20.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename resourcequota 02/06/23 18:32:20.21
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:32:20.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:32:20.232
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 02/06/23 18:32:37.239
STEP: Creating a ResourceQuota 02/06/23 18:32:42.244
STEP: Ensuring resource quota status is calculated 02/06/23 18:32:42.25
STEP: Creating a ConfigMap 02/06/23 18:32:44.256
STEP: Ensuring resource quota status captures configMap creation 02/06/23 18:32:44.272
STEP: Deleting a ConfigMap 02/06/23 18:32:46.277
STEP: Ensuring resource quota status released usage 02/06/23 18:32:46.283
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb  6 18:32:48.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5110" for this suite. 02/06/23 18:32:48.294
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":242,"skipped":4587,"failed":0}
------------------------------
• [SLOW TEST] [28.092 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:32:20.209
    Feb  6 18:32:20.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename resourcequota 02/06/23 18:32:20.21
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:32:20.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:32:20.232
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 02/06/23 18:32:37.239
    STEP: Creating a ResourceQuota 02/06/23 18:32:42.244
    STEP: Ensuring resource quota status is calculated 02/06/23 18:32:42.25
    STEP: Creating a ConfigMap 02/06/23 18:32:44.256
    STEP: Ensuring resource quota status captures configMap creation 02/06/23 18:32:44.272
    STEP: Deleting a ConfigMap 02/06/23 18:32:46.277
    STEP: Ensuring resource quota status released usage 02/06/23 18:32:46.283
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb  6 18:32:48.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5110" for this suite. 02/06/23 18:32:48.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:32:48.303
Feb  6 18:32:48.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename runtimeclass 02/06/23 18:32:48.304
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:32:48.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:32:48.323
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb  6 18:32:48.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8790" for this suite. 02/06/23 18:32:48.339
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":243,"skipped":4622,"failed":0}
------------------------------
• [0.042 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:32:48.303
    Feb  6 18:32:48.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename runtimeclass 02/06/23 18:32:48.304
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:32:48.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:32:48.323
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb  6 18:32:48.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8790" for this suite. 02/06/23 18:32:48.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:32:48.348
Feb  6 18:32:48.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-runtime 02/06/23 18:32:48.349
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:32:48.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:32:48.372
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/06/23 18:32:48.384
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/06/23 18:33:10.501
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/06/23 18:33:10.504
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/06/23 18:33:10.509
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/06/23 18:33:10.509
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/06/23 18:33:10.551
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/06/23 18:33:15.582
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/06/23 18:33:17.596
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/06/23 18:33:17.602
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/06/23 18:33:17.602
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/06/23 18:33:17.624
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/06/23 18:33:18.637
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/06/23 18:33:23.668
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/06/23 18:33:23.674
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/06/23 18:33:23.674
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb  6 18:33:23.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3738" for this suite. 02/06/23 18:33:23.706
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":244,"skipped":4662,"failed":0}
------------------------------
• [SLOW TEST] [35.366 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:32:48.348
    Feb  6 18:32:48.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-runtime 02/06/23 18:32:48.349
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:32:48.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:32:48.372
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/06/23 18:32:48.384
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/06/23 18:33:10.501
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/06/23 18:33:10.504
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/06/23 18:33:10.509
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/06/23 18:33:10.509
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/06/23 18:33:10.551
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/06/23 18:33:15.582
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/06/23 18:33:17.596
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/06/23 18:33:17.602
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/06/23 18:33:17.602
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/06/23 18:33:17.624
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/06/23 18:33:18.637
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/06/23 18:33:23.668
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/06/23 18:33:23.674
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/06/23 18:33:23.674
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb  6 18:33:23.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3738" for this suite. 02/06/23 18:33:23.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:33:23.715
Feb  6 18:33:23.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename daemonsets 02/06/23 18:33:23.716
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:23.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:23.733
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 02/06/23 18:33:23.765
STEP: Check that daemon pods launch on every node of the cluster. 02/06/23 18:33:23.772
Feb  6 18:33:23.779: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:23.779: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:23.779: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:23.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:33:23.783: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:33:24.789: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:24.789: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:24.789: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:24.792: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:33:24.793: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:33:25.791: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:25.792: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:25.792: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:25.796: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:33:25.797: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:33:26.789: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:26.789: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:26.789: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:26.793: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb  6 18:33:26.793: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
Feb  6 18:33:27.790: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:27.790: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:27.790: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  6 18:33:27.793: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb  6 18:33:27.793: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 02/06/23 18:33:27.796
Feb  6 18:33:27.800: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 02/06/23 18:33:27.8
Feb  6 18:33:27.808: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 02/06/23 18:33:27.808
Feb  6 18:33:27.810: INFO: Observed &DaemonSet event: ADDED
Feb  6 18:33:27.810: INFO: Observed &DaemonSet event: MODIFIED
Feb  6 18:33:27.811: INFO: Observed &DaemonSet event: MODIFIED
Feb  6 18:33:27.811: INFO: Observed &DaemonSet event: MODIFIED
Feb  6 18:33:27.811: INFO: Observed &DaemonSet event: MODIFIED
Feb  6 18:33:27.811: INFO: Found daemon set daemon-set in namespace daemonsets-7632 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb  6 18:33:27.811: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 02/06/23 18:33:27.811
STEP: watching for the daemon set status to be patched 02/06/23 18:33:27.819
Feb  6 18:33:27.821: INFO: Observed &DaemonSet event: ADDED
Feb  6 18:33:27.821: INFO: Observed &DaemonSet event: MODIFIED
Feb  6 18:33:27.821: INFO: Observed &DaemonSet event: MODIFIED
Feb  6 18:33:27.822: INFO: Observed &DaemonSet event: MODIFIED
Feb  6 18:33:27.822: INFO: Observed &DaemonSet event: MODIFIED
Feb  6 18:33:27.822: INFO: Observed daemon set daemon-set in namespace daemonsets-7632 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb  6 18:33:27.822: INFO: Observed &DaemonSet event: MODIFIED
Feb  6 18:33:27.822: INFO: Found daemon set daemon-set in namespace daemonsets-7632 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Feb  6 18:33:27.822: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/06/23 18:33:27.83
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7632, will wait for the garbage collector to delete the pods 02/06/23 18:33:27.83
Feb  6 18:33:27.902: INFO: Deleting DaemonSet.extensions daemon-set took: 7.731637ms
Feb  6 18:33:28.004: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.006718ms
Feb  6 18:33:30.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:33:30.009: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb  6 18:33:30.011: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"63723"},"items":null}

Feb  6 18:33:30.014: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"63723"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb  6 18:33:30.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7632" for this suite. 02/06/23 18:33:30.032
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":245,"skipped":4679,"failed":0}
------------------------------
• [SLOW TEST] [6.327 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:33:23.715
    Feb  6 18:33:23.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename daemonsets 02/06/23 18:33:23.716
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:23.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:23.733
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 02/06/23 18:33:23.765
    STEP: Check that daemon pods launch on every node of the cluster. 02/06/23 18:33:23.772
    Feb  6 18:33:23.779: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:23.779: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:23.779: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:23.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:33:23.783: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:33:24.789: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:24.789: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:24.789: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:24.792: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:33:24.793: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:33:25.791: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:25.792: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:25.792: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:25.796: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:33:25.797: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:33:26.789: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:26.789: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:26.789: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:26.793: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb  6 18:33:26.793: INFO: Node tneyla25-md-0-68cbcb4798-5xxmw is running 0 daemon pod, expected 1
    Feb  6 18:33:27.790: INFO: DaemonSet pods can't tolerate node tneyla25-bdbfp with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:27.790: INFO: DaemonSet pods can't tolerate node tneyla25-kns8g with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:27.790: INFO: DaemonSet pods can't tolerate node tneyla25-pnn8z with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb  6 18:33:27.793: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb  6 18:33:27.793: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 02/06/23 18:33:27.796
    Feb  6 18:33:27.800: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 02/06/23 18:33:27.8
    Feb  6 18:33:27.808: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 02/06/23 18:33:27.808
    Feb  6 18:33:27.810: INFO: Observed &DaemonSet event: ADDED
    Feb  6 18:33:27.810: INFO: Observed &DaemonSet event: MODIFIED
    Feb  6 18:33:27.811: INFO: Observed &DaemonSet event: MODIFIED
    Feb  6 18:33:27.811: INFO: Observed &DaemonSet event: MODIFIED
    Feb  6 18:33:27.811: INFO: Observed &DaemonSet event: MODIFIED
    Feb  6 18:33:27.811: INFO: Found daemon set daemon-set in namespace daemonsets-7632 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb  6 18:33:27.811: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 02/06/23 18:33:27.811
    STEP: watching for the daemon set status to be patched 02/06/23 18:33:27.819
    Feb  6 18:33:27.821: INFO: Observed &DaemonSet event: ADDED
    Feb  6 18:33:27.821: INFO: Observed &DaemonSet event: MODIFIED
    Feb  6 18:33:27.821: INFO: Observed &DaemonSet event: MODIFIED
    Feb  6 18:33:27.822: INFO: Observed &DaemonSet event: MODIFIED
    Feb  6 18:33:27.822: INFO: Observed &DaemonSet event: MODIFIED
    Feb  6 18:33:27.822: INFO: Observed daemon set daemon-set in namespace daemonsets-7632 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb  6 18:33:27.822: INFO: Observed &DaemonSet event: MODIFIED
    Feb  6 18:33:27.822: INFO: Found daemon set daemon-set in namespace daemonsets-7632 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Feb  6 18:33:27.822: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/06/23 18:33:27.83
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7632, will wait for the garbage collector to delete the pods 02/06/23 18:33:27.83
    Feb  6 18:33:27.902: INFO: Deleting DaemonSet.extensions daemon-set took: 7.731637ms
    Feb  6 18:33:28.004: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.006718ms
    Feb  6 18:33:30.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:33:30.009: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb  6 18:33:30.011: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"63723"},"items":null}

    Feb  6 18:33:30.014: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"63723"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 18:33:30.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7632" for this suite. 02/06/23 18:33:30.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:33:30.045
Feb  6 18:33:30.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename replicaset 02/06/23 18:33:30.046
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:30.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:30.063
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/06/23 18:33:30.067
Feb  6 18:33:30.077: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7699" to be "running and ready"
Feb  6 18:33:30.080: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.22685ms
Feb  6 18:33:30.080: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:33:32.086: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008941957s
Feb  6 18:33:32.086: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:33:34.086: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.008813867s
Feb  6 18:33:34.086: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Feb  6 18:33:34.086: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 02/06/23 18:33:34.089
STEP: Then the orphan pod is adopted 02/06/23 18:33:34.096
STEP: When the matched label of one of its pods change 02/06/23 18:33:35.105
Feb  6 18:33:35.108: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 02/06/23 18:33:35.12
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb  6 18:33:36.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7699" for this suite. 02/06/23 18:33:36.146
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":246,"skipped":4732,"failed":0}
------------------------------
• [SLOW TEST] [6.109 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:33:30.045
    Feb  6 18:33:30.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename replicaset 02/06/23 18:33:30.046
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:30.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:30.063
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/06/23 18:33:30.067
    Feb  6 18:33:30.077: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7699" to be "running and ready"
    Feb  6 18:33:30.080: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.22685ms
    Feb  6 18:33:30.080: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:33:32.086: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008941957s
    Feb  6 18:33:32.086: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:33:34.086: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.008813867s
    Feb  6 18:33:34.086: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Feb  6 18:33:34.086: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 02/06/23 18:33:34.089
    STEP: Then the orphan pod is adopted 02/06/23 18:33:34.096
    STEP: When the matched label of one of its pods change 02/06/23 18:33:35.105
    Feb  6 18:33:35.108: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/06/23 18:33:35.12
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb  6 18:33:36.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7699" for this suite. 02/06/23 18:33:36.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:33:36.155
Feb  6 18:33:36.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename events 02/06/23 18:33:36.156
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:36.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:36.175
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 02/06/23 18:33:36.178
STEP: listing all events in all namespaces 02/06/23 18:33:36.182
STEP: patching the test event 02/06/23 18:33:36.185
STEP: fetching the test event 02/06/23 18:33:36.191
STEP: updating the test event 02/06/23 18:33:36.193
STEP: getting the test event 02/06/23 18:33:36.203
STEP: deleting the test event 02/06/23 18:33:36.206
STEP: listing all events in all namespaces 02/06/23 18:33:36.215
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Feb  6 18:33:36.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5090" for this suite. 02/06/23 18:33:36.223
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":247,"skipped":4740,"failed":0}
------------------------------
• [0.074 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:33:36.155
    Feb  6 18:33:36.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename events 02/06/23 18:33:36.156
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:36.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:36.175
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 02/06/23 18:33:36.178
    STEP: listing all events in all namespaces 02/06/23 18:33:36.182
    STEP: patching the test event 02/06/23 18:33:36.185
    STEP: fetching the test event 02/06/23 18:33:36.191
    STEP: updating the test event 02/06/23 18:33:36.193
    STEP: getting the test event 02/06/23 18:33:36.203
    STEP: deleting the test event 02/06/23 18:33:36.206
    STEP: listing all events in all namespaces 02/06/23 18:33:36.215
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Feb  6 18:33:36.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5090" for this suite. 02/06/23 18:33:36.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:33:36.231
Feb  6 18:33:36.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename var-expansion 02/06/23 18:33:36.233
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:36.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:36.25
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Feb  6 18:33:36.262: INFO: Waiting up to 2m0s for pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b" in namespace "var-expansion-3107" to be "container 0 failed with reason CreateContainerConfigError"
Feb  6 18:33:36.271: INFO: Pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.550729ms
Feb  6 18:33:38.275: INFO: Pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013294741s
Feb  6 18:33:40.276: INFO: Pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013462095s
Feb  6 18:33:40.276: INFO: Pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb  6 18:33:40.276: INFO: Deleting pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b" in namespace "var-expansion-3107"
Feb  6 18:33:40.285: INFO: Wait up to 5m0s for pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb  6 18:33:42.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3107" for this suite. 02/06/23 18:33:42.3
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":248,"skipped":4754,"failed":0}
------------------------------
• [SLOW TEST] [6.075 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:33:36.231
    Feb  6 18:33:36.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename var-expansion 02/06/23 18:33:36.233
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:36.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:36.25
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Feb  6 18:33:36.262: INFO: Waiting up to 2m0s for pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b" in namespace "var-expansion-3107" to be "container 0 failed with reason CreateContainerConfigError"
    Feb  6 18:33:36.271: INFO: Pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.550729ms
    Feb  6 18:33:38.275: INFO: Pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013294741s
    Feb  6 18:33:40.276: INFO: Pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013462095s
    Feb  6 18:33:40.276: INFO: Pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb  6 18:33:40.276: INFO: Deleting pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b" in namespace "var-expansion-3107"
    Feb  6 18:33:40.285: INFO: Wait up to 5m0s for pod "var-expansion-3eef9d40-30b5-47b3-aa55-5be5f8d4970b" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb  6 18:33:42.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3107" for this suite. 02/06/23 18:33:42.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:33:42.318
Feb  6 18:33:42.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename dns 02/06/23 18:33:42.319
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:42.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:42.338
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/06/23 18:33:42.342
Feb  6 18:33:42.355: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5313  ce93457d-9445-4416-95ac-9cbff4ddb3f6 63945 0 2023-02-06 18:33:42 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-06 18:33:42 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2cjw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2cjw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  6 18:33:42.355: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5313" to be "running and ready"
Feb  6 18:33:42.363: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.54377ms
Feb  6 18:33:42.363: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:33:44.368: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013292225s
Feb  6 18:33:44.368: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:33:46.367: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.012313677s
Feb  6 18:33:46.368: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Feb  6 18:33:46.368: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 02/06/23 18:33:46.368
Feb  6 18:33:46.368: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5313 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:33:46.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:33:46.368: INFO: ExecWithOptions: Clientset creation
Feb  6 18:33:46.368: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5313/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 02/06/23 18:33:46.463
Feb  6 18:33:46.463: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5313 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:33:46.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:33:46.464: INFO: ExecWithOptions: Clientset creation
Feb  6 18:33:46.464: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5313/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb  6 18:33:46.556: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb  6 18:33:46.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5313" for this suite. 02/06/23 18:33:46.58
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":249,"skipped":4809,"failed":0}
------------------------------
• [4.271 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:33:42.318
    Feb  6 18:33:42.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename dns 02/06/23 18:33:42.319
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:42.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:42.338
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/06/23 18:33:42.342
    Feb  6 18:33:42.355: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5313  ce93457d-9445-4416-95ac-9cbff4ddb3f6 63945 0 2023-02-06 18:33:42 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-06 18:33:42 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2cjw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2cjw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb  6 18:33:42.355: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5313" to be "running and ready"
    Feb  6 18:33:42.363: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.54377ms
    Feb  6 18:33:42.363: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:33:44.368: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013292225s
    Feb  6 18:33:44.368: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:33:46.367: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.012313677s
    Feb  6 18:33:46.368: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Feb  6 18:33:46.368: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 02/06/23 18:33:46.368
    Feb  6 18:33:46.368: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5313 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:33:46.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:33:46.368: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:33:46.368: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5313/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 02/06/23 18:33:46.463
    Feb  6 18:33:46.463: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5313 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:33:46.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:33:46.464: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:33:46.464: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5313/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb  6 18:33:46.556: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb  6 18:33:46.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5313" for this suite. 02/06/23 18:33:46.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:33:46.595
Feb  6 18:33:46.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 18:33:46.596
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:46.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:46.618
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-108ce59b-055d-40b8-80a5-53ed51e3ed58 02/06/23 18:33:46.621
STEP: Creating a pod to test consume configMaps 02/06/23 18:33:46.626
Feb  6 18:33:46.635: INFO: Waiting up to 5m0s for pod "pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce" in namespace "configmap-3407" to be "Succeeded or Failed"
Feb  6 18:33:46.638: INFO: Pod "pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.167222ms
Feb  6 18:33:48.644: INFO: Pod "pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009537162s
Feb  6 18:33:50.645: INFO: Pod "pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009900001s
Feb  6 18:33:52.643: INFO: Pod "pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00861156s
STEP: Saw pod success 02/06/23 18:33:52.643
Feb  6 18:33:52.644: INFO: Pod "pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce" satisfied condition "Succeeded or Failed"
Feb  6 18:33:52.647: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:33:52.66
Feb  6 18:33:52.671: INFO: Waiting for pod pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce to disappear
Feb  6 18:33:52.674: INFO: Pod pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 18:33:52.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3407" for this suite. 02/06/23 18:33:52.681
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":250,"skipped":4833,"failed":0}
------------------------------
• [SLOW TEST] [6.092 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:33:46.595
    Feb  6 18:33:46.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 18:33:46.596
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:46.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:46.618
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-108ce59b-055d-40b8-80a5-53ed51e3ed58 02/06/23 18:33:46.621
    STEP: Creating a pod to test consume configMaps 02/06/23 18:33:46.626
    Feb  6 18:33:46.635: INFO: Waiting up to 5m0s for pod "pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce" in namespace "configmap-3407" to be "Succeeded or Failed"
    Feb  6 18:33:46.638: INFO: Pod "pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.167222ms
    Feb  6 18:33:48.644: INFO: Pod "pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009537162s
    Feb  6 18:33:50.645: INFO: Pod "pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009900001s
    Feb  6 18:33:52.643: INFO: Pod "pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00861156s
    STEP: Saw pod success 02/06/23 18:33:52.643
    Feb  6 18:33:52.644: INFO: Pod "pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce" satisfied condition "Succeeded or Failed"
    Feb  6 18:33:52.647: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:33:52.66
    Feb  6 18:33:52.671: INFO: Waiting for pod pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce to disappear
    Feb  6 18:33:52.674: INFO: Pod pod-configmaps-093a474c-d347-4a47-b7ea-19962db80fce no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 18:33:52.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3407" for this suite. 02/06/23 18:33:52.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:33:52.692
Feb  6 18:33:52.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename init-container 02/06/23 18:33:52.693
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:52.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:52.714
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 02/06/23 18:33:52.717
Feb  6 18:33:52.717: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb  6 18:33:58.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3526" for this suite. 02/06/23 18:33:58.077
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":251,"skipped":4848,"failed":0}
------------------------------
• [SLOW TEST] [5.393 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:33:52.692
    Feb  6 18:33:52.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename init-container 02/06/23 18:33:52.693
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:52.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:52.714
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 02/06/23 18:33:52.717
    Feb  6 18:33:52.717: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb  6 18:33:58.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3526" for this suite. 02/06/23 18:33:58.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:33:58.092
Feb  6 18:33:58.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 18:33:58.093
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:58.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:58.118
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-c0c095aa-81a0-4a09-a79c-d8ef75a6c56f 02/06/23 18:33:58.126
STEP: Creating the pod 02/06/23 18:33:58.132
Feb  6 18:33:58.142: INFO: Waiting up to 5m0s for pod "pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625" in namespace "configmap-9674" to be "running and ready"
Feb  6 18:33:58.147: INFO: Pod "pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625": Phase="Pending", Reason="", readiness=false. Elapsed: 4.625817ms
Feb  6 18:33:58.147: INFO: The phase of Pod pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:34:00.152: INFO: Pod "pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010037083s
Feb  6 18:34:00.152: INFO: The phase of Pod pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:34:02.152: INFO: Pod "pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625": Phase="Running", Reason="", readiness=true. Elapsed: 4.010242099s
Feb  6 18:34:02.152: INFO: The phase of Pod pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625 is Running (Ready = true)
Feb  6 18:34:02.152: INFO: Pod "pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-c0c095aa-81a0-4a09-a79c-d8ef75a6c56f 02/06/23 18:34:02.161
STEP: waiting to observe update in volume 02/06/23 18:34:02.166
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 18:34:04.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9674" for this suite. 02/06/23 18:34:04.187
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":252,"skipped":4886,"failed":0}
------------------------------
• [SLOW TEST] [6.101 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:33:58.092
    Feb  6 18:33:58.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 18:33:58.093
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:33:58.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:33:58.118
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-c0c095aa-81a0-4a09-a79c-d8ef75a6c56f 02/06/23 18:33:58.126
    STEP: Creating the pod 02/06/23 18:33:58.132
    Feb  6 18:33:58.142: INFO: Waiting up to 5m0s for pod "pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625" in namespace "configmap-9674" to be "running and ready"
    Feb  6 18:33:58.147: INFO: Pod "pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625": Phase="Pending", Reason="", readiness=false. Elapsed: 4.625817ms
    Feb  6 18:33:58.147: INFO: The phase of Pod pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:34:00.152: INFO: Pod "pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010037083s
    Feb  6 18:34:00.152: INFO: The phase of Pod pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:34:02.152: INFO: Pod "pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625": Phase="Running", Reason="", readiness=true. Elapsed: 4.010242099s
    Feb  6 18:34:02.152: INFO: The phase of Pod pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625 is Running (Ready = true)
    Feb  6 18:34:02.152: INFO: Pod "pod-configmaps-378ecb13-5508-4d8a-ab08-faa14ca5c625" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-c0c095aa-81a0-4a09-a79c-d8ef75a6c56f 02/06/23 18:34:02.161
    STEP: waiting to observe update in volume 02/06/23 18:34:02.166
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 18:34:04.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9674" for this suite. 02/06/23 18:34:04.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:34:04.197
Feb  6 18:34:04.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:34:04.199
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:34:04.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:34:04.218
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 02/06/23 18:34:04.222
Feb  6 18:34:04.230: INFO: Waiting up to 5m0s for pod "annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7" in namespace "projected-4582" to be "running and ready"
Feb  6 18:34:04.233: INFO: Pod "annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.967857ms
Feb  6 18:34:04.233: INFO: The phase of Pod annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:34:06.240: INFO: Pod "annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009797613s
Feb  6 18:34:06.240: INFO: The phase of Pod annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:34:08.237: INFO: Pod "annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7": Phase="Running", Reason="", readiness=true. Elapsed: 4.007464209s
Feb  6 18:34:08.237: INFO: The phase of Pod annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7 is Running (Ready = true)
Feb  6 18:34:08.237: INFO: Pod "annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7" satisfied condition "running and ready"
Feb  6 18:34:08.764: INFO: Successfully updated pod "annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb  6 18:34:12.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4582" for this suite. 02/06/23 18:34:12.804
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":253,"skipped":4958,"failed":0}
------------------------------
• [SLOW TEST] [8.613 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:34:04.197
    Feb  6 18:34:04.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:34:04.199
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:34:04.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:34:04.218
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 02/06/23 18:34:04.222
    Feb  6 18:34:04.230: INFO: Waiting up to 5m0s for pod "annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7" in namespace "projected-4582" to be "running and ready"
    Feb  6 18:34:04.233: INFO: Pod "annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.967857ms
    Feb  6 18:34:04.233: INFO: The phase of Pod annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:34:06.240: INFO: Pod "annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009797613s
    Feb  6 18:34:06.240: INFO: The phase of Pod annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:34:08.237: INFO: Pod "annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7": Phase="Running", Reason="", readiness=true. Elapsed: 4.007464209s
    Feb  6 18:34:08.237: INFO: The phase of Pod annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7 is Running (Ready = true)
    Feb  6 18:34:08.237: INFO: Pod "annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7" satisfied condition "running and ready"
    Feb  6 18:34:08.764: INFO: Successfully updated pod "annotationupdate3a3a7b89-eb55-4295-8707-911fdc7e12d7"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb  6 18:34:12.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4582" for this suite. 02/06/23 18:34:12.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:34:12.816
Feb  6 18:34:12.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename deployment 02/06/23 18:34:12.817
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:34:12.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:34:12.845
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 02/06/23 18:34:12.853
STEP: waiting for Deployment to be created 02/06/23 18:34:12.858
STEP: waiting for all Replicas to be Ready 02/06/23 18:34:12.86
Feb  6 18:34:12.861: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  6 18:34:12.861: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  6 18:34:12.873: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  6 18:34:12.873: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  6 18:34:12.894: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  6 18:34:12.894: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  6 18:34:12.934: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  6 18:34:12.934: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  6 18:34:15.922: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb  6 18:34:15.922: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb  6 18:34:16.138: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 02/06/23 18:34:16.138
W0206 18:34:16.150616      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb  6 18:34:16.153: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 02/06/23 18:34:16.153
Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:16.157: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:16.157: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:16.169: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:16.170: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:16.208: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:16.208: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:16.218: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
Feb  6 18:34:16.218: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
Feb  6 18:34:16.228: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
Feb  6 18:34:16.229: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
Feb  6 18:34:19.166: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:19.166: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:19.210: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
STEP: listing Deployments 02/06/23 18:34:19.21
Feb  6 18:34:19.218: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 02/06/23 18:34:19.218
Feb  6 18:34:19.241: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 02/06/23 18:34:19.241
Feb  6 18:34:19.250: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb  6 18:34:19.261: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb  6 18:34:19.286: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb  6 18:34:19.314: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb  6 18:34:19.323: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb  6 18:34:19.339: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb  6 18:34:22.841: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb  6 18:34:23.191: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Feb  6 18:34:23.248: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb  6 18:34:23.256: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb  6 18:34:26.974: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 02/06/23 18:34:27.008
STEP: fetching the DeploymentStatus 02/06/23 18:34:27.016
Feb  6 18:34:27.022: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
Feb  6 18:34:27.022: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
Feb  6 18:34:27.022: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
Feb  6 18:34:27.023: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
Feb  6 18:34:27.023: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
Feb  6 18:34:27.024: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
Feb  6 18:34:27.024: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:27.024: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 3
Feb  6 18:34:27.024: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:27.024: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
Feb  6 18:34:27.024: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 3
STEP: deleting the Deployment 02/06/23 18:34:27.024
Feb  6 18:34:27.035: INFO: observed event type MODIFIED
Feb  6 18:34:27.035: INFO: observed event type MODIFIED
Feb  6 18:34:27.036: INFO: observed event type MODIFIED
Feb  6 18:34:27.036: INFO: observed event type MODIFIED
Feb  6 18:34:27.036: INFO: observed event type MODIFIED
Feb  6 18:34:27.036: INFO: observed event type MODIFIED
Feb  6 18:34:27.036: INFO: observed event type MODIFIED
Feb  6 18:34:27.037: INFO: observed event type MODIFIED
Feb  6 18:34:27.037: INFO: observed event type MODIFIED
Feb  6 18:34:27.037: INFO: observed event type MODIFIED
Feb  6 18:34:27.037: INFO: observed event type MODIFIED
Feb  6 18:34:27.037: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb  6 18:34:27.064: INFO: Log out all the ReplicaSets if there is no deployment created
Feb  6 18:34:27.069: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-1929  392d4825-283c-49b6-897f-57784414764c 64702 4 2023-02-06 18:34:16 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment eb859a2f-22d1-4821-b343-6cff3f9be520 0xc003b9c257 0xc003b9c258}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb859a2f-22d1-4821-b343-6cff3f9be520\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:34:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b9c2e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Feb  6 18:34:27.074: INFO: pod: "test-deployment-54cc775c4b-rvgtp":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-rvgtp test-deployment-54cc775c4b- deployment-1929  b8ceb2c1-9637-4289-bf20-3b906a7433ae 64698 0 2023-02-06 18:34:19 +0000 UTC 2023-02-06 18:34:27 +0000 UTC 0xc00437a9c0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 392d4825-283c-49b6-897f-57784414764c 0xc00437a9f7 0xc00437a9f8}] [] [{kube-controller-manager Update v1 2023-02-06 18:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"392d4825-283c-49b6-897f-57784414764c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 18:34:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2c8c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2c8c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:192.168.3.26,StartTime:2023-02-06 18:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 18:34:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://d49e99154edb7f8e290b08bec6d1a4168c30f6af9aa100bc234a97e2c95264fa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.3.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb  6 18:34:27.075: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-1929  41219255-782c-43e1-8bba-3c5727bd072a 64694 2 2023-02-06 18:34:19 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment eb859a2f-22d1-4821-b343-6cff3f9be520 0xc003b9c347 0xc003b9c348}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:34:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb859a2f-22d1-4821-b343-6cff3f9be520\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:34:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b9c3d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Feb  6 18:34:27.080: INFO: pod: "test-deployment-7c7d8d58c8-2q5m5":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2q5m5 test-deployment-7c7d8d58c8- deployment-1929  6e2de6d7-3578-4761-80a9-68e23dd06dea 64693 0 2023-02-06 18:34:23 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 41219255-782c-43e1-8bba-3c5727bd072a 0xc00437b677 0xc00437b678}] [] [{kube-controller-manager Update v1 2023-02-06 18:34:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41219255-782c-43e1-8bba-3c5727bd072a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 18:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c22w8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c22w8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:192.168.1.162,StartTime:2023-02-06 18:34:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 18:34:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1e41f4434f6941ecf740aa954ac6ce03eb3f63b26962691af0f4625fb92e0409,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.1.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb  6 18:34:27.080: INFO: pod: "test-deployment-7c7d8d58c8-ndc2v":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-ndc2v test-deployment-7c7d8d58c8- deployment-1929  38a627ef-e8ab-49f3-885d-563fb09a06b4 64627 0 2023-02-06 18:34:19 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 41219255-782c-43e1-8bba-3c5727bd072a 0xc00437b857 0xc00437b858}] [] [{kube-controller-manager Update v1 2023-02-06 18:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41219255-782c-43e1-8bba-3c5727bd072a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 18:34:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-66cqr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-66cqr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.58,StartTime:2023-02-06 18:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 18:34:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cec50afa391148b2a0c6b64d42da056e9f9dcad6076524c69a0f9b9c22818060,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb  6 18:34:27.080: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-1929  bbaaca14-57ba-4dc7-8980-772c1c36f7de 64546 3 2023-02-06 18:34:12 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment eb859a2f-22d1-4821-b343-6cff3f9be520 0xc003b9c437 0xc003b9c438}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb859a2f-22d1-4821-b343-6cff3f9be520\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:34:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b9c4c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb  6 18:34:27.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1929" for this suite. 02/06/23 18:34:27.095
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":254,"skipped":4967,"failed":0}
------------------------------
• [SLOW TEST] [14.296 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:34:12.816
    Feb  6 18:34:12.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename deployment 02/06/23 18:34:12.817
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:34:12.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:34:12.845
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 02/06/23 18:34:12.853
    STEP: waiting for Deployment to be created 02/06/23 18:34:12.858
    STEP: waiting for all Replicas to be Ready 02/06/23 18:34:12.86
    Feb  6 18:34:12.861: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb  6 18:34:12.861: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb  6 18:34:12.873: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb  6 18:34:12.873: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb  6 18:34:12.894: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb  6 18:34:12.894: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb  6 18:34:12.934: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb  6 18:34:12.934: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb  6 18:34:15.922: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb  6 18:34:15.922: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb  6 18:34:16.138: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 02/06/23 18:34:16.138
    W0206 18:34:16.150616      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb  6 18:34:16.153: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 02/06/23 18:34:16.153
    Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
    Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
    Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
    Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
    Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
    Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
    Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
    Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 0
    Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:16.156: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:16.157: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:16.157: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:16.169: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:16.170: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:16.208: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:16.208: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:16.218: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    Feb  6 18:34:16.218: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    Feb  6 18:34:16.228: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    Feb  6 18:34:16.229: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    Feb  6 18:34:19.166: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:19.166: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:19.210: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    STEP: listing Deployments 02/06/23 18:34:19.21
    Feb  6 18:34:19.218: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 02/06/23 18:34:19.218
    Feb  6 18:34:19.241: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 02/06/23 18:34:19.241
    Feb  6 18:34:19.250: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb  6 18:34:19.261: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb  6 18:34:19.286: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb  6 18:34:19.314: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb  6 18:34:19.323: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb  6 18:34:19.339: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb  6 18:34:22.841: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb  6 18:34:23.191: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Feb  6 18:34:23.248: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb  6 18:34:23.256: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb  6 18:34:26.974: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 02/06/23 18:34:27.008
    STEP: fetching the DeploymentStatus 02/06/23 18:34:27.016
    Feb  6 18:34:27.022: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    Feb  6 18:34:27.022: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    Feb  6 18:34:27.022: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    Feb  6 18:34:27.023: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    Feb  6 18:34:27.023: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    Feb  6 18:34:27.024: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 1
    Feb  6 18:34:27.024: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:27.024: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 3
    Feb  6 18:34:27.024: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:27.024: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 2
    Feb  6 18:34:27.024: INFO: observed Deployment test-deployment in namespace deployment-1929 with ReadyReplicas 3
    STEP: deleting the Deployment 02/06/23 18:34:27.024
    Feb  6 18:34:27.035: INFO: observed event type MODIFIED
    Feb  6 18:34:27.035: INFO: observed event type MODIFIED
    Feb  6 18:34:27.036: INFO: observed event type MODIFIED
    Feb  6 18:34:27.036: INFO: observed event type MODIFIED
    Feb  6 18:34:27.036: INFO: observed event type MODIFIED
    Feb  6 18:34:27.036: INFO: observed event type MODIFIED
    Feb  6 18:34:27.036: INFO: observed event type MODIFIED
    Feb  6 18:34:27.037: INFO: observed event type MODIFIED
    Feb  6 18:34:27.037: INFO: observed event type MODIFIED
    Feb  6 18:34:27.037: INFO: observed event type MODIFIED
    Feb  6 18:34:27.037: INFO: observed event type MODIFIED
    Feb  6 18:34:27.037: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb  6 18:34:27.064: INFO: Log out all the ReplicaSets if there is no deployment created
    Feb  6 18:34:27.069: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-1929  392d4825-283c-49b6-897f-57784414764c 64702 4 2023-02-06 18:34:16 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment eb859a2f-22d1-4821-b343-6cff3f9be520 0xc003b9c257 0xc003b9c258}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb859a2f-22d1-4821-b343-6cff3f9be520\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:34:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b9c2e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Feb  6 18:34:27.074: INFO: pod: "test-deployment-54cc775c4b-rvgtp":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-rvgtp test-deployment-54cc775c4b- deployment-1929  b8ceb2c1-9637-4289-bf20-3b906a7433ae 64698 0 2023-02-06 18:34:19 +0000 UTC 2023-02-06 18:34:27 +0000 UTC 0xc00437a9c0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 392d4825-283c-49b6-897f-57784414764c 0xc00437a9f7 0xc00437a9f8}] [] [{kube-controller-manager Update v1 2023-02-06 18:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"392d4825-283c-49b6-897f-57784414764c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 18:34:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.3.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2c8c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2c8c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-6j94p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.89,PodIP:192.168.3.26,StartTime:2023-02-06 18:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 18:34:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://d49e99154edb7f8e290b08bec6d1a4168c30f6af9aa100bc234a97e2c95264fa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.3.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb  6 18:34:27.075: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-1929  41219255-782c-43e1-8bba-3c5727bd072a 64694 2 2023-02-06 18:34:19 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment eb859a2f-22d1-4821-b343-6cff3f9be520 0xc003b9c347 0xc003b9c348}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:34:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb859a2f-22d1-4821-b343-6cff3f9be520\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:34:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b9c3d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Feb  6 18:34:27.080: INFO: pod: "test-deployment-7c7d8d58c8-2q5m5":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2q5m5 test-deployment-7c7d8d58c8- deployment-1929  6e2de6d7-3578-4761-80a9-68e23dd06dea 64693 0 2023-02-06 18:34:23 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 41219255-782c-43e1-8bba-3c5727bd072a 0xc00437b677 0xc00437b678}] [] [{kube-controller-manager Update v1 2023-02-06 18:34:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41219255-782c-43e1-8bba-3c5727bd072a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 18:34:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c22w8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c22w8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-lz542,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.148.90,PodIP:192.168.1.162,StartTime:2023-02-06 18:34:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 18:34:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1e41f4434f6941ecf740aa954ac6ce03eb3f63b26962691af0f4625fb92e0409,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.1.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb  6 18:34:27.080: INFO: pod: "test-deployment-7c7d8d58c8-ndc2v":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-ndc2v test-deployment-7c7d8d58c8- deployment-1929  38a627ef-e8ab-49f3-885d-563fb09a06b4 64627 0 2023-02-06 18:34:19 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 41219255-782c-43e1-8bba-3c5727bd072a 0xc00437b857 0xc00437b858}] [] [{kube-controller-manager Update v1 2023-02-06 18:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41219255-782c-43e1-8bba-3c5727bd072a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-06 18:34:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-66cqr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-66cqr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tneyla25-md-0-68cbcb4798-5xxmw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-06 18:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:195.17.4.133,PodIP:192.168.2.58,StartTime:2023-02-06 18:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-06 18:34:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cec50afa391148b2a0c6b64d42da056e9f9dcad6076524c69a0f9b9c22818060,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb  6 18:34:27.080: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-1929  bbaaca14-57ba-4dc7-8980-772c1c36f7de 64546 3 2023-02-06 18:34:12 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment eb859a2f-22d1-4821-b343-6cff3f9be520 0xc003b9c437 0xc003b9c438}] [] [{kube-controller-manager Update apps/v1 2023-02-06 18:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb859a2f-22d1-4821-b343-6cff3f9be520\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-06 18:34:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b9c4c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb  6 18:34:27.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1929" for this suite. 02/06/23 18:34:27.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:34:27.119
Feb  6 18:34:27.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename statefulset 02/06/23 18:34:27.12
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:34:27.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:34:27.143
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5128 02/06/23 18:34:27.146
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 02/06/23 18:34:27.155
Feb  6 18:34:27.179: INFO: Found 0 stateful pods, waiting for 3
Feb  6 18:34:37.184: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 18:34:37.184: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 18:34:37.184: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Feb  6 18:34:47.184: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 18:34:47.184: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 18:34:47.184: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/06/23 18:34:47.194
Feb  6 18:34:47.217: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/06/23 18:34:47.217
STEP: Not applying an update when the partition is greater than the number of replicas 02/06/23 18:34:57.253
STEP: Performing a canary update 02/06/23 18:34:57.253
Feb  6 18:34:57.274: INFO: Updating stateful set ss2
Feb  6 18:34:57.285: INFO: Waiting for Pod statefulset-5128/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 02/06/23 18:35:07.293
Feb  6 18:35:07.372: INFO: Found 2 stateful pods, waiting for 3
Feb  6 18:35:17.378: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 18:35:17.378: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 18:35:17.378: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 02/06/23 18:35:17.384
Feb  6 18:35:17.405: INFO: Updating stateful set ss2
Feb  6 18:35:17.413: INFO: Waiting for Pod statefulset-5128/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Feb  6 18:35:27.446: INFO: Updating stateful set ss2
Feb  6 18:35:27.456: INFO: Waiting for StatefulSet statefulset-5128/ss2 to complete update
Feb  6 18:35:27.456: INFO: Waiting for Pod statefulset-5128/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb  6 18:35:37.464: INFO: Deleting all statefulset in ns statefulset-5128
Feb  6 18:35:37.468: INFO: Scaling statefulset ss2 to 0
Feb  6 18:35:47.485: INFO: Waiting for statefulset status.replicas updated to 0
Feb  6 18:35:47.489: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb  6 18:35:47.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5128" for this suite. 02/06/23 18:35:47.526
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":255,"skipped":4985,"failed":0}
------------------------------
• [SLOW TEST] [80.416 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:34:27.119
    Feb  6 18:34:27.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename statefulset 02/06/23 18:34:27.12
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:34:27.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:34:27.143
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5128 02/06/23 18:34:27.146
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 02/06/23 18:34:27.155
    Feb  6 18:34:27.179: INFO: Found 0 stateful pods, waiting for 3
    Feb  6 18:34:37.184: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 18:34:37.184: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 18:34:37.184: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
    Feb  6 18:34:47.184: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 18:34:47.184: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 18:34:47.184: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/06/23 18:34:47.194
    Feb  6 18:34:47.217: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/06/23 18:34:47.217
    STEP: Not applying an update when the partition is greater than the number of replicas 02/06/23 18:34:57.253
    STEP: Performing a canary update 02/06/23 18:34:57.253
    Feb  6 18:34:57.274: INFO: Updating stateful set ss2
    Feb  6 18:34:57.285: INFO: Waiting for Pod statefulset-5128/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 02/06/23 18:35:07.293
    Feb  6 18:35:07.372: INFO: Found 2 stateful pods, waiting for 3
    Feb  6 18:35:17.378: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 18:35:17.378: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 18:35:17.378: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 02/06/23 18:35:17.384
    Feb  6 18:35:17.405: INFO: Updating stateful set ss2
    Feb  6 18:35:17.413: INFO: Waiting for Pod statefulset-5128/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Feb  6 18:35:27.446: INFO: Updating stateful set ss2
    Feb  6 18:35:27.456: INFO: Waiting for StatefulSet statefulset-5128/ss2 to complete update
    Feb  6 18:35:27.456: INFO: Waiting for Pod statefulset-5128/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb  6 18:35:37.464: INFO: Deleting all statefulset in ns statefulset-5128
    Feb  6 18:35:37.468: INFO: Scaling statefulset ss2 to 0
    Feb  6 18:35:47.485: INFO: Waiting for statefulset status.replicas updated to 0
    Feb  6 18:35:47.489: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb  6 18:35:47.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5128" for this suite. 02/06/23 18:35:47.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:35:47.542
Feb  6 18:35:47.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 18:35:47.543
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:35:47.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:35:47.563
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb  6 18:35:47.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6760" for this suite. 02/06/23 18:35:47.607
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":256,"skipped":5043,"failed":0}
------------------------------
• [0.073 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:35:47.542
    Feb  6 18:35:47.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 18:35:47.543
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:35:47.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:35:47.563
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 18:35:47.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6760" for this suite. 02/06/23 18:35:47.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:35:47.617
Feb  6 18:35:47.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:35:47.618
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:35:47.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:35:47.646
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-3d769a4a-61fd-4f54-ad80-7deeb377404c 02/06/23 18:35:47.649
STEP: Creating a pod to test consume secrets 02/06/23 18:35:47.657
Feb  6 18:35:47.665: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70" in namespace "projected-8124" to be "Succeeded or Failed"
Feb  6 18:35:47.668: INFO: Pod "pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.933364ms
Feb  6 18:35:49.673: INFO: Pod "pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00795754s
Feb  6 18:35:51.673: INFO: Pod "pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007989603s
Feb  6 18:35:53.673: INFO: Pod "pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008532433s
STEP: Saw pod success 02/06/23 18:35:53.673
Feb  6 18:35:53.674: INFO: Pod "pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70" satisfied condition "Succeeded or Failed"
Feb  6 18:35:53.678: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/06/23 18:35:53.696
Feb  6 18:35:53.711: INFO: Waiting for pod pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70 to disappear
Feb  6 18:35:53.714: INFO: Pod pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb  6 18:35:53.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8124" for this suite. 02/06/23 18:35:53.719
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":257,"skipped":5061,"failed":0}
------------------------------
• [SLOW TEST] [6.110 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:35:47.617
    Feb  6 18:35:47.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:35:47.618
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:35:47.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:35:47.646
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-3d769a4a-61fd-4f54-ad80-7deeb377404c 02/06/23 18:35:47.649
    STEP: Creating a pod to test consume secrets 02/06/23 18:35:47.657
    Feb  6 18:35:47.665: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70" in namespace "projected-8124" to be "Succeeded or Failed"
    Feb  6 18:35:47.668: INFO: Pod "pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.933364ms
    Feb  6 18:35:49.673: INFO: Pod "pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00795754s
    Feb  6 18:35:51.673: INFO: Pod "pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007989603s
    Feb  6 18:35:53.673: INFO: Pod "pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008532433s
    STEP: Saw pod success 02/06/23 18:35:53.673
    Feb  6 18:35:53.674: INFO: Pod "pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70" satisfied condition "Succeeded or Failed"
    Feb  6 18:35:53.678: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 18:35:53.696
    Feb  6 18:35:53.711: INFO: Waiting for pod pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70 to disappear
    Feb  6 18:35:53.714: INFO: Pod pod-projected-secrets-76bc2e9a-faf2-4fbb-8daa-45f41c34ac70 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb  6 18:35:53.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8124" for this suite. 02/06/23 18:35:53.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:35:53.732
Feb  6 18:35:53.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 18:35:53.734
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:35:53.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:35:53.754
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 02/06/23 18:35:53.757
Feb  6 18:35:53.771: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd" in namespace "emptydir-8548" to be "running"
Feb  6 18:35:53.786: INFO: Pod "pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.77456ms
Feb  6 18:35:55.790: INFO: Pod "pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018744932s
Feb  6 18:35:57.790: INFO: Pod "pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd": Phase="Running", Reason="", readiness=false. Elapsed: 4.019627944s
Feb  6 18:35:57.790: INFO: Pod "pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd" satisfied condition "running"
STEP: Reading file content from the nginx-container 02/06/23 18:35:57.79
Feb  6 18:35:57.791: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8548 PodName:pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:35:57.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:35:57.791: INFO: ExecWithOptions: Clientset creation
Feb  6 18:35:57.791: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8548/pods/pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Feb  6 18:35:57.859: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 18:35:57.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8548" for this suite. 02/06/23 18:35:57.865
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":258,"skipped":5080,"failed":0}
------------------------------
• [4.140 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:35:53.732
    Feb  6 18:35:53.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 18:35:53.734
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:35:53.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:35:53.754
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 02/06/23 18:35:53.757
    Feb  6 18:35:53.771: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd" in namespace "emptydir-8548" to be "running"
    Feb  6 18:35:53.786: INFO: Pod "pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.77456ms
    Feb  6 18:35:55.790: INFO: Pod "pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018744932s
    Feb  6 18:35:57.790: INFO: Pod "pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd": Phase="Running", Reason="", readiness=false. Elapsed: 4.019627944s
    Feb  6 18:35:57.790: INFO: Pod "pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd" satisfied condition "running"
    STEP: Reading file content from the nginx-container 02/06/23 18:35:57.79
    Feb  6 18:35:57.791: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8548 PodName:pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:35:57.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:35:57.791: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:35:57.791: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8548/pods/pod-sharedvolume-9eda12b6-9312-4fa6-8ab2-e4bf4b1c39cd/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Feb  6 18:35:57.859: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 18:35:57.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8548" for this suite. 02/06/23 18:35:57.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:35:57.875
Feb  6 18:35:57.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename resourcequota 02/06/23 18:35:57.877
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:35:57.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:35:57.899
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 02/06/23 18:35:57.901
STEP: Creating a ResourceQuota 02/06/23 18:36:02.906
STEP: Ensuring resource quota status is calculated 02/06/23 18:36:02.913
STEP: Creating a ReplicaSet 02/06/23 18:36:04.918
STEP: Ensuring resource quota status captures replicaset creation 02/06/23 18:36:04.933
STEP: Deleting a ReplicaSet 02/06/23 18:36:06.937
STEP: Ensuring resource quota status released usage 02/06/23 18:36:06.945
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb  6 18:36:08.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3141" for this suite. 02/06/23 18:36:08.956
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":259,"skipped":5085,"failed":0}
------------------------------
• [SLOW TEST] [11.091 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:35:57.875
    Feb  6 18:35:57.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename resourcequota 02/06/23 18:35:57.877
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:35:57.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:35:57.899
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 02/06/23 18:35:57.901
    STEP: Creating a ResourceQuota 02/06/23 18:36:02.906
    STEP: Ensuring resource quota status is calculated 02/06/23 18:36:02.913
    STEP: Creating a ReplicaSet 02/06/23 18:36:04.918
    STEP: Ensuring resource quota status captures replicaset creation 02/06/23 18:36:04.933
    STEP: Deleting a ReplicaSet 02/06/23 18:36:06.937
    STEP: Ensuring resource quota status released usage 02/06/23 18:36:06.945
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb  6 18:36:08.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3141" for this suite. 02/06/23 18:36:08.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:36:08.974
Feb  6 18:36:08.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 18:36:08.975
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:08.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:08.995
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-fce519b6-f0ea-4aec-ab9e-3e5d04b174a8 02/06/23 18:36:08.999
STEP: Creating a pod to test consume secrets 02/06/23 18:36:09.005
Feb  6 18:36:09.018: INFO: Waiting up to 5m0s for pod "pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0" in namespace "secrets-9769" to be "Succeeded or Failed"
Feb  6 18:36:09.025: INFO: Pod "pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.030164ms
Feb  6 18:36:11.030: INFO: Pod "pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012389722s
Feb  6 18:36:13.029: INFO: Pod "pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011892092s
Feb  6 18:36:15.029: INFO: Pod "pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011678201s
STEP: Saw pod success 02/06/23 18:36:15.029
Feb  6 18:36:15.030: INFO: Pod "pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0" satisfied condition "Succeeded or Failed"
Feb  6 18:36:15.033: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0 container secret-volume-test: <nil>
STEP: delete the pod 02/06/23 18:36:15.04
Feb  6 18:36:15.057: INFO: Waiting for pod pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0 to disappear
Feb  6 18:36:15.060: INFO: Pod pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb  6 18:36:15.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9769" for this suite. 02/06/23 18:36:15.065
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":260,"skipped":5120,"failed":0}
------------------------------
• [SLOW TEST] [6.099 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:36:08.974
    Feb  6 18:36:08.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 18:36:08.975
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:08.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:08.995
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-fce519b6-f0ea-4aec-ab9e-3e5d04b174a8 02/06/23 18:36:08.999
    STEP: Creating a pod to test consume secrets 02/06/23 18:36:09.005
    Feb  6 18:36:09.018: INFO: Waiting up to 5m0s for pod "pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0" in namespace "secrets-9769" to be "Succeeded or Failed"
    Feb  6 18:36:09.025: INFO: Pod "pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.030164ms
    Feb  6 18:36:11.030: INFO: Pod "pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012389722s
    Feb  6 18:36:13.029: INFO: Pod "pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011892092s
    Feb  6 18:36:15.029: INFO: Pod "pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011678201s
    STEP: Saw pod success 02/06/23 18:36:15.029
    Feb  6 18:36:15.030: INFO: Pod "pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0" satisfied condition "Succeeded or Failed"
    Feb  6 18:36:15.033: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0 container secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 18:36:15.04
    Feb  6 18:36:15.057: INFO: Waiting for pod pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0 to disappear
    Feb  6 18:36:15.060: INFO: Pod pod-secrets-f1a10d11-a00c-4b05-942b-1a5f1d6d50f0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 18:36:15.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9769" for this suite. 02/06/23 18:36:15.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:36:15.073
Feb  6 18:36:15.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 18:36:15.075
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:15.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:15.097
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/06/23 18:36:15.1
Feb  6 18:36:15.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-5905 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Feb  6 18:36:15.202: INFO: stderr: ""
Feb  6 18:36:15.203: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 02/06/23 18:36:15.203
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Feb  6 18:36:15.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-5905 delete pods e2e-test-httpd-pod'
Feb  6 18:36:19.477: INFO: stderr: ""
Feb  6 18:36:19.477: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 18:36:19.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5905" for this suite. 02/06/23 18:36:19.482
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":261,"skipped":5129,"failed":0}
------------------------------
• [4.416 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:36:15.073
    Feb  6 18:36:15.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 18:36:15.075
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:15.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:15.097
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/06/23 18:36:15.1
    Feb  6 18:36:15.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-5905 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Feb  6 18:36:15.202: INFO: stderr: ""
    Feb  6 18:36:15.203: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 02/06/23 18:36:15.203
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Feb  6 18:36:15.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-5905 delete pods e2e-test-httpd-pod'
    Feb  6 18:36:19.477: INFO: stderr: ""
    Feb  6 18:36:19.477: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 18:36:19.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5905" for this suite. 02/06/23 18:36:19.482
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:36:19.489
Feb  6 18:36:19.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 18:36:19.491
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:19.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:19.513
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 02/06/23 18:36:19.516
Feb  6 18:36:19.516: INFO: Creating e2e-svc-a-fczhs
Feb  6 18:36:19.532: INFO: Creating e2e-svc-b-lhtsf
Feb  6 18:36:19.550: INFO: Creating e2e-svc-c-p6pxq
STEP: deleting service collection 02/06/23 18:36:19.576
Feb  6 18:36:19.658: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 18:36:19.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4711" for this suite. 02/06/23 18:36:19.665
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":262,"skipped":5133,"failed":0}
------------------------------
• [0.183 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:36:19.489
    Feb  6 18:36:19.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 18:36:19.491
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:19.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:19.513
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 02/06/23 18:36:19.516
    Feb  6 18:36:19.516: INFO: Creating e2e-svc-a-fczhs
    Feb  6 18:36:19.532: INFO: Creating e2e-svc-b-lhtsf
    Feb  6 18:36:19.550: INFO: Creating e2e-svc-c-p6pxq
    STEP: deleting service collection 02/06/23 18:36:19.576
    Feb  6 18:36:19.658: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 18:36:19.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4711" for this suite. 02/06/23 18:36:19.665
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:36:19.679
Feb  6 18:36:19.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename runtimeclass 02/06/23 18:36:19.681
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:19.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:19.706
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Feb  6 18:36:19.729: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8175 to be scheduled
Feb  6 18:36:19.736: INFO: 1 pods are not scheduled: [runtimeclass-8175/test-runtimeclass-runtimeclass-8175-preconfigured-handler-cm5g5(d4d37346-8397-4b94-8a9f-fc50f2791987)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb  6 18:36:21.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8175" for this suite. 02/06/23 18:36:21.753
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":263,"skipped":5135,"failed":0}
------------------------------
• [2.082 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:36:19.679
    Feb  6 18:36:19.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename runtimeclass 02/06/23 18:36:19.681
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:19.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:19.706
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Feb  6 18:36:19.729: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8175 to be scheduled
    Feb  6 18:36:19.736: INFO: 1 pods are not scheduled: [runtimeclass-8175/test-runtimeclass-runtimeclass-8175-preconfigured-handler-cm5g5(d4d37346-8397-4b94-8a9f-fc50f2791987)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb  6 18:36:21.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8175" for this suite. 02/06/23 18:36:21.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:36:21.764
Feb  6 18:36:21.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 18:36:21.765
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:21.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:21.779
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 02/06/23 18:36:21.783
Feb  6 18:36:21.791: INFO: Waiting up to 5m0s for pod "pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4" in namespace "emptydir-2451" to be "Succeeded or Failed"
Feb  6 18:36:21.795: INFO: Pod "pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075842ms
Feb  6 18:36:23.802: INFO: Pod "pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010301947s
Feb  6 18:36:25.800: INFO: Pod "pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008845291s
Feb  6 18:36:27.799: INFO: Pod "pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008169678s
STEP: Saw pod success 02/06/23 18:36:27.799
Feb  6 18:36:27.800: INFO: Pod "pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4" satisfied condition "Succeeded or Failed"
Feb  6 18:36:27.807: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4 container test-container: <nil>
STEP: delete the pod 02/06/23 18:36:27.814
Feb  6 18:36:27.830: INFO: Waiting for pod pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4 to disappear
Feb  6 18:36:27.835: INFO: Pod pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 18:36:27.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2451" for this suite. 02/06/23 18:36:27.842
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":264,"skipped":5152,"failed":0}
------------------------------
• [SLOW TEST] [6.086 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:36:21.764
    Feb  6 18:36:21.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 18:36:21.765
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:21.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:21.779
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/06/23 18:36:21.783
    Feb  6 18:36:21.791: INFO: Waiting up to 5m0s for pod "pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4" in namespace "emptydir-2451" to be "Succeeded or Failed"
    Feb  6 18:36:21.795: INFO: Pod "pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075842ms
    Feb  6 18:36:23.802: INFO: Pod "pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010301947s
    Feb  6 18:36:25.800: INFO: Pod "pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008845291s
    Feb  6 18:36:27.799: INFO: Pod "pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008169678s
    STEP: Saw pod success 02/06/23 18:36:27.799
    Feb  6 18:36:27.800: INFO: Pod "pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4" satisfied condition "Succeeded or Failed"
    Feb  6 18:36:27.807: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4 container test-container: <nil>
    STEP: delete the pod 02/06/23 18:36:27.814
    Feb  6 18:36:27.830: INFO: Waiting for pod pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4 to disappear
    Feb  6 18:36:27.835: INFO: Pod pod-6149b08b-dea4-445c-9f1e-fe90529dc3e4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 18:36:27.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2451" for this suite. 02/06/23 18:36:27.842
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:36:27.853
Feb  6 18:36:27.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 18:36:27.854
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:27.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:27.888
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 18:36:27.909
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:36:28.34
STEP: Deploying the webhook pod 02/06/23 18:36:28.35
STEP: Wait for the deployment to be ready 02/06/23 18:36:28.367
Feb  6 18:36:28.380: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 18:36:30.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 36, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 36, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 36, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 36, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 18:36:32.394
STEP: Verifying the service has paired with the endpoint 02/06/23 18:36:32.414
Feb  6 18:36:33.414: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 02/06/23 18:36:33.428
STEP: create a pod 02/06/23 18:36:33.454
Feb  6 18:36:33.467: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3723" to be "running"
Feb  6 18:36:33.475: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.480342ms
Feb  6 18:36:35.480: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013196052s
Feb  6 18:36:37.481: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014136265s
Feb  6 18:36:37.481: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 02/06/23 18:36:37.481
Feb  6 18:36:37.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=webhook-3723 attach --namespace=webhook-3723 to-be-attached-pod -i -c=container1'
Feb  6 18:36:37.596: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:36:37.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3723" for this suite. 02/06/23 18:36:37.609
STEP: Destroying namespace "webhook-3723-markers" for this suite. 02/06/23 18:36:37.615
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":265,"skipped":5154,"failed":0}
------------------------------
• [SLOW TEST] [9.847 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:36:27.853
    Feb  6 18:36:27.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 18:36:27.854
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:27.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:27.888
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 18:36:27.909
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:36:28.34
    STEP: Deploying the webhook pod 02/06/23 18:36:28.35
    STEP: Wait for the deployment to be ready 02/06/23 18:36:28.367
    Feb  6 18:36:28.380: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 18:36:30.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 36, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 36, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 36, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 36, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 18:36:32.394
    STEP: Verifying the service has paired with the endpoint 02/06/23 18:36:32.414
    Feb  6 18:36:33.414: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 02/06/23 18:36:33.428
    STEP: create a pod 02/06/23 18:36:33.454
    Feb  6 18:36:33.467: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3723" to be "running"
    Feb  6 18:36:33.475: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.480342ms
    Feb  6 18:36:35.480: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013196052s
    Feb  6 18:36:37.481: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014136265s
    Feb  6 18:36:37.481: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 02/06/23 18:36:37.481
    Feb  6 18:36:37.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=webhook-3723 attach --namespace=webhook-3723 to-be-attached-pod -i -c=container1'
    Feb  6 18:36:37.596: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:36:37.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3723" for this suite. 02/06/23 18:36:37.609
    STEP: Destroying namespace "webhook-3723-markers" for this suite. 02/06/23 18:36:37.615
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:36:37.704
Feb  6 18:36:37.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 18:36:37.706
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:37.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:37.734
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-4633 02/06/23 18:36:37.739
STEP: creating service affinity-nodeport in namespace services-4633 02/06/23 18:36:37.74
STEP: creating replication controller affinity-nodeport in namespace services-4633 02/06/23 18:36:37.764
I0206 18:36:37.777161      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4633, replica count: 3
I0206 18:36:40.828385      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 18:36:43.830130      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  6 18:36:43.841: INFO: Creating new exec pod
Feb  6 18:36:43.846: INFO: Waiting up to 5m0s for pod "execpod-affinity6g22h" in namespace "services-4633" to be "running"
Feb  6 18:36:43.852: INFO: Pod "execpod-affinity6g22h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.956982ms
Feb  6 18:36:45.857: INFO: Pod "execpod-affinity6g22h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010835868s
Feb  6 18:36:47.857: INFO: Pod "execpod-affinity6g22h": Phase="Running", Reason="", readiness=true. Elapsed: 4.010721649s
Feb  6 18:36:47.857: INFO: Pod "execpod-affinity6g22h" satisfied condition "running"
Feb  6 18:36:48.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-4633 exec execpod-affinity6g22h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Feb  6 18:36:49.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb  6 18:36:49.034: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:36:49.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-4633 exec execpod-affinity6g22h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.159.201 80'
Feb  6 18:36:49.201: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.159.201 80\nConnection to 10.106.159.201 80 port [tcp/http] succeeded!\n"
Feb  6 18:36:49.201: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:36:49.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-4633 exec execpod-affinity6g22h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.4.133 32121'
Feb  6 18:36:49.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.4.133 32121\nConnection to 195.17.4.133 32121 port [tcp/*] succeeded!\n"
Feb  6 18:36:49.366: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:36:49.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-4633 exec execpod-affinity6g22h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.89 32121'
Feb  6 18:36:49.533: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.148.89 32121\nConnection to 195.17.148.89 32121 port [tcp/*] succeeded!\n"
Feb  6 18:36:49.533: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:36:49.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-4633 exec execpod-affinity6g22h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://195.17.4.133:32121/ ; done'
Feb  6 18:36:49.802: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n"
Feb  6 18:36:49.802: INFO: stdout: "\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf"
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
Feb  6 18:36:49.802: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4633, will wait for the garbage collector to delete the pods 02/06/23 18:36:49.825
Feb  6 18:36:49.888: INFO: Deleting ReplicationController affinity-nodeport took: 6.91413ms
Feb  6 18:36:49.988: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.112019ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 18:36:52.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4633" for this suite. 02/06/23 18:36:52.45
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":266,"skipped":5182,"failed":0}
------------------------------
• [SLOW TEST] [14.752 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:36:37.704
    Feb  6 18:36:37.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 18:36:37.706
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:37.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:37.734
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-4633 02/06/23 18:36:37.739
    STEP: creating service affinity-nodeport in namespace services-4633 02/06/23 18:36:37.74
    STEP: creating replication controller affinity-nodeport in namespace services-4633 02/06/23 18:36:37.764
    I0206 18:36:37.777161      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4633, replica count: 3
    I0206 18:36:40.828385      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 18:36:43.830130      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb  6 18:36:43.841: INFO: Creating new exec pod
    Feb  6 18:36:43.846: INFO: Waiting up to 5m0s for pod "execpod-affinity6g22h" in namespace "services-4633" to be "running"
    Feb  6 18:36:43.852: INFO: Pod "execpod-affinity6g22h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.956982ms
    Feb  6 18:36:45.857: INFO: Pod "execpod-affinity6g22h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010835868s
    Feb  6 18:36:47.857: INFO: Pod "execpod-affinity6g22h": Phase="Running", Reason="", readiness=true. Elapsed: 4.010721649s
    Feb  6 18:36:47.857: INFO: Pod "execpod-affinity6g22h" satisfied condition "running"
    Feb  6 18:36:48.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-4633 exec execpod-affinity6g22h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Feb  6 18:36:49.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Feb  6 18:36:49.034: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:36:49.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-4633 exec execpod-affinity6g22h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.159.201 80'
    Feb  6 18:36:49.201: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.159.201 80\nConnection to 10.106.159.201 80 port [tcp/http] succeeded!\n"
    Feb  6 18:36:49.201: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:36:49.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-4633 exec execpod-affinity6g22h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.4.133 32121'
    Feb  6 18:36:49.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.4.133 32121\nConnection to 195.17.4.133 32121 port [tcp/*] succeeded!\n"
    Feb  6 18:36:49.366: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:36:49.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-4633 exec execpod-affinity6g22h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.89 32121'
    Feb  6 18:36:49.533: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.148.89 32121\nConnection to 195.17.148.89 32121 port [tcp/*] succeeded!\n"
    Feb  6 18:36:49.533: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:36:49.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-4633 exec execpod-affinity6g22h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://195.17.4.133:32121/ ; done'
    Feb  6 18:36:49.802: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n+ echo\n+ curl -q -s --connect-timeout 2 http://195.17.4.133:32121/\n"
    Feb  6 18:36:49.802: INFO: stdout: "\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf\naffinity-nodeport-hh7nf"
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Received response from host: affinity-nodeport-hh7nf
    Feb  6 18:36:49.802: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-4633, will wait for the garbage collector to delete the pods 02/06/23 18:36:49.825
    Feb  6 18:36:49.888: INFO: Deleting ReplicationController affinity-nodeport took: 6.91413ms
    Feb  6 18:36:49.988: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.112019ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 18:36:52.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4633" for this suite. 02/06/23 18:36:52.45
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:36:52.46
Feb  6 18:36:52.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 18:36:52.461
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:52.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:52.481
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-1c8a6630-ff53-49ce-8a33-d6d976b2554e 02/06/23 18:36:52.509
STEP: Creating a pod to test consume secrets 02/06/23 18:36:52.514
Feb  6 18:36:52.523: INFO: Waiting up to 5m0s for pod "pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083" in namespace "secrets-7507" to be "Succeeded or Failed"
Feb  6 18:36:52.533: INFO: Pod "pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083": Phase="Pending", Reason="", readiness=false. Elapsed: 9.480864ms
Feb  6 18:36:54.539: INFO: Pod "pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015548706s
Feb  6 18:36:56.541: INFO: Pod "pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018082096s
Feb  6 18:36:58.539: INFO: Pod "pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015372707s
STEP: Saw pod success 02/06/23 18:36:58.539
Feb  6 18:36:58.539: INFO: Pod "pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083" satisfied condition "Succeeded or Failed"
Feb  6 18:36:58.543: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083 container secret-volume-test: <nil>
STEP: delete the pod 02/06/23 18:36:58.554
Feb  6 18:36:58.568: INFO: Waiting for pod pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083 to disappear
Feb  6 18:36:58.571: INFO: Pod pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb  6 18:36:58.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7507" for this suite. 02/06/23 18:36:58.576
STEP: Destroying namespace "secret-namespace-2271" for this suite. 02/06/23 18:36:58.582
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":267,"skipped":5208,"failed":0}
------------------------------
• [SLOW TEST] [6.129 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:36:52.46
    Feb  6 18:36:52.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 18:36:52.461
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:52.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:52.481
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-1c8a6630-ff53-49ce-8a33-d6d976b2554e 02/06/23 18:36:52.509
    STEP: Creating a pod to test consume secrets 02/06/23 18:36:52.514
    Feb  6 18:36:52.523: INFO: Waiting up to 5m0s for pod "pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083" in namespace "secrets-7507" to be "Succeeded or Failed"
    Feb  6 18:36:52.533: INFO: Pod "pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083": Phase="Pending", Reason="", readiness=false. Elapsed: 9.480864ms
    Feb  6 18:36:54.539: INFO: Pod "pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015548706s
    Feb  6 18:36:56.541: INFO: Pod "pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018082096s
    Feb  6 18:36:58.539: INFO: Pod "pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015372707s
    STEP: Saw pod success 02/06/23 18:36:58.539
    Feb  6 18:36:58.539: INFO: Pod "pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083" satisfied condition "Succeeded or Failed"
    Feb  6 18:36:58.543: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083 container secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 18:36:58.554
    Feb  6 18:36:58.568: INFO: Waiting for pod pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083 to disappear
    Feb  6 18:36:58.571: INFO: Pod pod-secrets-f0447875-6ddf-4093-831c-eb85bd11a083 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 18:36:58.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7507" for this suite. 02/06/23 18:36:58.576
    STEP: Destroying namespace "secret-namespace-2271" for this suite. 02/06/23 18:36:58.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:36:58.59
Feb  6 18:36:58.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 18:36:58.591
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:58.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:58.61
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-4eb46fbb-106d-40fd-ab37-c74190931ca8 02/06/23 18:36:58.613
STEP: Creating a pod to test consume configMaps 02/06/23 18:36:58.618
Feb  6 18:36:58.627: INFO: Waiting up to 5m0s for pod "pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe" in namespace "configmap-2046" to be "Succeeded or Failed"
Feb  6 18:36:58.636: INFO: Pod "pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.295283ms
Feb  6 18:37:00.640: INFO: Pod "pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012975133s
Feb  6 18:37:02.644: INFO: Pod "pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016472273s
Feb  6 18:37:04.642: INFO: Pod "pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014526661s
STEP: Saw pod success 02/06/23 18:37:04.642
Feb  6 18:37:04.642: INFO: Pod "pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe" satisfied condition "Succeeded or Failed"
Feb  6 18:37:04.645: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:37:04.654
Feb  6 18:37:04.668: INFO: Waiting for pod pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe to disappear
Feb  6 18:37:04.671: INFO: Pod pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 18:37:04.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2046" for this suite. 02/06/23 18:37:04.676
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":268,"skipped":5226,"failed":0}
------------------------------
• [SLOW TEST] [6.095 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:36:58.59
    Feb  6 18:36:58.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 18:36:58.591
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:36:58.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:36:58.61
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-4eb46fbb-106d-40fd-ab37-c74190931ca8 02/06/23 18:36:58.613
    STEP: Creating a pod to test consume configMaps 02/06/23 18:36:58.618
    Feb  6 18:36:58.627: INFO: Waiting up to 5m0s for pod "pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe" in namespace "configmap-2046" to be "Succeeded or Failed"
    Feb  6 18:36:58.636: INFO: Pod "pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.295283ms
    Feb  6 18:37:00.640: INFO: Pod "pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012975133s
    Feb  6 18:37:02.644: INFO: Pod "pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016472273s
    Feb  6 18:37:04.642: INFO: Pod "pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014526661s
    STEP: Saw pod success 02/06/23 18:37:04.642
    Feb  6 18:37:04.642: INFO: Pod "pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe" satisfied condition "Succeeded or Failed"
    Feb  6 18:37:04.645: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:37:04.654
    Feb  6 18:37:04.668: INFO: Waiting for pod pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe to disappear
    Feb  6 18:37:04.671: INFO: Pod pod-configmaps-ff3bcba9-6da5-458b-8c02-9b07b53165fe no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 18:37:04.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2046" for this suite. 02/06/23 18:37:04.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:37:04.686
Feb  6 18:37:04.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubelet-test 02/06/23 18:37:04.687
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:37:04.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:37:04.705
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Feb  6 18:37:04.717: INFO: Waiting up to 5m0s for pod "busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd" in namespace "kubelet-test-8462" to be "running and ready"
Feb  6 18:37:04.720: INFO: Pod "busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.906738ms
Feb  6 18:37:04.720: INFO: The phase of Pod busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:37:06.726: INFO: Pod "busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009235698s
Feb  6 18:37:06.727: INFO: The phase of Pod busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:37:08.726: INFO: Pod "busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd": Phase="Running", Reason="", readiness=true. Elapsed: 4.008987968s
Feb  6 18:37:08.726: INFO: The phase of Pod busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd is Running (Ready = true)
Feb  6 18:37:08.727: INFO: Pod "busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb  6 18:37:08.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8462" for this suite. 02/06/23 18:37:08.745
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":269,"skipped":5233,"failed":0}
------------------------------
• [4.067 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:37:04.686
    Feb  6 18:37:04.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubelet-test 02/06/23 18:37:04.687
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:37:04.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:37:04.705
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Feb  6 18:37:04.717: INFO: Waiting up to 5m0s for pod "busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd" in namespace "kubelet-test-8462" to be "running and ready"
    Feb  6 18:37:04.720: INFO: Pod "busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.906738ms
    Feb  6 18:37:04.720: INFO: The phase of Pod busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:37:06.726: INFO: Pod "busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009235698s
    Feb  6 18:37:06.727: INFO: The phase of Pod busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:37:08.726: INFO: Pod "busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd": Phase="Running", Reason="", readiness=true. Elapsed: 4.008987968s
    Feb  6 18:37:08.726: INFO: The phase of Pod busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd is Running (Ready = true)
    Feb  6 18:37:08.727: INFO: Pod "busybox-scheduling-86659589-3e64-4f19-8666-48f3961d8fbd" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb  6 18:37:08.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8462" for this suite. 02/06/23 18:37:08.745
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:37:08.753
Feb  6 18:37:08.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename security-context-test 02/06/23 18:37:08.754
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:37:08.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:37:08.775
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Feb  6 18:37:08.788: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca" in namespace "security-context-test-7338" to be "Succeeded or Failed"
Feb  6 18:37:08.791: INFO: Pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.078734ms
Feb  6 18:37:10.807: INFO: Pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019507309s
Feb  6 18:37:12.795: INFO: Pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007210659s
Feb  6 18:37:14.797: INFO: Pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009445843s
Feb  6 18:37:16.799: INFO: Pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.010591929s
Feb  6 18:37:16.799: INFO: Pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb  6 18:37:16.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7338" for this suite. 02/06/23 18:37:16.811
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":270,"skipped":5233,"failed":0}
------------------------------
• [SLOW TEST] [8.065 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:37:08.753
    Feb  6 18:37:08.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename security-context-test 02/06/23 18:37:08.754
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:37:08.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:37:08.775
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Feb  6 18:37:08.788: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca" in namespace "security-context-test-7338" to be "Succeeded or Failed"
    Feb  6 18:37:08.791: INFO: Pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.078734ms
    Feb  6 18:37:10.807: INFO: Pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019507309s
    Feb  6 18:37:12.795: INFO: Pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007210659s
    Feb  6 18:37:14.797: INFO: Pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009445843s
    Feb  6 18:37:16.799: INFO: Pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.010591929s
    Feb  6 18:37:16.799: INFO: Pod "alpine-nnp-false-2b5839b7-8105-4538-a1e6-57959171e5ca" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb  6 18:37:16.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7338" for this suite. 02/06/23 18:37:16.811
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:37:16.82
Feb  6 18:37:16.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:37:16.821
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:37:16.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:37:16.845
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-5f671352-ab98-4e30-ac44-8ddd0778f417 02/06/23 18:37:16.849
STEP: Creating secret with name secret-projected-all-test-volume-972697e8-9626-439d-8bd1-c0cd7eec3630 02/06/23 18:37:16.854
STEP: Creating a pod to test Check all projections for projected volume plugin 02/06/23 18:37:16.86
Feb  6 18:37:16.869: INFO: Waiting up to 5m0s for pod "projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597" in namespace "projected-6642" to be "Succeeded or Failed"
Feb  6 18:37:16.875: INFO: Pod "projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597": Phase="Pending", Reason="", readiness=false. Elapsed: 6.595739ms
Feb  6 18:37:18.881: INFO: Pod "projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011789494s
Feb  6 18:37:20.879: INFO: Pod "projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010608448s
Feb  6 18:37:22.882: INFO: Pod "projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013249864s
STEP: Saw pod success 02/06/23 18:37:22.882
Feb  6 18:37:22.882: INFO: Pod "projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597" satisfied condition "Succeeded or Failed"
Feb  6 18:37:22.885: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597 container projected-all-volume-test: <nil>
STEP: delete the pod 02/06/23 18:37:22.894
Feb  6 18:37:22.907: INFO: Waiting for pod projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597 to disappear
Feb  6 18:37:22.910: INFO: Pod projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Feb  6 18:37:22.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6642" for this suite. 02/06/23 18:37:22.915
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":271,"skipped":5234,"failed":0}
------------------------------
• [SLOW TEST] [6.102 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:37:16.82
    Feb  6 18:37:16.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:37:16.821
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:37:16.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:37:16.845
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-5f671352-ab98-4e30-ac44-8ddd0778f417 02/06/23 18:37:16.849
    STEP: Creating secret with name secret-projected-all-test-volume-972697e8-9626-439d-8bd1-c0cd7eec3630 02/06/23 18:37:16.854
    STEP: Creating a pod to test Check all projections for projected volume plugin 02/06/23 18:37:16.86
    Feb  6 18:37:16.869: INFO: Waiting up to 5m0s for pod "projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597" in namespace "projected-6642" to be "Succeeded or Failed"
    Feb  6 18:37:16.875: INFO: Pod "projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597": Phase="Pending", Reason="", readiness=false. Elapsed: 6.595739ms
    Feb  6 18:37:18.881: INFO: Pod "projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011789494s
    Feb  6 18:37:20.879: INFO: Pod "projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010608448s
    Feb  6 18:37:22.882: INFO: Pod "projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013249864s
    STEP: Saw pod success 02/06/23 18:37:22.882
    Feb  6 18:37:22.882: INFO: Pod "projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597" satisfied condition "Succeeded or Failed"
    Feb  6 18:37:22.885: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597 container projected-all-volume-test: <nil>
    STEP: delete the pod 02/06/23 18:37:22.894
    Feb  6 18:37:22.907: INFO: Waiting for pod projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597 to disappear
    Feb  6 18:37:22.910: INFO: Pod projected-volume-8df89963-4f3c-4e34-8c2d-e4f2e2d5e597 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Feb  6 18:37:22.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6642" for this suite. 02/06/23 18:37:22.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:37:22.923
Feb  6 18:37:22.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename statefulset 02/06/23 18:37:22.924
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:37:22.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:37:22.945
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6158 02/06/23 18:37:22.948
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 02/06/23 18:37:22.954
STEP: Creating pod with conflicting port in namespace statefulset-6158 02/06/23 18:37:22.966
STEP: Waiting until pod test-pod will start running in namespace statefulset-6158 02/06/23 18:37:22.976
Feb  6 18:37:22.976: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6158" to be "running"
Feb  6 18:37:22.980: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076499ms
Feb  6 18:37:24.985: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009286558s
Feb  6 18:37:26.984: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008141832s
Feb  6 18:37:26.984: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-6158 02/06/23 18:37:26.984
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6158 02/06/23 18:37:26.993
Feb  6 18:37:27.006: INFO: Observed stateful pod in namespace: statefulset-6158, name: ss-0, uid: 8987f956-87de-4db5-a85f-0c59902e6ab0, status phase: Pending. Waiting for statefulset controller to delete.
Feb  6 18:37:27.028: INFO: Observed stateful pod in namespace: statefulset-6158, name: ss-0, uid: 8987f956-87de-4db5-a85f-0c59902e6ab0, status phase: Failed. Waiting for statefulset controller to delete.
Feb  6 18:37:27.044: INFO: Observed stateful pod in namespace: statefulset-6158, name: ss-0, uid: 8987f956-87de-4db5-a85f-0c59902e6ab0, status phase: Failed. Waiting for statefulset controller to delete.
Feb  6 18:37:27.049: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6158
STEP: Removing pod with conflicting port in namespace statefulset-6158 02/06/23 18:37:27.05
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6158 and will be in running state 02/06/23 18:37:27.074
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb  6 18:37:31.093: INFO: Deleting all statefulset in ns statefulset-6158
Feb  6 18:37:31.096: INFO: Scaling statefulset ss to 0
Feb  6 18:37:41.114: INFO: Waiting for statefulset status.replicas updated to 0
Feb  6 18:37:41.117: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb  6 18:37:41.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6158" for this suite. 02/06/23 18:37:41.136
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":272,"skipped":5241,"failed":0}
------------------------------
• [SLOW TEST] [18.219 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:37:22.923
    Feb  6 18:37:22.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename statefulset 02/06/23 18:37:22.924
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:37:22.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:37:22.945
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6158 02/06/23 18:37:22.948
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 02/06/23 18:37:22.954
    STEP: Creating pod with conflicting port in namespace statefulset-6158 02/06/23 18:37:22.966
    STEP: Waiting until pod test-pod will start running in namespace statefulset-6158 02/06/23 18:37:22.976
    Feb  6 18:37:22.976: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6158" to be "running"
    Feb  6 18:37:22.980: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076499ms
    Feb  6 18:37:24.985: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009286558s
    Feb  6 18:37:26.984: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008141832s
    Feb  6 18:37:26.984: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-6158 02/06/23 18:37:26.984
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6158 02/06/23 18:37:26.993
    Feb  6 18:37:27.006: INFO: Observed stateful pod in namespace: statefulset-6158, name: ss-0, uid: 8987f956-87de-4db5-a85f-0c59902e6ab0, status phase: Pending. Waiting for statefulset controller to delete.
    Feb  6 18:37:27.028: INFO: Observed stateful pod in namespace: statefulset-6158, name: ss-0, uid: 8987f956-87de-4db5-a85f-0c59902e6ab0, status phase: Failed. Waiting for statefulset controller to delete.
    Feb  6 18:37:27.044: INFO: Observed stateful pod in namespace: statefulset-6158, name: ss-0, uid: 8987f956-87de-4db5-a85f-0c59902e6ab0, status phase: Failed. Waiting for statefulset controller to delete.
    Feb  6 18:37:27.049: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6158
    STEP: Removing pod with conflicting port in namespace statefulset-6158 02/06/23 18:37:27.05
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6158 and will be in running state 02/06/23 18:37:27.074
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb  6 18:37:31.093: INFO: Deleting all statefulset in ns statefulset-6158
    Feb  6 18:37:31.096: INFO: Scaling statefulset ss to 0
    Feb  6 18:37:41.114: INFO: Waiting for statefulset status.replicas updated to 0
    Feb  6 18:37:41.117: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb  6 18:37:41.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6158" for this suite. 02/06/23 18:37:41.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:37:41.144
Feb  6 18:37:41.144: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename gc 02/06/23 18:37:41.145
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:37:41.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:37:41.166
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 02/06/23 18:37:41.173
STEP: delete the rc 02/06/23 18:37:46.37
STEP: wait for the rc to be deleted 02/06/23 18:37:46.537
Feb  6 18:37:48.319: INFO: 81 pods remaining
Feb  6 18:37:49.046: INFO: 80 pods has nil DeletionTimestamp
Feb  6 18:37:49.047: INFO: 
Feb  6 18:37:49.747: INFO: 65 pods remaining
Feb  6 18:37:49.747: INFO: 60 pods has nil DeletionTimestamp
Feb  6 18:37:49.785: INFO: 
Feb  6 18:37:50.760: INFO: 51 pods remaining
Feb  6 18:37:50.760: INFO: 40 pods has nil DeletionTimestamp
Feb  6 18:37:50.763: INFO: 
Feb  6 18:37:52.096: INFO: 33 pods remaining
Feb  6 18:37:52.096: INFO: 20 pods has nil DeletionTimestamp
Feb  6 18:37:52.096: INFO: 
Feb  6 18:37:53.055: INFO: 26 pods remaining
Feb  6 18:37:53.055: INFO: 10 pods has nil DeletionTimestamp
Feb  6 18:37:53.055: INFO: 
Feb  6 18:37:53.608: INFO: 17 pods remaining
Feb  6 18:37:53.608: INFO: 0 pods has nil DeletionTimestamp
Feb  6 18:37:53.608: INFO: 
Feb  6 18:37:54.868: INFO: 8 pods remaining
Feb  6 18:37:54.891: INFO: 0 pods has nil DeletionTimestamp
Feb  6 18:37:54.899: INFO: 
Feb  6 18:37:55.736: INFO: 8 pods remaining
Feb  6 18:37:55.737: INFO: 0 pods has nil DeletionTimestamp
Feb  6 18:37:55.737: INFO: 
STEP: Gathering metrics 02/06/23 18:37:56.603
Feb  6 18:37:56.633: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tneyla25-pnn8z" in namespace "kube-system" to be "running and ready"
Feb  6 18:37:56.640: INFO: Pod "kube-controller-manager-tneyla25-pnn8z": Phase="Running", Reason="", readiness=true. Elapsed: 6.530052ms
Feb  6 18:37:56.640: INFO: The phase of Pod kube-controller-manager-tneyla25-pnn8z is Running (Ready = true)
Feb  6 18:37:56.640: INFO: Pod "kube-controller-manager-tneyla25-pnn8z" satisfied condition "running and ready"
Feb  6 18:37:56.784: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb  6 18:37:56.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4455" for this suite. 02/06/23 18:37:56.797
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":273,"skipped":5249,"failed":0}
------------------------------
• [SLOW TEST] [15.668 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:37:41.144
    Feb  6 18:37:41.144: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename gc 02/06/23 18:37:41.145
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:37:41.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:37:41.166
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 02/06/23 18:37:41.173
    STEP: delete the rc 02/06/23 18:37:46.37
    STEP: wait for the rc to be deleted 02/06/23 18:37:46.537
    Feb  6 18:37:48.319: INFO: 81 pods remaining
    Feb  6 18:37:49.046: INFO: 80 pods has nil DeletionTimestamp
    Feb  6 18:37:49.047: INFO: 
    Feb  6 18:37:49.747: INFO: 65 pods remaining
    Feb  6 18:37:49.747: INFO: 60 pods has nil DeletionTimestamp
    Feb  6 18:37:49.785: INFO: 
    Feb  6 18:37:50.760: INFO: 51 pods remaining
    Feb  6 18:37:50.760: INFO: 40 pods has nil DeletionTimestamp
    Feb  6 18:37:50.763: INFO: 
    Feb  6 18:37:52.096: INFO: 33 pods remaining
    Feb  6 18:37:52.096: INFO: 20 pods has nil DeletionTimestamp
    Feb  6 18:37:52.096: INFO: 
    Feb  6 18:37:53.055: INFO: 26 pods remaining
    Feb  6 18:37:53.055: INFO: 10 pods has nil DeletionTimestamp
    Feb  6 18:37:53.055: INFO: 
    Feb  6 18:37:53.608: INFO: 17 pods remaining
    Feb  6 18:37:53.608: INFO: 0 pods has nil DeletionTimestamp
    Feb  6 18:37:53.608: INFO: 
    Feb  6 18:37:54.868: INFO: 8 pods remaining
    Feb  6 18:37:54.891: INFO: 0 pods has nil DeletionTimestamp
    Feb  6 18:37:54.899: INFO: 
    Feb  6 18:37:55.736: INFO: 8 pods remaining
    Feb  6 18:37:55.737: INFO: 0 pods has nil DeletionTimestamp
    Feb  6 18:37:55.737: INFO: 
    STEP: Gathering metrics 02/06/23 18:37:56.603
    Feb  6 18:37:56.633: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tneyla25-pnn8z" in namespace "kube-system" to be "running and ready"
    Feb  6 18:37:56.640: INFO: Pod "kube-controller-manager-tneyla25-pnn8z": Phase="Running", Reason="", readiness=true. Elapsed: 6.530052ms
    Feb  6 18:37:56.640: INFO: The phase of Pod kube-controller-manager-tneyla25-pnn8z is Running (Ready = true)
    Feb  6 18:37:56.640: INFO: Pod "kube-controller-manager-tneyla25-pnn8z" satisfied condition "running and ready"
    Feb  6 18:37:56.784: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb  6 18:37:56.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4455" for this suite. 02/06/23 18:37:56.797
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:37:56.812
Feb  6 18:37:56.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename disruption 02/06/23 18:37:56.813
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:37:56.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:37:56.87
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 02/06/23 18:37:56.879
STEP: Waiting for all pods to be running 02/06/23 18:37:58.923
Feb  6 18:37:58.934: INFO: running pods: 0 < 3
Feb  6 18:38:00.938: INFO: running pods: 0 < 3
Feb  6 18:38:02.939: INFO: running pods: 0 < 3
Feb  6 18:38:04.940: INFO: running pods: 0 < 3
Feb  6 18:38:06.941: INFO: running pods: 0 < 3
Feb  6 18:38:08.939: INFO: running pods: 0 < 3
Feb  6 18:38:10.938: INFO: running pods: 1 < 3
Feb  6 18:38:12.938: INFO: running pods: 1 < 3
Feb  6 18:38:14.939: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb  6 18:38:16.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1162" for this suite. 02/06/23 18:38:16.945
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":274,"skipped":5249,"failed":0}
------------------------------
• [SLOW TEST] [20.141 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:37:56.812
    Feb  6 18:37:56.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename disruption 02/06/23 18:37:56.813
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:37:56.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:37:56.87
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 02/06/23 18:37:56.879
    STEP: Waiting for all pods to be running 02/06/23 18:37:58.923
    Feb  6 18:37:58.934: INFO: running pods: 0 < 3
    Feb  6 18:38:00.938: INFO: running pods: 0 < 3
    Feb  6 18:38:02.939: INFO: running pods: 0 < 3
    Feb  6 18:38:04.940: INFO: running pods: 0 < 3
    Feb  6 18:38:06.941: INFO: running pods: 0 < 3
    Feb  6 18:38:08.939: INFO: running pods: 0 < 3
    Feb  6 18:38:10.938: INFO: running pods: 1 < 3
    Feb  6 18:38:12.938: INFO: running pods: 1 < 3
    Feb  6 18:38:14.939: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb  6 18:38:16.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1162" for this suite. 02/06/23 18:38:16.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:38:16.955
Feb  6 18:38:16.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename replicaset 02/06/23 18:38:16.956
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:38:16.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:38:16.974
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/06/23 18:38:16.978
Feb  6 18:38:16.989: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb  6 18:38:21.994: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/06/23 18:38:21.994
STEP: getting scale subresource 02/06/23 18:38:21.994
STEP: updating a scale subresource 02/06/23 18:38:21.998
STEP: verifying the replicaset Spec.Replicas was modified 02/06/23 18:38:22.004
STEP: Patch a scale subresource 02/06/23 18:38:22.009
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb  6 18:38:22.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4295" for this suite. 02/06/23 18:38:22.049
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":275,"skipped":5255,"failed":0}
------------------------------
• [SLOW TEST] [5.112 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:38:16.955
    Feb  6 18:38:16.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename replicaset 02/06/23 18:38:16.956
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:38:16.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:38:16.974
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/06/23 18:38:16.978
    Feb  6 18:38:16.989: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb  6 18:38:21.994: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/06/23 18:38:21.994
    STEP: getting scale subresource 02/06/23 18:38:21.994
    STEP: updating a scale subresource 02/06/23 18:38:21.998
    STEP: verifying the replicaset Spec.Replicas was modified 02/06/23 18:38:22.004
    STEP: Patch a scale subresource 02/06/23 18:38:22.009
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb  6 18:38:22.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4295" for this suite. 02/06/23 18:38:22.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:38:22.073
Feb  6 18:38:22.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename gc 02/06/23 18:38:22.075
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:38:22.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:38:22.095
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 02/06/23 18:38:22.104
STEP: delete the rc 02/06/23 18:38:27.546
STEP: wait for the rc to be deleted 02/06/23 18:38:27.594
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/06/23 18:38:32.652
STEP: Gathering metrics 02/06/23 18:39:02.758
Feb  6 18:39:03.288: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tneyla25-pnn8z" in namespace "kube-system" to be "running and ready"
Feb  6 18:39:03.292: INFO: Pod "kube-controller-manager-tneyla25-pnn8z": Phase="Running", Reason="", readiness=true. Elapsed: 3.704415ms
Feb  6 18:39:03.292: INFO: The phase of Pod kube-controller-manager-tneyla25-pnn8z is Running (Ready = true)
Feb  6 18:39:03.292: INFO: Pod "kube-controller-manager-tneyla25-pnn8z" satisfied condition "running and ready"
Feb  6 18:39:03.440: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb  6 18:39:03.440: INFO: Deleting pod "simpletest.rc-25m6p" in namespace "gc-7509"
Feb  6 18:39:03.462: INFO: Deleting pod "simpletest.rc-2f8dx" in namespace "gc-7509"
Feb  6 18:39:03.485: INFO: Deleting pod "simpletest.rc-42bdg" in namespace "gc-7509"
Feb  6 18:39:03.502: INFO: Deleting pod "simpletest.rc-42q7w" in namespace "gc-7509"
Feb  6 18:39:03.527: INFO: Deleting pod "simpletest.rc-4swtn" in namespace "gc-7509"
Feb  6 18:39:03.544: INFO: Deleting pod "simpletest.rc-4v7vl" in namespace "gc-7509"
Feb  6 18:39:03.568: INFO: Deleting pod "simpletest.rc-4xmzp" in namespace "gc-7509"
Feb  6 18:39:03.587: INFO: Deleting pod "simpletest.rc-56rcg" in namespace "gc-7509"
Feb  6 18:39:03.613: INFO: Deleting pod "simpletest.rc-5f7qp" in namespace "gc-7509"
Feb  6 18:39:03.640: INFO: Deleting pod "simpletest.rc-5sjv8" in namespace "gc-7509"
Feb  6 18:39:03.654: INFO: Deleting pod "simpletest.rc-6g5mk" in namespace "gc-7509"
Feb  6 18:39:03.679: INFO: Deleting pod "simpletest.rc-6jwv5" in namespace "gc-7509"
Feb  6 18:39:03.693: INFO: Deleting pod "simpletest.rc-6pnw7" in namespace "gc-7509"
Feb  6 18:39:03.827: INFO: Deleting pod "simpletest.rc-6znxp" in namespace "gc-7509"
Feb  6 18:39:03.865: INFO: Deleting pod "simpletest.rc-772c4" in namespace "gc-7509"
Feb  6 18:39:03.913: INFO: Deleting pod "simpletest.rc-77b6v" in namespace "gc-7509"
Feb  6 18:39:04.019: INFO: Deleting pod "simpletest.rc-79l2n" in namespace "gc-7509"
Feb  6 18:39:04.046: INFO: Deleting pod "simpletest.rc-7ppkn" in namespace "gc-7509"
Feb  6 18:39:04.291: INFO: Deleting pod "simpletest.rc-7sgbb" in namespace "gc-7509"
Feb  6 18:39:04.329: INFO: Deleting pod "simpletest.rc-8nw2k" in namespace "gc-7509"
Feb  6 18:39:04.396: INFO: Deleting pod "simpletest.rc-8zrrp" in namespace "gc-7509"
Feb  6 18:39:04.613: INFO: Deleting pod "simpletest.rc-9778z" in namespace "gc-7509"
Feb  6 18:39:04.634: INFO: Deleting pod "simpletest.rc-99vt8" in namespace "gc-7509"
Feb  6 18:39:04.683: INFO: Deleting pod "simpletest.rc-9bfss" in namespace "gc-7509"
Feb  6 18:39:04.759: INFO: Deleting pod "simpletest.rc-9jwlq" in namespace "gc-7509"
Feb  6 18:39:04.786: INFO: Deleting pod "simpletest.rc-9xn24" in namespace "gc-7509"
Feb  6 18:39:04.850: INFO: Deleting pod "simpletest.rc-b5hqq" in namespace "gc-7509"
Feb  6 18:39:04.949: INFO: Deleting pod "simpletest.rc-bhmwh" in namespace "gc-7509"
Feb  6 18:39:05.311: INFO: Deleting pod "simpletest.rc-bkvx6" in namespace "gc-7509"
Feb  6 18:39:05.336: INFO: Deleting pod "simpletest.rc-btsps" in namespace "gc-7509"
Feb  6 18:39:05.640: INFO: Deleting pod "simpletest.rc-bz7ph" in namespace "gc-7509"
Feb  6 18:39:05.678: INFO: Deleting pod "simpletest.rc-cxt52" in namespace "gc-7509"
Feb  6 18:39:05.710: INFO: Deleting pod "simpletest.rc-d7f25" in namespace "gc-7509"
Feb  6 18:39:05.731: INFO: Deleting pod "simpletest.rc-d84bk" in namespace "gc-7509"
Feb  6 18:39:05.765: INFO: Deleting pod "simpletest.rc-dfp5s" in namespace "gc-7509"
Feb  6 18:39:05.809: INFO: Deleting pod "simpletest.rc-dnxsc" in namespace "gc-7509"
Feb  6 18:39:05.829: INFO: Deleting pod "simpletest.rc-dqjc4" in namespace "gc-7509"
Feb  6 18:39:05.849: INFO: Deleting pod "simpletest.rc-dsnrd" in namespace "gc-7509"
Feb  6 18:39:05.868: INFO: Deleting pod "simpletest.rc-f7phr" in namespace "gc-7509"
Feb  6 18:39:05.889: INFO: Deleting pod "simpletest.rc-fbvck" in namespace "gc-7509"
Feb  6 18:39:05.912: INFO: Deleting pod "simpletest.rc-fgb76" in namespace "gc-7509"
Feb  6 18:39:05.944: INFO: Deleting pod "simpletest.rc-fgztg" in namespace "gc-7509"
Feb  6 18:39:05.973: INFO: Deleting pod "simpletest.rc-fswrj" in namespace "gc-7509"
Feb  6 18:39:06.004: INFO: Deleting pod "simpletest.rc-gc7dm" in namespace "gc-7509"
Feb  6 18:39:06.027: INFO: Deleting pod "simpletest.rc-gdb7l" in namespace "gc-7509"
Feb  6 18:39:06.047: INFO: Deleting pod "simpletest.rc-gppwh" in namespace "gc-7509"
Feb  6 18:39:06.067: INFO: Deleting pod "simpletest.rc-h2ft6" in namespace "gc-7509"
Feb  6 18:39:06.087: INFO: Deleting pod "simpletest.rc-h9tsv" in namespace "gc-7509"
Feb  6 18:39:06.117: INFO: Deleting pod "simpletest.rc-j4scv" in namespace "gc-7509"
Feb  6 18:39:06.142: INFO: Deleting pod "simpletest.rc-j8d42" in namespace "gc-7509"
Feb  6 18:39:06.169: INFO: Deleting pod "simpletest.rc-jb8q6" in namespace "gc-7509"
Feb  6 18:39:06.195: INFO: Deleting pod "simpletest.rc-jd8t6" in namespace "gc-7509"
Feb  6 18:39:06.223: INFO: Deleting pod "simpletest.rc-jdvzf" in namespace "gc-7509"
Feb  6 18:39:06.237: INFO: Deleting pod "simpletest.rc-jfj29" in namespace "gc-7509"
Feb  6 18:39:06.263: INFO: Deleting pod "simpletest.rc-jr8jx" in namespace "gc-7509"
Feb  6 18:39:06.280: INFO: Deleting pod "simpletest.rc-jtlqq" in namespace "gc-7509"
Feb  6 18:39:06.297: INFO: Deleting pod "simpletest.rc-kb9lg" in namespace "gc-7509"
Feb  6 18:39:06.320: INFO: Deleting pod "simpletest.rc-kshtc" in namespace "gc-7509"
Feb  6 18:39:06.336: INFO: Deleting pod "simpletest.rc-mfv7v" in namespace "gc-7509"
Feb  6 18:39:06.355: INFO: Deleting pod "simpletest.rc-mkg4j" in namespace "gc-7509"
Feb  6 18:39:06.377: INFO: Deleting pod "simpletest.rc-mnnrv" in namespace "gc-7509"
Feb  6 18:39:06.396: INFO: Deleting pod "simpletest.rc-n49hb" in namespace "gc-7509"
Feb  6 18:39:06.434: INFO: Deleting pod "simpletest.rc-n9g7h" in namespace "gc-7509"
Feb  6 18:39:06.451: INFO: Deleting pod "simpletest.rc-nfp29" in namespace "gc-7509"
Feb  6 18:39:06.467: INFO: Deleting pod "simpletest.rc-nfpgn" in namespace "gc-7509"
Feb  6 18:39:06.492: INFO: Deleting pod "simpletest.rc-p6c5x" in namespace "gc-7509"
Feb  6 18:39:06.522: INFO: Deleting pod "simpletest.rc-p8hhb" in namespace "gc-7509"
Feb  6 18:39:06.585: INFO: Deleting pod "simpletest.rc-pr9rf" in namespace "gc-7509"
Feb  6 18:39:06.724: INFO: Deleting pod "simpletest.rc-pt2kg" in namespace "gc-7509"
Feb  6 18:39:06.786: INFO: Deleting pod "simpletest.rc-pztmp" in namespace "gc-7509"
Feb  6 18:39:06.803: INFO: Deleting pod "simpletest.rc-q75sb" in namespace "gc-7509"
Feb  6 18:39:06.820: INFO: Deleting pod "simpletest.rc-qhzhh" in namespace "gc-7509"
Feb  6 18:39:06.846: INFO: Deleting pod "simpletest.rc-rx8n7" in namespace "gc-7509"
Feb  6 18:39:06.866: INFO: Deleting pod "simpletest.rc-rzmzj" in namespace "gc-7509"
Feb  6 18:39:06.890: INFO: Deleting pod "simpletest.rc-s2xx5" in namespace "gc-7509"
Feb  6 18:39:06.922: INFO: Deleting pod "simpletest.rc-s52dj" in namespace "gc-7509"
Feb  6 18:39:07.046: INFO: Deleting pod "simpletest.rc-scqgx" in namespace "gc-7509"
Feb  6 18:39:07.676: INFO: Deleting pod "simpletest.rc-sddr5" in namespace "gc-7509"
Feb  6 18:39:07.808: INFO: Deleting pod "simpletest.rc-szzp2" in namespace "gc-7509"
Feb  6 18:39:07.833: INFO: Deleting pod "simpletest.rc-t44xz" in namespace "gc-7509"
Feb  6 18:39:07.857: INFO: Deleting pod "simpletest.rc-t5mm5" in namespace "gc-7509"
Feb  6 18:39:07.986: INFO: Deleting pod "simpletest.rc-tct5l" in namespace "gc-7509"
Feb  6 18:39:08.154: INFO: Deleting pod "simpletest.rc-td4pt" in namespace "gc-7509"
Feb  6 18:39:08.227: INFO: Deleting pod "simpletest.rc-tfw86" in namespace "gc-7509"
Feb  6 18:39:08.272: INFO: Deleting pod "simpletest.rc-tjpm8" in namespace "gc-7509"
Feb  6 18:39:08.531: INFO: Deleting pod "simpletest.rc-tmj8p" in namespace "gc-7509"
Feb  6 18:39:08.554: INFO: Deleting pod "simpletest.rc-v9wfr" in namespace "gc-7509"
Feb  6 18:39:08.580: INFO: Deleting pod "simpletest.rc-vsrnj" in namespace "gc-7509"
Feb  6 18:39:08.598: INFO: Deleting pod "simpletest.rc-w75dn" in namespace "gc-7509"
Feb  6 18:39:08.618: INFO: Deleting pod "simpletest.rc-w9nkb" in namespace "gc-7509"
Feb  6 18:39:08.647: INFO: Deleting pod "simpletest.rc-wgnrw" in namespace "gc-7509"
Feb  6 18:39:08.675: INFO: Deleting pod "simpletest.rc-x4c95" in namespace "gc-7509"
Feb  6 18:39:08.709: INFO: Deleting pod "simpletest.rc-xd484" in namespace "gc-7509"
Feb  6 18:39:08.729: INFO: Deleting pod "simpletest.rc-xdprd" in namespace "gc-7509"
Feb  6 18:39:08.748: INFO: Deleting pod "simpletest.rc-xlrds" in namespace "gc-7509"
Feb  6 18:39:08.765: INFO: Deleting pod "simpletest.rc-xmdxh" in namespace "gc-7509"
Feb  6 18:39:08.791: INFO: Deleting pod "simpletest.rc-z4drx" in namespace "gc-7509"
Feb  6 18:39:08.810: INFO: Deleting pod "simpletest.rc-z6ctk" in namespace "gc-7509"
Feb  6 18:39:08.826: INFO: Deleting pod "simpletest.rc-zccnt" in namespace "gc-7509"
Feb  6 18:39:08.851: INFO: Deleting pod "simpletest.rc-zh9gx" in namespace "gc-7509"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb  6 18:39:08.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7509" for this suite. 02/06/23 18:39:08.888
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":276,"skipped":5289,"failed":0}
------------------------------
• [SLOW TEST] [46.823 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:38:22.073
    Feb  6 18:38:22.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename gc 02/06/23 18:38:22.075
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:38:22.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:38:22.095
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 02/06/23 18:38:22.104
    STEP: delete the rc 02/06/23 18:38:27.546
    STEP: wait for the rc to be deleted 02/06/23 18:38:27.594
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/06/23 18:38:32.652
    STEP: Gathering metrics 02/06/23 18:39:02.758
    Feb  6 18:39:03.288: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tneyla25-pnn8z" in namespace "kube-system" to be "running and ready"
    Feb  6 18:39:03.292: INFO: Pod "kube-controller-manager-tneyla25-pnn8z": Phase="Running", Reason="", readiness=true. Elapsed: 3.704415ms
    Feb  6 18:39:03.292: INFO: The phase of Pod kube-controller-manager-tneyla25-pnn8z is Running (Ready = true)
    Feb  6 18:39:03.292: INFO: Pod "kube-controller-manager-tneyla25-pnn8z" satisfied condition "running and ready"
    Feb  6 18:39:03.440: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb  6 18:39:03.440: INFO: Deleting pod "simpletest.rc-25m6p" in namespace "gc-7509"
    Feb  6 18:39:03.462: INFO: Deleting pod "simpletest.rc-2f8dx" in namespace "gc-7509"
    Feb  6 18:39:03.485: INFO: Deleting pod "simpletest.rc-42bdg" in namespace "gc-7509"
    Feb  6 18:39:03.502: INFO: Deleting pod "simpletest.rc-42q7w" in namespace "gc-7509"
    Feb  6 18:39:03.527: INFO: Deleting pod "simpletest.rc-4swtn" in namespace "gc-7509"
    Feb  6 18:39:03.544: INFO: Deleting pod "simpletest.rc-4v7vl" in namespace "gc-7509"
    Feb  6 18:39:03.568: INFO: Deleting pod "simpletest.rc-4xmzp" in namespace "gc-7509"
    Feb  6 18:39:03.587: INFO: Deleting pod "simpletest.rc-56rcg" in namespace "gc-7509"
    Feb  6 18:39:03.613: INFO: Deleting pod "simpletest.rc-5f7qp" in namespace "gc-7509"
    Feb  6 18:39:03.640: INFO: Deleting pod "simpletest.rc-5sjv8" in namespace "gc-7509"
    Feb  6 18:39:03.654: INFO: Deleting pod "simpletest.rc-6g5mk" in namespace "gc-7509"
    Feb  6 18:39:03.679: INFO: Deleting pod "simpletest.rc-6jwv5" in namespace "gc-7509"
    Feb  6 18:39:03.693: INFO: Deleting pod "simpletest.rc-6pnw7" in namespace "gc-7509"
    Feb  6 18:39:03.827: INFO: Deleting pod "simpletest.rc-6znxp" in namespace "gc-7509"
    Feb  6 18:39:03.865: INFO: Deleting pod "simpletest.rc-772c4" in namespace "gc-7509"
    Feb  6 18:39:03.913: INFO: Deleting pod "simpletest.rc-77b6v" in namespace "gc-7509"
    Feb  6 18:39:04.019: INFO: Deleting pod "simpletest.rc-79l2n" in namespace "gc-7509"
    Feb  6 18:39:04.046: INFO: Deleting pod "simpletest.rc-7ppkn" in namespace "gc-7509"
    Feb  6 18:39:04.291: INFO: Deleting pod "simpletest.rc-7sgbb" in namespace "gc-7509"
    Feb  6 18:39:04.329: INFO: Deleting pod "simpletest.rc-8nw2k" in namespace "gc-7509"
    Feb  6 18:39:04.396: INFO: Deleting pod "simpletest.rc-8zrrp" in namespace "gc-7509"
    Feb  6 18:39:04.613: INFO: Deleting pod "simpletest.rc-9778z" in namespace "gc-7509"
    Feb  6 18:39:04.634: INFO: Deleting pod "simpletest.rc-99vt8" in namespace "gc-7509"
    Feb  6 18:39:04.683: INFO: Deleting pod "simpletest.rc-9bfss" in namespace "gc-7509"
    Feb  6 18:39:04.759: INFO: Deleting pod "simpletest.rc-9jwlq" in namespace "gc-7509"
    Feb  6 18:39:04.786: INFO: Deleting pod "simpletest.rc-9xn24" in namespace "gc-7509"
    Feb  6 18:39:04.850: INFO: Deleting pod "simpletest.rc-b5hqq" in namespace "gc-7509"
    Feb  6 18:39:04.949: INFO: Deleting pod "simpletest.rc-bhmwh" in namespace "gc-7509"
    Feb  6 18:39:05.311: INFO: Deleting pod "simpletest.rc-bkvx6" in namespace "gc-7509"
    Feb  6 18:39:05.336: INFO: Deleting pod "simpletest.rc-btsps" in namespace "gc-7509"
    Feb  6 18:39:05.640: INFO: Deleting pod "simpletest.rc-bz7ph" in namespace "gc-7509"
    Feb  6 18:39:05.678: INFO: Deleting pod "simpletest.rc-cxt52" in namespace "gc-7509"
    Feb  6 18:39:05.710: INFO: Deleting pod "simpletest.rc-d7f25" in namespace "gc-7509"
    Feb  6 18:39:05.731: INFO: Deleting pod "simpletest.rc-d84bk" in namespace "gc-7509"
    Feb  6 18:39:05.765: INFO: Deleting pod "simpletest.rc-dfp5s" in namespace "gc-7509"
    Feb  6 18:39:05.809: INFO: Deleting pod "simpletest.rc-dnxsc" in namespace "gc-7509"
    Feb  6 18:39:05.829: INFO: Deleting pod "simpletest.rc-dqjc4" in namespace "gc-7509"
    Feb  6 18:39:05.849: INFO: Deleting pod "simpletest.rc-dsnrd" in namespace "gc-7509"
    Feb  6 18:39:05.868: INFO: Deleting pod "simpletest.rc-f7phr" in namespace "gc-7509"
    Feb  6 18:39:05.889: INFO: Deleting pod "simpletest.rc-fbvck" in namespace "gc-7509"
    Feb  6 18:39:05.912: INFO: Deleting pod "simpletest.rc-fgb76" in namespace "gc-7509"
    Feb  6 18:39:05.944: INFO: Deleting pod "simpletest.rc-fgztg" in namespace "gc-7509"
    Feb  6 18:39:05.973: INFO: Deleting pod "simpletest.rc-fswrj" in namespace "gc-7509"
    Feb  6 18:39:06.004: INFO: Deleting pod "simpletest.rc-gc7dm" in namespace "gc-7509"
    Feb  6 18:39:06.027: INFO: Deleting pod "simpletest.rc-gdb7l" in namespace "gc-7509"
    Feb  6 18:39:06.047: INFO: Deleting pod "simpletest.rc-gppwh" in namespace "gc-7509"
    Feb  6 18:39:06.067: INFO: Deleting pod "simpletest.rc-h2ft6" in namespace "gc-7509"
    Feb  6 18:39:06.087: INFO: Deleting pod "simpletest.rc-h9tsv" in namespace "gc-7509"
    Feb  6 18:39:06.117: INFO: Deleting pod "simpletest.rc-j4scv" in namespace "gc-7509"
    Feb  6 18:39:06.142: INFO: Deleting pod "simpletest.rc-j8d42" in namespace "gc-7509"
    Feb  6 18:39:06.169: INFO: Deleting pod "simpletest.rc-jb8q6" in namespace "gc-7509"
    Feb  6 18:39:06.195: INFO: Deleting pod "simpletest.rc-jd8t6" in namespace "gc-7509"
    Feb  6 18:39:06.223: INFO: Deleting pod "simpletest.rc-jdvzf" in namespace "gc-7509"
    Feb  6 18:39:06.237: INFO: Deleting pod "simpletest.rc-jfj29" in namespace "gc-7509"
    Feb  6 18:39:06.263: INFO: Deleting pod "simpletest.rc-jr8jx" in namespace "gc-7509"
    Feb  6 18:39:06.280: INFO: Deleting pod "simpletest.rc-jtlqq" in namespace "gc-7509"
    Feb  6 18:39:06.297: INFO: Deleting pod "simpletest.rc-kb9lg" in namespace "gc-7509"
    Feb  6 18:39:06.320: INFO: Deleting pod "simpletest.rc-kshtc" in namespace "gc-7509"
    Feb  6 18:39:06.336: INFO: Deleting pod "simpletest.rc-mfv7v" in namespace "gc-7509"
    Feb  6 18:39:06.355: INFO: Deleting pod "simpletest.rc-mkg4j" in namespace "gc-7509"
    Feb  6 18:39:06.377: INFO: Deleting pod "simpletest.rc-mnnrv" in namespace "gc-7509"
    Feb  6 18:39:06.396: INFO: Deleting pod "simpletest.rc-n49hb" in namespace "gc-7509"
    Feb  6 18:39:06.434: INFO: Deleting pod "simpletest.rc-n9g7h" in namespace "gc-7509"
    Feb  6 18:39:06.451: INFO: Deleting pod "simpletest.rc-nfp29" in namespace "gc-7509"
    Feb  6 18:39:06.467: INFO: Deleting pod "simpletest.rc-nfpgn" in namespace "gc-7509"
    Feb  6 18:39:06.492: INFO: Deleting pod "simpletest.rc-p6c5x" in namespace "gc-7509"
    Feb  6 18:39:06.522: INFO: Deleting pod "simpletest.rc-p8hhb" in namespace "gc-7509"
    Feb  6 18:39:06.585: INFO: Deleting pod "simpletest.rc-pr9rf" in namespace "gc-7509"
    Feb  6 18:39:06.724: INFO: Deleting pod "simpletest.rc-pt2kg" in namespace "gc-7509"
    Feb  6 18:39:06.786: INFO: Deleting pod "simpletest.rc-pztmp" in namespace "gc-7509"
    Feb  6 18:39:06.803: INFO: Deleting pod "simpletest.rc-q75sb" in namespace "gc-7509"
    Feb  6 18:39:06.820: INFO: Deleting pod "simpletest.rc-qhzhh" in namespace "gc-7509"
    Feb  6 18:39:06.846: INFO: Deleting pod "simpletest.rc-rx8n7" in namespace "gc-7509"
    Feb  6 18:39:06.866: INFO: Deleting pod "simpletest.rc-rzmzj" in namespace "gc-7509"
    Feb  6 18:39:06.890: INFO: Deleting pod "simpletest.rc-s2xx5" in namespace "gc-7509"
    Feb  6 18:39:06.922: INFO: Deleting pod "simpletest.rc-s52dj" in namespace "gc-7509"
    Feb  6 18:39:07.046: INFO: Deleting pod "simpletest.rc-scqgx" in namespace "gc-7509"
    Feb  6 18:39:07.676: INFO: Deleting pod "simpletest.rc-sddr5" in namespace "gc-7509"
    Feb  6 18:39:07.808: INFO: Deleting pod "simpletest.rc-szzp2" in namespace "gc-7509"
    Feb  6 18:39:07.833: INFO: Deleting pod "simpletest.rc-t44xz" in namespace "gc-7509"
    Feb  6 18:39:07.857: INFO: Deleting pod "simpletest.rc-t5mm5" in namespace "gc-7509"
    Feb  6 18:39:07.986: INFO: Deleting pod "simpletest.rc-tct5l" in namespace "gc-7509"
    Feb  6 18:39:08.154: INFO: Deleting pod "simpletest.rc-td4pt" in namespace "gc-7509"
    Feb  6 18:39:08.227: INFO: Deleting pod "simpletest.rc-tfw86" in namespace "gc-7509"
    Feb  6 18:39:08.272: INFO: Deleting pod "simpletest.rc-tjpm8" in namespace "gc-7509"
    Feb  6 18:39:08.531: INFO: Deleting pod "simpletest.rc-tmj8p" in namespace "gc-7509"
    Feb  6 18:39:08.554: INFO: Deleting pod "simpletest.rc-v9wfr" in namespace "gc-7509"
    Feb  6 18:39:08.580: INFO: Deleting pod "simpletest.rc-vsrnj" in namespace "gc-7509"
    Feb  6 18:39:08.598: INFO: Deleting pod "simpletest.rc-w75dn" in namespace "gc-7509"
    Feb  6 18:39:08.618: INFO: Deleting pod "simpletest.rc-w9nkb" in namespace "gc-7509"
    Feb  6 18:39:08.647: INFO: Deleting pod "simpletest.rc-wgnrw" in namespace "gc-7509"
    Feb  6 18:39:08.675: INFO: Deleting pod "simpletest.rc-x4c95" in namespace "gc-7509"
    Feb  6 18:39:08.709: INFO: Deleting pod "simpletest.rc-xd484" in namespace "gc-7509"
    Feb  6 18:39:08.729: INFO: Deleting pod "simpletest.rc-xdprd" in namespace "gc-7509"
    Feb  6 18:39:08.748: INFO: Deleting pod "simpletest.rc-xlrds" in namespace "gc-7509"
    Feb  6 18:39:08.765: INFO: Deleting pod "simpletest.rc-xmdxh" in namespace "gc-7509"
    Feb  6 18:39:08.791: INFO: Deleting pod "simpletest.rc-z4drx" in namespace "gc-7509"
    Feb  6 18:39:08.810: INFO: Deleting pod "simpletest.rc-z6ctk" in namespace "gc-7509"
    Feb  6 18:39:08.826: INFO: Deleting pod "simpletest.rc-zccnt" in namespace "gc-7509"
    Feb  6 18:39:08.851: INFO: Deleting pod "simpletest.rc-zh9gx" in namespace "gc-7509"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb  6 18:39:08.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7509" for this suite. 02/06/23 18:39:08.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:39:08.9
Feb  6 18:39:08.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename statefulset 02/06/23 18:39:08.901
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:39:08.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:39:08.924
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6400 02/06/23 18:39:08.927
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-6400 02/06/23 18:39:08.942
Feb  6 18:39:08.958: INFO: Found 0 stateful pods, waiting for 1
Feb  6 18:39:18.965: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb  6 18:39:28.966: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 02/06/23 18:39:28.972
STEP: Getting /status 02/06/23 18:39:28.985
Feb  6 18:39:28.989: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 02/06/23 18:39:28.989
Feb  6 18:39:28.999: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 02/06/23 18:39:28.999
Feb  6 18:39:29.001: INFO: Observed &StatefulSet event: ADDED
Feb  6 18:39:29.001: INFO: Found Statefulset ss in namespace statefulset-6400 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb  6 18:39:29.001: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 02/06/23 18:39:29.001
Feb  6 18:39:29.002: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb  6 18:39:29.012: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 02/06/23 18:39:29.012
Feb  6 18:39:29.014: INFO: Observed &StatefulSet event: ADDED
Feb  6 18:39:29.014: INFO: Observed Statefulset ss in namespace statefulset-6400 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb  6 18:39:29.014: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb  6 18:39:29.014: INFO: Deleting all statefulset in ns statefulset-6400
Feb  6 18:39:29.017: INFO: Scaling statefulset ss to 0
Feb  6 18:39:39.035: INFO: Waiting for statefulset status.replicas updated to 0
Feb  6 18:39:39.038: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb  6 18:39:39.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6400" for this suite. 02/06/23 18:39:39.084
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":277,"skipped":5295,"failed":0}
------------------------------
• [SLOW TEST] [30.193 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:39:08.9
    Feb  6 18:39:08.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename statefulset 02/06/23 18:39:08.901
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:39:08.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:39:08.924
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6400 02/06/23 18:39:08.927
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-6400 02/06/23 18:39:08.942
    Feb  6 18:39:08.958: INFO: Found 0 stateful pods, waiting for 1
    Feb  6 18:39:18.965: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Feb  6 18:39:28.966: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 02/06/23 18:39:28.972
    STEP: Getting /status 02/06/23 18:39:28.985
    Feb  6 18:39:28.989: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 02/06/23 18:39:28.989
    Feb  6 18:39:28.999: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 02/06/23 18:39:28.999
    Feb  6 18:39:29.001: INFO: Observed &StatefulSet event: ADDED
    Feb  6 18:39:29.001: INFO: Found Statefulset ss in namespace statefulset-6400 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb  6 18:39:29.001: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 02/06/23 18:39:29.001
    Feb  6 18:39:29.002: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb  6 18:39:29.012: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 02/06/23 18:39:29.012
    Feb  6 18:39:29.014: INFO: Observed &StatefulSet event: ADDED
    Feb  6 18:39:29.014: INFO: Observed Statefulset ss in namespace statefulset-6400 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb  6 18:39:29.014: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb  6 18:39:29.014: INFO: Deleting all statefulset in ns statefulset-6400
    Feb  6 18:39:29.017: INFO: Scaling statefulset ss to 0
    Feb  6 18:39:39.035: INFO: Waiting for statefulset status.replicas updated to 0
    Feb  6 18:39:39.038: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb  6 18:39:39.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6400" for this suite. 02/06/23 18:39:39.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:39:39.102
Feb  6 18:39:39.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 18:39:39.104
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:39:39.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:39:39.121
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 02/06/23 18:39:39.125
Feb  6 18:39:39.135: INFO: Waiting up to 5m0s for pod "downward-api-ad8978be-0a10-427b-b618-7136024c3531" in namespace "downward-api-4828" to be "Succeeded or Failed"
Feb  6 18:39:39.142: INFO: Pod "downward-api-ad8978be-0a10-427b-b618-7136024c3531": Phase="Pending", Reason="", readiness=false. Elapsed: 6.109309ms
Feb  6 18:39:41.146: INFO: Pod "downward-api-ad8978be-0a10-427b-b618-7136024c3531": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010866242s
Feb  6 18:39:43.148: INFO: Pod "downward-api-ad8978be-0a10-427b-b618-7136024c3531": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012779749s
Feb  6 18:39:45.149: INFO: Pod "downward-api-ad8978be-0a10-427b-b618-7136024c3531": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013097581s
STEP: Saw pod success 02/06/23 18:39:45.149
Feb  6 18:39:45.149: INFO: Pod "downward-api-ad8978be-0a10-427b-b618-7136024c3531" satisfied condition "Succeeded or Failed"
Feb  6 18:39:45.154: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downward-api-ad8978be-0a10-427b-b618-7136024c3531 container dapi-container: <nil>
STEP: delete the pod 02/06/23 18:39:45.167
Feb  6 18:39:45.179: INFO: Waiting for pod downward-api-ad8978be-0a10-427b-b618-7136024c3531 to disappear
Feb  6 18:39:45.184: INFO: Pod downward-api-ad8978be-0a10-427b-b618-7136024c3531 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb  6 18:39:45.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4828" for this suite. 02/06/23 18:39:45.19
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":278,"skipped":5340,"failed":0}
------------------------------
• [SLOW TEST] [6.101 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:39:39.102
    Feb  6 18:39:39.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 18:39:39.104
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:39:39.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:39:39.121
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 02/06/23 18:39:39.125
    Feb  6 18:39:39.135: INFO: Waiting up to 5m0s for pod "downward-api-ad8978be-0a10-427b-b618-7136024c3531" in namespace "downward-api-4828" to be "Succeeded or Failed"
    Feb  6 18:39:39.142: INFO: Pod "downward-api-ad8978be-0a10-427b-b618-7136024c3531": Phase="Pending", Reason="", readiness=false. Elapsed: 6.109309ms
    Feb  6 18:39:41.146: INFO: Pod "downward-api-ad8978be-0a10-427b-b618-7136024c3531": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010866242s
    Feb  6 18:39:43.148: INFO: Pod "downward-api-ad8978be-0a10-427b-b618-7136024c3531": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012779749s
    Feb  6 18:39:45.149: INFO: Pod "downward-api-ad8978be-0a10-427b-b618-7136024c3531": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013097581s
    STEP: Saw pod success 02/06/23 18:39:45.149
    Feb  6 18:39:45.149: INFO: Pod "downward-api-ad8978be-0a10-427b-b618-7136024c3531" satisfied condition "Succeeded or Failed"
    Feb  6 18:39:45.154: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downward-api-ad8978be-0a10-427b-b618-7136024c3531 container dapi-container: <nil>
    STEP: delete the pod 02/06/23 18:39:45.167
    Feb  6 18:39:45.179: INFO: Waiting for pod downward-api-ad8978be-0a10-427b-b618-7136024c3531 to disappear
    Feb  6 18:39:45.184: INFO: Pod downward-api-ad8978be-0a10-427b-b618-7136024c3531 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb  6 18:39:45.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4828" for this suite. 02/06/23 18:39:45.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:39:45.21
Feb  6 18:39:45.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-runtime 02/06/23 18:39:45.211
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:39:45.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:39:45.233
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 02/06/23 18:39:45.236
STEP: wait for the container to reach Succeeded 02/06/23 18:39:45.244
STEP: get the container status 02/06/23 18:39:50.274
STEP: the container should be terminated 02/06/23 18:39:50.277
STEP: the termination message should be set 02/06/23 18:39:50.278
Feb  6 18:39:50.278: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/06/23 18:39:50.278
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb  6 18:39:50.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7569" for this suite. 02/06/23 18:39:50.307
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":279,"skipped":5387,"failed":0}
------------------------------
• [SLOW TEST] [5.106 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:39:45.21
    Feb  6 18:39:45.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-runtime 02/06/23 18:39:45.211
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:39:45.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:39:45.233
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 02/06/23 18:39:45.236
    STEP: wait for the container to reach Succeeded 02/06/23 18:39:45.244
    STEP: get the container status 02/06/23 18:39:50.274
    STEP: the container should be terminated 02/06/23 18:39:50.277
    STEP: the termination message should be set 02/06/23 18:39:50.278
    Feb  6 18:39:50.278: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/06/23 18:39:50.278
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb  6 18:39:50.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7569" for this suite. 02/06/23 18:39:50.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:39:50.317
Feb  6 18:39:50.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename events 02/06/23 18:39:50.319
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:39:50.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:39:50.341
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 02/06/23 18:39:50.344
STEP: get a list of Events with a label in the current namespace 02/06/23 18:39:50.363
STEP: delete a list of events 02/06/23 18:39:50.366
Feb  6 18:39:50.366: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/06/23 18:39:50.395
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Feb  6 18:39:50.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6477" for this suite. 02/06/23 18:39:50.404
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":280,"skipped":5402,"failed":0}
------------------------------
• [0.095 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:39:50.317
    Feb  6 18:39:50.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename events 02/06/23 18:39:50.319
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:39:50.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:39:50.341
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 02/06/23 18:39:50.344
    STEP: get a list of Events with a label in the current namespace 02/06/23 18:39:50.363
    STEP: delete a list of events 02/06/23 18:39:50.366
    Feb  6 18:39:50.366: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/06/23 18:39:50.395
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Feb  6 18:39:50.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6477" for this suite. 02/06/23 18:39:50.404
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:39:50.416
Feb  6 18:39:50.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pods 02/06/23 18:39:50.417
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:39:50.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:39:50.439
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Feb  6 18:39:50.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: creating the pod 02/06/23 18:39:50.445
STEP: submitting the pod to kubernetes 02/06/23 18:39:50.445
Feb  6 18:39:50.461: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c" in namespace "pods-5330" to be "running and ready"
Feb  6 18:39:50.469: INFO: Pod "pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.613564ms
Feb  6 18:39:50.469: INFO: The phase of Pod pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:39:52.474: INFO: Pod "pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013156286s
Feb  6 18:39:52.474: INFO: The phase of Pod pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:39:54.474: INFO: Pod "pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c": Phase="Running", Reason="", readiness=true. Elapsed: 4.013353017s
Feb  6 18:39:54.474: INFO: The phase of Pod pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c is Running (Ready = true)
Feb  6 18:39:54.474: INFO: Pod "pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb  6 18:39:54.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5330" for this suite. 02/06/23 18:39:54.497
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":281,"skipped":5406,"failed":0}
------------------------------
• [4.090 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:39:50.416
    Feb  6 18:39:50.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pods 02/06/23 18:39:50.417
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:39:50.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:39:50.439
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Feb  6 18:39:50.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: creating the pod 02/06/23 18:39:50.445
    STEP: submitting the pod to kubernetes 02/06/23 18:39:50.445
    Feb  6 18:39:50.461: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c" in namespace "pods-5330" to be "running and ready"
    Feb  6 18:39:50.469: INFO: Pod "pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.613564ms
    Feb  6 18:39:50.469: INFO: The phase of Pod pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:39:52.474: INFO: Pod "pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013156286s
    Feb  6 18:39:52.474: INFO: The phase of Pod pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:39:54.474: INFO: Pod "pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c": Phase="Running", Reason="", readiness=true. Elapsed: 4.013353017s
    Feb  6 18:39:54.474: INFO: The phase of Pod pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c is Running (Ready = true)
    Feb  6 18:39:54.474: INFO: Pod "pod-logs-websocket-a8e9c9ae-1d14-483b-8a6b-14c84ce09d8c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb  6 18:39:54.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5330" for this suite. 02/06/23 18:39:54.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:39:54.511
Feb  6 18:39:54.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 18:39:54.512
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:39:54.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:39:54.534
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/06/23 18:39:54.537
Feb  6 18:39:54.548: INFO: Waiting up to 5m0s for pod "pod-7ff8c22f-e7ff-4023-ba50-8377435158b5" in namespace "emptydir-1554" to be "Succeeded or Failed"
Feb  6 18:39:54.553: INFO: Pod "pod-7ff8c22f-e7ff-4023-ba50-8377435158b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.356997ms
Feb  6 18:39:56.558: INFO: Pod "pod-7ff8c22f-e7ff-4023-ba50-8377435158b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01008998s
Feb  6 18:39:58.558: INFO: Pod "pod-7ff8c22f-e7ff-4023-ba50-8377435158b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009835302s
Feb  6 18:40:00.561: INFO: Pod "pod-7ff8c22f-e7ff-4023-ba50-8377435158b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012548149s
STEP: Saw pod success 02/06/23 18:40:00.561
Feb  6 18:40:00.561: INFO: Pod "pod-7ff8c22f-e7ff-4023-ba50-8377435158b5" satisfied condition "Succeeded or Failed"
Feb  6 18:40:00.564: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-7ff8c22f-e7ff-4023-ba50-8377435158b5 container test-container: <nil>
STEP: delete the pod 02/06/23 18:40:00.569
Feb  6 18:40:00.588: INFO: Waiting for pod pod-7ff8c22f-e7ff-4023-ba50-8377435158b5 to disappear
Feb  6 18:40:00.593: INFO: Pod pod-7ff8c22f-e7ff-4023-ba50-8377435158b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 18:40:00.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1554" for this suite. 02/06/23 18:40:00.6
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":282,"skipped":5419,"failed":0}
------------------------------
• [SLOW TEST] [6.097 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:39:54.511
    Feb  6 18:39:54.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 18:39:54.512
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:39:54.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:39:54.534
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/06/23 18:39:54.537
    Feb  6 18:39:54.548: INFO: Waiting up to 5m0s for pod "pod-7ff8c22f-e7ff-4023-ba50-8377435158b5" in namespace "emptydir-1554" to be "Succeeded or Failed"
    Feb  6 18:39:54.553: INFO: Pod "pod-7ff8c22f-e7ff-4023-ba50-8377435158b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.356997ms
    Feb  6 18:39:56.558: INFO: Pod "pod-7ff8c22f-e7ff-4023-ba50-8377435158b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01008998s
    Feb  6 18:39:58.558: INFO: Pod "pod-7ff8c22f-e7ff-4023-ba50-8377435158b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009835302s
    Feb  6 18:40:00.561: INFO: Pod "pod-7ff8c22f-e7ff-4023-ba50-8377435158b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012548149s
    STEP: Saw pod success 02/06/23 18:40:00.561
    Feb  6 18:40:00.561: INFO: Pod "pod-7ff8c22f-e7ff-4023-ba50-8377435158b5" satisfied condition "Succeeded or Failed"
    Feb  6 18:40:00.564: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-7ff8c22f-e7ff-4023-ba50-8377435158b5 container test-container: <nil>
    STEP: delete the pod 02/06/23 18:40:00.569
    Feb  6 18:40:00.588: INFO: Waiting for pod pod-7ff8c22f-e7ff-4023-ba50-8377435158b5 to disappear
    Feb  6 18:40:00.593: INFO: Pod pod-7ff8c22f-e7ff-4023-ba50-8377435158b5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 18:40:00.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1554" for this suite. 02/06/23 18:40:00.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:40:00.612
Feb  6 18:40:00.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:40:00.613
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:00.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:00.634
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-3cd0ecfa-8e13-4344-b2e9-910c27176cb0 02/06/23 18:40:00.637
STEP: Creating a pod to test consume secrets 02/06/23 18:40:00.643
Feb  6 18:40:00.656: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834" in namespace "projected-8746" to be "Succeeded or Failed"
Feb  6 18:40:00.661: INFO: Pod "pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834": Phase="Pending", Reason="", readiness=false. Elapsed: 5.705061ms
Feb  6 18:40:02.666: INFO: Pod "pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010752097s
Feb  6 18:40:04.667: INFO: Pod "pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010860949s
Feb  6 18:40:06.667: INFO: Pod "pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01115743s
STEP: Saw pod success 02/06/23 18:40:06.667
Feb  6 18:40:06.667: INFO: Pod "pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834" satisfied condition "Succeeded or Failed"
Feb  6 18:40:06.670: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834 container secret-volume-test: <nil>
STEP: delete the pod 02/06/23 18:40:06.677
Feb  6 18:40:06.693: INFO: Waiting for pod pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834 to disappear
Feb  6 18:40:06.698: INFO: Pod pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb  6 18:40:06.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8746" for this suite. 02/06/23 18:40:06.703
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":283,"skipped":5424,"failed":0}
------------------------------
• [SLOW TEST] [6.097 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:40:00.612
    Feb  6 18:40:00.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:40:00.613
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:00.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:00.634
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-3cd0ecfa-8e13-4344-b2e9-910c27176cb0 02/06/23 18:40:00.637
    STEP: Creating a pod to test consume secrets 02/06/23 18:40:00.643
    Feb  6 18:40:00.656: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834" in namespace "projected-8746" to be "Succeeded or Failed"
    Feb  6 18:40:00.661: INFO: Pod "pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834": Phase="Pending", Reason="", readiness=false. Elapsed: 5.705061ms
    Feb  6 18:40:02.666: INFO: Pod "pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010752097s
    Feb  6 18:40:04.667: INFO: Pod "pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010860949s
    Feb  6 18:40:06.667: INFO: Pod "pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01115743s
    STEP: Saw pod success 02/06/23 18:40:06.667
    Feb  6 18:40:06.667: INFO: Pod "pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834" satisfied condition "Succeeded or Failed"
    Feb  6 18:40:06.670: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834 container secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 18:40:06.677
    Feb  6 18:40:06.693: INFO: Waiting for pod pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834 to disappear
    Feb  6 18:40:06.698: INFO: Pod pod-projected-secrets-e1d903fd-9e72-4216-ad07-0b6adfb8f834 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb  6 18:40:06.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8746" for this suite. 02/06/23 18:40:06.703
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:40:06.71
Feb  6 18:40:06.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 18:40:06.711
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:06.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:06.733
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 02/06/23 18:40:06.736
Feb  6 18:40:06.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087" in namespace "downward-api-9258" to be "Succeeded or Failed"
Feb  6 18:40:06.754: INFO: Pod "downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087": Phase="Pending", Reason="", readiness=false. Elapsed: 9.704068ms
Feb  6 18:40:08.759: INFO: Pod "downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015196892s
Feb  6 18:40:10.764: INFO: Pod "downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019640909s
Feb  6 18:40:12.760: INFO: Pod "downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016028679s
STEP: Saw pod success 02/06/23 18:40:12.76
Feb  6 18:40:12.760: INFO: Pod "downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087" satisfied condition "Succeeded or Failed"
Feb  6 18:40:12.764: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087 container client-container: <nil>
STEP: delete the pod 02/06/23 18:40:12.772
Feb  6 18:40:12.785: INFO: Waiting for pod downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087 to disappear
Feb  6 18:40:12.788: INFO: Pod downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb  6 18:40:12.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9258" for this suite. 02/06/23 18:40:12.794
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":284,"skipped":5428,"failed":0}
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:40:06.71
    Feb  6 18:40:06.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 18:40:06.711
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:06.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:06.733
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 02/06/23 18:40:06.736
    Feb  6 18:40:06.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087" in namespace "downward-api-9258" to be "Succeeded or Failed"
    Feb  6 18:40:06.754: INFO: Pod "downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087": Phase="Pending", Reason="", readiness=false. Elapsed: 9.704068ms
    Feb  6 18:40:08.759: INFO: Pod "downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015196892s
    Feb  6 18:40:10.764: INFO: Pod "downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019640909s
    Feb  6 18:40:12.760: INFO: Pod "downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016028679s
    STEP: Saw pod success 02/06/23 18:40:12.76
    Feb  6 18:40:12.760: INFO: Pod "downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087" satisfied condition "Succeeded or Failed"
    Feb  6 18:40:12.764: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087 container client-container: <nil>
    STEP: delete the pod 02/06/23 18:40:12.772
    Feb  6 18:40:12.785: INFO: Waiting for pod downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087 to disappear
    Feb  6 18:40:12.788: INFO: Pod downwardapi-volume-e3e7ad36-f7c7-4de8-a37b-c24243551087 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb  6 18:40:12.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9258" for this suite. 02/06/23 18:40:12.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:40:12.806
Feb  6 18:40:12.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:40:12.806
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:12.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:12.829
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 02/06/23 18:40:12.835
Feb  6 18:40:12.845: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb" in namespace "projected-2133" to be "Succeeded or Failed"
Feb  6 18:40:12.852: INFO: Pod "downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.380428ms
Feb  6 18:40:14.857: INFO: Pod "downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011572001s
Feb  6 18:40:16.859: INFO: Pod "downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013166582s
Feb  6 18:40:18.858: INFO: Pod "downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012239323s
STEP: Saw pod success 02/06/23 18:40:18.858
Feb  6 18:40:18.858: INFO: Pod "downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb" satisfied condition "Succeeded or Failed"
Feb  6 18:40:18.865: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb container client-container: <nil>
STEP: delete the pod 02/06/23 18:40:18.872
Feb  6 18:40:18.890: INFO: Waiting for pod downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb to disappear
Feb  6 18:40:18.893: INFO: Pod downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb  6 18:40:18.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2133" for this suite. 02/06/23 18:40:18.898
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":285,"skipped":5454,"failed":0}
------------------------------
• [SLOW TEST] [6.098 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:40:12.806
    Feb  6 18:40:12.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:40:12.806
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:12.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:12.829
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 02/06/23 18:40:12.835
    Feb  6 18:40:12.845: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb" in namespace "projected-2133" to be "Succeeded or Failed"
    Feb  6 18:40:12.852: INFO: Pod "downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.380428ms
    Feb  6 18:40:14.857: INFO: Pod "downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011572001s
    Feb  6 18:40:16.859: INFO: Pod "downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013166582s
    Feb  6 18:40:18.858: INFO: Pod "downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012239323s
    STEP: Saw pod success 02/06/23 18:40:18.858
    Feb  6 18:40:18.858: INFO: Pod "downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb" satisfied condition "Succeeded or Failed"
    Feb  6 18:40:18.865: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb container client-container: <nil>
    STEP: delete the pod 02/06/23 18:40:18.872
    Feb  6 18:40:18.890: INFO: Waiting for pod downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb to disappear
    Feb  6 18:40:18.893: INFO: Pod downwardapi-volume-fdaa53f3-6344-4382-af2f-0e13c034eeeb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb  6 18:40:18.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2133" for this suite. 02/06/23 18:40:18.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:40:18.908
Feb  6 18:40:18.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubelet-test 02/06/23 18:40:18.909
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:18.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:18.93
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 02/06/23 18:40:18.946
Feb  6 18:40:18.946: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasescda29436-59b5-41f1-bf1f-38c1d991d905" in namespace "kubelet-test-4536" to be "completed"
Feb  6 18:40:18.951: INFO: Pod "agnhost-host-aliasescda29436-59b5-41f1-bf1f-38c1d991d905": Phase="Pending", Reason="", readiness=false. Elapsed: 4.30509ms
Feb  6 18:40:20.955: INFO: Pod "agnhost-host-aliasescda29436-59b5-41f1-bf1f-38c1d991d905": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008636245s
Feb  6 18:40:22.956: INFO: Pod "agnhost-host-aliasescda29436-59b5-41f1-bf1f-38c1d991d905": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01001664s
Feb  6 18:40:24.955: INFO: Pod "agnhost-host-aliasescda29436-59b5-41f1-bf1f-38c1d991d905": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008808112s
Feb  6 18:40:24.955: INFO: Pod "agnhost-host-aliasescda29436-59b5-41f1-bf1f-38c1d991d905" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb  6 18:40:24.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4536" for this suite. 02/06/23 18:40:24.967
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":286,"skipped":5461,"failed":0}
------------------------------
• [SLOW TEST] [6.065 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:40:18.908
    Feb  6 18:40:18.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubelet-test 02/06/23 18:40:18.909
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:18.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:18.93
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 02/06/23 18:40:18.946
    Feb  6 18:40:18.946: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasescda29436-59b5-41f1-bf1f-38c1d991d905" in namespace "kubelet-test-4536" to be "completed"
    Feb  6 18:40:18.951: INFO: Pod "agnhost-host-aliasescda29436-59b5-41f1-bf1f-38c1d991d905": Phase="Pending", Reason="", readiness=false. Elapsed: 4.30509ms
    Feb  6 18:40:20.955: INFO: Pod "agnhost-host-aliasescda29436-59b5-41f1-bf1f-38c1d991d905": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008636245s
    Feb  6 18:40:22.956: INFO: Pod "agnhost-host-aliasescda29436-59b5-41f1-bf1f-38c1d991d905": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01001664s
    Feb  6 18:40:24.955: INFO: Pod "agnhost-host-aliasescda29436-59b5-41f1-bf1f-38c1d991d905": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008808112s
    Feb  6 18:40:24.955: INFO: Pod "agnhost-host-aliasescda29436-59b5-41f1-bf1f-38c1d991d905" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb  6 18:40:24.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4536" for this suite. 02/06/23 18:40:24.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:40:24.977
Feb  6 18:40:24.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 18:40:24.978
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:24.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:24.999
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-139 02/06/23 18:40:25.003
STEP: creating service affinity-clusterip-transition in namespace services-139 02/06/23 18:40:25.004
STEP: creating replication controller affinity-clusterip-transition in namespace services-139 02/06/23 18:40:25.023
I0206 18:40:25.044350      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-139, replica count: 3
I0206 18:40:28.095576      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 18:40:31.097874      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  6 18:40:31.104: INFO: Creating new exec pod
Feb  6 18:40:31.109: INFO: Waiting up to 5m0s for pod "execpod-affinitywdm2d" in namespace "services-139" to be "running"
Feb  6 18:40:31.113: INFO: Pod "execpod-affinitywdm2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.001134ms
Feb  6 18:40:33.117: INFO: Pod "execpod-affinitywdm2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008294775s
Feb  6 18:40:35.119: INFO: Pod "execpod-affinitywdm2d": Phase="Running", Reason="", readiness=true. Elapsed: 4.010279009s
Feb  6 18:40:35.119: INFO: Pod "execpod-affinitywdm2d" satisfied condition "running"
Feb  6 18:40:36.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-139 exec execpod-affinitywdm2d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Feb  6 18:40:36.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb  6 18:40:36.290: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:40:36.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-139 exec execpod-affinitywdm2d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.4.77 80'
Feb  6 18:40:36.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.4.77 80\nConnection to 10.98.4.77 80 port [tcp/http] succeeded!\n"
Feb  6 18:40:36.461: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:40:36.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-139 exec execpod-affinitywdm2d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.4.77:80/ ; done'
Feb  6 18:40:36.777: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n"
Feb  6 18:40:36.777: INFO: stdout: "\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-jp6xv\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-jp6xv\naffinity-clusterip-transition-jp6xv"
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-jp6xv
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-jp6xv
Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-jp6xv
Feb  6 18:40:36.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-139 exec execpod-affinitywdm2d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.4.77:80/ ; done'
Feb  6 18:40:37.075: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n"
Feb  6 18:40:37.075: INFO: stdout: "\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk"
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
Feb  6 18:40:37.075: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-139, will wait for the garbage collector to delete the pods 02/06/23 18:40:37.091
Feb  6 18:40:37.163: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.319813ms
Feb  6 18:40:37.263: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.590989ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 18:40:39.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-139" for this suite. 02/06/23 18:40:39.508
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":287,"skipped":5470,"failed":0}
------------------------------
• [SLOW TEST] [14.548 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:40:24.977
    Feb  6 18:40:24.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 18:40:24.978
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:24.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:24.999
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-139 02/06/23 18:40:25.003
    STEP: creating service affinity-clusterip-transition in namespace services-139 02/06/23 18:40:25.004
    STEP: creating replication controller affinity-clusterip-transition in namespace services-139 02/06/23 18:40:25.023
    I0206 18:40:25.044350      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-139, replica count: 3
    I0206 18:40:28.095576      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 18:40:31.097874      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb  6 18:40:31.104: INFO: Creating new exec pod
    Feb  6 18:40:31.109: INFO: Waiting up to 5m0s for pod "execpod-affinitywdm2d" in namespace "services-139" to be "running"
    Feb  6 18:40:31.113: INFO: Pod "execpod-affinitywdm2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.001134ms
    Feb  6 18:40:33.117: INFO: Pod "execpod-affinitywdm2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008294775s
    Feb  6 18:40:35.119: INFO: Pod "execpod-affinitywdm2d": Phase="Running", Reason="", readiness=true. Elapsed: 4.010279009s
    Feb  6 18:40:35.119: INFO: Pod "execpod-affinitywdm2d" satisfied condition "running"
    Feb  6 18:40:36.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-139 exec execpod-affinitywdm2d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Feb  6 18:40:36.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Feb  6 18:40:36.290: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:40:36.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-139 exec execpod-affinitywdm2d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.4.77 80'
    Feb  6 18:40:36.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.4.77 80\nConnection to 10.98.4.77 80 port [tcp/http] succeeded!\n"
    Feb  6 18:40:36.461: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:40:36.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-139 exec execpod-affinitywdm2d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.4.77:80/ ; done'
    Feb  6 18:40:36.777: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n"
    Feb  6 18:40:36.777: INFO: stdout: "\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-jp6xv\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-tkkqm\naffinity-clusterip-transition-jp6xv\naffinity-clusterip-transition-jp6xv"
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-jp6xv
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-tkkqm
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-jp6xv
    Feb  6 18:40:36.777: INFO: Received response from host: affinity-clusterip-transition-jp6xv
    Feb  6 18:40:36.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-139 exec execpod-affinitywdm2d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.4.77:80/ ; done'
    Feb  6 18:40:37.075: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.4.77:80/\n"
    Feb  6 18:40:37.075: INFO: stdout: "\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk\naffinity-clusterip-transition-246nk"
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Received response from host: affinity-clusterip-transition-246nk
    Feb  6 18:40:37.075: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-139, will wait for the garbage collector to delete the pods 02/06/23 18:40:37.091
    Feb  6 18:40:37.163: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.319813ms
    Feb  6 18:40:37.263: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.590989ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 18:40:39.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-139" for this suite. 02/06/23 18:40:39.508
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:40:39.532
Feb  6 18:40:39.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 18:40:39.533
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:39.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:39.556
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Feb  6 18:40:39.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/06/23 18:40:47.971
Feb  6 18:40:47.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-3482 --namespace=crd-publish-openapi-3482 create -f -'
Feb  6 18:40:49.560: INFO: stderr: ""
Feb  6 18:40:49.560: INFO: stdout: "e2e-test-crd-publish-openapi-2599-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb  6 18:40:49.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-3482 --namespace=crd-publish-openapi-3482 delete e2e-test-crd-publish-openapi-2599-crds test-cr'
Feb  6 18:40:49.683: INFO: stderr: ""
Feb  6 18:40:49.683: INFO: stdout: "e2e-test-crd-publish-openapi-2599-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb  6 18:40:49.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-3482 --namespace=crd-publish-openapi-3482 apply -f -'
Feb  6 18:40:50.119: INFO: stderr: ""
Feb  6 18:40:50.119: INFO: stdout: "e2e-test-crd-publish-openapi-2599-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb  6 18:40:50.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-3482 --namespace=crd-publish-openapi-3482 delete e2e-test-crd-publish-openapi-2599-crds test-cr'
Feb  6 18:40:50.213: INFO: stderr: ""
Feb  6 18:40:50.213: INFO: stdout: "e2e-test-crd-publish-openapi-2599-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/06/23 18:40:50.213
Feb  6 18:40:50.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-3482 explain e2e-test-crd-publish-openapi-2599-crds'
Feb  6 18:40:50.624: INFO: stderr: ""
Feb  6 18:40:50.624: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2599-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:40:58.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3482" for this suite. 02/06/23 18:40:58.796
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":288,"skipped":5478,"failed":0}
------------------------------
• [SLOW TEST] [19.273 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:40:39.532
    Feb  6 18:40:39.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 18:40:39.533
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:39.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:39.556
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Feb  6 18:40:39.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/06/23 18:40:47.971
    Feb  6 18:40:47.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-3482 --namespace=crd-publish-openapi-3482 create -f -'
    Feb  6 18:40:49.560: INFO: stderr: ""
    Feb  6 18:40:49.560: INFO: stdout: "e2e-test-crd-publish-openapi-2599-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb  6 18:40:49.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-3482 --namespace=crd-publish-openapi-3482 delete e2e-test-crd-publish-openapi-2599-crds test-cr'
    Feb  6 18:40:49.683: INFO: stderr: ""
    Feb  6 18:40:49.683: INFO: stdout: "e2e-test-crd-publish-openapi-2599-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Feb  6 18:40:49.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-3482 --namespace=crd-publish-openapi-3482 apply -f -'
    Feb  6 18:40:50.119: INFO: stderr: ""
    Feb  6 18:40:50.119: INFO: stdout: "e2e-test-crd-publish-openapi-2599-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb  6 18:40:50.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-3482 --namespace=crd-publish-openapi-3482 delete e2e-test-crd-publish-openapi-2599-crds test-cr'
    Feb  6 18:40:50.213: INFO: stderr: ""
    Feb  6 18:40:50.213: INFO: stdout: "e2e-test-crd-publish-openapi-2599-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/06/23 18:40:50.213
    Feb  6 18:40:50.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=crd-publish-openapi-3482 explain e2e-test-crd-publish-openapi-2599-crds'
    Feb  6 18:40:50.624: INFO: stderr: ""
    Feb  6 18:40:50.624: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2599-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:40:58.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3482" for this suite. 02/06/23 18:40:58.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:40:58.806
Feb  6 18:40:58.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename dns 02/06/23 18:40:58.807
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:58.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:58.843
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 02/06/23 18:40:58.846
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5252.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5252.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 02/06/23 18:40:58.853
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5252.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5252.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 02/06/23 18:40:58.853
STEP: creating a pod to probe DNS 02/06/23 18:40:58.853
STEP: submitting the pod to kubernetes 02/06/23 18:40:58.853
Feb  6 18:40:58.868: INFO: Waiting up to 15m0s for pod "dns-test-acf7f16d-26ee-447f-ad31-f1b576559dbf" in namespace "dns-5252" to be "running"
Feb  6 18:40:58.875: INFO: Pod "dns-test-acf7f16d-26ee-447f-ad31-f1b576559dbf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.181169ms
Feb  6 18:41:00.883: INFO: Pod "dns-test-acf7f16d-26ee-447f-ad31-f1b576559dbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014602124s
Feb  6 18:41:02.881: INFO: Pod "dns-test-acf7f16d-26ee-447f-ad31-f1b576559dbf": Phase="Running", Reason="", readiness=true. Elapsed: 4.012628818s
Feb  6 18:41:02.881: INFO: Pod "dns-test-acf7f16d-26ee-447f-ad31-f1b576559dbf" satisfied condition "running"
STEP: retrieving the pod 02/06/23 18:41:02.881
STEP: looking for the results for each expected name from probers 02/06/23 18:41:02.885
Feb  6 18:41:02.905: INFO: DNS probes using dns-5252/dns-test-acf7f16d-26ee-447f-ad31-f1b576559dbf succeeded

STEP: deleting the pod 02/06/23 18:41:02.905
STEP: deleting the test headless service 02/06/23 18:41:02.927
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb  6 18:41:02.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5252" for this suite. 02/06/23 18:41:02.951
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":289,"skipped":5495,"failed":0}
------------------------------
• [4.159 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:40:58.806
    Feb  6 18:40:58.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename dns 02/06/23 18:40:58.807
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:40:58.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:40:58.843
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 02/06/23 18:40:58.846
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5252.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5252.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     02/06/23 18:40:58.853
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5252.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5252.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     02/06/23 18:40:58.853
    STEP: creating a pod to probe DNS 02/06/23 18:40:58.853
    STEP: submitting the pod to kubernetes 02/06/23 18:40:58.853
    Feb  6 18:40:58.868: INFO: Waiting up to 15m0s for pod "dns-test-acf7f16d-26ee-447f-ad31-f1b576559dbf" in namespace "dns-5252" to be "running"
    Feb  6 18:40:58.875: INFO: Pod "dns-test-acf7f16d-26ee-447f-ad31-f1b576559dbf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.181169ms
    Feb  6 18:41:00.883: INFO: Pod "dns-test-acf7f16d-26ee-447f-ad31-f1b576559dbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014602124s
    Feb  6 18:41:02.881: INFO: Pod "dns-test-acf7f16d-26ee-447f-ad31-f1b576559dbf": Phase="Running", Reason="", readiness=true. Elapsed: 4.012628818s
    Feb  6 18:41:02.881: INFO: Pod "dns-test-acf7f16d-26ee-447f-ad31-f1b576559dbf" satisfied condition "running"
    STEP: retrieving the pod 02/06/23 18:41:02.881
    STEP: looking for the results for each expected name from probers 02/06/23 18:41:02.885
    Feb  6 18:41:02.905: INFO: DNS probes using dns-5252/dns-test-acf7f16d-26ee-447f-ad31-f1b576559dbf succeeded

    STEP: deleting the pod 02/06/23 18:41:02.905
    STEP: deleting the test headless service 02/06/23 18:41:02.927
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb  6 18:41:02.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5252" for this suite. 02/06/23 18:41:02.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:41:02.975
Feb  6 18:41:02.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename resourcequota 02/06/23 18:41:02.976
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:41:03.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:41:03.004
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 02/06/23 18:41:03.006
STEP: Creating a ResourceQuota 02/06/23 18:41:08.012
STEP: Ensuring resource quota status is calculated 02/06/23 18:41:08.02
STEP: Creating a Pod that fits quota 02/06/23 18:41:10.027
STEP: Ensuring ResourceQuota status captures the pod usage 02/06/23 18:41:10.049
STEP: Not allowing a pod to be created that exceeds remaining quota 02/06/23 18:41:12.055
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/06/23 18:41:12.058
STEP: Ensuring a pod cannot update its resource requirements 02/06/23 18:41:12.06
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/06/23 18:41:12.066
STEP: Deleting the pod 02/06/23 18:41:14.074
STEP: Ensuring resource quota status released the pod usage 02/06/23 18:41:14.096
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb  6 18:41:16.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9711" for this suite. 02/06/23 18:41:16.115
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":290,"skipped":5534,"failed":0}
------------------------------
• [SLOW TEST] [13.150 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:41:02.975
    Feb  6 18:41:02.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename resourcequota 02/06/23 18:41:02.976
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:41:03.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:41:03.004
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 02/06/23 18:41:03.006
    STEP: Creating a ResourceQuota 02/06/23 18:41:08.012
    STEP: Ensuring resource quota status is calculated 02/06/23 18:41:08.02
    STEP: Creating a Pod that fits quota 02/06/23 18:41:10.027
    STEP: Ensuring ResourceQuota status captures the pod usage 02/06/23 18:41:10.049
    STEP: Not allowing a pod to be created that exceeds remaining quota 02/06/23 18:41:12.055
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/06/23 18:41:12.058
    STEP: Ensuring a pod cannot update its resource requirements 02/06/23 18:41:12.06
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/06/23 18:41:12.066
    STEP: Deleting the pod 02/06/23 18:41:14.074
    STEP: Ensuring resource quota status released the pod usage 02/06/23 18:41:14.096
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb  6 18:41:16.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9711" for this suite. 02/06/23 18:41:16.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:41:16.129
Feb  6 18:41:16.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename subpath 02/06/23 18:41:16.13
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:41:16.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:41:16.156
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/06/23 18:41:16.159
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-ndmt 02/06/23 18:41:16.171
STEP: Creating a pod to test atomic-volume-subpath 02/06/23 18:41:16.171
Feb  6 18:41:16.184: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ndmt" in namespace "subpath-9407" to be "Succeeded or Failed"
Feb  6 18:41:16.193: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.247001ms
Feb  6 18:41:18.207: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022965242s
Feb  6 18:41:20.199: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 4.015000962s
Feb  6 18:41:22.198: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 6.0135882s
Feb  6 18:41:24.199: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 8.01442592s
Feb  6 18:41:26.198: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 10.013525792s
Feb  6 18:41:28.198: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 12.013970312s
Feb  6 18:41:30.197: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 14.012834504s
Feb  6 18:41:32.198: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 16.013765306s
Feb  6 18:41:34.202: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 18.018187499s
Feb  6 18:41:36.202: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 20.017346434s
Feb  6 18:41:38.198: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 22.01400063s
Feb  6 18:41:40.200: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=false. Elapsed: 24.015569334s
Feb  6 18:41:42.198: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.013328037s
STEP: Saw pod success 02/06/23 18:41:42.198
Feb  6 18:41:42.198: INFO: Pod "pod-subpath-test-configmap-ndmt" satisfied condition "Succeeded or Failed"
Feb  6 18:41:42.203: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-subpath-test-configmap-ndmt container test-container-subpath-configmap-ndmt: <nil>
STEP: delete the pod 02/06/23 18:41:42.225
Feb  6 18:41:42.244: INFO: Waiting for pod pod-subpath-test-configmap-ndmt to disappear
Feb  6 18:41:42.247: INFO: Pod pod-subpath-test-configmap-ndmt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ndmt 02/06/23 18:41:42.247
Feb  6 18:41:42.248: INFO: Deleting pod "pod-subpath-test-configmap-ndmt" in namespace "subpath-9407"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb  6 18:41:42.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9407" for this suite. 02/06/23 18:41:42.257
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":291,"skipped":5552,"failed":0}
------------------------------
• [SLOW TEST] [26.136 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:41:16.129
    Feb  6 18:41:16.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename subpath 02/06/23 18:41:16.13
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:41:16.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:41:16.156
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/06/23 18:41:16.159
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-ndmt 02/06/23 18:41:16.171
    STEP: Creating a pod to test atomic-volume-subpath 02/06/23 18:41:16.171
    Feb  6 18:41:16.184: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ndmt" in namespace "subpath-9407" to be "Succeeded or Failed"
    Feb  6 18:41:16.193: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.247001ms
    Feb  6 18:41:18.207: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022965242s
    Feb  6 18:41:20.199: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 4.015000962s
    Feb  6 18:41:22.198: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 6.0135882s
    Feb  6 18:41:24.199: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 8.01442592s
    Feb  6 18:41:26.198: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 10.013525792s
    Feb  6 18:41:28.198: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 12.013970312s
    Feb  6 18:41:30.197: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 14.012834504s
    Feb  6 18:41:32.198: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 16.013765306s
    Feb  6 18:41:34.202: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 18.018187499s
    Feb  6 18:41:36.202: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 20.017346434s
    Feb  6 18:41:38.198: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=true. Elapsed: 22.01400063s
    Feb  6 18:41:40.200: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Running", Reason="", readiness=false. Elapsed: 24.015569334s
    Feb  6 18:41:42.198: INFO: Pod "pod-subpath-test-configmap-ndmt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.013328037s
    STEP: Saw pod success 02/06/23 18:41:42.198
    Feb  6 18:41:42.198: INFO: Pod "pod-subpath-test-configmap-ndmt" satisfied condition "Succeeded or Failed"
    Feb  6 18:41:42.203: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-subpath-test-configmap-ndmt container test-container-subpath-configmap-ndmt: <nil>
    STEP: delete the pod 02/06/23 18:41:42.225
    Feb  6 18:41:42.244: INFO: Waiting for pod pod-subpath-test-configmap-ndmt to disappear
    Feb  6 18:41:42.247: INFO: Pod pod-subpath-test-configmap-ndmt no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-ndmt 02/06/23 18:41:42.247
    Feb  6 18:41:42.248: INFO: Deleting pod "pod-subpath-test-configmap-ndmt" in namespace "subpath-9407"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb  6 18:41:42.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9407" for this suite. 02/06/23 18:41:42.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:41:42.266
Feb  6 18:41:42.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename replication-controller 02/06/23 18:41:42.267
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:41:42.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:41:42.289
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Feb  6 18:41:42.291: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/06/23 18:41:43.305
STEP: Checking rc "condition-test" has the desired failure condition set 02/06/23 18:41:43.313
STEP: Scaling down rc "condition-test" to satisfy pod quota 02/06/23 18:41:44.323
Feb  6 18:41:44.337: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 02/06/23 18:41:44.337
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb  6 18:41:44.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1906" for this suite. 02/06/23 18:41:44.349
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":292,"skipped":5573,"failed":0}
------------------------------
• [2.092 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:41:42.266
    Feb  6 18:41:42.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename replication-controller 02/06/23 18:41:42.267
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:41:42.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:41:42.289
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Feb  6 18:41:42.291: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/06/23 18:41:43.305
    STEP: Checking rc "condition-test" has the desired failure condition set 02/06/23 18:41:43.313
    STEP: Scaling down rc "condition-test" to satisfy pod quota 02/06/23 18:41:44.323
    Feb  6 18:41:44.337: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 02/06/23 18:41:44.337
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb  6 18:41:44.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1906" for this suite. 02/06/23 18:41:44.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:41:44.374
Feb  6 18:41:44.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename statefulset 02/06/23 18:41:44.376
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:41:44.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:41:44.402
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-584 02/06/23 18:41:44.409
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Feb  6 18:41:44.437: INFO: Found 0 stateful pods, waiting for 1
Feb  6 18:41:54.442: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 02/06/23 18:41:54.449
W0206 18:41:54.462514      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb  6 18:41:54.479: INFO: Found 1 stateful pods, waiting for 2
Feb  6 18:42:04.486: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  6 18:42:04.486: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 02/06/23 18:42:04.494
STEP: Delete all of the StatefulSets 02/06/23 18:42:04.499
STEP: Verify that StatefulSets have been deleted 02/06/23 18:42:04.509
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb  6 18:42:04.514: INFO: Deleting all statefulset in ns statefulset-584
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb  6 18:42:04.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-584" for this suite. 02/06/23 18:42:04.536
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":293,"skipped":5594,"failed":0}
------------------------------
• [SLOW TEST] [20.169 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:41:44.374
    Feb  6 18:41:44.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename statefulset 02/06/23 18:41:44.376
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:41:44.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:41:44.402
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-584 02/06/23 18:41:44.409
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Feb  6 18:41:44.437: INFO: Found 0 stateful pods, waiting for 1
    Feb  6 18:41:54.442: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 02/06/23 18:41:54.449
    W0206 18:41:54.462514      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb  6 18:41:54.479: INFO: Found 1 stateful pods, waiting for 2
    Feb  6 18:42:04.486: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb  6 18:42:04.486: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 02/06/23 18:42:04.494
    STEP: Delete all of the StatefulSets 02/06/23 18:42:04.499
    STEP: Verify that StatefulSets have been deleted 02/06/23 18:42:04.509
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb  6 18:42:04.514: INFO: Deleting all statefulset in ns statefulset-584
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb  6 18:42:04.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-584" for this suite. 02/06/23 18:42:04.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:42:04.544
Feb  6 18:42:04.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename replicaset 02/06/23 18:42:04.545
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:42:04.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:42:04.579
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 02/06/23 18:42:04.582
STEP: Verify that the required pods have come up 02/06/23 18:42:04.588
Feb  6 18:42:04.591: INFO: Pod name sample-pod: Found 0 pods out of 3
Feb  6 18:42:09.597: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 02/06/23 18:42:09.597
Feb  6 18:42:09.601: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 02/06/23 18:42:09.601
STEP: DeleteCollection of the ReplicaSets 02/06/23 18:42:09.607
STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/06/23 18:42:09.618
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb  6 18:42:09.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8678" for this suite. 02/06/23 18:42:09.633
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":294,"skipped":5599,"failed":0}
------------------------------
• [SLOW TEST] [5.104 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:42:04.544
    Feb  6 18:42:04.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename replicaset 02/06/23 18:42:04.545
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:42:04.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:42:04.579
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 02/06/23 18:42:04.582
    STEP: Verify that the required pods have come up 02/06/23 18:42:04.588
    Feb  6 18:42:04.591: INFO: Pod name sample-pod: Found 0 pods out of 3
    Feb  6 18:42:09.597: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 02/06/23 18:42:09.597
    Feb  6 18:42:09.601: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 02/06/23 18:42:09.601
    STEP: DeleteCollection of the ReplicaSets 02/06/23 18:42:09.607
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/06/23 18:42:09.618
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb  6 18:42:09.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8678" for this suite. 02/06/23 18:42:09.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:42:09.65
Feb  6 18:42:09.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 18:42:09.651
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:42:09.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:42:09.697
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 18:42:09.721
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:42:10.061
STEP: Deploying the webhook pod 02/06/23 18:42:10.071
STEP: Wait for the deployment to be ready 02/06/23 18:42:10.088
Feb  6 18:42:10.101: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  6 18:42:12.117: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 42, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 42, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 42, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 42, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 18:42:14.122
STEP: Verifying the service has paired with the endpoint 02/06/23 18:42:14.162
Feb  6 18:42:15.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/06/23 18:42:15.167
STEP: create a namespace for the webhook 02/06/23 18:42:15.195
STEP: create a configmap should be unconditionally rejected by the webhook 02/06/23 18:42:15.207
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:42:15.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4136" for this suite. 02/06/23 18:42:15.284
STEP: Destroying namespace "webhook-4136-markers" for this suite. 02/06/23 18:42:15.301
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":295,"skipped":5604,"failed":0}
------------------------------
• [SLOW TEST] [5.731 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:42:09.65
    Feb  6 18:42:09.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 18:42:09.651
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:42:09.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:42:09.697
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 18:42:09.721
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:42:10.061
    STEP: Deploying the webhook pod 02/06/23 18:42:10.071
    STEP: Wait for the deployment to be ready 02/06/23 18:42:10.088
    Feb  6 18:42:10.101: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb  6 18:42:12.117: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 42, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 42, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 42, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 42, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 18:42:14.122
    STEP: Verifying the service has paired with the endpoint 02/06/23 18:42:14.162
    Feb  6 18:42:15.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/06/23 18:42:15.167
    STEP: create a namespace for the webhook 02/06/23 18:42:15.195
    STEP: create a configmap should be unconditionally rejected by the webhook 02/06/23 18:42:15.207
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:42:15.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4136" for this suite. 02/06/23 18:42:15.284
    STEP: Destroying namespace "webhook-4136-markers" for this suite. 02/06/23 18:42:15.301
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:42:15.382
Feb  6 18:42:15.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename csistoragecapacity 02/06/23 18:42:15.383
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:42:15.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:42:15.425
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 02/06/23 18:42:15.429
STEP: getting /apis/storage.k8s.io 02/06/23 18:42:15.432
STEP: getting /apis/storage.k8s.io/v1 02/06/23 18:42:15.433
STEP: creating 02/06/23 18:42:15.434
STEP: watching 02/06/23 18:42:15.462
Feb  6 18:42:15.463: INFO: starting watch
STEP: getting 02/06/23 18:42:15.471
STEP: listing in namespace 02/06/23 18:42:15.475
STEP: listing across namespaces 02/06/23 18:42:15.478
STEP: patching 02/06/23 18:42:15.482
STEP: updating 02/06/23 18:42:15.493
Feb  6 18:42:15.499: INFO: waiting for watch events with expected annotations in namespace
Feb  6 18:42:15.500: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 02/06/23 18:42:15.5
STEP: deleting a collection 02/06/23 18:42:15.519
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Feb  6 18:42:15.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-3843" for this suite. 02/06/23 18:42:15.547
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":296,"skipped":5635,"failed":0}
------------------------------
• [0.173 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:42:15.382
    Feb  6 18:42:15.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename csistoragecapacity 02/06/23 18:42:15.383
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:42:15.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:42:15.425
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 02/06/23 18:42:15.429
    STEP: getting /apis/storage.k8s.io 02/06/23 18:42:15.432
    STEP: getting /apis/storage.k8s.io/v1 02/06/23 18:42:15.433
    STEP: creating 02/06/23 18:42:15.434
    STEP: watching 02/06/23 18:42:15.462
    Feb  6 18:42:15.463: INFO: starting watch
    STEP: getting 02/06/23 18:42:15.471
    STEP: listing in namespace 02/06/23 18:42:15.475
    STEP: listing across namespaces 02/06/23 18:42:15.478
    STEP: patching 02/06/23 18:42:15.482
    STEP: updating 02/06/23 18:42:15.493
    Feb  6 18:42:15.499: INFO: waiting for watch events with expected annotations in namespace
    Feb  6 18:42:15.500: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 02/06/23 18:42:15.5
    STEP: deleting a collection 02/06/23 18:42:15.519
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Feb  6 18:42:15.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-3843" for this suite. 02/06/23 18:42:15.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:42:15.559
Feb  6 18:42:15.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename dns 02/06/23 18:42:15.56
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:42:15.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:42:15.585
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 02/06/23 18:42:15.588
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8.svc;check="$$(dig +notcp +noall +answer +search 185.49.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.49.185_udp@PTR;check="$$(dig +tcp +noall +answer +search 185.49.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.49.185_tcp@PTR;sleep 1; done
 02/06/23 18:42:15.623
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8.svc;check="$$(dig +notcp +noall +answer +search 185.49.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.49.185_udp@PTR;check="$$(dig +tcp +noall +answer +search 185.49.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.49.185_tcp@PTR;sleep 1; done
 02/06/23 18:42:15.623
STEP: creating a pod to probe DNS 02/06/23 18:42:15.623
STEP: submitting the pod to kubernetes 02/06/23 18:42:15.623
Feb  6 18:42:15.646: INFO: Waiting up to 15m0s for pod "dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3" in namespace "dns-8" to be "running"
Feb  6 18:42:15.650: INFO: Pod "dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.796238ms
Feb  6 18:42:17.655: INFO: Pod "dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00881357s
Feb  6 18:42:19.655: INFO: Pod "dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009223441s
Feb  6 18:42:21.656: INFO: Pod "dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3": Phase="Running", Reason="", readiness=true. Elapsed: 6.010231631s
Feb  6 18:42:21.656: INFO: Pod "dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3" satisfied condition "running"
STEP: retrieving the pod 02/06/23 18:42:21.656
STEP: looking for the results for each expected name from probers 02/06/23 18:42:21.66
Feb  6 18:42:21.668: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.672: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.676: INFO: Unable to read wheezy_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.680: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.684: INFO: Unable to read wheezy_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.690: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.694: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.698: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.728: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.734: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.739: INFO: Unable to read jessie_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.744: INFO: Unable to read jessie_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.749: INFO: Unable to read jessie_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.752: INFO: Unable to read jessie_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.757: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.761: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:21.777: INFO: Lookups using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8 wheezy_tcp@dns-test-service.dns-8 wheezy_udp@dns-test-service.dns-8.svc wheezy_tcp@dns-test-service.dns-8.svc wheezy_udp@_http._tcp.dns-test-service.dns-8.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8 jessie_tcp@dns-test-service.dns-8 jessie_udp@dns-test-service.dns-8.svc jessie_tcp@dns-test-service.dns-8.svc jessie_udp@_http._tcp.dns-test-service.dns-8.svc jessie_tcp@_http._tcp.dns-test-service.dns-8.svc]

Feb  6 18:42:26.784: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.788: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.792: INFO: Unable to read wheezy_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.800: INFO: Unable to read wheezy_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.804: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.808: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.812: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.831: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.835: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.839: INFO: Unable to read jessie_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.843: INFO: Unable to read jessie_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.847: INFO: Unable to read jessie_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.851: INFO: Unable to read jessie_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.855: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.859: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:26.875: INFO: Lookups using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8 wheezy_tcp@dns-test-service.dns-8 wheezy_udp@dns-test-service.dns-8.svc wheezy_tcp@dns-test-service.dns-8.svc wheezy_udp@_http._tcp.dns-test-service.dns-8.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8 jessie_tcp@dns-test-service.dns-8 jessie_udp@dns-test-service.dns-8.svc jessie_tcp@dns-test-service.dns-8.svc jessie_udp@_http._tcp.dns-test-service.dns-8.svc jessie_tcp@_http._tcp.dns-test-service.dns-8.svc]

Feb  6 18:42:31.787: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.791: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.797: INFO: Unable to read wheezy_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.802: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.807: INFO: Unable to read wheezy_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.812: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.816: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.820: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.841: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.845: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.851: INFO: Unable to read jessie_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.855: INFO: Unable to read jessie_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.859: INFO: Unable to read jessie_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.864: INFO: Unable to read jessie_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.868: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.875: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:31.890: INFO: Lookups using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8 wheezy_tcp@dns-test-service.dns-8 wheezy_udp@dns-test-service.dns-8.svc wheezy_tcp@dns-test-service.dns-8.svc wheezy_udp@_http._tcp.dns-test-service.dns-8.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8 jessie_tcp@dns-test-service.dns-8 jessie_udp@dns-test-service.dns-8.svc jessie_tcp@dns-test-service.dns-8.svc jessie_udp@_http._tcp.dns-test-service.dns-8.svc jessie_tcp@_http._tcp.dns-test-service.dns-8.svc]

Feb  6 18:42:36.784: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.789: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.795: INFO: Unable to read wheezy_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.800: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.804: INFO: Unable to read wheezy_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.809: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.814: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.818: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.839: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.843: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.849: INFO: Unable to read jessie_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.855: INFO: Unable to read jessie_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.860: INFO: Unable to read jessie_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.869: INFO: Unable to read jessie_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.875: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.879: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:36.896: INFO: Lookups using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8 wheezy_tcp@dns-test-service.dns-8 wheezy_udp@dns-test-service.dns-8.svc wheezy_tcp@dns-test-service.dns-8.svc wheezy_udp@_http._tcp.dns-test-service.dns-8.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8 jessie_tcp@dns-test-service.dns-8 jessie_udp@dns-test-service.dns-8.svc jessie_tcp@dns-test-service.dns-8.svc jessie_udp@_http._tcp.dns-test-service.dns-8.svc jessie_tcp@_http._tcp.dns-test-service.dns-8.svc]

Feb  6 18:42:41.784: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.789: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.803: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.807: INFO: Unable to read wheezy_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.811: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.815: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.819: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.846: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.850: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.854: INFO: Unable to read jessie_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.860: INFO: Unable to read jessie_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.866: INFO: Unable to read jessie_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.871: INFO: Unable to read jessie_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.875: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.879: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:41.894: INFO: Lookups using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8 wheezy_tcp@dns-test-service.dns-8 wheezy_udp@dns-test-service.dns-8.svc wheezy_tcp@dns-test-service.dns-8.svc wheezy_udp@_http._tcp.dns-test-service.dns-8.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8 jessie_tcp@dns-test-service.dns-8 jessie_udp@dns-test-service.dns-8.svc jessie_tcp@dns-test-service.dns-8.svc jessie_udp@_http._tcp.dns-test-service.dns-8.svc jessie_tcp@_http._tcp.dns-test-service.dns-8.svc]

Feb  6 18:42:46.859: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.891: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.895: INFO: Unable to read wheezy_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.899: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.906: INFO: Unable to read wheezy_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.910: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.917: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.921: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.941: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.946: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.950: INFO: Unable to read jessie_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.957: INFO: Unable to read jessie_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.966: INFO: Unable to read jessie_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.972: INFO: Unable to read jessie_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.977: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.982: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
Feb  6 18:42:46.997: INFO: Lookups using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8 wheezy_tcp@dns-test-service.dns-8 wheezy_udp@dns-test-service.dns-8.svc wheezy_tcp@dns-test-service.dns-8.svc wheezy_udp@_http._tcp.dns-test-service.dns-8.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8 jessie_tcp@dns-test-service.dns-8 jessie_udp@dns-test-service.dns-8.svc jessie_tcp@dns-test-service.dns-8.svc jessie_udp@_http._tcp.dns-test-service.dns-8.svc jessie_tcp@_http._tcp.dns-test-service.dns-8.svc]

Feb  6 18:42:51.886: INFO: DNS probes using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 succeeded

STEP: deleting the pod 02/06/23 18:42:51.886
STEP: deleting the test service 02/06/23 18:42:51.902
STEP: deleting the test headless service 02/06/23 18:42:51.959
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb  6 18:42:52.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8" for this suite. 02/06/23 18:42:52.007
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":297,"skipped":5659,"failed":0}
------------------------------
• [SLOW TEST] [36.458 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:42:15.559
    Feb  6 18:42:15.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename dns 02/06/23 18:42:15.56
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:42:15.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:42:15.585
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 02/06/23 18:42:15.588
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8.svc;check="$$(dig +notcp +noall +answer +search 185.49.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.49.185_udp@PTR;check="$$(dig +tcp +noall +answer +search 185.49.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.49.185_tcp@PTR;sleep 1; done
     02/06/23 18:42:15.623
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8.svc;check="$$(dig +notcp +noall +answer +search 185.49.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.49.185_udp@PTR;check="$$(dig +tcp +noall +answer +search 185.49.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.49.185_tcp@PTR;sleep 1; done
     02/06/23 18:42:15.623
    STEP: creating a pod to probe DNS 02/06/23 18:42:15.623
    STEP: submitting the pod to kubernetes 02/06/23 18:42:15.623
    Feb  6 18:42:15.646: INFO: Waiting up to 15m0s for pod "dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3" in namespace "dns-8" to be "running"
    Feb  6 18:42:15.650: INFO: Pod "dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.796238ms
    Feb  6 18:42:17.655: INFO: Pod "dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00881357s
    Feb  6 18:42:19.655: INFO: Pod "dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009223441s
    Feb  6 18:42:21.656: INFO: Pod "dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3": Phase="Running", Reason="", readiness=true. Elapsed: 6.010231631s
    Feb  6 18:42:21.656: INFO: Pod "dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3" satisfied condition "running"
    STEP: retrieving the pod 02/06/23 18:42:21.656
    STEP: looking for the results for each expected name from probers 02/06/23 18:42:21.66
    Feb  6 18:42:21.668: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.672: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.676: INFO: Unable to read wheezy_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.680: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.684: INFO: Unable to read wheezy_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.690: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.694: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.698: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.728: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.734: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.739: INFO: Unable to read jessie_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.744: INFO: Unable to read jessie_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.749: INFO: Unable to read jessie_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.752: INFO: Unable to read jessie_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.757: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.761: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:21.777: INFO: Lookups using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8 wheezy_tcp@dns-test-service.dns-8 wheezy_udp@dns-test-service.dns-8.svc wheezy_tcp@dns-test-service.dns-8.svc wheezy_udp@_http._tcp.dns-test-service.dns-8.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8 jessie_tcp@dns-test-service.dns-8 jessie_udp@dns-test-service.dns-8.svc jessie_tcp@dns-test-service.dns-8.svc jessie_udp@_http._tcp.dns-test-service.dns-8.svc jessie_tcp@_http._tcp.dns-test-service.dns-8.svc]

    Feb  6 18:42:26.784: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.788: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.792: INFO: Unable to read wheezy_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.800: INFO: Unable to read wheezy_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.804: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.808: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.812: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.831: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.835: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.839: INFO: Unable to read jessie_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.843: INFO: Unable to read jessie_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.847: INFO: Unable to read jessie_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.851: INFO: Unable to read jessie_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.855: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.859: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:26.875: INFO: Lookups using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8 wheezy_tcp@dns-test-service.dns-8 wheezy_udp@dns-test-service.dns-8.svc wheezy_tcp@dns-test-service.dns-8.svc wheezy_udp@_http._tcp.dns-test-service.dns-8.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8 jessie_tcp@dns-test-service.dns-8 jessie_udp@dns-test-service.dns-8.svc jessie_tcp@dns-test-service.dns-8.svc jessie_udp@_http._tcp.dns-test-service.dns-8.svc jessie_tcp@_http._tcp.dns-test-service.dns-8.svc]

    Feb  6 18:42:31.787: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.791: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.797: INFO: Unable to read wheezy_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.802: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.807: INFO: Unable to read wheezy_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.812: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.816: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.820: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.841: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.845: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.851: INFO: Unable to read jessie_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.855: INFO: Unable to read jessie_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.859: INFO: Unable to read jessie_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.864: INFO: Unable to read jessie_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.868: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.875: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:31.890: INFO: Lookups using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8 wheezy_tcp@dns-test-service.dns-8 wheezy_udp@dns-test-service.dns-8.svc wheezy_tcp@dns-test-service.dns-8.svc wheezy_udp@_http._tcp.dns-test-service.dns-8.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8 jessie_tcp@dns-test-service.dns-8 jessie_udp@dns-test-service.dns-8.svc jessie_tcp@dns-test-service.dns-8.svc jessie_udp@_http._tcp.dns-test-service.dns-8.svc jessie_tcp@_http._tcp.dns-test-service.dns-8.svc]

    Feb  6 18:42:36.784: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.789: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.795: INFO: Unable to read wheezy_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.800: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.804: INFO: Unable to read wheezy_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.809: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.814: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.818: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.839: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.843: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.849: INFO: Unable to read jessie_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.855: INFO: Unable to read jessie_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.860: INFO: Unable to read jessie_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.869: INFO: Unable to read jessie_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.875: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.879: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:36.896: INFO: Lookups using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8 wheezy_tcp@dns-test-service.dns-8 wheezy_udp@dns-test-service.dns-8.svc wheezy_tcp@dns-test-service.dns-8.svc wheezy_udp@_http._tcp.dns-test-service.dns-8.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8 jessie_tcp@dns-test-service.dns-8 jessie_udp@dns-test-service.dns-8.svc jessie_tcp@dns-test-service.dns-8.svc jessie_udp@_http._tcp.dns-test-service.dns-8.svc jessie_tcp@_http._tcp.dns-test-service.dns-8.svc]

    Feb  6 18:42:41.784: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.789: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.803: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.807: INFO: Unable to read wheezy_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.811: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.815: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.819: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.846: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.850: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.854: INFO: Unable to read jessie_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.860: INFO: Unable to read jessie_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.866: INFO: Unable to read jessie_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.871: INFO: Unable to read jessie_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.875: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.879: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:41.894: INFO: Lookups using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8 wheezy_tcp@dns-test-service.dns-8 wheezy_udp@dns-test-service.dns-8.svc wheezy_tcp@dns-test-service.dns-8.svc wheezy_udp@_http._tcp.dns-test-service.dns-8.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8 jessie_tcp@dns-test-service.dns-8 jessie_udp@dns-test-service.dns-8.svc jessie_tcp@dns-test-service.dns-8.svc jessie_udp@_http._tcp.dns-test-service.dns-8.svc jessie_tcp@_http._tcp.dns-test-service.dns-8.svc]

    Feb  6 18:42:46.859: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.891: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.895: INFO: Unable to read wheezy_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.899: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.906: INFO: Unable to read wheezy_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.910: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.917: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.921: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.941: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.946: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.950: INFO: Unable to read jessie_udp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.957: INFO: Unable to read jessie_tcp@dns-test-service.dns-8 from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.966: INFO: Unable to read jessie_udp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.972: INFO: Unable to read jessie_tcp@dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.977: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.982: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8.svc from pod dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3: the server could not find the requested resource (get pods dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3)
    Feb  6 18:42:46.997: INFO: Lookups using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8 wheezy_tcp@dns-test-service.dns-8 wheezy_udp@dns-test-service.dns-8.svc wheezy_tcp@dns-test-service.dns-8.svc wheezy_udp@_http._tcp.dns-test-service.dns-8.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8 jessie_tcp@dns-test-service.dns-8 jessie_udp@dns-test-service.dns-8.svc jessie_tcp@dns-test-service.dns-8.svc jessie_udp@_http._tcp.dns-test-service.dns-8.svc jessie_tcp@_http._tcp.dns-test-service.dns-8.svc]

    Feb  6 18:42:51.886: INFO: DNS probes using dns-8/dns-test-e368aa65-ce69-4350-885d-a8ec192b7ef3 succeeded

    STEP: deleting the pod 02/06/23 18:42:51.886
    STEP: deleting the test service 02/06/23 18:42:51.902
    STEP: deleting the test headless service 02/06/23 18:42:51.959
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb  6 18:42:52.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8" for this suite. 02/06/23 18:42:52.007
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:42:52.025
Feb  6 18:42:52.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pods 02/06/23 18:42:52.027
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:42:52.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:42:52.056
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Feb  6 18:42:52.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: creating the pod 02/06/23 18:42:52.061
STEP: submitting the pod to kubernetes 02/06/23 18:42:52.062
Feb  6 18:42:52.081: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae" in namespace "pods-7283" to be "running and ready"
Feb  6 18:42:52.085: INFO: Pod "pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae": Phase="Pending", Reason="", readiness=false. Elapsed: 3.563568ms
Feb  6 18:42:52.085: INFO: The phase of Pod pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:42:54.089: INFO: Pod "pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007868711s
Feb  6 18:42:54.089: INFO: The phase of Pod pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:42:56.089: INFO: Pod "pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae": Phase="Running", Reason="", readiness=true. Elapsed: 4.008062058s
Feb  6 18:42:56.089: INFO: The phase of Pod pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae is Running (Ready = true)
Feb  6 18:42:56.089: INFO: Pod "pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb  6 18:42:56.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7283" for this suite. 02/06/23 18:42:56.201
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":298,"skipped":5661,"failed":0}
------------------------------
• [4.187 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:42:52.025
    Feb  6 18:42:52.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pods 02/06/23 18:42:52.027
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:42:52.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:42:52.056
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Feb  6 18:42:52.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: creating the pod 02/06/23 18:42:52.061
    STEP: submitting the pod to kubernetes 02/06/23 18:42:52.062
    Feb  6 18:42:52.081: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae" in namespace "pods-7283" to be "running and ready"
    Feb  6 18:42:52.085: INFO: Pod "pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae": Phase="Pending", Reason="", readiness=false. Elapsed: 3.563568ms
    Feb  6 18:42:52.085: INFO: The phase of Pod pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:42:54.089: INFO: Pod "pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007868711s
    Feb  6 18:42:54.089: INFO: The phase of Pod pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:42:56.089: INFO: Pod "pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae": Phase="Running", Reason="", readiness=true. Elapsed: 4.008062058s
    Feb  6 18:42:56.089: INFO: The phase of Pod pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae is Running (Ready = true)
    Feb  6 18:42:56.089: INFO: Pod "pod-exec-websocket-58a563a8-b88d-4ffa-b528-0f2ccb718dae" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb  6 18:42:56.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7283" for this suite. 02/06/23 18:42:56.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:42:56.219
Feb  6 18:42:56.219: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename daemonsets 02/06/23 18:42:56.22
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:42:56.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:42:56.242
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Feb  6 18:42:56.275: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 02/06/23 18:42:56.303
Feb  6 18:42:56.313: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:42:56.313: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 02/06/23 18:42:56.313
Feb  6 18:42:56.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:42:56.421: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
Feb  6 18:42:57.428: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:42:57.428: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
Feb  6 18:42:58.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:42:58.430: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
Feb  6 18:42:59.427: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:42:59.427: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
Feb  6 18:43:00.430: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb  6 18:43:00.430: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 02/06/23 18:43:00.434
Feb  6 18:43:00.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb  6 18:43:00.475: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Feb  6 18:43:01.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:43:01.482: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/06/23 18:43:01.482
Feb  6 18:43:01.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:43:01.507: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
Feb  6 18:43:02.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:43:02.513: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
Feb  6 18:43:03.512: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:43:03.512: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
Feb  6 18:43:04.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:43:04.517: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
Feb  6 18:43:05.515: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:43:05.515: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
Feb  6 18:43:06.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb  6 18:43:06.513: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/06/23 18:43:06.522
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3802, will wait for the garbage collector to delete the pods 02/06/23 18:43:06.522
Feb  6 18:43:06.587: INFO: Deleting DaemonSet.extensions daemon-set took: 7.949985ms
Feb  6 18:43:06.687: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.685935ms
Feb  6 18:43:08.792: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb  6 18:43:08.792: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb  6 18:43:08.797: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74720"},"items":null}

Feb  6 18:43:08.800: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74720"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb  6 18:43:08.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3802" for this suite. 02/06/23 18:43:08.842
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":299,"skipped":5697,"failed":0}
------------------------------
• [SLOW TEST] [12.639 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:42:56.219
    Feb  6 18:42:56.219: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename daemonsets 02/06/23 18:42:56.22
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:42:56.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:42:56.242
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Feb  6 18:42:56.275: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 02/06/23 18:42:56.303
    Feb  6 18:42:56.313: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:42:56.313: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 02/06/23 18:42:56.313
    Feb  6 18:42:56.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:42:56.421: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
    Feb  6 18:42:57.428: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:42:57.428: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
    Feb  6 18:42:58.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:42:58.430: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
    Feb  6 18:42:59.427: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:42:59.427: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
    Feb  6 18:43:00.430: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb  6 18:43:00.430: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 02/06/23 18:43:00.434
    Feb  6 18:43:00.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb  6 18:43:00.475: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Feb  6 18:43:01.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:43:01.482: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/06/23 18:43:01.482
    Feb  6 18:43:01.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:43:01.507: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
    Feb  6 18:43:02.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:43:02.513: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
    Feb  6 18:43:03.512: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:43:03.512: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
    Feb  6 18:43:04.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:43:04.517: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
    Feb  6 18:43:05.515: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:43:05.515: INFO: Node tneyla25-md-0-68cbcb4798-lz542 is running 0 daemon pod, expected 1
    Feb  6 18:43:06.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb  6 18:43:06.513: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/06/23 18:43:06.522
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3802, will wait for the garbage collector to delete the pods 02/06/23 18:43:06.522
    Feb  6 18:43:06.587: INFO: Deleting DaemonSet.extensions daemon-set took: 7.949985ms
    Feb  6 18:43:06.687: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.685935ms
    Feb  6 18:43:08.792: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb  6 18:43:08.792: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb  6 18:43:08.797: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74720"},"items":null}

    Feb  6 18:43:08.800: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74720"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 18:43:08.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3802" for this suite. 02/06/23 18:43:08.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:43:08.871
Feb  6 18:43:08.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-probe 02/06/23 18:43:08.873
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:43:08.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:43:08.898
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf in namespace container-probe-9175 02/06/23 18:43:08.9
Feb  6 18:43:08.910: INFO: Waiting up to 5m0s for pod "test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf" in namespace "container-probe-9175" to be "not pending"
Feb  6 18:43:08.915: INFO: Pod "test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.401279ms
Feb  6 18:43:10.927: INFO: Pod "test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017133755s
Feb  6 18:43:12.922: INFO: Pod "test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf": Phase="Running", Reason="", readiness=true. Elapsed: 4.011604165s
Feb  6 18:43:12.922: INFO: Pod "test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf" satisfied condition "not pending"
Feb  6 18:43:12.922: INFO: Started pod test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf in namespace container-probe-9175
STEP: checking the pod's current state and verifying that restartCount is present 02/06/23 18:43:12.922
Feb  6 18:43:12.926: INFO: Initial restart count of pod test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf is 0
STEP: deleting the pod 02/06/23 18:47:13.638
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb  6 18:47:13.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9175" for this suite. 02/06/23 18:47:13.661
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":300,"skipped":5709,"failed":0}
------------------------------
• [SLOW TEST] [244.798 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:43:08.871
    Feb  6 18:43:08.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-probe 02/06/23 18:43:08.873
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:43:08.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:43:08.898
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf in namespace container-probe-9175 02/06/23 18:43:08.9
    Feb  6 18:43:08.910: INFO: Waiting up to 5m0s for pod "test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf" in namespace "container-probe-9175" to be "not pending"
    Feb  6 18:43:08.915: INFO: Pod "test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.401279ms
    Feb  6 18:43:10.927: INFO: Pod "test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017133755s
    Feb  6 18:43:12.922: INFO: Pod "test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf": Phase="Running", Reason="", readiness=true. Elapsed: 4.011604165s
    Feb  6 18:43:12.922: INFO: Pod "test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf" satisfied condition "not pending"
    Feb  6 18:43:12.922: INFO: Started pod test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf in namespace container-probe-9175
    STEP: checking the pod's current state and verifying that restartCount is present 02/06/23 18:43:12.922
    Feb  6 18:43:12.926: INFO: Initial restart count of pod test-webserver-f1ec6954-a12c-415a-95d7-05ecc0fc83bf is 0
    STEP: deleting the pod 02/06/23 18:47:13.638
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb  6 18:47:13.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9175" for this suite. 02/06/23 18:47:13.661
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:47:13.67
Feb  6 18:47:13.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/06/23 18:47:13.672
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:13.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:13.704
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 02/06/23 18:47:13.707
STEP: Creating hostNetwork=false pod 02/06/23 18:47:13.708
Feb  6 18:47:13.720: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4766" to be "running and ready"
Feb  6 18:47:13.730: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.422043ms
Feb  6 18:47:13.730: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:47:15.735: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015201867s
Feb  6 18:47:15.735: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:47:17.735: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014906634s
Feb  6 18:47:17.735: INFO: The phase of Pod test-pod is Running (Ready = true)
Feb  6 18:47:17.735: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 02/06/23 18:47:17.742
Feb  6 18:47:17.748: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4766" to be "running and ready"
Feb  6 18:47:17.756: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.915777ms
Feb  6 18:47:17.756: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:47:19.761: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012950866s
Feb  6 18:47:19.761: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Feb  6 18:47:19.761: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 02/06/23 18:47:19.765
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/06/23 18:47:19.765
Feb  6 18:47:19.765: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:47:19.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:47:19.766: INFO: ExecWithOptions: Clientset creation
Feb  6 18:47:19.766: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb  6 18:47:19.859: INFO: Exec stderr: ""
Feb  6 18:47:19.859: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:47:19.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:47:19.860: INFO: ExecWithOptions: Clientset creation
Feb  6 18:47:19.860: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb  6 18:47:19.943: INFO: Exec stderr: ""
Feb  6 18:47:19.943: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:47:19.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:47:19.945: INFO: ExecWithOptions: Clientset creation
Feb  6 18:47:19.945: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb  6 18:47:20.024: INFO: Exec stderr: ""
Feb  6 18:47:20.024: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:47:20.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:47:20.026: INFO: ExecWithOptions: Clientset creation
Feb  6 18:47:20.026: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb  6 18:47:20.111: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/06/23 18:47:20.111
Feb  6 18:47:20.111: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:47:20.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:47:20.112: INFO: ExecWithOptions: Clientset creation
Feb  6 18:47:20.112: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb  6 18:47:20.194: INFO: Exec stderr: ""
Feb  6 18:47:20.195: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:47:20.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:47:20.195: INFO: ExecWithOptions: Clientset creation
Feb  6 18:47:20.195: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb  6 18:47:20.268: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/06/23 18:47:20.268
Feb  6 18:47:20.268: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:47:20.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:47:20.269: INFO: ExecWithOptions: Clientset creation
Feb  6 18:47:20.269: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb  6 18:47:20.346: INFO: Exec stderr: ""
Feb  6 18:47:20.346: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:47:20.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:47:20.346: INFO: ExecWithOptions: Clientset creation
Feb  6 18:47:20.346: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb  6 18:47:20.431: INFO: Exec stderr: ""
Feb  6 18:47:20.431: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:47:20.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:47:20.432: INFO: ExecWithOptions: Clientset creation
Feb  6 18:47:20.432: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb  6 18:47:20.513: INFO: Exec stderr: ""
Feb  6 18:47:20.513: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:47:20.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:47:20.514: INFO: ExecWithOptions: Clientset creation
Feb  6 18:47:20.514: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb  6 18:47:20.591: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Feb  6 18:47:20.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4766" for this suite. 02/06/23 18:47:20.6
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":301,"skipped":5710,"failed":0}
------------------------------
• [SLOW TEST] [6.939 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:47:13.67
    Feb  6 18:47:13.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/06/23 18:47:13.672
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:13.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:13.704
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 02/06/23 18:47:13.707
    STEP: Creating hostNetwork=false pod 02/06/23 18:47:13.708
    Feb  6 18:47:13.720: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4766" to be "running and ready"
    Feb  6 18:47:13.730: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.422043ms
    Feb  6 18:47:13.730: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:47:15.735: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015201867s
    Feb  6 18:47:15.735: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:47:17.735: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014906634s
    Feb  6 18:47:17.735: INFO: The phase of Pod test-pod is Running (Ready = true)
    Feb  6 18:47:17.735: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 02/06/23 18:47:17.742
    Feb  6 18:47:17.748: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4766" to be "running and ready"
    Feb  6 18:47:17.756: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.915777ms
    Feb  6 18:47:17.756: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:47:19.761: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012950866s
    Feb  6 18:47:19.761: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Feb  6 18:47:19.761: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 02/06/23 18:47:19.765
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/06/23 18:47:19.765
    Feb  6 18:47:19.765: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:47:19.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:47:19.766: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:47:19.766: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb  6 18:47:19.859: INFO: Exec stderr: ""
    Feb  6 18:47:19.859: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:47:19.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:47:19.860: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:47:19.860: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb  6 18:47:19.943: INFO: Exec stderr: ""
    Feb  6 18:47:19.943: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:47:19.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:47:19.945: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:47:19.945: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb  6 18:47:20.024: INFO: Exec stderr: ""
    Feb  6 18:47:20.024: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:47:20.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:47:20.026: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:47:20.026: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb  6 18:47:20.111: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/06/23 18:47:20.111
    Feb  6 18:47:20.111: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:47:20.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:47:20.112: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:47:20.112: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb  6 18:47:20.194: INFO: Exec stderr: ""
    Feb  6 18:47:20.195: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:47:20.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:47:20.195: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:47:20.195: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb  6 18:47:20.268: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/06/23 18:47:20.268
    Feb  6 18:47:20.268: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:47:20.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:47:20.269: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:47:20.269: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb  6 18:47:20.346: INFO: Exec stderr: ""
    Feb  6 18:47:20.346: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:47:20.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:47:20.346: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:47:20.346: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb  6 18:47:20.431: INFO: Exec stderr: ""
    Feb  6 18:47:20.431: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:47:20.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:47:20.432: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:47:20.432: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb  6 18:47:20.513: INFO: Exec stderr: ""
    Feb  6 18:47:20.513: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4766 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:47:20.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:47:20.514: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:47:20.514: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4766/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb  6 18:47:20.591: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Feb  6 18:47:20.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-4766" for this suite. 02/06/23 18:47:20.6
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:47:20.615
Feb  6 18:47:20.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 18:47:20.615
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:20.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:20.642
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-04f8bc4e-f522-41c8-884c-864e425964a3 02/06/23 18:47:20.645
STEP: Creating a pod to test consume configMaps 02/06/23 18:47:20.66
Feb  6 18:47:20.687: INFO: Waiting up to 5m0s for pod "pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4" in namespace "configmap-6700" to be "Succeeded or Failed"
Feb  6 18:47:20.697: INFO: Pod "pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.946812ms
Feb  6 18:47:22.702: INFO: Pod "pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015448548s
Feb  6 18:47:24.703: INFO: Pod "pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016220221s
Feb  6 18:47:26.703: INFO: Pod "pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016574684s
STEP: Saw pod success 02/06/23 18:47:26.703
Feb  6 18:47:26.704: INFO: Pod "pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4" satisfied condition "Succeeded or Failed"
Feb  6 18:47:26.711: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4 container configmap-volume-test: <nil>
STEP: delete the pod 02/06/23 18:47:26.734
Feb  6 18:47:26.750: INFO: Waiting for pod pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4 to disappear
Feb  6 18:47:26.755: INFO: Pod pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 18:47:26.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6700" for this suite. 02/06/23 18:47:26.761
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":302,"skipped":5714,"failed":0}
------------------------------
• [SLOW TEST] [6.155 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:47:20.615
    Feb  6 18:47:20.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 18:47:20.615
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:20.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:20.642
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-04f8bc4e-f522-41c8-884c-864e425964a3 02/06/23 18:47:20.645
    STEP: Creating a pod to test consume configMaps 02/06/23 18:47:20.66
    Feb  6 18:47:20.687: INFO: Waiting up to 5m0s for pod "pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4" in namespace "configmap-6700" to be "Succeeded or Failed"
    Feb  6 18:47:20.697: INFO: Pod "pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.946812ms
    Feb  6 18:47:22.702: INFO: Pod "pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015448548s
    Feb  6 18:47:24.703: INFO: Pod "pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016220221s
    Feb  6 18:47:26.703: INFO: Pod "pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016574684s
    STEP: Saw pod success 02/06/23 18:47:26.703
    Feb  6 18:47:26.704: INFO: Pod "pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4" satisfied condition "Succeeded or Failed"
    Feb  6 18:47:26.711: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4 container configmap-volume-test: <nil>
    STEP: delete the pod 02/06/23 18:47:26.734
    Feb  6 18:47:26.750: INFO: Waiting for pod pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4 to disappear
    Feb  6 18:47:26.755: INFO: Pod pod-configmaps-7eb273db-9f4f-4390-b05b-1620decc79d4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 18:47:26.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6700" for this suite. 02/06/23 18:47:26.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:47:26.774
Feb  6 18:47:26.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename sysctl 02/06/23 18:47:26.775
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:26.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:26.802
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 02/06/23 18:47:26.805
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Feb  6 18:47:26.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8394" for this suite. 02/06/23 18:47:26.817
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":303,"skipped":5727,"failed":0}
------------------------------
• [0.050 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:47:26.774
    Feb  6 18:47:26.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename sysctl 02/06/23 18:47:26.775
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:26.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:26.802
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 02/06/23 18:47:26.805
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb  6 18:47:26.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-8394" for this suite. 02/06/23 18:47:26.817
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:47:26.826
Feb  6 18:47:26.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 18:47:26.827
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:26.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:26.854
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-05b0c3cd-a70d-4ca2-962b-03b1e8d56b0a 02/06/23 18:47:26.857
STEP: Creating a pod to test consume secrets 02/06/23 18:47:26.865
Feb  6 18:47:26.881: INFO: Waiting up to 5m0s for pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1" in namespace "secrets-7464" to be "Succeeded or Failed"
Feb  6 18:47:26.888: INFO: Pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.819334ms
Feb  6 18:47:28.893: INFO: Pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01188726s
Feb  6 18:47:30.895: INFO: Pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1": Phase="Running", Reason="", readiness=true. Elapsed: 4.013509152s
Feb  6 18:47:32.894: INFO: Pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1": Phase="Running", Reason="", readiness=false. Elapsed: 6.012921544s
Feb  6 18:47:34.895: INFO: Pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013624273s
STEP: Saw pod success 02/06/23 18:47:34.895
Feb  6 18:47:34.895: INFO: Pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1" satisfied condition "Succeeded or Failed"
Feb  6 18:47:34.900: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1 container secret-volume-test: <nil>
STEP: delete the pod 02/06/23 18:47:34.91
Feb  6 18:47:34.933: INFO: Waiting for pod pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1 to disappear
Feb  6 18:47:34.936: INFO: Pod pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb  6 18:47:34.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7464" for this suite. 02/06/23 18:47:34.944
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":304,"skipped":5728,"failed":0}
------------------------------
• [SLOW TEST] [8.130 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:47:26.826
    Feb  6 18:47:26.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 18:47:26.827
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:26.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:26.854
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-05b0c3cd-a70d-4ca2-962b-03b1e8d56b0a 02/06/23 18:47:26.857
    STEP: Creating a pod to test consume secrets 02/06/23 18:47:26.865
    Feb  6 18:47:26.881: INFO: Waiting up to 5m0s for pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1" in namespace "secrets-7464" to be "Succeeded or Failed"
    Feb  6 18:47:26.888: INFO: Pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.819334ms
    Feb  6 18:47:28.893: INFO: Pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01188726s
    Feb  6 18:47:30.895: INFO: Pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1": Phase="Running", Reason="", readiness=true. Elapsed: 4.013509152s
    Feb  6 18:47:32.894: INFO: Pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1": Phase="Running", Reason="", readiness=false. Elapsed: 6.012921544s
    Feb  6 18:47:34.895: INFO: Pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013624273s
    STEP: Saw pod success 02/06/23 18:47:34.895
    Feb  6 18:47:34.895: INFO: Pod "pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1" satisfied condition "Succeeded or Failed"
    Feb  6 18:47:34.900: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1 container secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 18:47:34.91
    Feb  6 18:47:34.933: INFO: Waiting for pod pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1 to disappear
    Feb  6 18:47:34.936: INFO: Pod pod-secrets-a372c23f-5a8d-4107-a784-5f145deb6aa1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 18:47:34.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7464" for this suite. 02/06/23 18:47:34.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:47:34.957
Feb  6 18:47:34.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename sysctl 02/06/23 18:47:34.959
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:34.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:34.99
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/06/23 18:47:34.995
STEP: Watching for error events or started pod 02/06/23 18:47:35.009
STEP: Waiting for pod completion 02/06/23 18:47:39.014
Feb  6 18:47:39.014: INFO: Waiting up to 3m0s for pod "sysctl-68818f56-d558-4043-b13f-4d7d68457a89" in namespace "sysctl-8789" to be "completed"
Feb  6 18:47:39.018: INFO: Pod "sysctl-68818f56-d558-4043-b13f-4d7d68457a89": Phase="Pending", Reason="", readiness=false. Elapsed: 3.212552ms
Feb  6 18:47:41.024: INFO: Pod "sysctl-68818f56-d558-4043-b13f-4d7d68457a89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009812282s
Feb  6 18:47:41.024: INFO: Pod "sysctl-68818f56-d558-4043-b13f-4d7d68457a89" satisfied condition "completed"
STEP: Checking that the pod succeeded 02/06/23 18:47:41.028
STEP: Getting logs from the pod 02/06/23 18:47:41.028
STEP: Checking that the sysctl is actually updated 02/06/23 18:47:41.036
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Feb  6 18:47:41.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8789" for this suite. 02/06/23 18:47:41.042
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":305,"skipped":5734,"failed":0}
------------------------------
• [SLOW TEST] [6.092 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:47:34.957
    Feb  6 18:47:34.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename sysctl 02/06/23 18:47:34.959
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:34.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:34.99
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/06/23 18:47:34.995
    STEP: Watching for error events or started pod 02/06/23 18:47:35.009
    STEP: Waiting for pod completion 02/06/23 18:47:39.014
    Feb  6 18:47:39.014: INFO: Waiting up to 3m0s for pod "sysctl-68818f56-d558-4043-b13f-4d7d68457a89" in namespace "sysctl-8789" to be "completed"
    Feb  6 18:47:39.018: INFO: Pod "sysctl-68818f56-d558-4043-b13f-4d7d68457a89": Phase="Pending", Reason="", readiness=false. Elapsed: 3.212552ms
    Feb  6 18:47:41.024: INFO: Pod "sysctl-68818f56-d558-4043-b13f-4d7d68457a89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009812282s
    Feb  6 18:47:41.024: INFO: Pod "sysctl-68818f56-d558-4043-b13f-4d7d68457a89" satisfied condition "completed"
    STEP: Checking that the pod succeeded 02/06/23 18:47:41.028
    STEP: Getting logs from the pod 02/06/23 18:47:41.028
    STEP: Checking that the sysctl is actually updated 02/06/23 18:47:41.036
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb  6 18:47:41.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-8789" for this suite. 02/06/23 18:47:41.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:47:41.054
Feb  6 18:47:41.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 18:47:41.056
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:41.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:41.082
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/06/23 18:47:41.085
Feb  6 18:47:41.093: INFO: Waiting up to 5m0s for pod "pod-372630cc-22e9-476b-b020-b9378101a4f7" in namespace "emptydir-9289" to be "Succeeded or Failed"
Feb  6 18:47:41.097: INFO: Pod "pod-372630cc-22e9-476b-b020-b9378101a4f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.179977ms
Feb  6 18:47:43.101: INFO: Pod "pod-372630cc-22e9-476b-b020-b9378101a4f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007852471s
Feb  6 18:47:45.103: INFO: Pod "pod-372630cc-22e9-476b-b020-b9378101a4f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009650112s
Feb  6 18:47:47.109: INFO: Pod "pod-372630cc-22e9-476b-b020-b9378101a4f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015837625s
STEP: Saw pod success 02/06/23 18:47:47.109
Feb  6 18:47:47.109: INFO: Pod "pod-372630cc-22e9-476b-b020-b9378101a4f7" satisfied condition "Succeeded or Failed"
Feb  6 18:47:47.119: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-372630cc-22e9-476b-b020-b9378101a4f7 container test-container: <nil>
STEP: delete the pod 02/06/23 18:47:47.168
Feb  6 18:47:47.189: INFO: Waiting for pod pod-372630cc-22e9-476b-b020-b9378101a4f7 to disappear
Feb  6 18:47:47.193: INFO: Pod pod-372630cc-22e9-476b-b020-b9378101a4f7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 18:47:47.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9289" for this suite. 02/06/23 18:47:47.201
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":306,"skipped":5747,"failed":0}
------------------------------
• [SLOW TEST] [6.156 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:47:41.054
    Feb  6 18:47:41.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 18:47:41.056
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:41.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:41.082
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/06/23 18:47:41.085
    Feb  6 18:47:41.093: INFO: Waiting up to 5m0s for pod "pod-372630cc-22e9-476b-b020-b9378101a4f7" in namespace "emptydir-9289" to be "Succeeded or Failed"
    Feb  6 18:47:41.097: INFO: Pod "pod-372630cc-22e9-476b-b020-b9378101a4f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.179977ms
    Feb  6 18:47:43.101: INFO: Pod "pod-372630cc-22e9-476b-b020-b9378101a4f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007852471s
    Feb  6 18:47:45.103: INFO: Pod "pod-372630cc-22e9-476b-b020-b9378101a4f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009650112s
    Feb  6 18:47:47.109: INFO: Pod "pod-372630cc-22e9-476b-b020-b9378101a4f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015837625s
    STEP: Saw pod success 02/06/23 18:47:47.109
    Feb  6 18:47:47.109: INFO: Pod "pod-372630cc-22e9-476b-b020-b9378101a4f7" satisfied condition "Succeeded or Failed"
    Feb  6 18:47:47.119: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-372630cc-22e9-476b-b020-b9378101a4f7 container test-container: <nil>
    STEP: delete the pod 02/06/23 18:47:47.168
    Feb  6 18:47:47.189: INFO: Waiting for pod pod-372630cc-22e9-476b-b020-b9378101a4f7 to disappear
    Feb  6 18:47:47.193: INFO: Pod pod-372630cc-22e9-476b-b020-b9378101a4f7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 18:47:47.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9289" for this suite. 02/06/23 18:47:47.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:47:47.214
Feb  6 18:47:47.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename emptydir 02/06/23 18:47:47.215
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:47.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:47.243
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/06/23 18:47:47.245
Feb  6 18:47:47.255: INFO: Waiting up to 5m0s for pod "pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc" in namespace "emptydir-4744" to be "Succeeded or Failed"
Feb  6 18:47:47.261: INFO: Pod "pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.970677ms
Feb  6 18:47:49.266: INFO: Pod "pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011024921s
Feb  6 18:47:51.268: INFO: Pod "pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012675633s
Feb  6 18:47:53.267: INFO: Pod "pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011563167s
STEP: Saw pod success 02/06/23 18:47:53.267
Feb  6 18:47:53.267: INFO: Pod "pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc" satisfied condition "Succeeded or Failed"
Feb  6 18:47:53.270: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc container test-container: <nil>
STEP: delete the pod 02/06/23 18:47:53.278
Feb  6 18:47:53.293: INFO: Waiting for pod pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc to disappear
Feb  6 18:47:53.297: INFO: Pod pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb  6 18:47:53.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4744" for this suite. 02/06/23 18:47:53.303
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":307,"skipped":5771,"failed":0}
------------------------------
• [SLOW TEST] [6.096 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:47:47.214
    Feb  6 18:47:47.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename emptydir 02/06/23 18:47:47.215
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:47.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:47.243
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/06/23 18:47:47.245
    Feb  6 18:47:47.255: INFO: Waiting up to 5m0s for pod "pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc" in namespace "emptydir-4744" to be "Succeeded or Failed"
    Feb  6 18:47:47.261: INFO: Pod "pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.970677ms
    Feb  6 18:47:49.266: INFO: Pod "pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011024921s
    Feb  6 18:47:51.268: INFO: Pod "pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012675633s
    Feb  6 18:47:53.267: INFO: Pod "pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011563167s
    STEP: Saw pod success 02/06/23 18:47:53.267
    Feb  6 18:47:53.267: INFO: Pod "pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc" satisfied condition "Succeeded or Failed"
    Feb  6 18:47:53.270: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc container test-container: <nil>
    STEP: delete the pod 02/06/23 18:47:53.278
    Feb  6 18:47:53.293: INFO: Waiting for pod pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc to disappear
    Feb  6 18:47:53.297: INFO: Pod pod-4b1c7fd9-8d20-4c0d-a690-f221f299b0cc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb  6 18:47:53.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4744" for this suite. 02/06/23 18:47:53.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:47:53.311
Feb  6 18:47:53.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename sched-preemption 02/06/23 18:47:53.312
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:53.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:53.338
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb  6 18:47:53.358: INFO: Waiting up to 1m0s for all nodes to be ready
Feb  6 18:48:53.428: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:48:53.432
Feb  6 18:48:53.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename sched-preemption-path 02/06/23 18:48:53.434
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:48:53.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:48:53.457
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 02/06/23 18:48:53.461
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/06/23 18:48:53.461
Feb  6 18:48:53.472: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2335" to be "running"
Feb  6 18:48:53.475: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.194837ms
Feb  6 18:48:55.481: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008515052s
Feb  6 18:48:57.481: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.008910216s
Feb  6 18:48:57.481: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/06/23 18:48:57.484
Feb  6 18:48:57.496: INFO: found a healthy node: tneyla25-md-0-68cbcb4798-5xxmw
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Feb  6 18:49:11.597: INFO: pods created so far: [1 1 1]
Feb  6 18:49:11.597: INFO: length of pods created so far: 3
Feb  6 18:49:13.614: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Feb  6 18:49:20.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2335" for this suite. 02/06/23 18:49:20.622
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb  6 18:49:20.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1709" for this suite. 02/06/23 18:49:20.681
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":308,"skipped":5787,"failed":0}
------------------------------
• [SLOW TEST] [87.457 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:47:53.311
    Feb  6 18:47:53.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename sched-preemption 02/06/23 18:47:53.312
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:47:53.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:47:53.338
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb  6 18:47:53.358: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb  6 18:48:53.428: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:48:53.432
    Feb  6 18:48:53.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename sched-preemption-path 02/06/23 18:48:53.434
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:48:53.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:48:53.457
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 02/06/23 18:48:53.461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/06/23 18:48:53.461
    Feb  6 18:48:53.472: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2335" to be "running"
    Feb  6 18:48:53.475: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.194837ms
    Feb  6 18:48:55.481: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008515052s
    Feb  6 18:48:57.481: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.008910216s
    Feb  6 18:48:57.481: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/06/23 18:48:57.484
    Feb  6 18:48:57.496: INFO: found a healthy node: tneyla25-md-0-68cbcb4798-5xxmw
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Feb  6 18:49:11.597: INFO: pods created so far: [1 1 1]
    Feb  6 18:49:11.597: INFO: length of pods created so far: 3
    Feb  6 18:49:13.614: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Feb  6 18:49:20.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-2335" for this suite. 02/06/23 18:49:20.622
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 18:49:20.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-1709" for this suite. 02/06/23 18:49:20.681
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:49:20.776
Feb  6 18:49:20.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pods 02/06/23 18:49:20.777
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:49:20.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:49:20.813
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 02/06/23 18:49:20.816
Feb  6 18:49:20.826: INFO: created test-pod-1
Feb  6 18:49:20.837: INFO: created test-pod-2
Feb  6 18:49:20.848: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 02/06/23 18:49:20.848
Feb  6 18:49:20.849: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9110' to be running and ready
Feb  6 18:49:20.873: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb  6 18:49:20.873: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb  6 18:49:20.873: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb  6 18:49:20.873: INFO: 0 / 3 pods in namespace 'pods-9110' are running and ready (0 seconds elapsed)
Feb  6 18:49:20.873: INFO: expected 0 pod replicas in namespace 'pods-9110', 0 are Running and Ready.
Feb  6 18:49:20.873: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
Feb  6 18:49:20.873: INFO: test-pod-1  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
Feb  6 18:49:20.873: INFO: test-pod-2  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
Feb  6 18:49:20.873: INFO: test-pod-3  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
Feb  6 18:49:20.873: INFO: 
Feb  6 18:49:22.891: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb  6 18:49:22.891: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb  6 18:49:22.891: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb  6 18:49:22.891: INFO: 0 / 3 pods in namespace 'pods-9110' are running and ready (2 seconds elapsed)
Feb  6 18:49:22.891: INFO: expected 0 pod replicas in namespace 'pods-9110', 0 are Running and Ready.
Feb  6 18:49:22.891: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
Feb  6 18:49:22.891: INFO: test-pod-1  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
Feb  6 18:49:22.891: INFO: test-pod-2  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
Feb  6 18:49:22.891: INFO: test-pod-3  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
Feb  6 18:49:22.891: INFO: 
Feb  6 18:49:24.886: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb  6 18:49:24.887: INFO: 2 / 3 pods in namespace 'pods-9110' are running and ready (4 seconds elapsed)
Feb  6 18:49:24.887: INFO: expected 0 pod replicas in namespace 'pods-9110', 0 are Running and Ready.
Feb  6 18:49:24.887: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
Feb  6 18:49:24.887: INFO: test-pod-3  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
Feb  6 18:49:24.887: INFO: 
Feb  6 18:49:26.900: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb  6 18:49:26.900: INFO: 2 / 3 pods in namespace 'pods-9110' are running and ready (6 seconds elapsed)
Feb  6 18:49:26.901: INFO: expected 0 pod replicas in namespace 'pods-9110', 0 are Running and Ready.
Feb  6 18:49:26.901: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
Feb  6 18:49:26.901: INFO: test-pod-3  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
Feb  6 18:49:26.901: INFO: 
Feb  6 18:49:28.889: INFO: 3 / 3 pods in namespace 'pods-9110' are running and ready (8 seconds elapsed)
Feb  6 18:49:28.889: INFO: expected 0 pod replicas in namespace 'pods-9110', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 02/06/23 18:49:28.92
Feb  6 18:49:28.925: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb  6 18:49:29.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9110" for this suite. 02/06/23 18:49:29.936
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":309,"skipped":5832,"failed":0}
------------------------------
• [SLOW TEST] [9.174 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:49:20.776
    Feb  6 18:49:20.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pods 02/06/23 18:49:20.777
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:49:20.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:49:20.813
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 02/06/23 18:49:20.816
    Feb  6 18:49:20.826: INFO: created test-pod-1
    Feb  6 18:49:20.837: INFO: created test-pod-2
    Feb  6 18:49:20.848: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 02/06/23 18:49:20.848
    Feb  6 18:49:20.849: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9110' to be running and ready
    Feb  6 18:49:20.873: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb  6 18:49:20.873: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb  6 18:49:20.873: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb  6 18:49:20.873: INFO: 0 / 3 pods in namespace 'pods-9110' are running and ready (0 seconds elapsed)
    Feb  6 18:49:20.873: INFO: expected 0 pod replicas in namespace 'pods-9110', 0 are Running and Ready.
    Feb  6 18:49:20.873: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
    Feb  6 18:49:20.873: INFO: test-pod-1  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
    Feb  6 18:49:20.873: INFO: test-pod-2  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
    Feb  6 18:49:20.873: INFO: test-pod-3  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
    Feb  6 18:49:20.873: INFO: 
    Feb  6 18:49:22.891: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb  6 18:49:22.891: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb  6 18:49:22.891: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb  6 18:49:22.891: INFO: 0 / 3 pods in namespace 'pods-9110' are running and ready (2 seconds elapsed)
    Feb  6 18:49:22.891: INFO: expected 0 pod replicas in namespace 'pods-9110', 0 are Running and Ready.
    Feb  6 18:49:22.891: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
    Feb  6 18:49:22.891: INFO: test-pod-1  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
    Feb  6 18:49:22.891: INFO: test-pod-2  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
    Feb  6 18:49:22.891: INFO: test-pod-3  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
    Feb  6 18:49:22.891: INFO: 
    Feb  6 18:49:24.886: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb  6 18:49:24.887: INFO: 2 / 3 pods in namespace 'pods-9110' are running and ready (4 seconds elapsed)
    Feb  6 18:49:24.887: INFO: expected 0 pod replicas in namespace 'pods-9110', 0 are Running and Ready.
    Feb  6 18:49:24.887: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
    Feb  6 18:49:24.887: INFO: test-pod-3  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
    Feb  6 18:49:24.887: INFO: 
    Feb  6 18:49:26.900: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb  6 18:49:26.900: INFO: 2 / 3 pods in namespace 'pods-9110' are running and ready (6 seconds elapsed)
    Feb  6 18:49:26.901: INFO: expected 0 pod replicas in namespace 'pods-9110', 0 are Running and Ready.
    Feb  6 18:49:26.901: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
    Feb  6 18:49:26.901: INFO: test-pod-3  tneyla25-md-0-68cbcb4798-5xxmw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-06 18:49:20 +0000 UTC  }]
    Feb  6 18:49:26.901: INFO: 
    Feb  6 18:49:28.889: INFO: 3 / 3 pods in namespace 'pods-9110' are running and ready (8 seconds elapsed)
    Feb  6 18:49:28.889: INFO: expected 0 pod replicas in namespace 'pods-9110', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 02/06/23 18:49:28.92
    Feb  6 18:49:28.925: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb  6 18:49:29.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9110" for this suite. 02/06/23 18:49:29.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:49:29.954
Feb  6 18:49:29.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename resourcequota 02/06/23 18:49:29.956
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:49:29.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:49:29.979
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 02/06/23 18:49:29.982
STEP: Getting a ResourceQuota 02/06/23 18:49:29.987
STEP: Updating a ResourceQuota 02/06/23 18:49:29.99
STEP: Verifying a ResourceQuota was modified 02/06/23 18:49:30.001
STEP: Deleting a ResourceQuota 02/06/23 18:49:30.007
STEP: Verifying the deleted ResourceQuota 02/06/23 18:49:30.013
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb  6 18:49:30.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4216" for this suite. 02/06/23 18:49:30.022
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":310,"skipped":5845,"failed":0}
------------------------------
• [0.076 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:49:29.954
    Feb  6 18:49:29.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename resourcequota 02/06/23 18:49:29.956
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:49:29.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:49:29.979
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 02/06/23 18:49:29.982
    STEP: Getting a ResourceQuota 02/06/23 18:49:29.987
    STEP: Updating a ResourceQuota 02/06/23 18:49:29.99
    STEP: Verifying a ResourceQuota was modified 02/06/23 18:49:30.001
    STEP: Deleting a ResourceQuota 02/06/23 18:49:30.007
    STEP: Verifying the deleted ResourceQuota 02/06/23 18:49:30.013
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb  6 18:49:30.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4216" for this suite. 02/06/23 18:49:30.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:49:30.033
Feb  6 18:49:30.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename job 02/06/23 18:49:30.034
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:49:30.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:49:30.054
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 02/06/23 18:49:30.058
STEP: Ensuring active pods == parallelism 02/06/23 18:49:30.063
STEP: delete a job 02/06/23 18:49:36.069
STEP: deleting Job.batch foo in namespace job-1566, will wait for the garbage collector to delete the pods 02/06/23 18:49:36.07
Feb  6 18:49:36.133: INFO: Deleting Job.batch foo took: 9.102695ms
Feb  6 18:49:36.234: INFO: Terminating Job.batch foo pods took: 100.29621ms
STEP: Ensuring job was deleted 02/06/23 18:50:07.934
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb  6 18:50:07.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1566" for this suite. 02/06/23 18:50:07.946
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":311,"skipped":5858,"failed":0}
------------------------------
• [SLOW TEST] [37.921 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:49:30.033
    Feb  6 18:49:30.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename job 02/06/23 18:49:30.034
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:49:30.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:49:30.054
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 02/06/23 18:49:30.058
    STEP: Ensuring active pods == parallelism 02/06/23 18:49:30.063
    STEP: delete a job 02/06/23 18:49:36.069
    STEP: deleting Job.batch foo in namespace job-1566, will wait for the garbage collector to delete the pods 02/06/23 18:49:36.07
    Feb  6 18:49:36.133: INFO: Deleting Job.batch foo took: 9.102695ms
    Feb  6 18:49:36.234: INFO: Terminating Job.batch foo pods took: 100.29621ms
    STEP: Ensuring job was deleted 02/06/23 18:50:07.934
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb  6 18:50:07.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1566" for this suite. 02/06/23 18:50:07.946
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:50:07.954
Feb  6 18:50:07.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename job 02/06/23 18:50:07.955
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:07.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:07.985
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 02/06/23 18:50:07.992
STEP: Patching the Job 02/06/23 18:50:07.998
STEP: Watching for Job to be patched 02/06/23 18:50:08.025
Feb  6 18:50:08.028: INFO: Event ADDED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb  6 18:50:08.028: INFO: Event MODIFIED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb  6 18:50:08.028: INFO: Event MODIFIED found for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 02/06/23 18:50:08.028
STEP: Watching for Job to be updated 02/06/23 18:50:08.04
Feb  6 18:50:08.042: INFO: Event MODIFIED found for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb  6 18:50:08.042: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 02/06/23 18:50:08.042
Feb  6 18:50:08.053: INFO: Job: e2e-5pbrd as labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd]
STEP: Waiting for job to complete 02/06/23 18:50:08.053
STEP: Delete a job collection with a labelselector 02/06/23 18:50:22.059
STEP: Watching for Job to be deleted 02/06/23 18:50:22.074
Feb  6 18:50:22.078: INFO: Event MODIFIED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb  6 18:50:22.078: INFO: Event MODIFIED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb  6 18:50:22.078: INFO: Event MODIFIED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb  6 18:50:22.078: INFO: Event MODIFIED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb  6 18:50:22.078: INFO: Event MODIFIED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb  6 18:50:22.079: INFO: Event DELETED found for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 02/06/23 18:50:22.079
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb  6 18:50:22.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8231" for this suite. 02/06/23 18:50:22.094
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":312,"skipped":5858,"failed":0}
------------------------------
• [SLOW TEST] [14.148 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:50:07.954
    Feb  6 18:50:07.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename job 02/06/23 18:50:07.955
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:07.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:07.985
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 02/06/23 18:50:07.992
    STEP: Patching the Job 02/06/23 18:50:07.998
    STEP: Watching for Job to be patched 02/06/23 18:50:08.025
    Feb  6 18:50:08.028: INFO: Event ADDED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb  6 18:50:08.028: INFO: Event MODIFIED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb  6 18:50:08.028: INFO: Event MODIFIED found for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 02/06/23 18:50:08.028
    STEP: Watching for Job to be updated 02/06/23 18:50:08.04
    Feb  6 18:50:08.042: INFO: Event MODIFIED found for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb  6 18:50:08.042: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 02/06/23 18:50:08.042
    Feb  6 18:50:08.053: INFO: Job: e2e-5pbrd as labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd]
    STEP: Waiting for job to complete 02/06/23 18:50:08.053
    STEP: Delete a job collection with a labelselector 02/06/23 18:50:22.059
    STEP: Watching for Job to be deleted 02/06/23 18:50:22.074
    Feb  6 18:50:22.078: INFO: Event MODIFIED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb  6 18:50:22.078: INFO: Event MODIFIED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb  6 18:50:22.078: INFO: Event MODIFIED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb  6 18:50:22.078: INFO: Event MODIFIED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb  6 18:50:22.078: INFO: Event MODIFIED observed for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb  6 18:50:22.079: INFO: Event DELETED found for Job e2e-5pbrd in namespace job-8231 with labels: map[e2e-5pbrd:patched e2e-job-label:e2e-5pbrd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 02/06/23 18:50:22.079
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb  6 18:50:22.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8231" for this suite. 02/06/23 18:50:22.094
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:50:22.106
Feb  6 18:50:22.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:50:22.107
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:22.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:22.15
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-4122eb03-944c-4239-8bc0-8a9337c8f597 02/06/23 18:50:22.153
STEP: Creating a pod to test consume configMaps 02/06/23 18:50:22.158
Feb  6 18:50:22.168: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b" in namespace "projected-91" to be "Succeeded or Failed"
Feb  6 18:50:22.173: INFO: Pod "pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.901018ms
Feb  6 18:50:24.179: INFO: Pod "pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010688772s
Feb  6 18:50:26.178: INFO: Pod "pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009801457s
Feb  6 18:50:28.180: INFO: Pod "pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011954798s
STEP: Saw pod success 02/06/23 18:50:28.18
Feb  6 18:50:28.181: INFO: Pod "pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b" satisfied condition "Succeeded or Failed"
Feb  6 18:50:28.185: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:50:28.202
Feb  6 18:50:28.222: INFO: Waiting for pod pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b to disappear
Feb  6 18:50:28.225: INFO: Pod pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb  6 18:50:28.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-91" for this suite. 02/06/23 18:50:28.23
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":313,"skipped":5862,"failed":0}
------------------------------
• [SLOW TEST] [6.132 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:50:22.106
    Feb  6 18:50:22.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:50:22.107
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:22.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:22.15
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-4122eb03-944c-4239-8bc0-8a9337c8f597 02/06/23 18:50:22.153
    STEP: Creating a pod to test consume configMaps 02/06/23 18:50:22.158
    Feb  6 18:50:22.168: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b" in namespace "projected-91" to be "Succeeded or Failed"
    Feb  6 18:50:22.173: INFO: Pod "pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.901018ms
    Feb  6 18:50:24.179: INFO: Pod "pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010688772s
    Feb  6 18:50:26.178: INFO: Pod "pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009801457s
    Feb  6 18:50:28.180: INFO: Pod "pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011954798s
    STEP: Saw pod success 02/06/23 18:50:28.18
    Feb  6 18:50:28.181: INFO: Pod "pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b" satisfied condition "Succeeded or Failed"
    Feb  6 18:50:28.185: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:50:28.202
    Feb  6 18:50:28.222: INFO: Waiting for pod pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b to disappear
    Feb  6 18:50:28.225: INFO: Pod pod-projected-configmaps-d3d036c5-bb91-4240-8d1f-a1c66afe865b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb  6 18:50:28.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-91" for this suite. 02/06/23 18:50:28.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:50:28.239
Feb  6 18:50:28.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pods 02/06/23 18:50:28.241
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:28.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:28.267
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 02/06/23 18:50:28.271
STEP: submitting the pod to kubernetes 02/06/23 18:50:28.271
Feb  6 18:50:28.282: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00" in namespace "pods-5673" to be "running and ready"
Feb  6 18:50:28.290: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.459224ms
Feb  6 18:50:28.290: INFO: The phase of Pod pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:50:30.294: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012355635s
Feb  6 18:50:30.294: INFO: The phase of Pod pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:50:32.295: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00": Phase="Running", Reason="", readiness=true. Elapsed: 4.013748911s
Feb  6 18:50:32.296: INFO: The phase of Pod pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00 is Running (Ready = true)
Feb  6 18:50:32.296: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/06/23 18:50:32.299
STEP: updating the pod 02/06/23 18:50:32.304
Feb  6 18:50:32.818: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00"
Feb  6 18:50:32.818: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00" in namespace "pods-5673" to be "terminated with reason DeadlineExceeded"
Feb  6 18:50:32.821: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00": Phase="Running", Reason="", readiness=true. Elapsed: 2.966789ms
Feb  6 18:50:34.826: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00": Phase="Running", Reason="", readiness=false. Elapsed: 2.0087507s
Feb  6 18:50:36.826: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.00842608s
Feb  6 18:50:36.826: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb  6 18:50:36.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5673" for this suite. 02/06/23 18:50:36.831
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":314,"skipped":5880,"failed":0}
------------------------------
• [SLOW TEST] [8.599 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:50:28.239
    Feb  6 18:50:28.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pods 02/06/23 18:50:28.241
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:28.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:28.267
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 02/06/23 18:50:28.271
    STEP: submitting the pod to kubernetes 02/06/23 18:50:28.271
    Feb  6 18:50:28.282: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00" in namespace "pods-5673" to be "running and ready"
    Feb  6 18:50:28.290: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.459224ms
    Feb  6 18:50:28.290: INFO: The phase of Pod pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:50:30.294: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012355635s
    Feb  6 18:50:30.294: INFO: The phase of Pod pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:50:32.295: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00": Phase="Running", Reason="", readiness=true. Elapsed: 4.013748911s
    Feb  6 18:50:32.296: INFO: The phase of Pod pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00 is Running (Ready = true)
    Feb  6 18:50:32.296: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/06/23 18:50:32.299
    STEP: updating the pod 02/06/23 18:50:32.304
    Feb  6 18:50:32.818: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00"
    Feb  6 18:50:32.818: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00" in namespace "pods-5673" to be "terminated with reason DeadlineExceeded"
    Feb  6 18:50:32.821: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00": Phase="Running", Reason="", readiness=true. Elapsed: 2.966789ms
    Feb  6 18:50:34.826: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00": Phase="Running", Reason="", readiness=false. Elapsed: 2.0087507s
    Feb  6 18:50:36.826: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.00842608s
    Feb  6 18:50:36.826: INFO: Pod "pod-update-activedeadlineseconds-ddd13dcf-3e64-4b79-991d-28c09a8ddf00" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb  6 18:50:36.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5673" for this suite. 02/06/23 18:50:36.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:50:36.839
Feb  6 18:50:36.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:50:36.841
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:36.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:36.866
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 02/06/23 18:50:36.868
Feb  6 18:50:36.882: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812" in namespace "projected-144" to be "Succeeded or Failed"
Feb  6 18:50:36.891: INFO: Pod "downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812": Phase="Pending", Reason="", readiness=false. Elapsed: 8.999611ms
Feb  6 18:50:38.897: INFO: Pod "downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014785806s
Feb  6 18:50:40.898: INFO: Pod "downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015476338s
Feb  6 18:50:42.896: INFO: Pod "downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013982122s
STEP: Saw pod success 02/06/23 18:50:42.896
Feb  6 18:50:42.896: INFO: Pod "downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812" satisfied condition "Succeeded or Failed"
Feb  6 18:50:42.900: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812 container client-container: <nil>
STEP: delete the pod 02/06/23 18:50:42.909
Feb  6 18:50:42.927: INFO: Waiting for pod downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812 to disappear
Feb  6 18:50:42.932: INFO: Pod downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb  6 18:50:42.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-144" for this suite. 02/06/23 18:50:42.939
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":315,"skipped":5908,"failed":0}
------------------------------
• [SLOW TEST] [6.110 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:50:36.839
    Feb  6 18:50:36.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:50:36.841
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:36.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:36.866
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 02/06/23 18:50:36.868
    Feb  6 18:50:36.882: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812" in namespace "projected-144" to be "Succeeded or Failed"
    Feb  6 18:50:36.891: INFO: Pod "downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812": Phase="Pending", Reason="", readiness=false. Elapsed: 8.999611ms
    Feb  6 18:50:38.897: INFO: Pod "downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014785806s
    Feb  6 18:50:40.898: INFO: Pod "downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015476338s
    Feb  6 18:50:42.896: INFO: Pod "downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013982122s
    STEP: Saw pod success 02/06/23 18:50:42.896
    Feb  6 18:50:42.896: INFO: Pod "downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812" satisfied condition "Succeeded or Failed"
    Feb  6 18:50:42.900: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812 container client-container: <nil>
    STEP: delete the pod 02/06/23 18:50:42.909
    Feb  6 18:50:42.927: INFO: Waiting for pod downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812 to disappear
    Feb  6 18:50:42.932: INFO: Pod downwardapi-volume-6aa4c6d1-7f8b-4e4e-ae4e-1b0e72f26812 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb  6 18:50:42.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-144" for this suite. 02/06/23 18:50:42.939
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:50:42.95
Feb  6 18:50:42.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename dns 02/06/23 18:50:42.952
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:43.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:43.031
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2755.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2755.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 02/06/23 18:50:43.033
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2755.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2755.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 02/06/23 18:50:43.033
STEP: creating a pod to probe /etc/hosts 02/06/23 18:50:43.034
STEP: submitting the pod to kubernetes 02/06/23 18:50:43.034
Feb  6 18:50:43.046: INFO: Waiting up to 15m0s for pod "dns-test-3d3de2bf-e587-4a6d-b544-62837755ec35" in namespace "dns-2755" to be "running"
Feb  6 18:50:43.055: INFO: Pod "dns-test-3d3de2bf-e587-4a6d-b544-62837755ec35": Phase="Pending", Reason="", readiness=false. Elapsed: 8.597385ms
Feb  6 18:50:45.062: INFO: Pod "dns-test-3d3de2bf-e587-4a6d-b544-62837755ec35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01518308s
Feb  6 18:50:47.061: INFO: Pod "dns-test-3d3de2bf-e587-4a6d-b544-62837755ec35": Phase="Running", Reason="", readiness=true. Elapsed: 4.014514682s
Feb  6 18:50:47.061: INFO: Pod "dns-test-3d3de2bf-e587-4a6d-b544-62837755ec35" satisfied condition "running"
STEP: retrieving the pod 02/06/23 18:50:47.061
STEP: looking for the results for each expected name from probers 02/06/23 18:50:47.065
Feb  6 18:50:47.082: INFO: DNS probes using dns-2755/dns-test-3d3de2bf-e587-4a6d-b544-62837755ec35 succeeded

STEP: deleting the pod 02/06/23 18:50:47.082
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb  6 18:50:47.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2755" for this suite. 02/06/23 18:50:47.108
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":316,"skipped":5909,"failed":0}
------------------------------
• [4.167 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:50:42.95
    Feb  6 18:50:42.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename dns 02/06/23 18:50:42.952
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:43.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:43.031
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2755.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2755.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     02/06/23 18:50:43.033
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2755.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2755.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     02/06/23 18:50:43.033
    STEP: creating a pod to probe /etc/hosts 02/06/23 18:50:43.034
    STEP: submitting the pod to kubernetes 02/06/23 18:50:43.034
    Feb  6 18:50:43.046: INFO: Waiting up to 15m0s for pod "dns-test-3d3de2bf-e587-4a6d-b544-62837755ec35" in namespace "dns-2755" to be "running"
    Feb  6 18:50:43.055: INFO: Pod "dns-test-3d3de2bf-e587-4a6d-b544-62837755ec35": Phase="Pending", Reason="", readiness=false. Elapsed: 8.597385ms
    Feb  6 18:50:45.062: INFO: Pod "dns-test-3d3de2bf-e587-4a6d-b544-62837755ec35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01518308s
    Feb  6 18:50:47.061: INFO: Pod "dns-test-3d3de2bf-e587-4a6d-b544-62837755ec35": Phase="Running", Reason="", readiness=true. Elapsed: 4.014514682s
    Feb  6 18:50:47.061: INFO: Pod "dns-test-3d3de2bf-e587-4a6d-b544-62837755ec35" satisfied condition "running"
    STEP: retrieving the pod 02/06/23 18:50:47.061
    STEP: looking for the results for each expected name from probers 02/06/23 18:50:47.065
    Feb  6 18:50:47.082: INFO: DNS probes using dns-2755/dns-test-3d3de2bf-e587-4a6d-b544-62837755ec35 succeeded

    STEP: deleting the pod 02/06/23 18:50:47.082
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb  6 18:50:47.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2755" for this suite. 02/06/23 18:50:47.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:50:47.12
Feb  6 18:50:47.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 18:50:47.121
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:47.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:47.143
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-0daa7203-7df8-4fe8-8e79-1b035aca0d2f 02/06/23 18:50:47.146
STEP: Creating a pod to test consume secrets 02/06/23 18:50:47.151
Feb  6 18:50:47.161: INFO: Waiting up to 5m0s for pod "pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0" in namespace "secrets-4607" to be "Succeeded or Failed"
Feb  6 18:50:47.166: INFO: Pod "pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.92129ms
Feb  6 18:50:49.173: INFO: Pod "pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012067819s
Feb  6 18:50:51.171: INFO: Pod "pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010728189s
Feb  6 18:50:53.171: INFO: Pod "pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010446517s
STEP: Saw pod success 02/06/23 18:50:53.171
Feb  6 18:50:53.171: INFO: Pod "pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0" satisfied condition "Succeeded or Failed"
Feb  6 18:50:53.174: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0 container secret-volume-test: <nil>
STEP: delete the pod 02/06/23 18:50:53.183
Feb  6 18:50:53.195: INFO: Waiting for pod pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0 to disappear
Feb  6 18:50:53.198: INFO: Pod pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb  6 18:50:53.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4607" for this suite. 02/06/23 18:50:53.204
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":317,"skipped":5920,"failed":0}
------------------------------
• [SLOW TEST] [6.100 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:50:47.12
    Feb  6 18:50:47.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 18:50:47.121
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:47.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:47.143
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-0daa7203-7df8-4fe8-8e79-1b035aca0d2f 02/06/23 18:50:47.146
    STEP: Creating a pod to test consume secrets 02/06/23 18:50:47.151
    Feb  6 18:50:47.161: INFO: Waiting up to 5m0s for pod "pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0" in namespace "secrets-4607" to be "Succeeded or Failed"
    Feb  6 18:50:47.166: INFO: Pod "pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.92129ms
    Feb  6 18:50:49.173: INFO: Pod "pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012067819s
    Feb  6 18:50:51.171: INFO: Pod "pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010728189s
    Feb  6 18:50:53.171: INFO: Pod "pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010446517s
    STEP: Saw pod success 02/06/23 18:50:53.171
    Feb  6 18:50:53.171: INFO: Pod "pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0" satisfied condition "Succeeded or Failed"
    Feb  6 18:50:53.174: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0 container secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 18:50:53.183
    Feb  6 18:50:53.195: INFO: Waiting for pod pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0 to disappear
    Feb  6 18:50:53.198: INFO: Pod pod-secrets-82c71b90-4644-4642-88cf-b619a61b95a0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 18:50:53.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4607" for this suite. 02/06/23 18:50:53.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:50:53.225
Feb  6 18:50:53.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename svcaccounts 02/06/23 18:50:53.226
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:53.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:53.247
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  02/06/23 18:50:53.25
Feb  6 18:50:53.264: INFO: Waiting up to 5m0s for pod "test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2" in namespace "svcaccounts-6569" to be "Succeeded or Failed"
Feb  6 18:50:53.269: INFO: Pod "test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.989019ms
Feb  6 18:50:55.275: INFO: Pod "test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011394381s
Feb  6 18:50:57.274: INFO: Pod "test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010823511s
Feb  6 18:50:59.275: INFO: Pod "test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011465666s
STEP: Saw pod success 02/06/23 18:50:59.275
Feb  6 18:50:59.275: INFO: Pod "test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2" satisfied condition "Succeeded or Failed"
Feb  6 18:50:59.279: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2 container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:50:59.286
Feb  6 18:50:59.300: INFO: Waiting for pod test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2 to disappear
Feb  6 18:50:59.303: INFO: Pod test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb  6 18:50:59.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6569" for this suite. 02/06/23 18:50:59.308
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":318,"skipped":5966,"failed":0}
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:50:53.225
    Feb  6 18:50:53.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename svcaccounts 02/06/23 18:50:53.226
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:53.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:53.247
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  02/06/23 18:50:53.25
    Feb  6 18:50:53.264: INFO: Waiting up to 5m0s for pod "test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2" in namespace "svcaccounts-6569" to be "Succeeded or Failed"
    Feb  6 18:50:53.269: INFO: Pod "test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.989019ms
    Feb  6 18:50:55.275: INFO: Pod "test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011394381s
    Feb  6 18:50:57.274: INFO: Pod "test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010823511s
    Feb  6 18:50:59.275: INFO: Pod "test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011465666s
    STEP: Saw pod success 02/06/23 18:50:59.275
    Feb  6 18:50:59.275: INFO: Pod "test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2" satisfied condition "Succeeded or Failed"
    Feb  6 18:50:59.279: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2 container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:50:59.286
    Feb  6 18:50:59.300: INFO: Waiting for pod test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2 to disappear
    Feb  6 18:50:59.303: INFO: Pod test-pod-0604eb1a-2375-4e26-9cbc-5688cfd801a2 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb  6 18:50:59.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6569" for this suite. 02/06/23 18:50:59.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:50:59.317
Feb  6 18:50:59.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-lifecycle-hook 02/06/23 18:50:59.319
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:59.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:59.341
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/06/23 18:50:59.349
Feb  6 18:50:59.360: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-540" to be "running and ready"
Feb  6 18:50:59.363: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.007751ms
Feb  6 18:50:59.363: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:51:01.368: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007558867s
Feb  6 18:51:01.368: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:51:03.370: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.009173677s
Feb  6 18:51:03.370: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb  6 18:51:03.370: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 02/06/23 18:51:03.374
Feb  6 18:51:03.381: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-540" to be "running and ready"
Feb  6 18:51:03.391: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.186232ms
Feb  6 18:51:03.391: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:51:05.399: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017026568s
Feb  6 18:51:05.399: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:51:07.395: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.01324171s
Feb  6 18:51:07.395: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Feb  6 18:51:07.395: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/06/23 18:51:07.399
STEP: delete the pod with lifecycle hook 02/06/23 18:51:07.418
Feb  6 18:51:07.428: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  6 18:51:07.434: INFO: Pod pod-with-poststart-http-hook still exists
Feb  6 18:51:09.436: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  6 18:51:09.442: INFO: Pod pod-with-poststart-http-hook still exists
Feb  6 18:51:11.435: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  6 18:51:11.441: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb  6 18:51:11.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-540" for this suite. 02/06/23 18:51:11.446
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":319,"skipped":5983,"failed":0}
------------------------------
• [SLOW TEST] [12.136 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:50:59.317
    Feb  6 18:50:59.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/06/23 18:50:59.319
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:50:59.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:50:59.341
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/06/23 18:50:59.349
    Feb  6 18:50:59.360: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-540" to be "running and ready"
    Feb  6 18:50:59.363: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.007751ms
    Feb  6 18:50:59.363: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:51:01.368: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007558867s
    Feb  6 18:51:01.368: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:51:03.370: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.009173677s
    Feb  6 18:51:03.370: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb  6 18:51:03.370: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 02/06/23 18:51:03.374
    Feb  6 18:51:03.381: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-540" to be "running and ready"
    Feb  6 18:51:03.391: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.186232ms
    Feb  6 18:51:03.391: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:51:05.399: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017026568s
    Feb  6 18:51:05.399: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:51:07.395: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.01324171s
    Feb  6 18:51:07.395: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Feb  6 18:51:07.395: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/06/23 18:51:07.399
    STEP: delete the pod with lifecycle hook 02/06/23 18:51:07.418
    Feb  6 18:51:07.428: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb  6 18:51:07.434: INFO: Pod pod-with-poststart-http-hook still exists
    Feb  6 18:51:09.436: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb  6 18:51:09.442: INFO: Pod pod-with-poststart-http-hook still exists
    Feb  6 18:51:11.435: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb  6 18:51:11.441: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb  6 18:51:11.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-540" for this suite. 02/06/23 18:51:11.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:51:11.456
Feb  6 18:51:11.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pods 02/06/23 18:51:11.457
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:51:11.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:51:11.479
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 02/06/23 18:51:11.481
STEP: submitting the pod to kubernetes 02/06/23 18:51:11.481
Feb  6 18:51:11.491: INFO: Waiting up to 5m0s for pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4" in namespace "pods-3784" to be "running and ready"
Feb  6 18:51:11.499: INFO: Pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.952833ms
Feb  6 18:51:11.499: INFO: The phase of Pod pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:51:13.504: INFO: Pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012178195s
Feb  6 18:51:13.504: INFO: The phase of Pod pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:51:15.504: INFO: Pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4": Phase="Running", Reason="", readiness=true. Elapsed: 4.012665836s
Feb  6 18:51:15.505: INFO: The phase of Pod pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4 is Running (Ready = true)
Feb  6 18:51:15.505: INFO: Pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/06/23 18:51:15.509
STEP: updating the pod 02/06/23 18:51:15.513
Feb  6 18:51:16.028: INFO: Successfully updated pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4"
Feb  6 18:51:16.028: INFO: Waiting up to 5m0s for pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4" in namespace "pods-3784" to be "running"
Feb  6 18:51:16.042: INFO: Pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4": Phase="Running", Reason="", readiness=true. Elapsed: 13.565785ms
Feb  6 18:51:16.042: INFO: Pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 02/06/23 18:51:16.042
Feb  6 18:51:16.047: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb  6 18:51:16.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3784" for this suite. 02/06/23 18:51:16.053
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":320,"skipped":6004,"failed":0}
------------------------------
• [4.606 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:51:11.456
    Feb  6 18:51:11.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pods 02/06/23 18:51:11.457
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:51:11.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:51:11.479
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 02/06/23 18:51:11.481
    STEP: submitting the pod to kubernetes 02/06/23 18:51:11.481
    Feb  6 18:51:11.491: INFO: Waiting up to 5m0s for pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4" in namespace "pods-3784" to be "running and ready"
    Feb  6 18:51:11.499: INFO: Pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.952833ms
    Feb  6 18:51:11.499: INFO: The phase of Pod pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:51:13.504: INFO: Pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012178195s
    Feb  6 18:51:13.504: INFO: The phase of Pod pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:51:15.504: INFO: Pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4": Phase="Running", Reason="", readiness=true. Elapsed: 4.012665836s
    Feb  6 18:51:15.505: INFO: The phase of Pod pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4 is Running (Ready = true)
    Feb  6 18:51:15.505: INFO: Pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/06/23 18:51:15.509
    STEP: updating the pod 02/06/23 18:51:15.513
    Feb  6 18:51:16.028: INFO: Successfully updated pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4"
    Feb  6 18:51:16.028: INFO: Waiting up to 5m0s for pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4" in namespace "pods-3784" to be "running"
    Feb  6 18:51:16.042: INFO: Pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4": Phase="Running", Reason="", readiness=true. Elapsed: 13.565785ms
    Feb  6 18:51:16.042: INFO: Pod "pod-update-b69d8d83-ebc4-43d5-bdc4-c6fa289179a4" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 02/06/23 18:51:16.042
    Feb  6 18:51:16.047: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb  6 18:51:16.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3784" for this suite. 02/06/23 18:51:16.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:51:16.066
Feb  6 18:51:16.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename svc-latency 02/06/23 18:51:16.068
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:51:16.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:51:16.09
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Feb  6 18:51:16.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2123 02/06/23 18:51:16.093
I0206 18:51:16.102866      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2123, replica count: 1
I0206 18:51:17.156368      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 18:51:18.156732      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 18:51:19.157695      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  6 18:51:19.281: INFO: Created: latency-svc-cc4r2
Feb  6 18:51:19.293: INFO: Got endpoints: latency-svc-cc4r2 [33.568229ms]
Feb  6 18:51:19.325: INFO: Created: latency-svc-27rs5
Feb  6 18:51:19.344: INFO: Got endpoints: latency-svc-27rs5 [51.188491ms]
Feb  6 18:51:19.354: INFO: Created: latency-svc-gv9j8
Feb  6 18:51:19.358: INFO: Got endpoints: latency-svc-gv9j8 [65.074713ms]
Feb  6 18:51:19.373: INFO: Created: latency-svc-gphns
Feb  6 18:51:19.399: INFO: Got endpoints: latency-svc-gphns [105.138838ms]
Feb  6 18:51:19.418: INFO: Created: latency-svc-hgrpc
Feb  6 18:51:19.433: INFO: Got endpoints: latency-svc-hgrpc [139.507066ms]
Feb  6 18:51:19.442: INFO: Created: latency-svc-b5gv9
Feb  6 18:51:19.450: INFO: Got endpoints: latency-svc-b5gv9 [155.919907ms]
Feb  6 18:51:19.481: INFO: Created: latency-svc-kfkl4
Feb  6 18:51:19.495: INFO: Got endpoints: latency-svc-kfkl4 [200.650051ms]
Feb  6 18:51:19.506: INFO: Created: latency-svc-zppxh
Feb  6 18:51:19.520: INFO: Got endpoints: latency-svc-zppxh [225.596834ms]
Feb  6 18:51:19.528: INFO: Created: latency-svc-ngb5w
Feb  6 18:51:19.533: INFO: Got endpoints: latency-svc-ngb5w [238.71822ms]
Feb  6 18:51:19.554: INFO: Created: latency-svc-zmh6m
Feb  6 18:51:19.559: INFO: Got endpoints: latency-svc-zmh6m [264.18038ms]
Feb  6 18:51:19.575: INFO: Created: latency-svc-b9zc6
Feb  6 18:51:19.587: INFO: Got endpoints: latency-svc-b9zc6 [291.872216ms]
Feb  6 18:51:19.593: INFO: Created: latency-svc-sz5mv
Feb  6 18:51:19.599: INFO: Got endpoints: latency-svc-sz5mv [304.902304ms]
Feb  6 18:51:19.626: INFO: Created: latency-svc-ttrrq
Feb  6 18:51:19.626: INFO: Got endpoints: latency-svc-ttrrq [331.005193ms]
Feb  6 18:51:19.658: INFO: Created: latency-svc-wzrhv
Feb  6 18:51:19.665: INFO: Got endpoints: latency-svc-wzrhv [370.922025ms]
Feb  6 18:51:19.682: INFO: Created: latency-svc-mcf8m
Feb  6 18:51:19.702: INFO: Got endpoints: latency-svc-mcf8m [407.858164ms]
Feb  6 18:51:19.710: INFO: Created: latency-svc-qvr66
Feb  6 18:51:19.714: INFO: Got endpoints: latency-svc-qvr66 [369.631507ms]
Feb  6 18:51:19.736: INFO: Created: latency-svc-8wlcc
Feb  6 18:51:19.744: INFO: Got endpoints: latency-svc-8wlcc [386.179012ms]
Feb  6 18:51:19.757: INFO: Created: latency-svc-2hbcg
Feb  6 18:51:19.773: INFO: Got endpoints: latency-svc-2hbcg [374.772112ms]
Feb  6 18:51:19.782: INFO: Created: latency-svc-lzs8p
Feb  6 18:51:19.791: INFO: Got endpoints: latency-svc-lzs8p [357.505625ms]
Feb  6 18:51:19.806: INFO: Created: latency-svc-n8xhb
Feb  6 18:51:19.809: INFO: Created: latency-svc-b2925
Feb  6 18:51:19.833: INFO: Got endpoints: latency-svc-b2925 [538.586235ms]
Feb  6 18:51:19.833: INFO: Got endpoints: latency-svc-n8xhb [382.672725ms]
Feb  6 18:51:19.839: INFO: Created: latency-svc-k8d9g
Feb  6 18:51:19.852: INFO: Got endpoints: latency-svc-k8d9g [356.482885ms]
Feb  6 18:51:19.860: INFO: Created: latency-svc-xc8l9
Feb  6 18:51:19.874: INFO: Got endpoints: latency-svc-xc8l9 [353.339249ms]
Feb  6 18:51:19.881: INFO: Created: latency-svc-n5kk4
Feb  6 18:51:19.890: INFO: Got endpoints: latency-svc-n5kk4 [356.965892ms]
Feb  6 18:51:19.902: INFO: Created: latency-svc-qzwtt
Feb  6 18:51:19.911: INFO: Got endpoints: latency-svc-qzwtt [351.817658ms]
Feb  6 18:51:19.922: INFO: Created: latency-svc-pfmfv
Feb  6 18:51:19.943: INFO: Got endpoints: latency-svc-pfmfv [356.72711ms]
Feb  6 18:51:19.954: INFO: Created: latency-svc-9rt4v
Feb  6 18:51:19.973: INFO: Got endpoints: latency-svc-9rt4v [373.82396ms]
Feb  6 18:51:19.983: INFO: Created: latency-svc-ltgfx
Feb  6 18:51:19.998: INFO: Got endpoints: latency-svc-ltgfx [371.997355ms]
Feb  6 18:51:20.047: INFO: Created: latency-svc-42hn4
Feb  6 18:51:20.070: INFO: Got endpoints: latency-svc-42hn4 [404.795352ms]
Feb  6 18:51:20.086: INFO: Created: latency-svc-g97xk
Feb  6 18:51:20.111: INFO: Got endpoints: latency-svc-g97xk [408.073464ms]
Feb  6 18:51:20.125: INFO: Created: latency-svc-4w6l8
Feb  6 18:51:20.130: INFO: Got endpoints: latency-svc-4w6l8 [416.518349ms]
Feb  6 18:51:20.155: INFO: Created: latency-svc-7f8jv
Feb  6 18:51:20.169: INFO: Got endpoints: latency-svc-7f8jv [424.064379ms]
Feb  6 18:51:20.176: INFO: Created: latency-svc-6z54x
Feb  6 18:51:20.187: INFO: Got endpoints: latency-svc-6z54x [413.326421ms]
Feb  6 18:51:20.194: INFO: Created: latency-svc-4d6xb
Feb  6 18:51:20.205: INFO: Got endpoints: latency-svc-4d6xb [413.545023ms]
Feb  6 18:51:20.210: INFO: Created: latency-svc-5fg2w
Feb  6 18:51:20.226: INFO: Got endpoints: latency-svc-5fg2w [392.953841ms]
Feb  6 18:51:20.232: INFO: Created: latency-svc-85l4c
Feb  6 18:51:20.246: INFO: Got endpoints: latency-svc-85l4c [412.021928ms]
Feb  6 18:51:20.263: INFO: Created: latency-svc-7nqh6
Feb  6 18:51:20.264: INFO: Got endpoints: latency-svc-7nqh6 [412.23931ms]
Feb  6 18:51:20.285: INFO: Created: latency-svc-66t92
Feb  6 18:51:20.296: INFO: Got endpoints: latency-svc-66t92 [421.402316ms]
Feb  6 18:51:20.311: INFO: Created: latency-svc-4nrmj
Feb  6 18:51:20.331: INFO: Got endpoints: latency-svc-4nrmj [441.17972ms]
Feb  6 18:51:20.341: INFO: Created: latency-svc-rbtsc
Feb  6 18:51:20.348: INFO: Got endpoints: latency-svc-rbtsc [436.731392ms]
Feb  6 18:51:20.361: INFO: Created: latency-svc-np7bx
Feb  6 18:51:20.374: INFO: Got endpoints: latency-svc-np7bx [430.836441ms]
Feb  6 18:51:20.389: INFO: Created: latency-svc-rr6j9
Feb  6 18:51:20.395: INFO: Got endpoints: latency-svc-rr6j9 [421.525332ms]
Feb  6 18:51:20.411: INFO: Created: latency-svc-psgld
Feb  6 18:51:20.423: INFO: Got endpoints: latency-svc-psgld [424.941245ms]
Feb  6 18:51:20.431: INFO: Created: latency-svc-8mms6
Feb  6 18:51:20.444: INFO: Got endpoints: latency-svc-8mms6 [373.841544ms]
Feb  6 18:51:20.453: INFO: Created: latency-svc-8m9lp
Feb  6 18:51:20.467: INFO: Got endpoints: latency-svc-8m9lp [356.28263ms]
Feb  6 18:51:20.476: INFO: Created: latency-svc-pc8kb
Feb  6 18:51:20.479: INFO: Got endpoints: latency-svc-pc8kb [348.319114ms]
Feb  6 18:51:20.504: INFO: Created: latency-svc-g5r2n
Feb  6 18:51:20.512: INFO: Got endpoints: latency-svc-g5r2n [342.75046ms]
Feb  6 18:51:20.518: INFO: Created: latency-svc-vll5z
Feb  6 18:51:20.524: INFO: Got endpoints: latency-svc-vll5z [337.094345ms]
Feb  6 18:51:20.541: INFO: Created: latency-svc-2nt9g
Feb  6 18:51:20.549: INFO: Got endpoints: latency-svc-2nt9g [343.421479ms]
Feb  6 18:51:20.559: INFO: Created: latency-svc-8p97m
Feb  6 18:51:20.568: INFO: Got endpoints: latency-svc-8p97m [341.551892ms]
Feb  6 18:51:20.580: INFO: Created: latency-svc-brscz
Feb  6 18:51:20.587: INFO: Got endpoints: latency-svc-brscz [341.442803ms]
Feb  6 18:51:20.609: INFO: Created: latency-svc-vjs5m
Feb  6 18:51:20.625: INFO: Got endpoints: latency-svc-vjs5m [360.628509ms]
Feb  6 18:51:20.635: INFO: Created: latency-svc-gq2j6
Feb  6 18:51:20.648: INFO: Got endpoints: latency-svc-gq2j6 [351.99028ms]
Feb  6 18:51:20.654: INFO: Created: latency-svc-7qxsw
Feb  6 18:51:20.662: INFO: Got endpoints: latency-svc-7qxsw [330.607948ms]
Feb  6 18:51:20.670: INFO: Created: latency-svc-ngr56
Feb  6 18:51:20.680: INFO: Got endpoints: latency-svc-ngr56 [332.373504ms]
Feb  6 18:51:20.688: INFO: Created: latency-svc-s7rxg
Feb  6 18:51:20.697: INFO: Got endpoints: latency-svc-s7rxg [322.548209ms]
Feb  6 18:51:20.712: INFO: Created: latency-svc-vq7cf
Feb  6 18:51:20.723: INFO: Got endpoints: latency-svc-vq7cf [328.211083ms]
Feb  6 18:51:20.739: INFO: Created: latency-svc-48zjw
Feb  6 18:51:20.752: INFO: Got endpoints: latency-svc-48zjw [328.770999ms]
Feb  6 18:51:20.760: INFO: Created: latency-svc-ssltt
Feb  6 18:51:20.783: INFO: Got endpoints: latency-svc-ssltt [338.065589ms]
Feb  6 18:51:20.798: INFO: Created: latency-svc-wdxtj
Feb  6 18:51:20.808: INFO: Got endpoints: latency-svc-wdxtj [340.457792ms]
Feb  6 18:51:20.825: INFO: Created: latency-svc-6t2g5
Feb  6 18:51:20.844: INFO: Got endpoints: latency-svc-6t2g5 [364.983268ms]
Feb  6 18:51:20.853: INFO: Created: latency-svc-qzhnk
Feb  6 18:51:20.870: INFO: Created: latency-svc-s2fdn
Feb  6 18:51:20.893: INFO: Got endpoints: latency-svc-qzhnk [381.374437ms]
Feb  6 18:51:20.895: INFO: Created: latency-svc-m9krf
Feb  6 18:51:20.908: INFO: Created: latency-svc-k675w
Feb  6 18:51:20.938: INFO: Created: latency-svc-kj6cz
Feb  6 18:51:20.943: INFO: Got endpoints: latency-svc-s2fdn [418.809174ms]
Feb  6 18:51:20.956: INFO: Created: latency-svc-xrjs8
Feb  6 18:51:20.978: INFO: Created: latency-svc-9wqpd
Feb  6 18:51:20.991: INFO: Created: latency-svc-wlnxh
Feb  6 18:51:20.994: INFO: Got endpoints: latency-svc-m9krf [445.271747ms]
Feb  6 18:51:21.012: INFO: Created: latency-svc-6bw59
Feb  6 18:51:21.039: INFO: Created: latency-svc-tqhvp
Feb  6 18:51:21.043: INFO: Got endpoints: latency-svc-k675w [475.495403ms]
Feb  6 18:51:21.057: INFO: Created: latency-svc-q68zn
Feb  6 18:51:21.075: INFO: Created: latency-svc-dtw5c
Feb  6 18:51:21.098: INFO: Created: latency-svc-6h6mv
Feb  6 18:51:21.101: INFO: Got endpoints: latency-svc-kj6cz [513.42872ms]
Feb  6 18:51:21.118: INFO: Created: latency-svc-pq8b5
Feb  6 18:51:21.137: INFO: Created: latency-svc-47jt4
Feb  6 18:51:21.153: INFO: Got endpoints: latency-svc-xrjs8 [527.877202ms]
Feb  6 18:51:21.163: INFO: Created: latency-svc-pm9pv
Feb  6 18:51:21.188: INFO: Created: latency-svc-7pw2x
Feb  6 18:51:21.199: INFO: Got endpoints: latency-svc-9wqpd [551.162532ms]
Feb  6 18:51:21.208: INFO: Created: latency-svc-vcbkc
Feb  6 18:51:21.230: INFO: Created: latency-svc-rlznh
Feb  6 18:51:21.245: INFO: Got endpoints: latency-svc-wlnxh [582.563008ms]
Feb  6 18:51:21.268: INFO: Created: latency-svc-wgjlr
Feb  6 18:51:21.285: INFO: Created: latency-svc-nmnd9
Feb  6 18:51:21.297: INFO: Got endpoints: latency-svc-6bw59 [616.6609ms]
Feb  6 18:51:21.312: INFO: Created: latency-svc-tpdl4
Feb  6 18:51:21.333: INFO: Created: latency-svc-8qpkc
Feb  6 18:51:21.353: INFO: Got endpoints: latency-svc-tqhvp [655.001592ms]
Feb  6 18:51:21.378: INFO: Created: latency-svc-7zgk5
Feb  6 18:51:21.397: INFO: Got endpoints: latency-svc-q68zn [673.496311ms]
Feb  6 18:51:21.408: INFO: Created: latency-svc-nbbxc
Feb  6 18:51:21.433: INFO: Created: latency-svc-x57d5
Feb  6 18:51:21.448: INFO: Got endpoints: latency-svc-dtw5c [696.193217ms]
Feb  6 18:51:21.481: INFO: Created: latency-svc-5ff5p
Feb  6 18:51:21.496: INFO: Got endpoints: latency-svc-6h6mv [713.240307ms]
Feb  6 18:51:21.522: INFO: Created: latency-svc-r59gq
Feb  6 18:51:21.546: INFO: Got endpoints: latency-svc-pq8b5 [737.386044ms]
Feb  6 18:51:21.583: INFO: Created: latency-svc-r7rnz
Feb  6 18:51:21.602: INFO: Got endpoints: latency-svc-47jt4 [757.86117ms]
Feb  6 18:51:21.629: INFO: Created: latency-svc-qbptc
Feb  6 18:51:21.646: INFO: Got endpoints: latency-svc-pm9pv [752.710107ms]
Feb  6 18:51:21.687: INFO: Created: latency-svc-kzzqh
Feb  6 18:51:21.702: INFO: Got endpoints: latency-svc-7pw2x [758.166368ms]
Feb  6 18:51:21.727: INFO: Created: latency-svc-l7qsp
Feb  6 18:51:21.745: INFO: Got endpoints: latency-svc-vcbkc [751.161654ms]
Feb  6 18:51:21.771: INFO: Created: latency-svc-689xw
Feb  6 18:51:21.800: INFO: Got endpoints: latency-svc-rlznh [756.611672ms]
Feb  6 18:51:21.823: INFO: Created: latency-svc-4k7gn
Feb  6 18:51:21.845: INFO: Got endpoints: latency-svc-wgjlr [743.307277ms]
Feb  6 18:51:21.873: INFO: Created: latency-svc-wz2w5
Feb  6 18:51:21.907: INFO: Got endpoints: latency-svc-nmnd9 [753.295518ms]
Feb  6 18:51:21.929: INFO: Created: latency-svc-2xjv9
Feb  6 18:51:21.944: INFO: Got endpoints: latency-svc-tpdl4 [744.498409ms]
Feb  6 18:51:21.970: INFO: Created: latency-svc-vnl4z
Feb  6 18:51:21.990: INFO: Got endpoints: latency-svc-8qpkc [744.997464ms]
Feb  6 18:51:22.020: INFO: Created: latency-svc-mvq9n
Feb  6 18:51:22.038: INFO: Got endpoints: latency-svc-7zgk5 [740.70504ms]
Feb  6 18:51:22.063: INFO: Created: latency-svc-7m9ch
Feb  6 18:51:22.096: INFO: Got endpoints: latency-svc-nbbxc [742.618829ms]
Feb  6 18:51:22.124: INFO: Created: latency-svc-wc7dl
Feb  6 18:51:22.139: INFO: Got endpoints: latency-svc-x57d5 [742.114639ms]
Feb  6 18:51:22.174: INFO: Created: latency-svc-g65c4
Feb  6 18:51:22.191: INFO: Got endpoints: latency-svc-5ff5p [742.077276ms]
Feb  6 18:51:22.218: INFO: Created: latency-svc-4bmmn
Feb  6 18:51:22.240: INFO: Got endpoints: latency-svc-r59gq [743.421445ms]
Feb  6 18:51:22.260: INFO: Created: latency-svc-9fgts
Feb  6 18:51:22.294: INFO: Got endpoints: latency-svc-r7rnz [747.986755ms]
Feb  6 18:51:22.322: INFO: Created: latency-svc-b6cqg
Feb  6 18:51:22.345: INFO: Got endpoints: latency-svc-qbptc [743.027267ms]
Feb  6 18:51:22.372: INFO: Created: latency-svc-xjtbf
Feb  6 18:51:22.390: INFO: Got endpoints: latency-svc-kzzqh [743.554602ms]
Feb  6 18:51:22.419: INFO: Created: latency-svc-2lcvj
Feb  6 18:51:22.446: INFO: Got endpoints: latency-svc-l7qsp [744.173194ms]
Feb  6 18:51:22.478: INFO: Created: latency-svc-xdvjl
Feb  6 18:51:22.500: INFO: Got endpoints: latency-svc-689xw [754.339616ms]
Feb  6 18:51:22.520: INFO: Created: latency-svc-vhdxf
Feb  6 18:51:22.554: INFO: Got endpoints: latency-svc-4k7gn [754.497293ms]
Feb  6 18:51:22.586: INFO: Created: latency-svc-4f82w
Feb  6 18:51:22.597: INFO: Got endpoints: latency-svc-wz2w5 [752.904203ms]
Feb  6 18:51:22.620: INFO: Created: latency-svc-k5fp4
Feb  6 18:51:22.643: INFO: Got endpoints: latency-svc-2xjv9 [735.626714ms]
Feb  6 18:51:22.669: INFO: Created: latency-svc-fjrx2
Feb  6 18:51:22.692: INFO: Got endpoints: latency-svc-vnl4z [747.297949ms]
Feb  6 18:51:22.718: INFO: Created: latency-svc-w979q
Feb  6 18:51:22.741: INFO: Got endpoints: latency-svc-mvq9n [750.636074ms]
Feb  6 18:51:22.770: INFO: Created: latency-svc-s5fvr
Feb  6 18:51:22.793: INFO: Got endpoints: latency-svc-7m9ch [754.65422ms]
Feb  6 18:51:22.819: INFO: Created: latency-svc-vmcv5
Feb  6 18:51:22.843: INFO: Got endpoints: latency-svc-wc7dl [747.864299ms]
Feb  6 18:51:22.876: INFO: Created: latency-svc-f8hgs
Feb  6 18:51:22.893: INFO: Got endpoints: latency-svc-g65c4 [753.677292ms]
Feb  6 18:51:22.913: INFO: Created: latency-svc-gtlzs
Feb  6 18:51:22.944: INFO: Got endpoints: latency-svc-4bmmn [753.635109ms]
Feb  6 18:51:22.974: INFO: Created: latency-svc-ktsvt
Feb  6 18:51:22.991: INFO: Got endpoints: latency-svc-9fgts [751.756337ms]
Feb  6 18:51:23.012: INFO: Created: latency-svc-xmqv6
Feb  6 18:51:23.043: INFO: Got endpoints: latency-svc-b6cqg [748.660486ms]
Feb  6 18:51:23.071: INFO: Created: latency-svc-clll4
Feb  6 18:51:23.092: INFO: Got endpoints: latency-svc-xjtbf [746.816553ms]
Feb  6 18:51:23.118: INFO: Created: latency-svc-vtk46
Feb  6 18:51:23.146: INFO: Got endpoints: latency-svc-2lcvj [755.878284ms]
Feb  6 18:51:23.174: INFO: Created: latency-svc-dwx4h
Feb  6 18:51:23.197: INFO: Got endpoints: latency-svc-xdvjl [750.988524ms]
Feb  6 18:51:23.226: INFO: Created: latency-svc-pdrg5
Feb  6 18:51:23.239: INFO: Got endpoints: latency-svc-vhdxf [738.613478ms]
Feb  6 18:51:23.267: INFO: Created: latency-svc-jjckq
Feb  6 18:51:23.291: INFO: Got endpoints: latency-svc-4f82w [736.199742ms]
Feb  6 18:51:23.336: INFO: Created: latency-svc-q2d4d
Feb  6 18:51:23.356: INFO: Got endpoints: latency-svc-k5fp4 [758.351704ms]
Feb  6 18:51:23.418: INFO: Got endpoints: latency-svc-fjrx2 [774.932732ms]
Feb  6 18:51:23.453: INFO: Created: latency-svc-ndrmf
Feb  6 18:51:23.466: INFO: Got endpoints: latency-svc-w979q [772.766969ms]
Feb  6 18:51:23.479: INFO: Created: latency-svc-d29nv
Feb  6 18:51:23.493: INFO: Got endpoints: latency-svc-s5fvr [751.691964ms]
Feb  6 18:51:23.500: INFO: Created: latency-svc-p7xsz
Feb  6 18:51:23.524: INFO: Created: latency-svc-5pc6s
Feb  6 18:51:23.539: INFO: Got endpoints: latency-svc-vmcv5 [745.645744ms]
Feb  6 18:51:23.563: INFO: Created: latency-svc-qp4ms
Feb  6 18:51:23.592: INFO: Got endpoints: latency-svc-f8hgs [748.073146ms]
Feb  6 18:51:23.616: INFO: Created: latency-svc-5m5r5
Feb  6 18:51:23.641: INFO: Got endpoints: latency-svc-gtlzs [747.643629ms]
Feb  6 18:51:23.662: INFO: Created: latency-svc-kn795
Feb  6 18:51:23.691: INFO: Got endpoints: latency-svc-ktsvt [746.533274ms]
Feb  6 18:51:23.713: INFO: Created: latency-svc-m27b7
Feb  6 18:51:23.742: INFO: Got endpoints: latency-svc-xmqv6 [750.949356ms]
Feb  6 18:51:23.763: INFO: Created: latency-svc-tnr6f
Feb  6 18:51:23.796: INFO: Got endpoints: latency-svc-clll4 [753.780845ms]
Feb  6 18:51:23.817: INFO: Created: latency-svc-82cwr
Feb  6 18:51:23.842: INFO: Got endpoints: latency-svc-vtk46 [749.99315ms]
Feb  6 18:51:23.860: INFO: Created: latency-svc-9jfqg
Feb  6 18:51:23.892: INFO: Got endpoints: latency-svc-dwx4h [745.832555ms]
Feb  6 18:51:23.913: INFO: Created: latency-svc-s29qc
Feb  6 18:51:23.941: INFO: Got endpoints: latency-svc-pdrg5 [744.097254ms]
Feb  6 18:51:23.963: INFO: Created: latency-svc-bzwjl
Feb  6 18:51:23.992: INFO: Got endpoints: latency-svc-jjckq [753.028132ms]
Feb  6 18:51:24.010: INFO: Created: latency-svc-vwmb5
Feb  6 18:51:24.043: INFO: Got endpoints: latency-svc-q2d4d [750.820395ms]
Feb  6 18:51:24.072: INFO: Created: latency-svc-x54cl
Feb  6 18:51:24.094: INFO: Got endpoints: latency-svc-ndrmf [737.395255ms]
Feb  6 18:51:24.112: INFO: Created: latency-svc-z6fbv
Feb  6 18:51:24.143: INFO: Got endpoints: latency-svc-d29nv [724.977078ms]
Feb  6 18:51:24.164: INFO: Created: latency-svc-zncg7
Feb  6 18:51:24.190: INFO: Got endpoints: latency-svc-p7xsz [723.810526ms]
Feb  6 18:51:24.208: INFO: Created: latency-svc-wjw54
Feb  6 18:51:24.244: INFO: Got endpoints: latency-svc-5pc6s [750.818062ms]
Feb  6 18:51:24.269: INFO: Created: latency-svc-nrspj
Feb  6 18:51:24.290: INFO: Got endpoints: latency-svc-qp4ms [751.074502ms]
Feb  6 18:51:24.320: INFO: Created: latency-svc-gl2cc
Feb  6 18:51:24.345: INFO: Got endpoints: latency-svc-5m5r5 [753.313339ms]
Feb  6 18:51:24.372: INFO: Created: latency-svc-r8l49
Feb  6 18:51:24.392: INFO: Got endpoints: latency-svc-kn795 [751.397617ms]
Feb  6 18:51:24.416: INFO: Created: latency-svc-95xrs
Feb  6 18:51:24.442: INFO: Got endpoints: latency-svc-m27b7 [750.753727ms]
Feb  6 18:51:24.489: INFO: Created: latency-svc-hnt8g
Feb  6 18:51:24.493: INFO: Got endpoints: latency-svc-tnr6f [750.876717ms]
Feb  6 18:51:24.513: INFO: Created: latency-svc-7948b
Feb  6 18:51:24.542: INFO: Got endpoints: latency-svc-82cwr [745.607398ms]
Feb  6 18:51:24.564: INFO: Created: latency-svc-fvt7r
Feb  6 18:51:24.591: INFO: Got endpoints: latency-svc-9jfqg [749.273463ms]
Feb  6 18:51:24.611: INFO: Created: latency-svc-bssb5
Feb  6 18:51:24.640: INFO: Got endpoints: latency-svc-s29qc [747.570765ms]
Feb  6 18:51:24.658: INFO: Created: latency-svc-hmxzz
Feb  6 18:51:24.697: INFO: Got endpoints: latency-svc-bzwjl [755.407428ms]
Feb  6 18:51:24.718: INFO: Created: latency-svc-qwdl4
Feb  6 18:51:24.741: INFO: Got endpoints: latency-svc-vwmb5 [748.815845ms]
Feb  6 18:51:24.761: INFO: Created: latency-svc-zg24v
Feb  6 18:51:24.791: INFO: Got endpoints: latency-svc-x54cl [748.481871ms]
Feb  6 18:51:24.816: INFO: Created: latency-svc-l29ns
Feb  6 18:51:24.843: INFO: Got endpoints: latency-svc-z6fbv [748.654107ms]
Feb  6 18:51:24.862: INFO: Created: latency-svc-jc95d
Feb  6 18:51:24.894: INFO: Got endpoints: latency-svc-zncg7 [751.339545ms]
Feb  6 18:51:24.916: INFO: Created: latency-svc-6h2p5
Feb  6 18:51:24.945: INFO: Got endpoints: latency-svc-wjw54 [755.596522ms]
Feb  6 18:51:24.973: INFO: Created: latency-svc-hqpmz
Feb  6 18:51:24.993: INFO: Got endpoints: latency-svc-nrspj [748.801329ms]
Feb  6 18:51:25.016: INFO: Created: latency-svc-8l4c7
Feb  6 18:51:25.045: INFO: Got endpoints: latency-svc-gl2cc [754.802021ms]
Feb  6 18:51:25.066: INFO: Created: latency-svc-bk26j
Feb  6 18:51:25.093: INFO: Got endpoints: latency-svc-r8l49 [747.140732ms]
Feb  6 18:51:25.116: INFO: Created: latency-svc-xmfmf
Feb  6 18:51:25.139: INFO: Got endpoints: latency-svc-95xrs [746.520906ms]
Feb  6 18:51:25.162: INFO: Created: latency-svc-57fqk
Feb  6 18:51:25.194: INFO: Got endpoints: latency-svc-hnt8g [751.447133ms]
Feb  6 18:51:25.216: INFO: Created: latency-svc-f6wt8
Feb  6 18:51:25.244: INFO: Got endpoints: latency-svc-7948b [749.889225ms]
Feb  6 18:51:25.262: INFO: Created: latency-svc-2zcqn
Feb  6 18:51:25.293: INFO: Got endpoints: latency-svc-fvt7r [750.031358ms]
Feb  6 18:51:25.314: INFO: Created: latency-svc-lbd26
Feb  6 18:51:25.344: INFO: Got endpoints: latency-svc-bssb5 [752.093798ms]
Feb  6 18:51:25.375: INFO: Created: latency-svc-6rbps
Feb  6 18:51:25.389: INFO: Got endpoints: latency-svc-hmxzz [749.521129ms]
Feb  6 18:51:25.419: INFO: Created: latency-svc-9pgl6
Feb  6 18:51:25.443: INFO: Got endpoints: latency-svc-qwdl4 [745.478382ms]
Feb  6 18:51:25.465: INFO: Created: latency-svc-xwbjq
Feb  6 18:51:25.491: INFO: Got endpoints: latency-svc-zg24v [750.300211ms]
Feb  6 18:51:25.512: INFO: Created: latency-svc-jskfv
Feb  6 18:51:25.545: INFO: Got endpoints: latency-svc-l29ns [753.950077ms]
Feb  6 18:51:25.569: INFO: Created: latency-svc-9xh7j
Feb  6 18:51:25.597: INFO: Got endpoints: latency-svc-jc95d [753.898826ms]
Feb  6 18:51:25.644: INFO: Created: latency-svc-m87sz
Feb  6 18:51:25.650: INFO: Got endpoints: latency-svc-6h2p5 [755.509468ms]
Feb  6 18:51:25.672: INFO: Created: latency-svc-rdg7l
Feb  6 18:51:25.693: INFO: Got endpoints: latency-svc-hqpmz [746.557277ms]
Feb  6 18:51:25.716: INFO: Created: latency-svc-dz7xk
Feb  6 18:51:25.744: INFO: Got endpoints: latency-svc-8l4c7 [750.950306ms]
Feb  6 18:51:25.780: INFO: Created: latency-svc-d2zlq
Feb  6 18:51:25.791: INFO: Got endpoints: latency-svc-bk26j [745.925708ms]
Feb  6 18:51:25.813: INFO: Created: latency-svc-2jrxd
Feb  6 18:51:25.841: INFO: Got endpoints: latency-svc-xmfmf [747.867375ms]
Feb  6 18:51:25.874: INFO: Created: latency-svc-xksxg
Feb  6 18:51:25.893: INFO: Got endpoints: latency-svc-57fqk [753.856431ms]
Feb  6 18:51:25.918: INFO: Created: latency-svc-gs8cx
Feb  6 18:51:25.944: INFO: Got endpoints: latency-svc-f6wt8 [749.898666ms]
Feb  6 18:51:25.962: INFO: Created: latency-svc-7f9gz
Feb  6 18:51:25.994: INFO: Got endpoints: latency-svc-2zcqn [750.26107ms]
Feb  6 18:51:26.013: INFO: Created: latency-svc-tkvfj
Feb  6 18:51:26.042: INFO: Got endpoints: latency-svc-lbd26 [749.702354ms]
Feb  6 18:51:26.064: INFO: Created: latency-svc-29z9l
Feb  6 18:51:26.091: INFO: Got endpoints: latency-svc-6rbps [746.651842ms]
Feb  6 18:51:26.111: INFO: Created: latency-svc-h4hd9
Feb  6 18:51:26.138: INFO: Got endpoints: latency-svc-9pgl6 [749.068398ms]
Feb  6 18:51:26.159: INFO: Created: latency-svc-qmbcv
Feb  6 18:51:26.191: INFO: Got endpoints: latency-svc-xwbjq [747.970999ms]
Feb  6 18:51:26.216: INFO: Created: latency-svc-m6q2h
Feb  6 18:51:26.245: INFO: Got endpoints: latency-svc-jskfv [753.266262ms]
Feb  6 18:51:26.263: INFO: Created: latency-svc-f859n
Feb  6 18:51:26.293: INFO: Got endpoints: latency-svc-9xh7j [747.89847ms]
Feb  6 18:51:26.324: INFO: Created: latency-svc-zdppf
Feb  6 18:51:26.343: INFO: Got endpoints: latency-svc-m87sz [746.543944ms]
Feb  6 18:51:26.364: INFO: Created: latency-svc-f5fl2
Feb  6 18:51:26.401: INFO: Got endpoints: latency-svc-rdg7l [750.81139ms]
Feb  6 18:51:26.425: INFO: Created: latency-svc-r922j
Feb  6 18:51:26.442: INFO: Got endpoints: latency-svc-dz7xk [749.078059ms]
Feb  6 18:51:26.465: INFO: Created: latency-svc-djtbj
Feb  6 18:51:26.499: INFO: Got endpoints: latency-svc-d2zlq [755.183749ms]
Feb  6 18:51:26.523: INFO: Created: latency-svc-x86ph
Feb  6 18:51:26.543: INFO: Got endpoints: latency-svc-2jrxd [751.286324ms]
Feb  6 18:51:26.564: INFO: Created: latency-svc-7kzm4
Feb  6 18:51:26.592: INFO: Got endpoints: latency-svc-xksxg [751.438615ms]
Feb  6 18:51:26.623: INFO: Created: latency-svc-7cc7p
Feb  6 18:51:26.645: INFO: Got endpoints: latency-svc-gs8cx [751.583728ms]
Feb  6 18:51:26.667: INFO: Created: latency-svc-2qgq5
Feb  6 18:51:26.693: INFO: Got endpoints: latency-svc-7f9gz [749.447493ms]
Feb  6 18:51:26.724: INFO: Created: latency-svc-whc76
Feb  6 18:51:26.743: INFO: Got endpoints: latency-svc-tkvfj [748.359076ms]
Feb  6 18:51:26.763: INFO: Created: latency-svc-hw68q
Feb  6 18:51:26.792: INFO: Got endpoints: latency-svc-29z9l [748.960637ms]
Feb  6 18:51:26.814: INFO: Created: latency-svc-cwnkr
Feb  6 18:51:26.846: INFO: Got endpoints: latency-svc-h4hd9 [755.616553ms]
Feb  6 18:51:26.869: INFO: Created: latency-svc-tf49s
Feb  6 18:51:26.892: INFO: Got endpoints: latency-svc-qmbcv [753.295664ms]
Feb  6 18:51:26.911: INFO: Created: latency-svc-htsps
Feb  6 18:51:26.943: INFO: Got endpoints: latency-svc-m6q2h [751.548289ms]
Feb  6 18:51:26.971: INFO: Created: latency-svc-57vkl
Feb  6 18:51:26.997: INFO: Got endpoints: latency-svc-f859n [751.888867ms]
Feb  6 18:51:27.022: INFO: Created: latency-svc-dcr74
Feb  6 18:51:27.046: INFO: Got endpoints: latency-svc-zdppf [752.38918ms]
Feb  6 18:51:27.074: INFO: Created: latency-svc-hk9k2
Feb  6 18:51:27.092: INFO: Got endpoints: latency-svc-f5fl2 [748.571281ms]
Feb  6 18:51:27.126: INFO: Created: latency-svc-8drjd
Feb  6 18:51:27.141: INFO: Got endpoints: latency-svc-r922j [739.290601ms]
Feb  6 18:51:27.190: INFO: Got endpoints: latency-svc-djtbj [748.152777ms]
Feb  6 18:51:27.243: INFO: Got endpoints: latency-svc-x86ph [743.013437ms]
Feb  6 18:51:27.292: INFO: Got endpoints: latency-svc-7kzm4 [748.145924ms]
Feb  6 18:51:27.347: INFO: Got endpoints: latency-svc-7cc7p [754.943554ms]
Feb  6 18:51:27.397: INFO: Got endpoints: latency-svc-2qgq5 [751.754432ms]
Feb  6 18:51:27.444: INFO: Got endpoints: latency-svc-whc76 [750.180278ms]
Feb  6 18:51:27.494: INFO: Got endpoints: latency-svc-hw68q [750.765445ms]
Feb  6 18:51:27.539: INFO: Got endpoints: latency-svc-cwnkr [747.048624ms]
Feb  6 18:51:27.592: INFO: Got endpoints: latency-svc-tf49s [745.901804ms]
Feb  6 18:51:27.644: INFO: Got endpoints: latency-svc-htsps [752.278942ms]
Feb  6 18:51:27.692: INFO: Got endpoints: latency-svc-57vkl [749.418358ms]
Feb  6 18:51:27.743: INFO: Got endpoints: latency-svc-dcr74 [745.622174ms]
Feb  6 18:51:27.794: INFO: Got endpoints: latency-svc-hk9k2 [747.471248ms]
Feb  6 18:51:27.847: INFO: Got endpoints: latency-svc-8drjd [754.765259ms]
Feb  6 18:51:27.847: INFO: Latencies: [51.188491ms 65.074713ms 105.138838ms 139.507066ms 155.919907ms 200.650051ms 225.596834ms 238.71822ms 264.18038ms 291.872216ms 304.902304ms 322.548209ms 328.211083ms 328.770999ms 330.607948ms 331.005193ms 332.373504ms 337.094345ms 338.065589ms 340.457792ms 341.442803ms 341.551892ms 342.75046ms 343.421479ms 348.319114ms 351.817658ms 351.99028ms 353.339249ms 356.28263ms 356.482885ms 356.72711ms 356.965892ms 357.505625ms 360.628509ms 364.983268ms 369.631507ms 370.922025ms 371.997355ms 373.82396ms 373.841544ms 374.772112ms 381.374437ms 382.672725ms 386.179012ms 392.953841ms 404.795352ms 407.858164ms 408.073464ms 412.021928ms 412.23931ms 413.326421ms 413.545023ms 416.518349ms 418.809174ms 421.402316ms 421.525332ms 424.064379ms 424.941245ms 430.836441ms 436.731392ms 441.17972ms 445.271747ms 475.495403ms 513.42872ms 527.877202ms 538.586235ms 551.162532ms 582.563008ms 616.6609ms 655.001592ms 673.496311ms 696.193217ms 713.240307ms 723.810526ms 724.977078ms 735.626714ms 736.199742ms 737.386044ms 737.395255ms 738.613478ms 739.290601ms 740.70504ms 742.077276ms 742.114639ms 742.618829ms 743.013437ms 743.027267ms 743.307277ms 743.421445ms 743.554602ms 744.097254ms 744.173194ms 744.498409ms 744.997464ms 745.478382ms 745.607398ms 745.622174ms 745.645744ms 745.832555ms 745.901804ms 745.925708ms 746.520906ms 746.533274ms 746.543944ms 746.557277ms 746.651842ms 746.816553ms 747.048624ms 747.140732ms 747.297949ms 747.471248ms 747.570765ms 747.643629ms 747.864299ms 747.867375ms 747.89847ms 747.970999ms 747.986755ms 748.073146ms 748.145924ms 748.152777ms 748.359076ms 748.481871ms 748.571281ms 748.654107ms 748.660486ms 748.801329ms 748.815845ms 748.960637ms 749.068398ms 749.078059ms 749.273463ms 749.418358ms 749.447493ms 749.521129ms 749.702354ms 749.889225ms 749.898666ms 749.99315ms 750.031358ms 750.180278ms 750.26107ms 750.300211ms 750.636074ms 750.753727ms 750.765445ms 750.81139ms 750.818062ms 750.820395ms 750.876717ms 750.949356ms 750.950306ms 750.988524ms 751.074502ms 751.161654ms 751.286324ms 751.339545ms 751.397617ms 751.438615ms 751.447133ms 751.548289ms 751.583728ms 751.691964ms 751.754432ms 751.756337ms 751.888867ms 752.093798ms 752.278942ms 752.38918ms 752.710107ms 752.904203ms 753.028132ms 753.266262ms 753.295518ms 753.295664ms 753.313339ms 753.635109ms 753.677292ms 753.780845ms 753.856431ms 753.898826ms 753.950077ms 754.339616ms 754.497293ms 754.65422ms 754.765259ms 754.802021ms 754.943554ms 755.183749ms 755.407428ms 755.509468ms 755.596522ms 755.616553ms 755.878284ms 756.611672ms 757.86117ms 758.166368ms 758.351704ms 772.766969ms 774.932732ms]
Feb  6 18:51:27.847: INFO: 50 %ile: 745.925708ms
Feb  6 18:51:27.847: INFO: 90 %ile: 753.898826ms
Feb  6 18:51:27.847: INFO: 99 %ile: 772.766969ms
Feb  6 18:51:27.847: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Feb  6 18:51:27.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2123" for this suite. 02/06/23 18:51:27.858
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":321,"skipped":6017,"failed":0}
------------------------------
• [SLOW TEST] [11.805 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:51:16.066
    Feb  6 18:51:16.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename svc-latency 02/06/23 18:51:16.068
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:51:16.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:51:16.09
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Feb  6 18:51:16.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-2123 02/06/23 18:51:16.093
    I0206 18:51:16.102866      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2123, replica count: 1
    I0206 18:51:17.156368      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 18:51:18.156732      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 18:51:19.157695      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb  6 18:51:19.281: INFO: Created: latency-svc-cc4r2
    Feb  6 18:51:19.293: INFO: Got endpoints: latency-svc-cc4r2 [33.568229ms]
    Feb  6 18:51:19.325: INFO: Created: latency-svc-27rs5
    Feb  6 18:51:19.344: INFO: Got endpoints: latency-svc-27rs5 [51.188491ms]
    Feb  6 18:51:19.354: INFO: Created: latency-svc-gv9j8
    Feb  6 18:51:19.358: INFO: Got endpoints: latency-svc-gv9j8 [65.074713ms]
    Feb  6 18:51:19.373: INFO: Created: latency-svc-gphns
    Feb  6 18:51:19.399: INFO: Got endpoints: latency-svc-gphns [105.138838ms]
    Feb  6 18:51:19.418: INFO: Created: latency-svc-hgrpc
    Feb  6 18:51:19.433: INFO: Got endpoints: latency-svc-hgrpc [139.507066ms]
    Feb  6 18:51:19.442: INFO: Created: latency-svc-b5gv9
    Feb  6 18:51:19.450: INFO: Got endpoints: latency-svc-b5gv9 [155.919907ms]
    Feb  6 18:51:19.481: INFO: Created: latency-svc-kfkl4
    Feb  6 18:51:19.495: INFO: Got endpoints: latency-svc-kfkl4 [200.650051ms]
    Feb  6 18:51:19.506: INFO: Created: latency-svc-zppxh
    Feb  6 18:51:19.520: INFO: Got endpoints: latency-svc-zppxh [225.596834ms]
    Feb  6 18:51:19.528: INFO: Created: latency-svc-ngb5w
    Feb  6 18:51:19.533: INFO: Got endpoints: latency-svc-ngb5w [238.71822ms]
    Feb  6 18:51:19.554: INFO: Created: latency-svc-zmh6m
    Feb  6 18:51:19.559: INFO: Got endpoints: latency-svc-zmh6m [264.18038ms]
    Feb  6 18:51:19.575: INFO: Created: latency-svc-b9zc6
    Feb  6 18:51:19.587: INFO: Got endpoints: latency-svc-b9zc6 [291.872216ms]
    Feb  6 18:51:19.593: INFO: Created: latency-svc-sz5mv
    Feb  6 18:51:19.599: INFO: Got endpoints: latency-svc-sz5mv [304.902304ms]
    Feb  6 18:51:19.626: INFO: Created: latency-svc-ttrrq
    Feb  6 18:51:19.626: INFO: Got endpoints: latency-svc-ttrrq [331.005193ms]
    Feb  6 18:51:19.658: INFO: Created: latency-svc-wzrhv
    Feb  6 18:51:19.665: INFO: Got endpoints: latency-svc-wzrhv [370.922025ms]
    Feb  6 18:51:19.682: INFO: Created: latency-svc-mcf8m
    Feb  6 18:51:19.702: INFO: Got endpoints: latency-svc-mcf8m [407.858164ms]
    Feb  6 18:51:19.710: INFO: Created: latency-svc-qvr66
    Feb  6 18:51:19.714: INFO: Got endpoints: latency-svc-qvr66 [369.631507ms]
    Feb  6 18:51:19.736: INFO: Created: latency-svc-8wlcc
    Feb  6 18:51:19.744: INFO: Got endpoints: latency-svc-8wlcc [386.179012ms]
    Feb  6 18:51:19.757: INFO: Created: latency-svc-2hbcg
    Feb  6 18:51:19.773: INFO: Got endpoints: latency-svc-2hbcg [374.772112ms]
    Feb  6 18:51:19.782: INFO: Created: latency-svc-lzs8p
    Feb  6 18:51:19.791: INFO: Got endpoints: latency-svc-lzs8p [357.505625ms]
    Feb  6 18:51:19.806: INFO: Created: latency-svc-n8xhb
    Feb  6 18:51:19.809: INFO: Created: latency-svc-b2925
    Feb  6 18:51:19.833: INFO: Got endpoints: latency-svc-b2925 [538.586235ms]
    Feb  6 18:51:19.833: INFO: Got endpoints: latency-svc-n8xhb [382.672725ms]
    Feb  6 18:51:19.839: INFO: Created: latency-svc-k8d9g
    Feb  6 18:51:19.852: INFO: Got endpoints: latency-svc-k8d9g [356.482885ms]
    Feb  6 18:51:19.860: INFO: Created: latency-svc-xc8l9
    Feb  6 18:51:19.874: INFO: Got endpoints: latency-svc-xc8l9 [353.339249ms]
    Feb  6 18:51:19.881: INFO: Created: latency-svc-n5kk4
    Feb  6 18:51:19.890: INFO: Got endpoints: latency-svc-n5kk4 [356.965892ms]
    Feb  6 18:51:19.902: INFO: Created: latency-svc-qzwtt
    Feb  6 18:51:19.911: INFO: Got endpoints: latency-svc-qzwtt [351.817658ms]
    Feb  6 18:51:19.922: INFO: Created: latency-svc-pfmfv
    Feb  6 18:51:19.943: INFO: Got endpoints: latency-svc-pfmfv [356.72711ms]
    Feb  6 18:51:19.954: INFO: Created: latency-svc-9rt4v
    Feb  6 18:51:19.973: INFO: Got endpoints: latency-svc-9rt4v [373.82396ms]
    Feb  6 18:51:19.983: INFO: Created: latency-svc-ltgfx
    Feb  6 18:51:19.998: INFO: Got endpoints: latency-svc-ltgfx [371.997355ms]
    Feb  6 18:51:20.047: INFO: Created: latency-svc-42hn4
    Feb  6 18:51:20.070: INFO: Got endpoints: latency-svc-42hn4 [404.795352ms]
    Feb  6 18:51:20.086: INFO: Created: latency-svc-g97xk
    Feb  6 18:51:20.111: INFO: Got endpoints: latency-svc-g97xk [408.073464ms]
    Feb  6 18:51:20.125: INFO: Created: latency-svc-4w6l8
    Feb  6 18:51:20.130: INFO: Got endpoints: latency-svc-4w6l8 [416.518349ms]
    Feb  6 18:51:20.155: INFO: Created: latency-svc-7f8jv
    Feb  6 18:51:20.169: INFO: Got endpoints: latency-svc-7f8jv [424.064379ms]
    Feb  6 18:51:20.176: INFO: Created: latency-svc-6z54x
    Feb  6 18:51:20.187: INFO: Got endpoints: latency-svc-6z54x [413.326421ms]
    Feb  6 18:51:20.194: INFO: Created: latency-svc-4d6xb
    Feb  6 18:51:20.205: INFO: Got endpoints: latency-svc-4d6xb [413.545023ms]
    Feb  6 18:51:20.210: INFO: Created: latency-svc-5fg2w
    Feb  6 18:51:20.226: INFO: Got endpoints: latency-svc-5fg2w [392.953841ms]
    Feb  6 18:51:20.232: INFO: Created: latency-svc-85l4c
    Feb  6 18:51:20.246: INFO: Got endpoints: latency-svc-85l4c [412.021928ms]
    Feb  6 18:51:20.263: INFO: Created: latency-svc-7nqh6
    Feb  6 18:51:20.264: INFO: Got endpoints: latency-svc-7nqh6 [412.23931ms]
    Feb  6 18:51:20.285: INFO: Created: latency-svc-66t92
    Feb  6 18:51:20.296: INFO: Got endpoints: latency-svc-66t92 [421.402316ms]
    Feb  6 18:51:20.311: INFO: Created: latency-svc-4nrmj
    Feb  6 18:51:20.331: INFO: Got endpoints: latency-svc-4nrmj [441.17972ms]
    Feb  6 18:51:20.341: INFO: Created: latency-svc-rbtsc
    Feb  6 18:51:20.348: INFO: Got endpoints: latency-svc-rbtsc [436.731392ms]
    Feb  6 18:51:20.361: INFO: Created: latency-svc-np7bx
    Feb  6 18:51:20.374: INFO: Got endpoints: latency-svc-np7bx [430.836441ms]
    Feb  6 18:51:20.389: INFO: Created: latency-svc-rr6j9
    Feb  6 18:51:20.395: INFO: Got endpoints: latency-svc-rr6j9 [421.525332ms]
    Feb  6 18:51:20.411: INFO: Created: latency-svc-psgld
    Feb  6 18:51:20.423: INFO: Got endpoints: latency-svc-psgld [424.941245ms]
    Feb  6 18:51:20.431: INFO: Created: latency-svc-8mms6
    Feb  6 18:51:20.444: INFO: Got endpoints: latency-svc-8mms6 [373.841544ms]
    Feb  6 18:51:20.453: INFO: Created: latency-svc-8m9lp
    Feb  6 18:51:20.467: INFO: Got endpoints: latency-svc-8m9lp [356.28263ms]
    Feb  6 18:51:20.476: INFO: Created: latency-svc-pc8kb
    Feb  6 18:51:20.479: INFO: Got endpoints: latency-svc-pc8kb [348.319114ms]
    Feb  6 18:51:20.504: INFO: Created: latency-svc-g5r2n
    Feb  6 18:51:20.512: INFO: Got endpoints: latency-svc-g5r2n [342.75046ms]
    Feb  6 18:51:20.518: INFO: Created: latency-svc-vll5z
    Feb  6 18:51:20.524: INFO: Got endpoints: latency-svc-vll5z [337.094345ms]
    Feb  6 18:51:20.541: INFO: Created: latency-svc-2nt9g
    Feb  6 18:51:20.549: INFO: Got endpoints: latency-svc-2nt9g [343.421479ms]
    Feb  6 18:51:20.559: INFO: Created: latency-svc-8p97m
    Feb  6 18:51:20.568: INFO: Got endpoints: latency-svc-8p97m [341.551892ms]
    Feb  6 18:51:20.580: INFO: Created: latency-svc-brscz
    Feb  6 18:51:20.587: INFO: Got endpoints: latency-svc-brscz [341.442803ms]
    Feb  6 18:51:20.609: INFO: Created: latency-svc-vjs5m
    Feb  6 18:51:20.625: INFO: Got endpoints: latency-svc-vjs5m [360.628509ms]
    Feb  6 18:51:20.635: INFO: Created: latency-svc-gq2j6
    Feb  6 18:51:20.648: INFO: Got endpoints: latency-svc-gq2j6 [351.99028ms]
    Feb  6 18:51:20.654: INFO: Created: latency-svc-7qxsw
    Feb  6 18:51:20.662: INFO: Got endpoints: latency-svc-7qxsw [330.607948ms]
    Feb  6 18:51:20.670: INFO: Created: latency-svc-ngr56
    Feb  6 18:51:20.680: INFO: Got endpoints: latency-svc-ngr56 [332.373504ms]
    Feb  6 18:51:20.688: INFO: Created: latency-svc-s7rxg
    Feb  6 18:51:20.697: INFO: Got endpoints: latency-svc-s7rxg [322.548209ms]
    Feb  6 18:51:20.712: INFO: Created: latency-svc-vq7cf
    Feb  6 18:51:20.723: INFO: Got endpoints: latency-svc-vq7cf [328.211083ms]
    Feb  6 18:51:20.739: INFO: Created: latency-svc-48zjw
    Feb  6 18:51:20.752: INFO: Got endpoints: latency-svc-48zjw [328.770999ms]
    Feb  6 18:51:20.760: INFO: Created: latency-svc-ssltt
    Feb  6 18:51:20.783: INFO: Got endpoints: latency-svc-ssltt [338.065589ms]
    Feb  6 18:51:20.798: INFO: Created: latency-svc-wdxtj
    Feb  6 18:51:20.808: INFO: Got endpoints: latency-svc-wdxtj [340.457792ms]
    Feb  6 18:51:20.825: INFO: Created: latency-svc-6t2g5
    Feb  6 18:51:20.844: INFO: Got endpoints: latency-svc-6t2g5 [364.983268ms]
    Feb  6 18:51:20.853: INFO: Created: latency-svc-qzhnk
    Feb  6 18:51:20.870: INFO: Created: latency-svc-s2fdn
    Feb  6 18:51:20.893: INFO: Got endpoints: latency-svc-qzhnk [381.374437ms]
    Feb  6 18:51:20.895: INFO: Created: latency-svc-m9krf
    Feb  6 18:51:20.908: INFO: Created: latency-svc-k675w
    Feb  6 18:51:20.938: INFO: Created: latency-svc-kj6cz
    Feb  6 18:51:20.943: INFO: Got endpoints: latency-svc-s2fdn [418.809174ms]
    Feb  6 18:51:20.956: INFO: Created: latency-svc-xrjs8
    Feb  6 18:51:20.978: INFO: Created: latency-svc-9wqpd
    Feb  6 18:51:20.991: INFO: Created: latency-svc-wlnxh
    Feb  6 18:51:20.994: INFO: Got endpoints: latency-svc-m9krf [445.271747ms]
    Feb  6 18:51:21.012: INFO: Created: latency-svc-6bw59
    Feb  6 18:51:21.039: INFO: Created: latency-svc-tqhvp
    Feb  6 18:51:21.043: INFO: Got endpoints: latency-svc-k675w [475.495403ms]
    Feb  6 18:51:21.057: INFO: Created: latency-svc-q68zn
    Feb  6 18:51:21.075: INFO: Created: latency-svc-dtw5c
    Feb  6 18:51:21.098: INFO: Created: latency-svc-6h6mv
    Feb  6 18:51:21.101: INFO: Got endpoints: latency-svc-kj6cz [513.42872ms]
    Feb  6 18:51:21.118: INFO: Created: latency-svc-pq8b5
    Feb  6 18:51:21.137: INFO: Created: latency-svc-47jt4
    Feb  6 18:51:21.153: INFO: Got endpoints: latency-svc-xrjs8 [527.877202ms]
    Feb  6 18:51:21.163: INFO: Created: latency-svc-pm9pv
    Feb  6 18:51:21.188: INFO: Created: latency-svc-7pw2x
    Feb  6 18:51:21.199: INFO: Got endpoints: latency-svc-9wqpd [551.162532ms]
    Feb  6 18:51:21.208: INFO: Created: latency-svc-vcbkc
    Feb  6 18:51:21.230: INFO: Created: latency-svc-rlznh
    Feb  6 18:51:21.245: INFO: Got endpoints: latency-svc-wlnxh [582.563008ms]
    Feb  6 18:51:21.268: INFO: Created: latency-svc-wgjlr
    Feb  6 18:51:21.285: INFO: Created: latency-svc-nmnd9
    Feb  6 18:51:21.297: INFO: Got endpoints: latency-svc-6bw59 [616.6609ms]
    Feb  6 18:51:21.312: INFO: Created: latency-svc-tpdl4
    Feb  6 18:51:21.333: INFO: Created: latency-svc-8qpkc
    Feb  6 18:51:21.353: INFO: Got endpoints: latency-svc-tqhvp [655.001592ms]
    Feb  6 18:51:21.378: INFO: Created: latency-svc-7zgk5
    Feb  6 18:51:21.397: INFO: Got endpoints: latency-svc-q68zn [673.496311ms]
    Feb  6 18:51:21.408: INFO: Created: latency-svc-nbbxc
    Feb  6 18:51:21.433: INFO: Created: latency-svc-x57d5
    Feb  6 18:51:21.448: INFO: Got endpoints: latency-svc-dtw5c [696.193217ms]
    Feb  6 18:51:21.481: INFO: Created: latency-svc-5ff5p
    Feb  6 18:51:21.496: INFO: Got endpoints: latency-svc-6h6mv [713.240307ms]
    Feb  6 18:51:21.522: INFO: Created: latency-svc-r59gq
    Feb  6 18:51:21.546: INFO: Got endpoints: latency-svc-pq8b5 [737.386044ms]
    Feb  6 18:51:21.583: INFO: Created: latency-svc-r7rnz
    Feb  6 18:51:21.602: INFO: Got endpoints: latency-svc-47jt4 [757.86117ms]
    Feb  6 18:51:21.629: INFO: Created: latency-svc-qbptc
    Feb  6 18:51:21.646: INFO: Got endpoints: latency-svc-pm9pv [752.710107ms]
    Feb  6 18:51:21.687: INFO: Created: latency-svc-kzzqh
    Feb  6 18:51:21.702: INFO: Got endpoints: latency-svc-7pw2x [758.166368ms]
    Feb  6 18:51:21.727: INFO: Created: latency-svc-l7qsp
    Feb  6 18:51:21.745: INFO: Got endpoints: latency-svc-vcbkc [751.161654ms]
    Feb  6 18:51:21.771: INFO: Created: latency-svc-689xw
    Feb  6 18:51:21.800: INFO: Got endpoints: latency-svc-rlznh [756.611672ms]
    Feb  6 18:51:21.823: INFO: Created: latency-svc-4k7gn
    Feb  6 18:51:21.845: INFO: Got endpoints: latency-svc-wgjlr [743.307277ms]
    Feb  6 18:51:21.873: INFO: Created: latency-svc-wz2w5
    Feb  6 18:51:21.907: INFO: Got endpoints: latency-svc-nmnd9 [753.295518ms]
    Feb  6 18:51:21.929: INFO: Created: latency-svc-2xjv9
    Feb  6 18:51:21.944: INFO: Got endpoints: latency-svc-tpdl4 [744.498409ms]
    Feb  6 18:51:21.970: INFO: Created: latency-svc-vnl4z
    Feb  6 18:51:21.990: INFO: Got endpoints: latency-svc-8qpkc [744.997464ms]
    Feb  6 18:51:22.020: INFO: Created: latency-svc-mvq9n
    Feb  6 18:51:22.038: INFO: Got endpoints: latency-svc-7zgk5 [740.70504ms]
    Feb  6 18:51:22.063: INFO: Created: latency-svc-7m9ch
    Feb  6 18:51:22.096: INFO: Got endpoints: latency-svc-nbbxc [742.618829ms]
    Feb  6 18:51:22.124: INFO: Created: latency-svc-wc7dl
    Feb  6 18:51:22.139: INFO: Got endpoints: latency-svc-x57d5 [742.114639ms]
    Feb  6 18:51:22.174: INFO: Created: latency-svc-g65c4
    Feb  6 18:51:22.191: INFO: Got endpoints: latency-svc-5ff5p [742.077276ms]
    Feb  6 18:51:22.218: INFO: Created: latency-svc-4bmmn
    Feb  6 18:51:22.240: INFO: Got endpoints: latency-svc-r59gq [743.421445ms]
    Feb  6 18:51:22.260: INFO: Created: latency-svc-9fgts
    Feb  6 18:51:22.294: INFO: Got endpoints: latency-svc-r7rnz [747.986755ms]
    Feb  6 18:51:22.322: INFO: Created: latency-svc-b6cqg
    Feb  6 18:51:22.345: INFO: Got endpoints: latency-svc-qbptc [743.027267ms]
    Feb  6 18:51:22.372: INFO: Created: latency-svc-xjtbf
    Feb  6 18:51:22.390: INFO: Got endpoints: latency-svc-kzzqh [743.554602ms]
    Feb  6 18:51:22.419: INFO: Created: latency-svc-2lcvj
    Feb  6 18:51:22.446: INFO: Got endpoints: latency-svc-l7qsp [744.173194ms]
    Feb  6 18:51:22.478: INFO: Created: latency-svc-xdvjl
    Feb  6 18:51:22.500: INFO: Got endpoints: latency-svc-689xw [754.339616ms]
    Feb  6 18:51:22.520: INFO: Created: latency-svc-vhdxf
    Feb  6 18:51:22.554: INFO: Got endpoints: latency-svc-4k7gn [754.497293ms]
    Feb  6 18:51:22.586: INFO: Created: latency-svc-4f82w
    Feb  6 18:51:22.597: INFO: Got endpoints: latency-svc-wz2w5 [752.904203ms]
    Feb  6 18:51:22.620: INFO: Created: latency-svc-k5fp4
    Feb  6 18:51:22.643: INFO: Got endpoints: latency-svc-2xjv9 [735.626714ms]
    Feb  6 18:51:22.669: INFO: Created: latency-svc-fjrx2
    Feb  6 18:51:22.692: INFO: Got endpoints: latency-svc-vnl4z [747.297949ms]
    Feb  6 18:51:22.718: INFO: Created: latency-svc-w979q
    Feb  6 18:51:22.741: INFO: Got endpoints: latency-svc-mvq9n [750.636074ms]
    Feb  6 18:51:22.770: INFO: Created: latency-svc-s5fvr
    Feb  6 18:51:22.793: INFO: Got endpoints: latency-svc-7m9ch [754.65422ms]
    Feb  6 18:51:22.819: INFO: Created: latency-svc-vmcv5
    Feb  6 18:51:22.843: INFO: Got endpoints: latency-svc-wc7dl [747.864299ms]
    Feb  6 18:51:22.876: INFO: Created: latency-svc-f8hgs
    Feb  6 18:51:22.893: INFO: Got endpoints: latency-svc-g65c4 [753.677292ms]
    Feb  6 18:51:22.913: INFO: Created: latency-svc-gtlzs
    Feb  6 18:51:22.944: INFO: Got endpoints: latency-svc-4bmmn [753.635109ms]
    Feb  6 18:51:22.974: INFO: Created: latency-svc-ktsvt
    Feb  6 18:51:22.991: INFO: Got endpoints: latency-svc-9fgts [751.756337ms]
    Feb  6 18:51:23.012: INFO: Created: latency-svc-xmqv6
    Feb  6 18:51:23.043: INFO: Got endpoints: latency-svc-b6cqg [748.660486ms]
    Feb  6 18:51:23.071: INFO: Created: latency-svc-clll4
    Feb  6 18:51:23.092: INFO: Got endpoints: latency-svc-xjtbf [746.816553ms]
    Feb  6 18:51:23.118: INFO: Created: latency-svc-vtk46
    Feb  6 18:51:23.146: INFO: Got endpoints: latency-svc-2lcvj [755.878284ms]
    Feb  6 18:51:23.174: INFO: Created: latency-svc-dwx4h
    Feb  6 18:51:23.197: INFO: Got endpoints: latency-svc-xdvjl [750.988524ms]
    Feb  6 18:51:23.226: INFO: Created: latency-svc-pdrg5
    Feb  6 18:51:23.239: INFO: Got endpoints: latency-svc-vhdxf [738.613478ms]
    Feb  6 18:51:23.267: INFO: Created: latency-svc-jjckq
    Feb  6 18:51:23.291: INFO: Got endpoints: latency-svc-4f82w [736.199742ms]
    Feb  6 18:51:23.336: INFO: Created: latency-svc-q2d4d
    Feb  6 18:51:23.356: INFO: Got endpoints: latency-svc-k5fp4 [758.351704ms]
    Feb  6 18:51:23.418: INFO: Got endpoints: latency-svc-fjrx2 [774.932732ms]
    Feb  6 18:51:23.453: INFO: Created: latency-svc-ndrmf
    Feb  6 18:51:23.466: INFO: Got endpoints: latency-svc-w979q [772.766969ms]
    Feb  6 18:51:23.479: INFO: Created: latency-svc-d29nv
    Feb  6 18:51:23.493: INFO: Got endpoints: latency-svc-s5fvr [751.691964ms]
    Feb  6 18:51:23.500: INFO: Created: latency-svc-p7xsz
    Feb  6 18:51:23.524: INFO: Created: latency-svc-5pc6s
    Feb  6 18:51:23.539: INFO: Got endpoints: latency-svc-vmcv5 [745.645744ms]
    Feb  6 18:51:23.563: INFO: Created: latency-svc-qp4ms
    Feb  6 18:51:23.592: INFO: Got endpoints: latency-svc-f8hgs [748.073146ms]
    Feb  6 18:51:23.616: INFO: Created: latency-svc-5m5r5
    Feb  6 18:51:23.641: INFO: Got endpoints: latency-svc-gtlzs [747.643629ms]
    Feb  6 18:51:23.662: INFO: Created: latency-svc-kn795
    Feb  6 18:51:23.691: INFO: Got endpoints: latency-svc-ktsvt [746.533274ms]
    Feb  6 18:51:23.713: INFO: Created: latency-svc-m27b7
    Feb  6 18:51:23.742: INFO: Got endpoints: latency-svc-xmqv6 [750.949356ms]
    Feb  6 18:51:23.763: INFO: Created: latency-svc-tnr6f
    Feb  6 18:51:23.796: INFO: Got endpoints: latency-svc-clll4 [753.780845ms]
    Feb  6 18:51:23.817: INFO: Created: latency-svc-82cwr
    Feb  6 18:51:23.842: INFO: Got endpoints: latency-svc-vtk46 [749.99315ms]
    Feb  6 18:51:23.860: INFO: Created: latency-svc-9jfqg
    Feb  6 18:51:23.892: INFO: Got endpoints: latency-svc-dwx4h [745.832555ms]
    Feb  6 18:51:23.913: INFO: Created: latency-svc-s29qc
    Feb  6 18:51:23.941: INFO: Got endpoints: latency-svc-pdrg5 [744.097254ms]
    Feb  6 18:51:23.963: INFO: Created: latency-svc-bzwjl
    Feb  6 18:51:23.992: INFO: Got endpoints: latency-svc-jjckq [753.028132ms]
    Feb  6 18:51:24.010: INFO: Created: latency-svc-vwmb5
    Feb  6 18:51:24.043: INFO: Got endpoints: latency-svc-q2d4d [750.820395ms]
    Feb  6 18:51:24.072: INFO: Created: latency-svc-x54cl
    Feb  6 18:51:24.094: INFO: Got endpoints: latency-svc-ndrmf [737.395255ms]
    Feb  6 18:51:24.112: INFO: Created: latency-svc-z6fbv
    Feb  6 18:51:24.143: INFO: Got endpoints: latency-svc-d29nv [724.977078ms]
    Feb  6 18:51:24.164: INFO: Created: latency-svc-zncg7
    Feb  6 18:51:24.190: INFO: Got endpoints: latency-svc-p7xsz [723.810526ms]
    Feb  6 18:51:24.208: INFO: Created: latency-svc-wjw54
    Feb  6 18:51:24.244: INFO: Got endpoints: latency-svc-5pc6s [750.818062ms]
    Feb  6 18:51:24.269: INFO: Created: latency-svc-nrspj
    Feb  6 18:51:24.290: INFO: Got endpoints: latency-svc-qp4ms [751.074502ms]
    Feb  6 18:51:24.320: INFO: Created: latency-svc-gl2cc
    Feb  6 18:51:24.345: INFO: Got endpoints: latency-svc-5m5r5 [753.313339ms]
    Feb  6 18:51:24.372: INFO: Created: latency-svc-r8l49
    Feb  6 18:51:24.392: INFO: Got endpoints: latency-svc-kn795 [751.397617ms]
    Feb  6 18:51:24.416: INFO: Created: latency-svc-95xrs
    Feb  6 18:51:24.442: INFO: Got endpoints: latency-svc-m27b7 [750.753727ms]
    Feb  6 18:51:24.489: INFO: Created: latency-svc-hnt8g
    Feb  6 18:51:24.493: INFO: Got endpoints: latency-svc-tnr6f [750.876717ms]
    Feb  6 18:51:24.513: INFO: Created: latency-svc-7948b
    Feb  6 18:51:24.542: INFO: Got endpoints: latency-svc-82cwr [745.607398ms]
    Feb  6 18:51:24.564: INFO: Created: latency-svc-fvt7r
    Feb  6 18:51:24.591: INFO: Got endpoints: latency-svc-9jfqg [749.273463ms]
    Feb  6 18:51:24.611: INFO: Created: latency-svc-bssb5
    Feb  6 18:51:24.640: INFO: Got endpoints: latency-svc-s29qc [747.570765ms]
    Feb  6 18:51:24.658: INFO: Created: latency-svc-hmxzz
    Feb  6 18:51:24.697: INFO: Got endpoints: latency-svc-bzwjl [755.407428ms]
    Feb  6 18:51:24.718: INFO: Created: latency-svc-qwdl4
    Feb  6 18:51:24.741: INFO: Got endpoints: latency-svc-vwmb5 [748.815845ms]
    Feb  6 18:51:24.761: INFO: Created: latency-svc-zg24v
    Feb  6 18:51:24.791: INFO: Got endpoints: latency-svc-x54cl [748.481871ms]
    Feb  6 18:51:24.816: INFO: Created: latency-svc-l29ns
    Feb  6 18:51:24.843: INFO: Got endpoints: latency-svc-z6fbv [748.654107ms]
    Feb  6 18:51:24.862: INFO: Created: latency-svc-jc95d
    Feb  6 18:51:24.894: INFO: Got endpoints: latency-svc-zncg7 [751.339545ms]
    Feb  6 18:51:24.916: INFO: Created: latency-svc-6h2p5
    Feb  6 18:51:24.945: INFO: Got endpoints: latency-svc-wjw54 [755.596522ms]
    Feb  6 18:51:24.973: INFO: Created: latency-svc-hqpmz
    Feb  6 18:51:24.993: INFO: Got endpoints: latency-svc-nrspj [748.801329ms]
    Feb  6 18:51:25.016: INFO: Created: latency-svc-8l4c7
    Feb  6 18:51:25.045: INFO: Got endpoints: latency-svc-gl2cc [754.802021ms]
    Feb  6 18:51:25.066: INFO: Created: latency-svc-bk26j
    Feb  6 18:51:25.093: INFO: Got endpoints: latency-svc-r8l49 [747.140732ms]
    Feb  6 18:51:25.116: INFO: Created: latency-svc-xmfmf
    Feb  6 18:51:25.139: INFO: Got endpoints: latency-svc-95xrs [746.520906ms]
    Feb  6 18:51:25.162: INFO: Created: latency-svc-57fqk
    Feb  6 18:51:25.194: INFO: Got endpoints: latency-svc-hnt8g [751.447133ms]
    Feb  6 18:51:25.216: INFO: Created: latency-svc-f6wt8
    Feb  6 18:51:25.244: INFO: Got endpoints: latency-svc-7948b [749.889225ms]
    Feb  6 18:51:25.262: INFO: Created: latency-svc-2zcqn
    Feb  6 18:51:25.293: INFO: Got endpoints: latency-svc-fvt7r [750.031358ms]
    Feb  6 18:51:25.314: INFO: Created: latency-svc-lbd26
    Feb  6 18:51:25.344: INFO: Got endpoints: latency-svc-bssb5 [752.093798ms]
    Feb  6 18:51:25.375: INFO: Created: latency-svc-6rbps
    Feb  6 18:51:25.389: INFO: Got endpoints: latency-svc-hmxzz [749.521129ms]
    Feb  6 18:51:25.419: INFO: Created: latency-svc-9pgl6
    Feb  6 18:51:25.443: INFO: Got endpoints: latency-svc-qwdl4 [745.478382ms]
    Feb  6 18:51:25.465: INFO: Created: latency-svc-xwbjq
    Feb  6 18:51:25.491: INFO: Got endpoints: latency-svc-zg24v [750.300211ms]
    Feb  6 18:51:25.512: INFO: Created: latency-svc-jskfv
    Feb  6 18:51:25.545: INFO: Got endpoints: latency-svc-l29ns [753.950077ms]
    Feb  6 18:51:25.569: INFO: Created: latency-svc-9xh7j
    Feb  6 18:51:25.597: INFO: Got endpoints: latency-svc-jc95d [753.898826ms]
    Feb  6 18:51:25.644: INFO: Created: latency-svc-m87sz
    Feb  6 18:51:25.650: INFO: Got endpoints: latency-svc-6h2p5 [755.509468ms]
    Feb  6 18:51:25.672: INFO: Created: latency-svc-rdg7l
    Feb  6 18:51:25.693: INFO: Got endpoints: latency-svc-hqpmz [746.557277ms]
    Feb  6 18:51:25.716: INFO: Created: latency-svc-dz7xk
    Feb  6 18:51:25.744: INFO: Got endpoints: latency-svc-8l4c7 [750.950306ms]
    Feb  6 18:51:25.780: INFO: Created: latency-svc-d2zlq
    Feb  6 18:51:25.791: INFO: Got endpoints: latency-svc-bk26j [745.925708ms]
    Feb  6 18:51:25.813: INFO: Created: latency-svc-2jrxd
    Feb  6 18:51:25.841: INFO: Got endpoints: latency-svc-xmfmf [747.867375ms]
    Feb  6 18:51:25.874: INFO: Created: latency-svc-xksxg
    Feb  6 18:51:25.893: INFO: Got endpoints: latency-svc-57fqk [753.856431ms]
    Feb  6 18:51:25.918: INFO: Created: latency-svc-gs8cx
    Feb  6 18:51:25.944: INFO: Got endpoints: latency-svc-f6wt8 [749.898666ms]
    Feb  6 18:51:25.962: INFO: Created: latency-svc-7f9gz
    Feb  6 18:51:25.994: INFO: Got endpoints: latency-svc-2zcqn [750.26107ms]
    Feb  6 18:51:26.013: INFO: Created: latency-svc-tkvfj
    Feb  6 18:51:26.042: INFO: Got endpoints: latency-svc-lbd26 [749.702354ms]
    Feb  6 18:51:26.064: INFO: Created: latency-svc-29z9l
    Feb  6 18:51:26.091: INFO: Got endpoints: latency-svc-6rbps [746.651842ms]
    Feb  6 18:51:26.111: INFO: Created: latency-svc-h4hd9
    Feb  6 18:51:26.138: INFO: Got endpoints: latency-svc-9pgl6 [749.068398ms]
    Feb  6 18:51:26.159: INFO: Created: latency-svc-qmbcv
    Feb  6 18:51:26.191: INFO: Got endpoints: latency-svc-xwbjq [747.970999ms]
    Feb  6 18:51:26.216: INFO: Created: latency-svc-m6q2h
    Feb  6 18:51:26.245: INFO: Got endpoints: latency-svc-jskfv [753.266262ms]
    Feb  6 18:51:26.263: INFO: Created: latency-svc-f859n
    Feb  6 18:51:26.293: INFO: Got endpoints: latency-svc-9xh7j [747.89847ms]
    Feb  6 18:51:26.324: INFO: Created: latency-svc-zdppf
    Feb  6 18:51:26.343: INFO: Got endpoints: latency-svc-m87sz [746.543944ms]
    Feb  6 18:51:26.364: INFO: Created: latency-svc-f5fl2
    Feb  6 18:51:26.401: INFO: Got endpoints: latency-svc-rdg7l [750.81139ms]
    Feb  6 18:51:26.425: INFO: Created: latency-svc-r922j
    Feb  6 18:51:26.442: INFO: Got endpoints: latency-svc-dz7xk [749.078059ms]
    Feb  6 18:51:26.465: INFO: Created: latency-svc-djtbj
    Feb  6 18:51:26.499: INFO: Got endpoints: latency-svc-d2zlq [755.183749ms]
    Feb  6 18:51:26.523: INFO: Created: latency-svc-x86ph
    Feb  6 18:51:26.543: INFO: Got endpoints: latency-svc-2jrxd [751.286324ms]
    Feb  6 18:51:26.564: INFO: Created: latency-svc-7kzm4
    Feb  6 18:51:26.592: INFO: Got endpoints: latency-svc-xksxg [751.438615ms]
    Feb  6 18:51:26.623: INFO: Created: latency-svc-7cc7p
    Feb  6 18:51:26.645: INFO: Got endpoints: latency-svc-gs8cx [751.583728ms]
    Feb  6 18:51:26.667: INFO: Created: latency-svc-2qgq5
    Feb  6 18:51:26.693: INFO: Got endpoints: latency-svc-7f9gz [749.447493ms]
    Feb  6 18:51:26.724: INFO: Created: latency-svc-whc76
    Feb  6 18:51:26.743: INFO: Got endpoints: latency-svc-tkvfj [748.359076ms]
    Feb  6 18:51:26.763: INFO: Created: latency-svc-hw68q
    Feb  6 18:51:26.792: INFO: Got endpoints: latency-svc-29z9l [748.960637ms]
    Feb  6 18:51:26.814: INFO: Created: latency-svc-cwnkr
    Feb  6 18:51:26.846: INFO: Got endpoints: latency-svc-h4hd9 [755.616553ms]
    Feb  6 18:51:26.869: INFO: Created: latency-svc-tf49s
    Feb  6 18:51:26.892: INFO: Got endpoints: latency-svc-qmbcv [753.295664ms]
    Feb  6 18:51:26.911: INFO: Created: latency-svc-htsps
    Feb  6 18:51:26.943: INFO: Got endpoints: latency-svc-m6q2h [751.548289ms]
    Feb  6 18:51:26.971: INFO: Created: latency-svc-57vkl
    Feb  6 18:51:26.997: INFO: Got endpoints: latency-svc-f859n [751.888867ms]
    Feb  6 18:51:27.022: INFO: Created: latency-svc-dcr74
    Feb  6 18:51:27.046: INFO: Got endpoints: latency-svc-zdppf [752.38918ms]
    Feb  6 18:51:27.074: INFO: Created: latency-svc-hk9k2
    Feb  6 18:51:27.092: INFO: Got endpoints: latency-svc-f5fl2 [748.571281ms]
    Feb  6 18:51:27.126: INFO: Created: latency-svc-8drjd
    Feb  6 18:51:27.141: INFO: Got endpoints: latency-svc-r922j [739.290601ms]
    Feb  6 18:51:27.190: INFO: Got endpoints: latency-svc-djtbj [748.152777ms]
    Feb  6 18:51:27.243: INFO: Got endpoints: latency-svc-x86ph [743.013437ms]
    Feb  6 18:51:27.292: INFO: Got endpoints: latency-svc-7kzm4 [748.145924ms]
    Feb  6 18:51:27.347: INFO: Got endpoints: latency-svc-7cc7p [754.943554ms]
    Feb  6 18:51:27.397: INFO: Got endpoints: latency-svc-2qgq5 [751.754432ms]
    Feb  6 18:51:27.444: INFO: Got endpoints: latency-svc-whc76 [750.180278ms]
    Feb  6 18:51:27.494: INFO: Got endpoints: latency-svc-hw68q [750.765445ms]
    Feb  6 18:51:27.539: INFO: Got endpoints: latency-svc-cwnkr [747.048624ms]
    Feb  6 18:51:27.592: INFO: Got endpoints: latency-svc-tf49s [745.901804ms]
    Feb  6 18:51:27.644: INFO: Got endpoints: latency-svc-htsps [752.278942ms]
    Feb  6 18:51:27.692: INFO: Got endpoints: latency-svc-57vkl [749.418358ms]
    Feb  6 18:51:27.743: INFO: Got endpoints: latency-svc-dcr74 [745.622174ms]
    Feb  6 18:51:27.794: INFO: Got endpoints: latency-svc-hk9k2 [747.471248ms]
    Feb  6 18:51:27.847: INFO: Got endpoints: latency-svc-8drjd [754.765259ms]
    Feb  6 18:51:27.847: INFO: Latencies: [51.188491ms 65.074713ms 105.138838ms 139.507066ms 155.919907ms 200.650051ms 225.596834ms 238.71822ms 264.18038ms 291.872216ms 304.902304ms 322.548209ms 328.211083ms 328.770999ms 330.607948ms 331.005193ms 332.373504ms 337.094345ms 338.065589ms 340.457792ms 341.442803ms 341.551892ms 342.75046ms 343.421479ms 348.319114ms 351.817658ms 351.99028ms 353.339249ms 356.28263ms 356.482885ms 356.72711ms 356.965892ms 357.505625ms 360.628509ms 364.983268ms 369.631507ms 370.922025ms 371.997355ms 373.82396ms 373.841544ms 374.772112ms 381.374437ms 382.672725ms 386.179012ms 392.953841ms 404.795352ms 407.858164ms 408.073464ms 412.021928ms 412.23931ms 413.326421ms 413.545023ms 416.518349ms 418.809174ms 421.402316ms 421.525332ms 424.064379ms 424.941245ms 430.836441ms 436.731392ms 441.17972ms 445.271747ms 475.495403ms 513.42872ms 527.877202ms 538.586235ms 551.162532ms 582.563008ms 616.6609ms 655.001592ms 673.496311ms 696.193217ms 713.240307ms 723.810526ms 724.977078ms 735.626714ms 736.199742ms 737.386044ms 737.395255ms 738.613478ms 739.290601ms 740.70504ms 742.077276ms 742.114639ms 742.618829ms 743.013437ms 743.027267ms 743.307277ms 743.421445ms 743.554602ms 744.097254ms 744.173194ms 744.498409ms 744.997464ms 745.478382ms 745.607398ms 745.622174ms 745.645744ms 745.832555ms 745.901804ms 745.925708ms 746.520906ms 746.533274ms 746.543944ms 746.557277ms 746.651842ms 746.816553ms 747.048624ms 747.140732ms 747.297949ms 747.471248ms 747.570765ms 747.643629ms 747.864299ms 747.867375ms 747.89847ms 747.970999ms 747.986755ms 748.073146ms 748.145924ms 748.152777ms 748.359076ms 748.481871ms 748.571281ms 748.654107ms 748.660486ms 748.801329ms 748.815845ms 748.960637ms 749.068398ms 749.078059ms 749.273463ms 749.418358ms 749.447493ms 749.521129ms 749.702354ms 749.889225ms 749.898666ms 749.99315ms 750.031358ms 750.180278ms 750.26107ms 750.300211ms 750.636074ms 750.753727ms 750.765445ms 750.81139ms 750.818062ms 750.820395ms 750.876717ms 750.949356ms 750.950306ms 750.988524ms 751.074502ms 751.161654ms 751.286324ms 751.339545ms 751.397617ms 751.438615ms 751.447133ms 751.548289ms 751.583728ms 751.691964ms 751.754432ms 751.756337ms 751.888867ms 752.093798ms 752.278942ms 752.38918ms 752.710107ms 752.904203ms 753.028132ms 753.266262ms 753.295518ms 753.295664ms 753.313339ms 753.635109ms 753.677292ms 753.780845ms 753.856431ms 753.898826ms 753.950077ms 754.339616ms 754.497293ms 754.65422ms 754.765259ms 754.802021ms 754.943554ms 755.183749ms 755.407428ms 755.509468ms 755.596522ms 755.616553ms 755.878284ms 756.611672ms 757.86117ms 758.166368ms 758.351704ms 772.766969ms 774.932732ms]
    Feb  6 18:51:27.847: INFO: 50 %ile: 745.925708ms
    Feb  6 18:51:27.847: INFO: 90 %ile: 753.898826ms
    Feb  6 18:51:27.847: INFO: 99 %ile: 772.766969ms
    Feb  6 18:51:27.847: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Feb  6 18:51:27.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-2123" for this suite. 02/06/23 18:51:27.858
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:51:27.873
Feb  6 18:51:27.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 18:51:27.877
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:51:27.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:51:27.955
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-917 02/06/23 18:51:27.961
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/06/23 18:51:27.981
STEP: creating service externalsvc in namespace services-917 02/06/23 18:51:27.981
STEP: creating replication controller externalsvc in namespace services-917 02/06/23 18:51:28.014
I0206 18:51:28.025415      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-917, replica count: 2
I0206 18:51:31.076421      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 18:51:34.076896      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 02/06/23 18:51:34.084
Feb  6 18:51:34.122: INFO: Creating new exec pod
Feb  6 18:51:34.137: INFO: Waiting up to 5m0s for pod "execpodg597f" in namespace "services-917" to be "running"
Feb  6 18:51:34.150: INFO: Pod "execpodg597f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.677461ms
Feb  6 18:51:36.164: INFO: Pod "execpodg597f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026117818s
Feb  6 18:51:38.165: INFO: Pod "execpodg597f": Phase="Running", Reason="", readiness=true. Elapsed: 4.027040908s
Feb  6 18:51:38.165: INFO: Pod "execpodg597f" satisfied condition "running"
Feb  6 18:51:38.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-917 exec execpodg597f -- /bin/sh -x -c nslookup clusterip-service.services-917.svc.cluster.local'
Feb  6 18:51:38.381: INFO: stderr: "+ nslookup clusterip-service.services-917.svc.cluster.local\n"
Feb  6 18:51:38.381: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-917.svc.cluster.local\tcanonical name = externalsvc.services-917.svc.cluster.local.\nName:\texternalsvc.services-917.svc.cluster.local\nAddress: 10.99.43.48\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-917, will wait for the garbage collector to delete the pods 02/06/23 18:51:38.381
Feb  6 18:51:38.458: INFO: Deleting ReplicationController externalsvc took: 15.005029ms
Feb  6 18:51:38.560: INFO: Terminating ReplicationController externalsvc pods took: 101.432784ms
Feb  6 18:51:41.006: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 18:51:41.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-917" for this suite. 02/06/23 18:51:41.039
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":322,"skipped":6021,"failed":0}
------------------------------
• [SLOW TEST] [13.177 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:51:27.873
    Feb  6 18:51:27.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 18:51:27.877
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:51:27.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:51:27.955
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-917 02/06/23 18:51:27.961
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/06/23 18:51:27.981
    STEP: creating service externalsvc in namespace services-917 02/06/23 18:51:27.981
    STEP: creating replication controller externalsvc in namespace services-917 02/06/23 18:51:28.014
    I0206 18:51:28.025415      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-917, replica count: 2
    I0206 18:51:31.076421      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 18:51:34.076896      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 02/06/23 18:51:34.084
    Feb  6 18:51:34.122: INFO: Creating new exec pod
    Feb  6 18:51:34.137: INFO: Waiting up to 5m0s for pod "execpodg597f" in namespace "services-917" to be "running"
    Feb  6 18:51:34.150: INFO: Pod "execpodg597f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.677461ms
    Feb  6 18:51:36.164: INFO: Pod "execpodg597f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026117818s
    Feb  6 18:51:38.165: INFO: Pod "execpodg597f": Phase="Running", Reason="", readiness=true. Elapsed: 4.027040908s
    Feb  6 18:51:38.165: INFO: Pod "execpodg597f" satisfied condition "running"
    Feb  6 18:51:38.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-917 exec execpodg597f -- /bin/sh -x -c nslookup clusterip-service.services-917.svc.cluster.local'
    Feb  6 18:51:38.381: INFO: stderr: "+ nslookup clusterip-service.services-917.svc.cluster.local\n"
    Feb  6 18:51:38.381: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-917.svc.cluster.local\tcanonical name = externalsvc.services-917.svc.cluster.local.\nName:\texternalsvc.services-917.svc.cluster.local\nAddress: 10.99.43.48\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-917, will wait for the garbage collector to delete the pods 02/06/23 18:51:38.381
    Feb  6 18:51:38.458: INFO: Deleting ReplicationController externalsvc took: 15.005029ms
    Feb  6 18:51:38.560: INFO: Terminating ReplicationController externalsvc pods took: 101.432784ms
    Feb  6 18:51:41.006: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 18:51:41.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-917" for this suite. 02/06/23 18:51:41.039
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:51:41.056
Feb  6 18:51:41.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename configmap 02/06/23 18:51:41.057
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:51:41.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:51:41.133
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-d78a3fbc-e5b7-4e2f-a0d2-b763d842a26b 02/06/23 18:51:41.136
STEP: Creating a pod to test consume configMaps 02/06/23 18:51:41.142
Feb  6 18:51:41.155: INFO: Waiting up to 5m0s for pod "pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e" in namespace "configmap-2511" to be "Succeeded or Failed"
Feb  6 18:51:41.160: INFO: Pod "pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.498849ms
Feb  6 18:51:43.166: INFO: Pod "pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010803108s
Feb  6 18:51:45.166: INFO: Pod "pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010612051s
Feb  6 18:51:47.165: INFO: Pod "pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00999891s
STEP: Saw pod success 02/06/23 18:51:47.165
Feb  6 18:51:47.165: INFO: Pod "pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e" satisfied condition "Succeeded or Failed"
Feb  6 18:51:47.168: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:51:47.175
Feb  6 18:51:47.188: INFO: Waiting for pod pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e to disappear
Feb  6 18:51:47.191: INFO: Pod pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb  6 18:51:47.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2511" for this suite. 02/06/23 18:51:47.196
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":323,"skipped":6028,"failed":0}
------------------------------
• [SLOW TEST] [6.147 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:51:41.056
    Feb  6 18:51:41.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename configmap 02/06/23 18:51:41.057
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:51:41.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:51:41.133
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-d78a3fbc-e5b7-4e2f-a0d2-b763d842a26b 02/06/23 18:51:41.136
    STEP: Creating a pod to test consume configMaps 02/06/23 18:51:41.142
    Feb  6 18:51:41.155: INFO: Waiting up to 5m0s for pod "pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e" in namespace "configmap-2511" to be "Succeeded or Failed"
    Feb  6 18:51:41.160: INFO: Pod "pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.498849ms
    Feb  6 18:51:43.166: INFO: Pod "pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010803108s
    Feb  6 18:51:45.166: INFO: Pod "pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010612051s
    Feb  6 18:51:47.165: INFO: Pod "pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00999891s
    STEP: Saw pod success 02/06/23 18:51:47.165
    Feb  6 18:51:47.165: INFO: Pod "pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e" satisfied condition "Succeeded or Failed"
    Feb  6 18:51:47.168: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:51:47.175
    Feb  6 18:51:47.188: INFO: Waiting for pod pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e to disappear
    Feb  6 18:51:47.191: INFO: Pod pod-configmaps-8fd66bf1-0416-4ebc-adb5-e6031707737e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb  6 18:51:47.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2511" for this suite. 02/06/23 18:51:47.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:51:47.206
Feb  6 18:51:47.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename prestop 02/06/23 18:51:47.208
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:51:47.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:51:47.228
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-605 02/06/23 18:51:47.23
STEP: Waiting for pods to come up. 02/06/23 18:51:47.239
Feb  6 18:51:47.239: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-605" to be "running"
Feb  6 18:51:47.245: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 6.202431ms
Feb  6 18:51:49.251: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011884939s
Feb  6 18:51:51.251: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.011863552s
Feb  6 18:51:51.251: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-605 02/06/23 18:51:51.254
Feb  6 18:51:51.262: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-605" to be "running"
Feb  6 18:51:51.266: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.598525ms
Feb  6 18:51:53.271: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008663732s
Feb  6 18:51:55.271: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.008680771s
Feb  6 18:51:55.271: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 02/06/23 18:51:55.271
Feb  6 18:52:00.286: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 02/06/23 18:52:00.286
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Feb  6 18:52:00.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-605" for this suite. 02/06/23 18:52:00.318
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":324,"skipped":6052,"failed":0}
------------------------------
• [SLOW TEST] [13.123 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:51:47.206
    Feb  6 18:51:47.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename prestop 02/06/23 18:51:47.208
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:51:47.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:51:47.228
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-605 02/06/23 18:51:47.23
    STEP: Waiting for pods to come up. 02/06/23 18:51:47.239
    Feb  6 18:51:47.239: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-605" to be "running"
    Feb  6 18:51:47.245: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 6.202431ms
    Feb  6 18:51:49.251: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011884939s
    Feb  6 18:51:51.251: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.011863552s
    Feb  6 18:51:51.251: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-605 02/06/23 18:51:51.254
    Feb  6 18:51:51.262: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-605" to be "running"
    Feb  6 18:51:51.266: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.598525ms
    Feb  6 18:51:53.271: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008663732s
    Feb  6 18:51:55.271: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.008680771s
    Feb  6 18:51:55.271: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 02/06/23 18:51:55.271
    Feb  6 18:52:00.286: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 02/06/23 18:52:00.286
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Feb  6 18:52:00.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-605" for this suite. 02/06/23 18:52:00.318
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:52:00.331
Feb  6 18:52:00.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename endpointslice 02/06/23 18:52:00.333
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:52:00.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:52:00.364
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Feb  6 18:52:00.377: INFO: Endpoints addresses: [195.17.37.23 195.17.71.3 195.17.74.46] , ports: [6443]
Feb  6 18:52:00.377: INFO: EndpointSlices addresses: [195.17.37.23 195.17.71.3 195.17.74.46] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb  6 18:52:00.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3943" for this suite. 02/06/23 18:52:00.383
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":325,"skipped":6053,"failed":0}
------------------------------
• [0.060 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:52:00.331
    Feb  6 18:52:00.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename endpointslice 02/06/23 18:52:00.333
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:52:00.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:52:00.364
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Feb  6 18:52:00.377: INFO: Endpoints addresses: [195.17.37.23 195.17.71.3 195.17.74.46] , ports: [6443]
    Feb  6 18:52:00.377: INFO: EndpointSlices addresses: [195.17.37.23 195.17.71.3 195.17.74.46] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb  6 18:52:00.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3943" for this suite. 02/06/23 18:52:00.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:52:00.395
Feb  6 18:52:00.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pods 02/06/23 18:52:00.396
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:52:00.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:52:00.419
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 02/06/23 18:52:00.423
Feb  6 18:52:00.440: INFO: Waiting up to 5m0s for pod "pod-r5k69" in namespace "pods-7210" to be "running"
Feb  6 18:52:00.447: INFO: Pod "pod-r5k69": Phase="Pending", Reason="", readiness=false. Elapsed: 6.97069ms
Feb  6 18:52:02.454: INFO: Pod "pod-r5k69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014638471s
Feb  6 18:52:04.453: INFO: Pod "pod-r5k69": Phase="Running", Reason="", readiness=true. Elapsed: 4.013439719s
Feb  6 18:52:04.453: INFO: Pod "pod-r5k69" satisfied condition "running"
STEP: patching /status 02/06/23 18:52:04.453
Feb  6 18:52:04.464: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb  6 18:52:04.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7210" for this suite. 02/06/23 18:52:04.47
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":326,"skipped":6060,"failed":0}
------------------------------
• [4.084 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:52:00.395
    Feb  6 18:52:00.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pods 02/06/23 18:52:00.396
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:52:00.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:52:00.419
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 02/06/23 18:52:00.423
    Feb  6 18:52:00.440: INFO: Waiting up to 5m0s for pod "pod-r5k69" in namespace "pods-7210" to be "running"
    Feb  6 18:52:00.447: INFO: Pod "pod-r5k69": Phase="Pending", Reason="", readiness=false. Elapsed: 6.97069ms
    Feb  6 18:52:02.454: INFO: Pod "pod-r5k69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014638471s
    Feb  6 18:52:04.453: INFO: Pod "pod-r5k69": Phase="Running", Reason="", readiness=true. Elapsed: 4.013439719s
    Feb  6 18:52:04.453: INFO: Pod "pod-r5k69" satisfied condition "running"
    STEP: patching /status 02/06/23 18:52:04.453
    Feb  6 18:52:04.464: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb  6 18:52:04.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7210" for this suite. 02/06/23 18:52:04.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:52:04.484
Feb  6 18:52:04.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-runtime 02/06/23 18:52:04.485
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:52:04.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:52:04.509
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 02/06/23 18:52:04.512
STEP: wait for the container to reach Succeeded 02/06/23 18:52:04.521
STEP: get the container status 02/06/23 18:52:10.563
STEP: the container should be terminated 02/06/23 18:52:10.567
STEP: the termination message should be set 02/06/23 18:52:10.567
Feb  6 18:52:10.567: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 02/06/23 18:52:10.568
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb  6 18:52:10.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3192" for this suite. 02/06/23 18:52:10.595
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":327,"skipped":6152,"failed":0}
------------------------------
• [SLOW TEST] [6.124 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:52:04.484
    Feb  6 18:52:04.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-runtime 02/06/23 18:52:04.485
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:52:04.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:52:04.509
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 02/06/23 18:52:04.512
    STEP: wait for the container to reach Succeeded 02/06/23 18:52:04.521
    STEP: get the container status 02/06/23 18:52:10.563
    STEP: the container should be terminated 02/06/23 18:52:10.567
    STEP: the termination message should be set 02/06/23 18:52:10.567
    Feb  6 18:52:10.567: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 02/06/23 18:52:10.568
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb  6 18:52:10.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3192" for this suite. 02/06/23 18:52:10.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:52:10.614
Feb  6 18:52:10.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 18:52:10.615
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:52:10.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:52:10.649
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 02/06/23 18:52:10.653
Feb  6 18:52:10.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: rename a version 02/06/23 18:52:28.028
STEP: check the new version name is served 02/06/23 18:52:28.044
STEP: check the old version name is removed 02/06/23 18:52:35.615
STEP: check the other version is not changed 02/06/23 18:52:38.608
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:52:52.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2415" for this suite. 02/06/23 18:52:52.658
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":328,"skipped":6180,"failed":0}
------------------------------
• [SLOW TEST] [42.053 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:52:10.614
    Feb  6 18:52:10.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 18:52:10.615
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:52:10.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:52:10.649
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 02/06/23 18:52:10.653
    Feb  6 18:52:10.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: rename a version 02/06/23 18:52:28.028
    STEP: check the new version name is served 02/06/23 18:52:28.044
    STEP: check the old version name is removed 02/06/23 18:52:35.615
    STEP: check the other version is not changed 02/06/23 18:52:38.608
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:52:52.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2415" for this suite. 02/06/23 18:52:52.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:52:52.67
Feb  6 18:52:52.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename gc 02/06/23 18:52:52.672
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:52:52.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:52:52.693
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 02/06/23 18:52:52.696
STEP: delete the rc 02/06/23 18:52:57.71
STEP: wait for all pods to be garbage collected 02/06/23 18:52:57.718
STEP: Gathering metrics 02/06/23 18:53:02.729
Feb  6 18:53:02.765: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tneyla25-pnn8z" in namespace "kube-system" to be "running and ready"
Feb  6 18:53:02.769: INFO: Pod "kube-controller-manager-tneyla25-pnn8z": Phase="Running", Reason="", readiness=true. Elapsed: 3.768886ms
Feb  6 18:53:02.769: INFO: The phase of Pod kube-controller-manager-tneyla25-pnn8z is Running (Ready = true)
Feb  6 18:53:02.769: INFO: Pod "kube-controller-manager-tneyla25-pnn8z" satisfied condition "running and ready"
Feb  6 18:53:02.859: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb  6 18:53:02.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-666" for this suite. 02/06/23 18:53:02.865
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":329,"skipped":6202,"failed":0}
------------------------------
• [SLOW TEST] [10.201 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:52:52.67
    Feb  6 18:52:52.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename gc 02/06/23 18:52:52.672
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:52:52.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:52:52.693
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 02/06/23 18:52:52.696
    STEP: delete the rc 02/06/23 18:52:57.71
    STEP: wait for all pods to be garbage collected 02/06/23 18:52:57.718
    STEP: Gathering metrics 02/06/23 18:53:02.729
    Feb  6 18:53:02.765: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tneyla25-pnn8z" in namespace "kube-system" to be "running and ready"
    Feb  6 18:53:02.769: INFO: Pod "kube-controller-manager-tneyla25-pnn8z": Phase="Running", Reason="", readiness=true. Elapsed: 3.768886ms
    Feb  6 18:53:02.769: INFO: The phase of Pod kube-controller-manager-tneyla25-pnn8z is Running (Ready = true)
    Feb  6 18:53:02.769: INFO: Pod "kube-controller-manager-tneyla25-pnn8z" satisfied condition "running and ready"
    Feb  6 18:53:02.859: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb  6 18:53:02.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-666" for this suite. 02/06/23 18:53:02.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:53:02.874
Feb  6 18:53:02.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename proxy 02/06/23 18:53:02.876
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:53:02.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:53:02.899
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 02/06/23 18:53:02.924
STEP: creating replication controller proxy-service-ggdlz in namespace proxy-8701 02/06/23 18:53:02.924
I0206 18:53:02.936864      20 runners.go:193] Created replication controller with name: proxy-service-ggdlz, namespace: proxy-8701, replica count: 1
I0206 18:53:03.988219      20 runners.go:193] proxy-service-ggdlz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 18:53:04.988521      20 runners.go:193] proxy-service-ggdlz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 18:53:05.988790      20 runners.go:193] proxy-service-ggdlz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 18:53:06.989133      20 runners.go:193] proxy-service-ggdlz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  6 18:53:06.994: INFO: setup took 4.092131092s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/06/23 18:53:06.994
Feb  6 18:53:07.011: INFO: (0) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 16.515153ms)
Feb  6 18:53:07.011: INFO: (0) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 16.585366ms)
Feb  6 18:53:07.011: INFO: (0) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 16.662125ms)
Feb  6 18:53:07.011: INFO: (0) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 16.565557ms)
Feb  6 18:53:07.012: INFO: (0) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 16.962458ms)
Feb  6 18:53:07.013: INFO: (0) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 18.556235ms)
Feb  6 18:53:07.013: INFO: (0) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 18.850702ms)
Feb  6 18:53:07.013: INFO: (0) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 18.818155ms)
Feb  6 18:53:07.013: INFO: (0) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 19.00096ms)
Feb  6 18:53:07.013: INFO: (0) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 18.684388ms)
Feb  6 18:53:07.015: INFO: (0) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 20.736207ms)
Feb  6 18:53:07.015: INFO: (0) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 21.041305ms)
Feb  6 18:53:07.016: INFO: (0) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 21.254332ms)
Feb  6 18:53:07.016: INFO: (0) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 21.832048ms)
Feb  6 18:53:07.016: INFO: (0) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 21.296226ms)
Feb  6 18:53:07.016: INFO: (0) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 21.431154ms)
Feb  6 18:53:07.036: INFO: (1) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 19.593942ms)
Feb  6 18:53:07.036: INFO: (1) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 18.180396ms)
Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 19.568246ms)
Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 20.205325ms)
Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 19.739732ms)
Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 18.880166ms)
Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 20.191261ms)
Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 19.792219ms)
Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 21.253857ms)
Feb  6 18:53:07.038: INFO: (1) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 21.944994ms)
Feb  6 18:53:07.038: INFO: (1) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 20.027511ms)
Feb  6 18:53:07.044: INFO: (1) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 26.31242ms)
Feb  6 18:53:07.045: INFO: (1) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 27.740944ms)
Feb  6 18:53:07.046: INFO: (1) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 29.137421ms)
Feb  6 18:53:07.046: INFO: (1) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 28.793396ms)
Feb  6 18:53:07.046: INFO: (1) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 28.575029ms)
Feb  6 18:53:07.064: INFO: (2) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 16.605098ms)
Feb  6 18:53:07.064: INFO: (2) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 17.413327ms)
Feb  6 18:53:07.064: INFO: (2) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 17.572415ms)
Feb  6 18:53:07.064: INFO: (2) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 17.580921ms)
Feb  6 18:53:07.065: INFO: (2) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 17.951533ms)
Feb  6 18:53:07.065: INFO: (2) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 17.633994ms)
Feb  6 18:53:07.065: INFO: (2) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 18.097485ms)
Feb  6 18:53:07.065: INFO: (2) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 17.956765ms)
Feb  6 18:53:07.065: INFO: (2) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 18.548639ms)
Feb  6 18:53:07.068: INFO: (2) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 20.47373ms)
Feb  6 18:53:07.070: INFO: (2) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 22.983937ms)
Feb  6 18:53:07.074: INFO: (2) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 26.823233ms)
Feb  6 18:53:07.074: INFO: (2) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 26.776382ms)
Feb  6 18:53:07.074: INFO: (2) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 27.770523ms)
Feb  6 18:53:07.074: INFO: (2) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 27.213755ms)
Feb  6 18:53:07.075: INFO: (2) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 28.120841ms)
Feb  6 18:53:07.090: INFO: (3) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 14.784261ms)
Feb  6 18:53:07.090: INFO: (3) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 15.083574ms)
Feb  6 18:53:07.090: INFO: (3) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 14.79915ms)
Feb  6 18:53:07.090: INFO: (3) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 14.591314ms)
Feb  6 18:53:07.090: INFO: (3) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 15.201864ms)
Feb  6 18:53:07.091: INFO: (3) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 16.054877ms)
Feb  6 18:53:07.094: INFO: (3) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 19.265614ms)
Feb  6 18:53:07.094: INFO: (3) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 18.566757ms)
Feb  6 18:53:07.094: INFO: (3) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 18.630239ms)
Feb  6 18:53:07.095: INFO: (3) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 19.004259ms)
Feb  6 18:53:07.096: INFO: (3) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 21.119652ms)
Feb  6 18:53:07.096: INFO: (3) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 20.764905ms)
Feb  6 18:53:07.096: INFO: (3) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 20.721469ms)
Feb  6 18:53:07.096: INFO: (3) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 20.618698ms)
Feb  6 18:53:07.097: INFO: (3) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 21.113387ms)
Feb  6 18:53:07.097: INFO: (3) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 21.114222ms)
Feb  6 18:53:07.107: INFO: (4) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 9.657121ms)
Feb  6 18:53:07.108: INFO: (4) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 10.428797ms)
Feb  6 18:53:07.108: INFO: (4) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 10.656504ms)
Feb  6 18:53:07.108: INFO: (4) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 10.759785ms)
Feb  6 18:53:07.109: INFO: (4) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.093694ms)
Feb  6 18:53:07.109: INFO: (4) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 11.031681ms)
Feb  6 18:53:07.111: INFO: (4) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 13.70951ms)
Feb  6 18:53:07.111: INFO: (4) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 13.32275ms)
Feb  6 18:53:07.111: INFO: (4) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.828913ms)
Feb  6 18:53:07.111: INFO: (4) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 13.18021ms)
Feb  6 18:53:07.111: INFO: (4) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.998259ms)
Feb  6 18:53:07.112: INFO: (4) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 14.80886ms)
Feb  6 18:53:07.112: INFO: (4) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 13.654063ms)
Feb  6 18:53:07.113: INFO: (4) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 15.549488ms)
Feb  6 18:53:07.114: INFO: (4) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 16.084747ms)
Feb  6 18:53:07.114: INFO: (4) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 15.653481ms)
Feb  6 18:53:07.124: INFO: (5) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 9.00571ms)
Feb  6 18:53:07.127: INFO: (5) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.787031ms)
Feb  6 18:53:07.127: INFO: (5) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.097376ms)
Feb  6 18:53:07.127: INFO: (5) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 11.853948ms)
Feb  6 18:53:07.127: INFO: (5) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.134643ms)
Feb  6 18:53:07.127: INFO: (5) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 12.991439ms)
Feb  6 18:53:07.127: INFO: (5) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.873686ms)
Feb  6 18:53:07.128: INFO: (5) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 13.203044ms)
Feb  6 18:53:07.129: INFO: (5) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 14.750286ms)
Feb  6 18:53:07.130: INFO: (5) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 14.777469ms)
Feb  6 18:53:07.130: INFO: (5) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 14.764715ms)
Feb  6 18:53:07.130: INFO: (5) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 15.449416ms)
Feb  6 18:53:07.131: INFO: (5) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 16.479341ms)
Feb  6 18:53:07.131: INFO: (5) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 16.488511ms)
Feb  6 18:53:07.131: INFO: (5) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 17.155366ms)
Feb  6 18:53:07.131: INFO: (5) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 17.08431ms)
Feb  6 18:53:07.138: INFO: (6) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 6.021249ms)
Feb  6 18:53:07.144: INFO: (6) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 11.863328ms)
Feb  6 18:53:07.144: INFO: (6) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.481426ms)
Feb  6 18:53:07.145: INFO: (6) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.5945ms)
Feb  6 18:53:07.146: INFO: (6) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 14.372ms)
Feb  6 18:53:07.148: INFO: (6) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 14.933667ms)
Feb  6 18:53:07.148: INFO: (6) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 15.305423ms)
Feb  6 18:53:07.148: INFO: (6) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 16.007859ms)
Feb  6 18:53:07.148: INFO: (6) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 16.573131ms)
Feb  6 18:53:07.149: INFO: (6) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 16.95265ms)
Feb  6 18:53:07.155: INFO: (6) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 22.860116ms)
Feb  6 18:53:07.155: INFO: (6) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 22.446047ms)
Feb  6 18:53:07.155: INFO: (6) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 23.147764ms)
Feb  6 18:53:07.155: INFO: (6) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 22.729087ms)
Feb  6 18:53:07.155: INFO: (6) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 23.369528ms)
Feb  6 18:53:07.155: INFO: (6) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 22.944711ms)
Feb  6 18:53:07.168: INFO: (7) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.754637ms)
Feb  6 18:53:07.168: INFO: (7) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.376161ms)
Feb  6 18:53:07.168: INFO: (7) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.17792ms)
Feb  6 18:53:07.168: INFO: (7) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.245761ms)
Feb  6 18:53:07.169: INFO: (7) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 12.140708ms)
Feb  6 18:53:07.170: INFO: (7) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 14.560485ms)
Feb  6 18:53:07.170: INFO: (7) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 13.723564ms)
Feb  6 18:53:07.173: INFO: (7) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 17.174798ms)
Feb  6 18:53:07.173: INFO: (7) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 16.785647ms)
Feb  6 18:53:07.173: INFO: (7) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 16.896897ms)
Feb  6 18:53:07.173: INFO: (7) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 17.269791ms)
Feb  6 18:53:07.175: INFO: (7) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 18.400384ms)
Feb  6 18:53:07.176: INFO: (7) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 19.585918ms)
Feb  6 18:53:07.176: INFO: (7) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 19.74354ms)
Feb  6 18:53:07.176: INFO: (7) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 19.545921ms)
Feb  6 18:53:07.177: INFO: (7) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 20.263939ms)
Feb  6 18:53:07.187: INFO: (8) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 9.658058ms)
Feb  6 18:53:07.187: INFO: (8) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 9.682522ms)
Feb  6 18:53:07.187: INFO: (8) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 9.621198ms)
Feb  6 18:53:07.187: INFO: (8) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 10.466355ms)
Feb  6 18:53:07.188: INFO: (8) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 10.587305ms)
Feb  6 18:53:07.188: INFO: (8) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 11.455361ms)
Feb  6 18:53:07.188: INFO: (8) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 11.464168ms)
Feb  6 18:53:07.188: INFO: (8) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.484277ms)
Feb  6 18:53:07.189: INFO: (8) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 10.413752ms)
Feb  6 18:53:07.189: INFO: (8) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 10.49396ms)
Feb  6 18:53:07.189: INFO: (8) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 10.531809ms)
Feb  6 18:53:07.190: INFO: (8) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 13.225238ms)
Feb  6 18:53:07.192: INFO: (8) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 13.98609ms)
Feb  6 18:53:07.192: INFO: (8) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 14.959949ms)
Feb  6 18:53:07.193: INFO: (8) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 15.404194ms)
Feb  6 18:53:07.193: INFO: (8) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 15.528343ms)
Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.718812ms)
Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 11.291168ms)
Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 11.351862ms)
Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 10.742108ms)
Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 11.904279ms)
Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 11.691288ms)
Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.780842ms)
Feb  6 18:53:07.206: INFO: (9) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 11.750974ms)
Feb  6 18:53:07.207: INFO: (9) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 12.071008ms)
Feb  6 18:53:07.208: INFO: (9) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 14.265816ms)
Feb  6 18:53:07.208: INFO: (9) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 13.680397ms)
Feb  6 18:53:07.209: INFO: (9) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 15.225403ms)
Feb  6 18:53:07.211: INFO: (9) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 17.034416ms)
Feb  6 18:53:07.211: INFO: (9) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 17.216723ms)
Feb  6 18:53:07.212: INFO: (9) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 17.822853ms)
Feb  6 18:53:07.212: INFO: (9) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 18.025727ms)
Feb  6 18:53:07.218: INFO: (10) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 5.849289ms)
Feb  6 18:53:07.219: INFO: (10) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 6.238832ms)
Feb  6 18:53:07.221: INFO: (10) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 8.31489ms)
Feb  6 18:53:07.222: INFO: (10) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 8.922959ms)
Feb  6 18:53:07.222: INFO: (10) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 9.603221ms)
Feb  6 18:53:07.223: INFO: (10) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 10.042765ms)
Feb  6 18:53:07.224: INFO: (10) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.658805ms)
Feb  6 18:53:07.224: INFO: (10) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.826816ms)
Feb  6 18:53:07.225: INFO: (10) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.438524ms)
Feb  6 18:53:07.225: INFO: (10) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 12.100639ms)
Feb  6 18:53:07.226: INFO: (10) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 13.42519ms)
Feb  6 18:53:07.226: INFO: (10) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 14.018873ms)
Feb  6 18:53:07.226: INFO: (10) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 13.57922ms)
Feb  6 18:53:07.227: INFO: (10) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 14.140993ms)
Feb  6 18:53:07.227: INFO: (10) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 13.782945ms)
Feb  6 18:53:07.227: INFO: (10) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 13.96788ms)
Feb  6 18:53:07.237: INFO: (11) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 9.832762ms)
Feb  6 18:53:07.238: INFO: (11) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 10.236243ms)
Feb  6 18:53:07.239: INFO: (11) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.074249ms)
Feb  6 18:53:07.239: INFO: (11) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.797117ms)
Feb  6 18:53:07.239: INFO: (11) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.793239ms)
Feb  6 18:53:07.239: INFO: (11) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.53703ms)
Feb  6 18:53:07.239: INFO: (11) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 11.150967ms)
Feb  6 18:53:07.240: INFO: (11) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.029924ms)
Feb  6 18:53:07.241: INFO: (11) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 13.019453ms)
Feb  6 18:53:07.241: INFO: (11) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 13.275211ms)
Feb  6 18:53:07.243: INFO: (11) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 16.041201ms)
Feb  6 18:53:07.245: INFO: (11) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 16.559709ms)
Feb  6 18:53:07.245: INFO: (11) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 17.238286ms)
Feb  6 18:53:07.245: INFO: (11) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 17.77143ms)
Feb  6 18:53:07.246: INFO: (11) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 18.380594ms)
Feb  6 18:53:07.246: INFO: (11) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 17.451069ms)
Feb  6 18:53:07.252: INFO: (12) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 5.807403ms)
Feb  6 18:53:07.253: INFO: (12) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 6.923344ms)
Feb  6 18:53:07.258: INFO: (12) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.718265ms)
Feb  6 18:53:07.258: INFO: (12) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.022295ms)
Feb  6 18:53:07.258: INFO: (12) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.077297ms)
Feb  6 18:53:07.259: INFO: (12) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.405485ms)
Feb  6 18:53:07.259: INFO: (12) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 12.493044ms)
Feb  6 18:53:07.259: INFO: (12) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 12.678404ms)
Feb  6 18:53:07.261: INFO: (12) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 14.809325ms)
Feb  6 18:53:07.261: INFO: (12) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 15.344935ms)
Feb  6 18:53:07.263: INFO: (12) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 16.821168ms)
Feb  6 18:53:07.266: INFO: (12) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 19.87513ms)
Feb  6 18:53:07.267: INFO: (12) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 21.151224ms)
Feb  6 18:53:07.268: INFO: (12) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 21.571263ms)
Feb  6 18:53:07.268: INFO: (12) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 22.178487ms)
Feb  6 18:53:07.268: INFO: (12) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 21.850239ms)
Feb  6 18:53:07.275: INFO: (13) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 7.131602ms)
Feb  6 18:53:07.276: INFO: (13) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 7.353702ms)
Feb  6 18:53:07.278: INFO: (13) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 8.736705ms)
Feb  6 18:53:07.279: INFO: (13) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 9.911125ms)
Feb  6 18:53:07.279: INFO: (13) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 9.691379ms)
Feb  6 18:53:07.280: INFO: (13) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 11.416517ms)
Feb  6 18:53:07.280: INFO: (13) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 11.56165ms)
Feb  6 18:53:07.281: INFO: (13) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.377092ms)
Feb  6 18:53:07.282: INFO: (13) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 13.374913ms)
Feb  6 18:53:07.282: INFO: (13) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 13.187302ms)
Feb  6 18:53:07.283: INFO: (13) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 13.843765ms)
Feb  6 18:53:07.284: INFO: (13) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 15.231096ms)
Feb  6 18:53:07.284: INFO: (13) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 14.886246ms)
Feb  6 18:53:07.284: INFO: (13) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 15.515133ms)
Feb  6 18:53:07.284: INFO: (13) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 15.464132ms)
Feb  6 18:53:07.285: INFO: (13) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 15.716495ms)
Feb  6 18:53:07.293: INFO: (14) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 7.597848ms)
Feb  6 18:53:07.298: INFO: (14) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 12.983408ms)
Feb  6 18:53:07.299: INFO: (14) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 13.486116ms)
Feb  6 18:53:07.299: INFO: (14) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 13.364194ms)
Feb  6 18:53:07.299: INFO: (14) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 13.943161ms)
Feb  6 18:53:07.299: INFO: (14) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 13.940991ms)
Feb  6 18:53:07.299: INFO: (14) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 14.118513ms)
Feb  6 18:53:07.300: INFO: (14) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 14.584439ms)
Feb  6 18:53:07.300: INFO: (14) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 14.709279ms)
Feb  6 18:53:07.300: INFO: (14) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 14.571928ms)
Feb  6 18:53:07.300: INFO: (14) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 14.703747ms)
Feb  6 18:53:07.301: INFO: (14) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 15.702815ms)
Feb  6 18:53:07.303: INFO: (14) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 17.80448ms)
Feb  6 18:53:07.304: INFO: (14) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 18.290258ms)
Feb  6 18:53:07.304: INFO: (14) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 18.71744ms)
Feb  6 18:53:07.304: INFO: (14) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 18.669959ms)
Feb  6 18:53:07.315: INFO: (15) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 9.976256ms)
Feb  6 18:53:07.315: INFO: (15) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 10.444669ms)
Feb  6 18:53:07.315: INFO: (15) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 10.635645ms)
Feb  6 18:53:07.315: INFO: (15) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.133741ms)
Feb  6 18:53:07.316: INFO: (15) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 11.129364ms)
Feb  6 18:53:07.316: INFO: (15) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.05515ms)
Feb  6 18:53:07.316: INFO: (15) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 11.292047ms)
Feb  6 18:53:07.316: INFO: (15) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 11.645568ms)
Feb  6 18:53:07.316: INFO: (15) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 11.451985ms)
Feb  6 18:53:07.316: INFO: (15) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 11.614288ms)
Feb  6 18:53:07.317: INFO: (15) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.679705ms)
Feb  6 18:53:07.321: INFO: (15) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 16.17054ms)
Feb  6 18:53:07.321: INFO: (15) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 17.002577ms)
Feb  6 18:53:07.321: INFO: (15) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 16.75189ms)
Feb  6 18:53:07.322: INFO: (15) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 17.191922ms)
Feb  6 18:53:07.322: INFO: (15) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 17.491136ms)
Feb  6 18:53:07.334: INFO: (16) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.6761ms)
Feb  6 18:53:07.334: INFO: (16) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 10.778475ms)
Feb  6 18:53:07.335: INFO: (16) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 12.047792ms)
Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 12.605542ms)
Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 13.564675ms)
Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 12.96177ms)
Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 14.220351ms)
Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 13.59596ms)
Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 13.684696ms)
Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 14.122042ms)
Feb  6 18:53:07.337: INFO: (16) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 14.118208ms)
Feb  6 18:53:07.338: INFO: (16) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 15.578478ms)
Feb  6 18:53:07.339: INFO: (16) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 15.911011ms)
Feb  6 18:53:07.339: INFO: (16) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 16.307392ms)
Feb  6 18:53:07.339: INFO: (16) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 16.885988ms)
Feb  6 18:53:07.340: INFO: (16) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 16.485439ms)
Feb  6 18:53:07.351: INFO: (17) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.961653ms)
Feb  6 18:53:07.351: INFO: (17) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.999ms)
Feb  6 18:53:07.351: INFO: (17) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 11.358609ms)
Feb  6 18:53:07.351: INFO: (17) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 11.205557ms)
Feb  6 18:53:07.352: INFO: (17) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 11.522123ms)
Feb  6 18:53:07.352: INFO: (17) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.449309ms)
Feb  6 18:53:07.353: INFO: (17) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 13.422372ms)
Feb  6 18:53:07.354: INFO: (17) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 14.123668ms)
Feb  6 18:53:07.354: INFO: (17) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 14.540801ms)
Feb  6 18:53:07.354: INFO: (17) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 14.404502ms)
Feb  6 18:53:07.354: INFO: (17) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 14.857702ms)
Feb  6 18:53:07.356: INFO: (17) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 15.563527ms)
Feb  6 18:53:07.356: INFO: (17) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 16.687659ms)
Feb  6 18:53:07.357: INFO: (17) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 17.259572ms)
Feb  6 18:53:07.357: INFO: (17) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 17.205801ms)
Feb  6 18:53:07.357: INFO: (17) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 17.540225ms)
Feb  6 18:53:07.368: INFO: (18) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 9.678105ms)
Feb  6 18:53:07.369: INFO: (18) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 10.319329ms)
Feb  6 18:53:07.370: INFO: (18) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 11.968728ms)
Feb  6 18:53:07.370: INFO: (18) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 11.514604ms)
Feb  6 18:53:07.371: INFO: (18) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 12.599583ms)
Feb  6 18:53:07.371: INFO: (18) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.293592ms)
Feb  6 18:53:07.371: INFO: (18) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 12.825579ms)
Feb  6 18:53:07.372: INFO: (18) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 12.479483ms)
Feb  6 18:53:07.372: INFO: (18) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 13.134199ms)
Feb  6 18:53:07.372: INFO: (18) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 14.301206ms)
Feb  6 18:53:07.372: INFO: (18) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.927102ms)
Feb  6 18:53:07.372: INFO: (18) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 14.509072ms)
Feb  6 18:53:07.373: INFO: (18) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 14.751994ms)
Feb  6 18:53:07.373: INFO: (18) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 14.05344ms)
Feb  6 18:53:07.373: INFO: (18) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 15.051022ms)
Feb  6 18:53:07.374: INFO: (18) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 14.924353ms)
Feb  6 18:53:07.387: INFO: (19) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.791232ms)
Feb  6 18:53:07.387: INFO: (19) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 13.542973ms)
Feb  6 18:53:07.387: INFO: (19) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 13.459791ms)
Feb  6 18:53:07.388: INFO: (19) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 13.530987ms)
Feb  6 18:53:07.388: INFO: (19) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 13.671398ms)
Feb  6 18:53:07.388: INFO: (19) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 13.710618ms)
Feb  6 18:53:07.388: INFO: (19) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 14.339136ms)
Feb  6 18:53:07.388: INFO: (19) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 14.285978ms)
Feb  6 18:53:07.389: INFO: (19) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 14.479083ms)
Feb  6 18:53:07.389: INFO: (19) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 14.732223ms)
Feb  6 18:53:07.389: INFO: (19) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 15.026005ms)
Feb  6 18:53:07.390: INFO: (19) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 16.273728ms)
Feb  6 18:53:07.390: INFO: (19) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 16.406191ms)
Feb  6 18:53:07.390: INFO: (19) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 16.299385ms)
Feb  6 18:53:07.390: INFO: (19) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 16.3746ms)
Feb  6 18:53:07.391: INFO: (19) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 16.823275ms)
STEP: deleting ReplicationController proxy-service-ggdlz in namespace proxy-8701, will wait for the garbage collector to delete the pods 02/06/23 18:53:07.391
Feb  6 18:53:07.454: INFO: Deleting ReplicationController proxy-service-ggdlz took: 8.609377ms
Feb  6 18:53:07.554: INFO: Terminating ReplicationController proxy-service-ggdlz pods took: 100.413574ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb  6 18:53:10.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8701" for this suite. 02/06/23 18:53:10.565
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":330,"skipped":6222,"failed":0}
------------------------------
• [SLOW TEST] [7.700 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:53:02.874
    Feb  6 18:53:02.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename proxy 02/06/23 18:53:02.876
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:53:02.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:53:02.899
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 02/06/23 18:53:02.924
    STEP: creating replication controller proxy-service-ggdlz in namespace proxy-8701 02/06/23 18:53:02.924
    I0206 18:53:02.936864      20 runners.go:193] Created replication controller with name: proxy-service-ggdlz, namespace: proxy-8701, replica count: 1
    I0206 18:53:03.988219      20 runners.go:193] proxy-service-ggdlz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 18:53:04.988521      20 runners.go:193] proxy-service-ggdlz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 18:53:05.988790      20 runners.go:193] proxy-service-ggdlz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 18:53:06.989133      20 runners.go:193] proxy-service-ggdlz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb  6 18:53:06.994: INFO: setup took 4.092131092s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/06/23 18:53:06.994
    Feb  6 18:53:07.011: INFO: (0) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 16.515153ms)
    Feb  6 18:53:07.011: INFO: (0) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 16.585366ms)
    Feb  6 18:53:07.011: INFO: (0) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 16.662125ms)
    Feb  6 18:53:07.011: INFO: (0) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 16.565557ms)
    Feb  6 18:53:07.012: INFO: (0) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 16.962458ms)
    Feb  6 18:53:07.013: INFO: (0) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 18.556235ms)
    Feb  6 18:53:07.013: INFO: (0) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 18.850702ms)
    Feb  6 18:53:07.013: INFO: (0) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 18.818155ms)
    Feb  6 18:53:07.013: INFO: (0) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 19.00096ms)
    Feb  6 18:53:07.013: INFO: (0) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 18.684388ms)
    Feb  6 18:53:07.015: INFO: (0) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 20.736207ms)
    Feb  6 18:53:07.015: INFO: (0) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 21.041305ms)
    Feb  6 18:53:07.016: INFO: (0) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 21.254332ms)
    Feb  6 18:53:07.016: INFO: (0) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 21.832048ms)
    Feb  6 18:53:07.016: INFO: (0) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 21.296226ms)
    Feb  6 18:53:07.016: INFO: (0) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 21.431154ms)
    Feb  6 18:53:07.036: INFO: (1) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 19.593942ms)
    Feb  6 18:53:07.036: INFO: (1) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 18.180396ms)
    Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 19.568246ms)
    Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 20.205325ms)
    Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 19.739732ms)
    Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 18.880166ms)
    Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 20.191261ms)
    Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 19.792219ms)
    Feb  6 18:53:07.037: INFO: (1) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 21.253857ms)
    Feb  6 18:53:07.038: INFO: (1) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 21.944994ms)
    Feb  6 18:53:07.038: INFO: (1) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 20.027511ms)
    Feb  6 18:53:07.044: INFO: (1) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 26.31242ms)
    Feb  6 18:53:07.045: INFO: (1) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 27.740944ms)
    Feb  6 18:53:07.046: INFO: (1) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 29.137421ms)
    Feb  6 18:53:07.046: INFO: (1) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 28.793396ms)
    Feb  6 18:53:07.046: INFO: (1) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 28.575029ms)
    Feb  6 18:53:07.064: INFO: (2) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 16.605098ms)
    Feb  6 18:53:07.064: INFO: (2) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 17.413327ms)
    Feb  6 18:53:07.064: INFO: (2) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 17.572415ms)
    Feb  6 18:53:07.064: INFO: (2) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 17.580921ms)
    Feb  6 18:53:07.065: INFO: (2) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 17.951533ms)
    Feb  6 18:53:07.065: INFO: (2) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 17.633994ms)
    Feb  6 18:53:07.065: INFO: (2) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 18.097485ms)
    Feb  6 18:53:07.065: INFO: (2) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 17.956765ms)
    Feb  6 18:53:07.065: INFO: (2) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 18.548639ms)
    Feb  6 18:53:07.068: INFO: (2) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 20.47373ms)
    Feb  6 18:53:07.070: INFO: (2) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 22.983937ms)
    Feb  6 18:53:07.074: INFO: (2) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 26.823233ms)
    Feb  6 18:53:07.074: INFO: (2) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 26.776382ms)
    Feb  6 18:53:07.074: INFO: (2) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 27.770523ms)
    Feb  6 18:53:07.074: INFO: (2) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 27.213755ms)
    Feb  6 18:53:07.075: INFO: (2) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 28.120841ms)
    Feb  6 18:53:07.090: INFO: (3) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 14.784261ms)
    Feb  6 18:53:07.090: INFO: (3) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 15.083574ms)
    Feb  6 18:53:07.090: INFO: (3) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 14.79915ms)
    Feb  6 18:53:07.090: INFO: (3) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 14.591314ms)
    Feb  6 18:53:07.090: INFO: (3) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 15.201864ms)
    Feb  6 18:53:07.091: INFO: (3) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 16.054877ms)
    Feb  6 18:53:07.094: INFO: (3) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 19.265614ms)
    Feb  6 18:53:07.094: INFO: (3) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 18.566757ms)
    Feb  6 18:53:07.094: INFO: (3) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 18.630239ms)
    Feb  6 18:53:07.095: INFO: (3) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 19.004259ms)
    Feb  6 18:53:07.096: INFO: (3) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 21.119652ms)
    Feb  6 18:53:07.096: INFO: (3) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 20.764905ms)
    Feb  6 18:53:07.096: INFO: (3) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 20.721469ms)
    Feb  6 18:53:07.096: INFO: (3) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 20.618698ms)
    Feb  6 18:53:07.097: INFO: (3) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 21.113387ms)
    Feb  6 18:53:07.097: INFO: (3) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 21.114222ms)
    Feb  6 18:53:07.107: INFO: (4) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 9.657121ms)
    Feb  6 18:53:07.108: INFO: (4) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 10.428797ms)
    Feb  6 18:53:07.108: INFO: (4) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 10.656504ms)
    Feb  6 18:53:07.108: INFO: (4) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 10.759785ms)
    Feb  6 18:53:07.109: INFO: (4) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.093694ms)
    Feb  6 18:53:07.109: INFO: (4) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 11.031681ms)
    Feb  6 18:53:07.111: INFO: (4) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 13.70951ms)
    Feb  6 18:53:07.111: INFO: (4) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 13.32275ms)
    Feb  6 18:53:07.111: INFO: (4) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.828913ms)
    Feb  6 18:53:07.111: INFO: (4) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 13.18021ms)
    Feb  6 18:53:07.111: INFO: (4) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.998259ms)
    Feb  6 18:53:07.112: INFO: (4) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 14.80886ms)
    Feb  6 18:53:07.112: INFO: (4) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 13.654063ms)
    Feb  6 18:53:07.113: INFO: (4) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 15.549488ms)
    Feb  6 18:53:07.114: INFO: (4) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 16.084747ms)
    Feb  6 18:53:07.114: INFO: (4) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 15.653481ms)
    Feb  6 18:53:07.124: INFO: (5) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 9.00571ms)
    Feb  6 18:53:07.127: INFO: (5) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.787031ms)
    Feb  6 18:53:07.127: INFO: (5) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.097376ms)
    Feb  6 18:53:07.127: INFO: (5) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 11.853948ms)
    Feb  6 18:53:07.127: INFO: (5) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.134643ms)
    Feb  6 18:53:07.127: INFO: (5) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 12.991439ms)
    Feb  6 18:53:07.127: INFO: (5) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.873686ms)
    Feb  6 18:53:07.128: INFO: (5) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 13.203044ms)
    Feb  6 18:53:07.129: INFO: (5) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 14.750286ms)
    Feb  6 18:53:07.130: INFO: (5) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 14.777469ms)
    Feb  6 18:53:07.130: INFO: (5) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 14.764715ms)
    Feb  6 18:53:07.130: INFO: (5) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 15.449416ms)
    Feb  6 18:53:07.131: INFO: (5) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 16.479341ms)
    Feb  6 18:53:07.131: INFO: (5) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 16.488511ms)
    Feb  6 18:53:07.131: INFO: (5) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 17.155366ms)
    Feb  6 18:53:07.131: INFO: (5) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 17.08431ms)
    Feb  6 18:53:07.138: INFO: (6) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 6.021249ms)
    Feb  6 18:53:07.144: INFO: (6) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 11.863328ms)
    Feb  6 18:53:07.144: INFO: (6) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.481426ms)
    Feb  6 18:53:07.145: INFO: (6) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.5945ms)
    Feb  6 18:53:07.146: INFO: (6) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 14.372ms)
    Feb  6 18:53:07.148: INFO: (6) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 14.933667ms)
    Feb  6 18:53:07.148: INFO: (6) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 15.305423ms)
    Feb  6 18:53:07.148: INFO: (6) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 16.007859ms)
    Feb  6 18:53:07.148: INFO: (6) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 16.573131ms)
    Feb  6 18:53:07.149: INFO: (6) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 16.95265ms)
    Feb  6 18:53:07.155: INFO: (6) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 22.860116ms)
    Feb  6 18:53:07.155: INFO: (6) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 22.446047ms)
    Feb  6 18:53:07.155: INFO: (6) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 23.147764ms)
    Feb  6 18:53:07.155: INFO: (6) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 22.729087ms)
    Feb  6 18:53:07.155: INFO: (6) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 23.369528ms)
    Feb  6 18:53:07.155: INFO: (6) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 22.944711ms)
    Feb  6 18:53:07.168: INFO: (7) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.754637ms)
    Feb  6 18:53:07.168: INFO: (7) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.376161ms)
    Feb  6 18:53:07.168: INFO: (7) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.17792ms)
    Feb  6 18:53:07.168: INFO: (7) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.245761ms)
    Feb  6 18:53:07.169: INFO: (7) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 12.140708ms)
    Feb  6 18:53:07.170: INFO: (7) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 14.560485ms)
    Feb  6 18:53:07.170: INFO: (7) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 13.723564ms)
    Feb  6 18:53:07.173: INFO: (7) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 17.174798ms)
    Feb  6 18:53:07.173: INFO: (7) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 16.785647ms)
    Feb  6 18:53:07.173: INFO: (7) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 16.896897ms)
    Feb  6 18:53:07.173: INFO: (7) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 17.269791ms)
    Feb  6 18:53:07.175: INFO: (7) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 18.400384ms)
    Feb  6 18:53:07.176: INFO: (7) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 19.585918ms)
    Feb  6 18:53:07.176: INFO: (7) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 19.74354ms)
    Feb  6 18:53:07.176: INFO: (7) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 19.545921ms)
    Feb  6 18:53:07.177: INFO: (7) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 20.263939ms)
    Feb  6 18:53:07.187: INFO: (8) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 9.658058ms)
    Feb  6 18:53:07.187: INFO: (8) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 9.682522ms)
    Feb  6 18:53:07.187: INFO: (8) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 9.621198ms)
    Feb  6 18:53:07.187: INFO: (8) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 10.466355ms)
    Feb  6 18:53:07.188: INFO: (8) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 10.587305ms)
    Feb  6 18:53:07.188: INFO: (8) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 11.455361ms)
    Feb  6 18:53:07.188: INFO: (8) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 11.464168ms)
    Feb  6 18:53:07.188: INFO: (8) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.484277ms)
    Feb  6 18:53:07.189: INFO: (8) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 10.413752ms)
    Feb  6 18:53:07.189: INFO: (8) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 10.49396ms)
    Feb  6 18:53:07.189: INFO: (8) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 10.531809ms)
    Feb  6 18:53:07.190: INFO: (8) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 13.225238ms)
    Feb  6 18:53:07.192: INFO: (8) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 13.98609ms)
    Feb  6 18:53:07.192: INFO: (8) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 14.959949ms)
    Feb  6 18:53:07.193: INFO: (8) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 15.404194ms)
    Feb  6 18:53:07.193: INFO: (8) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 15.528343ms)
    Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.718812ms)
    Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 11.291168ms)
    Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 11.351862ms)
    Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 10.742108ms)
    Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 11.904279ms)
    Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 11.691288ms)
    Feb  6 18:53:07.205: INFO: (9) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.780842ms)
    Feb  6 18:53:07.206: INFO: (9) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 11.750974ms)
    Feb  6 18:53:07.207: INFO: (9) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 12.071008ms)
    Feb  6 18:53:07.208: INFO: (9) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 14.265816ms)
    Feb  6 18:53:07.208: INFO: (9) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 13.680397ms)
    Feb  6 18:53:07.209: INFO: (9) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 15.225403ms)
    Feb  6 18:53:07.211: INFO: (9) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 17.034416ms)
    Feb  6 18:53:07.211: INFO: (9) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 17.216723ms)
    Feb  6 18:53:07.212: INFO: (9) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 17.822853ms)
    Feb  6 18:53:07.212: INFO: (9) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 18.025727ms)
    Feb  6 18:53:07.218: INFO: (10) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 5.849289ms)
    Feb  6 18:53:07.219: INFO: (10) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 6.238832ms)
    Feb  6 18:53:07.221: INFO: (10) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 8.31489ms)
    Feb  6 18:53:07.222: INFO: (10) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 8.922959ms)
    Feb  6 18:53:07.222: INFO: (10) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 9.603221ms)
    Feb  6 18:53:07.223: INFO: (10) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 10.042765ms)
    Feb  6 18:53:07.224: INFO: (10) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.658805ms)
    Feb  6 18:53:07.224: INFO: (10) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.826816ms)
    Feb  6 18:53:07.225: INFO: (10) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.438524ms)
    Feb  6 18:53:07.225: INFO: (10) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 12.100639ms)
    Feb  6 18:53:07.226: INFO: (10) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 13.42519ms)
    Feb  6 18:53:07.226: INFO: (10) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 14.018873ms)
    Feb  6 18:53:07.226: INFO: (10) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 13.57922ms)
    Feb  6 18:53:07.227: INFO: (10) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 14.140993ms)
    Feb  6 18:53:07.227: INFO: (10) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 13.782945ms)
    Feb  6 18:53:07.227: INFO: (10) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 13.96788ms)
    Feb  6 18:53:07.237: INFO: (11) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 9.832762ms)
    Feb  6 18:53:07.238: INFO: (11) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 10.236243ms)
    Feb  6 18:53:07.239: INFO: (11) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.074249ms)
    Feb  6 18:53:07.239: INFO: (11) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.797117ms)
    Feb  6 18:53:07.239: INFO: (11) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.793239ms)
    Feb  6 18:53:07.239: INFO: (11) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.53703ms)
    Feb  6 18:53:07.239: INFO: (11) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 11.150967ms)
    Feb  6 18:53:07.240: INFO: (11) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.029924ms)
    Feb  6 18:53:07.241: INFO: (11) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 13.019453ms)
    Feb  6 18:53:07.241: INFO: (11) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 13.275211ms)
    Feb  6 18:53:07.243: INFO: (11) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 16.041201ms)
    Feb  6 18:53:07.245: INFO: (11) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 16.559709ms)
    Feb  6 18:53:07.245: INFO: (11) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 17.238286ms)
    Feb  6 18:53:07.245: INFO: (11) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 17.77143ms)
    Feb  6 18:53:07.246: INFO: (11) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 18.380594ms)
    Feb  6 18:53:07.246: INFO: (11) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 17.451069ms)
    Feb  6 18:53:07.252: INFO: (12) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 5.807403ms)
    Feb  6 18:53:07.253: INFO: (12) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 6.923344ms)
    Feb  6 18:53:07.258: INFO: (12) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.718265ms)
    Feb  6 18:53:07.258: INFO: (12) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.022295ms)
    Feb  6 18:53:07.258: INFO: (12) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.077297ms)
    Feb  6 18:53:07.259: INFO: (12) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.405485ms)
    Feb  6 18:53:07.259: INFO: (12) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 12.493044ms)
    Feb  6 18:53:07.259: INFO: (12) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 12.678404ms)
    Feb  6 18:53:07.261: INFO: (12) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 14.809325ms)
    Feb  6 18:53:07.261: INFO: (12) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 15.344935ms)
    Feb  6 18:53:07.263: INFO: (12) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 16.821168ms)
    Feb  6 18:53:07.266: INFO: (12) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 19.87513ms)
    Feb  6 18:53:07.267: INFO: (12) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 21.151224ms)
    Feb  6 18:53:07.268: INFO: (12) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 21.571263ms)
    Feb  6 18:53:07.268: INFO: (12) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 22.178487ms)
    Feb  6 18:53:07.268: INFO: (12) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 21.850239ms)
    Feb  6 18:53:07.275: INFO: (13) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 7.131602ms)
    Feb  6 18:53:07.276: INFO: (13) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 7.353702ms)
    Feb  6 18:53:07.278: INFO: (13) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 8.736705ms)
    Feb  6 18:53:07.279: INFO: (13) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 9.911125ms)
    Feb  6 18:53:07.279: INFO: (13) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 9.691379ms)
    Feb  6 18:53:07.280: INFO: (13) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 11.416517ms)
    Feb  6 18:53:07.280: INFO: (13) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 11.56165ms)
    Feb  6 18:53:07.281: INFO: (13) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.377092ms)
    Feb  6 18:53:07.282: INFO: (13) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 13.374913ms)
    Feb  6 18:53:07.282: INFO: (13) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 13.187302ms)
    Feb  6 18:53:07.283: INFO: (13) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 13.843765ms)
    Feb  6 18:53:07.284: INFO: (13) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 15.231096ms)
    Feb  6 18:53:07.284: INFO: (13) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 14.886246ms)
    Feb  6 18:53:07.284: INFO: (13) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 15.515133ms)
    Feb  6 18:53:07.284: INFO: (13) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 15.464132ms)
    Feb  6 18:53:07.285: INFO: (13) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 15.716495ms)
    Feb  6 18:53:07.293: INFO: (14) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 7.597848ms)
    Feb  6 18:53:07.298: INFO: (14) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 12.983408ms)
    Feb  6 18:53:07.299: INFO: (14) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 13.486116ms)
    Feb  6 18:53:07.299: INFO: (14) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 13.364194ms)
    Feb  6 18:53:07.299: INFO: (14) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 13.943161ms)
    Feb  6 18:53:07.299: INFO: (14) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 13.940991ms)
    Feb  6 18:53:07.299: INFO: (14) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 14.118513ms)
    Feb  6 18:53:07.300: INFO: (14) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 14.584439ms)
    Feb  6 18:53:07.300: INFO: (14) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 14.709279ms)
    Feb  6 18:53:07.300: INFO: (14) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 14.571928ms)
    Feb  6 18:53:07.300: INFO: (14) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 14.703747ms)
    Feb  6 18:53:07.301: INFO: (14) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 15.702815ms)
    Feb  6 18:53:07.303: INFO: (14) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 17.80448ms)
    Feb  6 18:53:07.304: INFO: (14) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 18.290258ms)
    Feb  6 18:53:07.304: INFO: (14) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 18.71744ms)
    Feb  6 18:53:07.304: INFO: (14) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 18.669959ms)
    Feb  6 18:53:07.315: INFO: (15) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 9.976256ms)
    Feb  6 18:53:07.315: INFO: (15) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 10.444669ms)
    Feb  6 18:53:07.315: INFO: (15) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 10.635645ms)
    Feb  6 18:53:07.315: INFO: (15) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.133741ms)
    Feb  6 18:53:07.316: INFO: (15) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 11.129364ms)
    Feb  6 18:53:07.316: INFO: (15) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.05515ms)
    Feb  6 18:53:07.316: INFO: (15) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 11.292047ms)
    Feb  6 18:53:07.316: INFO: (15) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 11.645568ms)
    Feb  6 18:53:07.316: INFO: (15) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 11.451985ms)
    Feb  6 18:53:07.316: INFO: (15) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 11.614288ms)
    Feb  6 18:53:07.317: INFO: (15) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.679705ms)
    Feb  6 18:53:07.321: INFO: (15) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 16.17054ms)
    Feb  6 18:53:07.321: INFO: (15) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 17.002577ms)
    Feb  6 18:53:07.321: INFO: (15) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 16.75189ms)
    Feb  6 18:53:07.322: INFO: (15) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 17.191922ms)
    Feb  6 18:53:07.322: INFO: (15) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 17.491136ms)
    Feb  6 18:53:07.334: INFO: (16) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 11.6761ms)
    Feb  6 18:53:07.334: INFO: (16) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 10.778475ms)
    Feb  6 18:53:07.335: INFO: (16) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 12.047792ms)
    Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 12.605542ms)
    Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 13.564675ms)
    Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 12.96177ms)
    Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 14.220351ms)
    Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 13.59596ms)
    Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 13.684696ms)
    Feb  6 18:53:07.336: INFO: (16) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 14.122042ms)
    Feb  6 18:53:07.337: INFO: (16) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 14.118208ms)
    Feb  6 18:53:07.338: INFO: (16) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 15.578478ms)
    Feb  6 18:53:07.339: INFO: (16) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 15.911011ms)
    Feb  6 18:53:07.339: INFO: (16) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 16.307392ms)
    Feb  6 18:53:07.339: INFO: (16) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 16.885988ms)
    Feb  6 18:53:07.340: INFO: (16) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 16.485439ms)
    Feb  6 18:53:07.351: INFO: (17) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.961653ms)
    Feb  6 18:53:07.351: INFO: (17) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 10.999ms)
    Feb  6 18:53:07.351: INFO: (17) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 11.358609ms)
    Feb  6 18:53:07.351: INFO: (17) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 11.205557ms)
    Feb  6 18:53:07.352: INFO: (17) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 11.522123ms)
    Feb  6 18:53:07.352: INFO: (17) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 11.449309ms)
    Feb  6 18:53:07.353: INFO: (17) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 13.422372ms)
    Feb  6 18:53:07.354: INFO: (17) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 14.123668ms)
    Feb  6 18:53:07.354: INFO: (17) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 14.540801ms)
    Feb  6 18:53:07.354: INFO: (17) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 14.404502ms)
    Feb  6 18:53:07.354: INFO: (17) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 14.857702ms)
    Feb  6 18:53:07.356: INFO: (17) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 15.563527ms)
    Feb  6 18:53:07.356: INFO: (17) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 16.687659ms)
    Feb  6 18:53:07.357: INFO: (17) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 17.259572ms)
    Feb  6 18:53:07.357: INFO: (17) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 17.205801ms)
    Feb  6 18:53:07.357: INFO: (17) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 17.540225ms)
    Feb  6 18:53:07.368: INFO: (18) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 9.678105ms)
    Feb  6 18:53:07.369: INFO: (18) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 10.319329ms)
    Feb  6 18:53:07.370: INFO: (18) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 11.968728ms)
    Feb  6 18:53:07.370: INFO: (18) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 11.514604ms)
    Feb  6 18:53:07.371: INFO: (18) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 12.599583ms)
    Feb  6 18:53:07.371: INFO: (18) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.293592ms)
    Feb  6 18:53:07.371: INFO: (18) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 12.825579ms)
    Feb  6 18:53:07.372: INFO: (18) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 12.479483ms)
    Feb  6 18:53:07.372: INFO: (18) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 13.134199ms)
    Feb  6 18:53:07.372: INFO: (18) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 14.301206ms)
    Feb  6 18:53:07.372: INFO: (18) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 12.927102ms)
    Feb  6 18:53:07.372: INFO: (18) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 14.509072ms)
    Feb  6 18:53:07.373: INFO: (18) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 14.751994ms)
    Feb  6 18:53:07.373: INFO: (18) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 14.05344ms)
    Feb  6 18:53:07.373: INFO: (18) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 15.051022ms)
    Feb  6 18:53:07.374: INFO: (18) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 14.924353ms)
    Feb  6 18:53:07.387: INFO: (19) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 12.791232ms)
    Feb  6 18:53:07.387: INFO: (19) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">test<... (200; 13.542973ms)
    Feb  6 18:53:07.387: INFO: (19) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:1080/proxy/rewriteme">... (200; 13.459791ms)
    Feb  6 18:53:07.388: INFO: (19) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:460/proxy/: tls baz (200; 13.530987ms)
    Feb  6 18:53:07.388: INFO: (19) /api/v1/namespaces/proxy-8701/pods/http:proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 13.671398ms)
    Feb  6 18:53:07.388: INFO: (19) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:160/proxy/: foo (200; 13.710618ms)
    Feb  6 18:53:07.388: INFO: (19) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname2/proxy/: bar (200; 14.339136ms)
    Feb  6 18:53:07.388: INFO: (19) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:443/proxy/tlsrewritem... (200; 14.285978ms)
    Feb  6 18:53:07.389: INFO: (19) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname1/proxy/: foo (200; 14.479083ms)
    Feb  6 18:53:07.389: INFO: (19) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/: <a href="/api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v/proxy/rewriteme">test</a> (200; 14.732223ms)
    Feb  6 18:53:07.389: INFO: (19) /api/v1/namespaces/proxy-8701/services/proxy-service-ggdlz:portname1/proxy/: foo (200; 15.026005ms)
    Feb  6 18:53:07.390: INFO: (19) /api/v1/namespaces/proxy-8701/services/http:proxy-service-ggdlz:portname2/proxy/: bar (200; 16.273728ms)
    Feb  6 18:53:07.390: INFO: (19) /api/v1/namespaces/proxy-8701/pods/https:proxy-service-ggdlz-mcv9v:462/proxy/: tls qux (200; 16.406191ms)
    Feb  6 18:53:07.390: INFO: (19) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname1/proxy/: tls baz (200; 16.299385ms)
    Feb  6 18:53:07.390: INFO: (19) /api/v1/namespaces/proxy-8701/services/https:proxy-service-ggdlz:tlsportname2/proxy/: tls qux (200; 16.3746ms)
    Feb  6 18:53:07.391: INFO: (19) /api/v1/namespaces/proxy-8701/pods/proxy-service-ggdlz-mcv9v:162/proxy/: bar (200; 16.823275ms)
    STEP: deleting ReplicationController proxy-service-ggdlz in namespace proxy-8701, will wait for the garbage collector to delete the pods 02/06/23 18:53:07.391
    Feb  6 18:53:07.454: INFO: Deleting ReplicationController proxy-service-ggdlz took: 8.609377ms
    Feb  6 18:53:07.554: INFO: Terminating ReplicationController proxy-service-ggdlz pods took: 100.413574ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb  6 18:53:10.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-8701" for this suite. 02/06/23 18:53:10.565
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:53:10.581
Feb  6 18:53:10.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 18:53:10.582
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:53:10.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:53:10.607
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-7c982bdb-33cb-4cfb-9ca3-912ac9eae1c6 02/06/23 18:53:10.611
STEP: Creating a pod to test consume secrets 02/06/23 18:53:10.619
Feb  6 18:53:10.630: INFO: Waiting up to 5m0s for pod "pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89" in namespace "secrets-8371" to be "Succeeded or Failed"
Feb  6 18:53:10.639: INFO: Pod "pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89": Phase="Pending", Reason="", readiness=false. Elapsed: 9.380718ms
Feb  6 18:53:12.646: INFO: Pod "pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015750807s
Feb  6 18:53:14.646: INFO: Pod "pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89": Phase="Running", Reason="", readiness=false. Elapsed: 4.015734665s
Feb  6 18:53:16.658: INFO: Pod "pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027875193s
STEP: Saw pod success 02/06/23 18:53:16.658
Feb  6 18:53:16.658: INFO: Pod "pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89" satisfied condition "Succeeded or Failed"
Feb  6 18:53:16.662: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89 container secret-volume-test: <nil>
STEP: delete the pod 02/06/23 18:53:16.681
Feb  6 18:53:16.696: INFO: Waiting for pod pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89 to disappear
Feb  6 18:53:16.700: INFO: Pod pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb  6 18:53:16.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8371" for this suite. 02/06/23 18:53:16.705
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":331,"skipped":6222,"failed":0}
------------------------------
• [SLOW TEST] [6.132 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:53:10.581
    Feb  6 18:53:10.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 18:53:10.582
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:53:10.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:53:10.607
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-7c982bdb-33cb-4cfb-9ca3-912ac9eae1c6 02/06/23 18:53:10.611
    STEP: Creating a pod to test consume secrets 02/06/23 18:53:10.619
    Feb  6 18:53:10.630: INFO: Waiting up to 5m0s for pod "pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89" in namespace "secrets-8371" to be "Succeeded or Failed"
    Feb  6 18:53:10.639: INFO: Pod "pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89": Phase="Pending", Reason="", readiness=false. Elapsed: 9.380718ms
    Feb  6 18:53:12.646: INFO: Pod "pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015750807s
    Feb  6 18:53:14.646: INFO: Pod "pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89": Phase="Running", Reason="", readiness=false. Elapsed: 4.015734665s
    Feb  6 18:53:16.658: INFO: Pod "pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027875193s
    STEP: Saw pod success 02/06/23 18:53:16.658
    Feb  6 18:53:16.658: INFO: Pod "pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89" satisfied condition "Succeeded or Failed"
    Feb  6 18:53:16.662: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89 container secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 18:53:16.681
    Feb  6 18:53:16.696: INFO: Waiting for pod pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89 to disappear
    Feb  6 18:53:16.700: INFO: Pod pod-secrets-5f60a059-1349-428f-9f58-c4b7bae7aa89 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 18:53:16.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8371" for this suite. 02/06/23 18:53:16.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:53:16.718
Feb  6 18:53:16.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename podtemplate 02/06/23 18:53:16.719
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:53:16.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:53:16.745
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 02/06/23 18:53:16.748
STEP: Replace a pod template 02/06/23 18:53:16.757
Feb  6 18:53:16.769: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb  6 18:53:16.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-76" for this suite. 02/06/23 18:53:16.774
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":332,"skipped":6245,"failed":0}
------------------------------
• [0.066 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:53:16.718
    Feb  6 18:53:16.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename podtemplate 02/06/23 18:53:16.719
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:53:16.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:53:16.745
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 02/06/23 18:53:16.748
    STEP: Replace a pod template 02/06/23 18:53:16.757
    Feb  6 18:53:16.769: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb  6 18:53:16.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-76" for this suite. 02/06/23 18:53:16.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:53:16.785
Feb  6 18:53:16.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename custom-resource-definition 02/06/23 18:53:16.786
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:53:16.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:53:16.808
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 02/06/23 18:53:16.811
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/06/23 18:53:16.813
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/06/23 18:53:16.813
STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/06/23 18:53:16.813
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/06/23 18:53:16.814
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/06/23 18:53:16.814
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/06/23 18:53:16.815
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:53:16.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-631" for this suite. 02/06/23 18:53:16.821
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":333,"skipped":6268,"failed":0}
------------------------------
• [0.045 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:53:16.785
    Feb  6 18:53:16.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename custom-resource-definition 02/06/23 18:53:16.786
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:53:16.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:53:16.808
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 02/06/23 18:53:16.811
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/06/23 18:53:16.813
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/06/23 18:53:16.813
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/06/23 18:53:16.813
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/06/23 18:53:16.814
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/06/23 18:53:16.814
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/06/23 18:53:16.815
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:53:16.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-631" for this suite. 02/06/23 18:53:16.821
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:53:16.831
Feb  6 18:53:16.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-probe 02/06/23 18:53:16.832
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:53:16.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:53:16.858
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-23522932-39eb-4255-916e-79225fa4114c in namespace container-probe-789 02/06/23 18:53:16.861
Feb  6 18:53:16.875: INFO: Waiting up to 5m0s for pod "liveness-23522932-39eb-4255-916e-79225fa4114c" in namespace "container-probe-789" to be "not pending"
Feb  6 18:53:16.882: INFO: Pod "liveness-23522932-39eb-4255-916e-79225fa4114c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.527688ms
Feb  6 18:53:18.888: INFO: Pod "liveness-23522932-39eb-4255-916e-79225fa4114c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013340988s
Feb  6 18:53:20.889: INFO: Pod "liveness-23522932-39eb-4255-916e-79225fa4114c": Phase="Running", Reason="", readiness=true. Elapsed: 4.013884817s
Feb  6 18:53:20.889: INFO: Pod "liveness-23522932-39eb-4255-916e-79225fa4114c" satisfied condition "not pending"
Feb  6 18:53:20.889: INFO: Started pod liveness-23522932-39eb-4255-916e-79225fa4114c in namespace container-probe-789
STEP: checking the pod's current state and verifying that restartCount is present 02/06/23 18:53:20.889
Feb  6 18:53:20.893: INFO: Initial restart count of pod liveness-23522932-39eb-4255-916e-79225fa4114c is 0
STEP: deleting the pod 02/06/23 18:57:21.738
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb  6 18:57:21.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-789" for this suite. 02/06/23 18:57:21.765
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":334,"skipped":6269,"failed":0}
------------------------------
• [SLOW TEST] [244.942 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:53:16.831
    Feb  6 18:53:16.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-probe 02/06/23 18:53:16.832
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:53:16.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:53:16.858
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-23522932-39eb-4255-916e-79225fa4114c in namespace container-probe-789 02/06/23 18:53:16.861
    Feb  6 18:53:16.875: INFO: Waiting up to 5m0s for pod "liveness-23522932-39eb-4255-916e-79225fa4114c" in namespace "container-probe-789" to be "not pending"
    Feb  6 18:53:16.882: INFO: Pod "liveness-23522932-39eb-4255-916e-79225fa4114c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.527688ms
    Feb  6 18:53:18.888: INFO: Pod "liveness-23522932-39eb-4255-916e-79225fa4114c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013340988s
    Feb  6 18:53:20.889: INFO: Pod "liveness-23522932-39eb-4255-916e-79225fa4114c": Phase="Running", Reason="", readiness=true. Elapsed: 4.013884817s
    Feb  6 18:53:20.889: INFO: Pod "liveness-23522932-39eb-4255-916e-79225fa4114c" satisfied condition "not pending"
    Feb  6 18:53:20.889: INFO: Started pod liveness-23522932-39eb-4255-916e-79225fa4114c in namespace container-probe-789
    STEP: checking the pod's current state and verifying that restartCount is present 02/06/23 18:53:20.889
    Feb  6 18:53:20.893: INFO: Initial restart count of pod liveness-23522932-39eb-4255-916e-79225fa4114c is 0
    STEP: deleting the pod 02/06/23 18:57:21.738
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb  6 18:57:21.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-789" for this suite. 02/06/23 18:57:21.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:57:21.777
Feb  6 18:57:21.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 18:57:21.778
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:57:21.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:57:21.807
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-1116 02/06/23 18:57:21.811
Feb  6 18:57:21.824: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1116" to be "running and ready"
Feb  6 18:57:21.833: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 8.606418ms
Feb  6 18:57:21.833: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:57:23.839: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.014736687s
Feb  6 18:57:23.839: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb  6 18:57:23.839: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Feb  6 18:57:23.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1116 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb  6 18:57:24.023: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb  6 18:57:24.023: INFO: stdout: "iptables"
Feb  6 18:57:24.023: INFO: proxyMode: iptables
Feb  6 18:57:24.049: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb  6 18:57:24.053: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1116 02/06/23 18:57:24.053
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1116 02/06/23 18:57:24.077
I0206 18:57:24.086936      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1116, replica count: 3
I0206 18:57:27.137873      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 18:57:30.138203      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  6 18:57:30.146: INFO: Creating new exec pod
Feb  6 18:57:30.155: INFO: Waiting up to 5m0s for pod "execpod-affinityrsqs5" in namespace "services-1116" to be "running"
Feb  6 18:57:30.160: INFO: Pod "execpod-affinityrsqs5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.114154ms
Feb  6 18:57:32.166: INFO: Pod "execpod-affinityrsqs5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010950806s
Feb  6 18:57:34.165: INFO: Pod "execpod-affinityrsqs5": Phase="Running", Reason="", readiness=true. Elapsed: 4.010624184s
Feb  6 18:57:34.165: INFO: Pod "execpod-affinityrsqs5" satisfied condition "running"
Feb  6 18:57:35.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1116 exec execpod-affinityrsqs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Feb  6 18:57:35.344: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Feb  6 18:57:35.344: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:57:35.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1116 exec execpod-affinityrsqs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.182.190 80'
Feb  6 18:57:35.518: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.182.190 80\nConnection to 10.100.182.190 80 port [tcp/http] succeeded!\n"
Feb  6 18:57:35.518: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:57:35.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1116 exec execpod-affinityrsqs5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.182.190:80/ ; done'
Feb  6 18:57:35.807: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n"
Feb  6 18:57:35.807: INFO: stdout: "\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x"
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
Feb  6 18:57:35.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1116 exec execpod-affinityrsqs5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.182.190:80/'
Feb  6 18:57:35.984: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n"
Feb  6 18:57:35.985: INFO: stdout: "affinity-clusterip-timeout-8lb7x"
Feb  6 18:57:55.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1116 exec execpod-affinityrsqs5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.182.190:80/'
Feb  6 18:57:56.176: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n"
Feb  6 18:57:56.176: INFO: stdout: "affinity-clusterip-timeout-xv5rz"
Feb  6 18:57:56.176: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1116, will wait for the garbage collector to delete the pods 02/06/23 18:57:56.2
Feb  6 18:57:56.266: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 11.657668ms
Feb  6 18:57:56.367: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.945636ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 18:57:58.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1116" for this suite. 02/06/23 18:57:58.814
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":335,"skipped":6319,"failed":0}
------------------------------
• [SLOW TEST] [37.048 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:57:21.777
    Feb  6 18:57:21.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 18:57:21.778
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:57:21.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:57:21.807
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-1116 02/06/23 18:57:21.811
    Feb  6 18:57:21.824: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1116" to be "running and ready"
    Feb  6 18:57:21.833: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 8.606418ms
    Feb  6 18:57:21.833: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:57:23.839: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.014736687s
    Feb  6 18:57:23.839: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Feb  6 18:57:23.839: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Feb  6 18:57:23.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1116 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Feb  6 18:57:24.023: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Feb  6 18:57:24.023: INFO: stdout: "iptables"
    Feb  6 18:57:24.023: INFO: proxyMode: iptables
    Feb  6 18:57:24.049: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Feb  6 18:57:24.053: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-1116 02/06/23 18:57:24.053
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-1116 02/06/23 18:57:24.077
    I0206 18:57:24.086936      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1116, replica count: 3
    I0206 18:57:27.137873      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 18:57:30.138203      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb  6 18:57:30.146: INFO: Creating new exec pod
    Feb  6 18:57:30.155: INFO: Waiting up to 5m0s for pod "execpod-affinityrsqs5" in namespace "services-1116" to be "running"
    Feb  6 18:57:30.160: INFO: Pod "execpod-affinityrsqs5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.114154ms
    Feb  6 18:57:32.166: INFO: Pod "execpod-affinityrsqs5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010950806s
    Feb  6 18:57:34.165: INFO: Pod "execpod-affinityrsqs5": Phase="Running", Reason="", readiness=true. Elapsed: 4.010624184s
    Feb  6 18:57:34.165: INFO: Pod "execpod-affinityrsqs5" satisfied condition "running"
    Feb  6 18:57:35.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1116 exec execpod-affinityrsqs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Feb  6 18:57:35.344: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Feb  6 18:57:35.344: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:57:35.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1116 exec execpod-affinityrsqs5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.182.190 80'
    Feb  6 18:57:35.518: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.182.190 80\nConnection to 10.100.182.190 80 port [tcp/http] succeeded!\n"
    Feb  6 18:57:35.518: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:57:35.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1116 exec execpod-affinityrsqs5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.182.190:80/ ; done'
    Feb  6 18:57:35.807: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n"
    Feb  6 18:57:35.807: INFO: stdout: "\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x\naffinity-clusterip-timeout-8lb7x"
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.807: INFO: Received response from host: affinity-clusterip-timeout-8lb7x
    Feb  6 18:57:35.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1116 exec execpod-affinityrsqs5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.182.190:80/'
    Feb  6 18:57:35.984: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n"
    Feb  6 18:57:35.985: INFO: stdout: "affinity-clusterip-timeout-8lb7x"
    Feb  6 18:57:55.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-1116 exec execpod-affinityrsqs5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.182.190:80/'
    Feb  6 18:57:56.176: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.182.190:80/\n"
    Feb  6 18:57:56.176: INFO: stdout: "affinity-clusterip-timeout-xv5rz"
    Feb  6 18:57:56.176: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1116, will wait for the garbage collector to delete the pods 02/06/23 18:57:56.2
    Feb  6 18:57:56.266: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 11.657668ms
    Feb  6 18:57:56.367: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.945636ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 18:57:58.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1116" for this suite. 02/06/23 18:57:58.814
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:57:58.827
Feb  6 18:57:58.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename pod-network-test 02/06/23 18:57:58.829
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:57:58.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:57:58.852
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-2378 02/06/23 18:57:58.855
STEP: creating a selector 02/06/23 18:57:58.855
STEP: Creating the service pods in kubernetes 02/06/23 18:57:58.855
Feb  6 18:57:58.855: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb  6 18:57:58.916: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2378" to be "running and ready"
Feb  6 18:57:58.926: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.465303ms
Feb  6 18:57:58.926: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:58:00.931: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014495673s
Feb  6 18:58:00.932: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:58:02.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01432887s
Feb  6 18:58:02.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:58:04.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015962909s
Feb  6 18:58:04.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:58:06.932: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015355612s
Feb  6 18:58:06.932: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:58:08.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015918665s
Feb  6 18:58:08.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:58:10.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01630612s
Feb  6 18:58:10.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:58:12.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014492844s
Feb  6 18:58:12.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:58:14.930: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01343645s
Feb  6 18:58:14.930: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:58:16.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.017327186s
Feb  6 18:58:16.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:58:18.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.015754919s
Feb  6 18:58:18.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb  6 18:58:20.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014594419s
Feb  6 18:58:20.931: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb  6 18:58:20.931: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb  6 18:58:20.934: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2378" to be "running and ready"
Feb  6 18:58:20.938: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.576827ms
Feb  6 18:58:20.938: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb  6 18:58:20.938: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb  6 18:58:20.941: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2378" to be "running and ready"
Feb  6 18:58:20.945: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.367591ms
Feb  6 18:58:20.945: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb  6 18:58:20.945: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/06/23 18:58:20.948
Feb  6 18:58:20.963: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2378" to be "running"
Feb  6 18:58:20.971: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.947865ms
Feb  6 18:58:22.977: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013339153s
Feb  6 18:58:24.976: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012806962s
Feb  6 18:58:24.976: INFO: Pod "test-container-pod" satisfied condition "running"
Feb  6 18:58:24.980: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2378" to be "running"
Feb  6 18:58:24.983: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.261758ms
Feb  6 18:58:24.983: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb  6 18:58:24.987: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb  6 18:58:24.987: INFO: Going to poll 192.168.2.48 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb  6 18:58:24.990: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.2.48:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2378 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:58:24.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:58:24.991: INFO: ExecWithOptions: Clientset creation
Feb  6 18:58:24.991: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2378/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.2.48%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb  6 18:58:25.073: INFO: Found all 1 expected endpoints: [netserver-0]
Feb  6 18:58:25.073: INFO: Going to poll 192.168.3.84 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb  6 18:58:25.077: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.3.84:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2378 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:58:25.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:58:25.078: INFO: ExecWithOptions: Clientset creation
Feb  6 18:58:25.078: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2378/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.3.84%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb  6 18:58:25.153: INFO: Found all 1 expected endpoints: [netserver-1]
Feb  6 18:58:25.153: INFO: Going to poll 192.168.1.6 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb  6 18:58:25.159: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.1.6:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2378 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  6 18:58:25.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 18:58:25.160: INFO: ExecWithOptions: Clientset creation
Feb  6 18:58:25.160: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2378/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.1.6%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb  6 18:58:25.229: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb  6 18:58:25.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2378" for this suite. 02/06/23 18:58:25.237
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":336,"skipped":6360,"failed":0}
------------------------------
• [SLOW TEST] [26.417 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:57:58.827
    Feb  6 18:57:58.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename pod-network-test 02/06/23 18:57:58.829
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:57:58.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:57:58.852
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-2378 02/06/23 18:57:58.855
    STEP: creating a selector 02/06/23 18:57:58.855
    STEP: Creating the service pods in kubernetes 02/06/23 18:57:58.855
    Feb  6 18:57:58.855: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb  6 18:57:58.916: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2378" to be "running and ready"
    Feb  6 18:57:58.926: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.465303ms
    Feb  6 18:57:58.926: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:58:00.931: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014495673s
    Feb  6 18:58:00.932: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:58:02.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01432887s
    Feb  6 18:58:02.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:58:04.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015962909s
    Feb  6 18:58:04.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:58:06.932: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015355612s
    Feb  6 18:58:06.932: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:58:08.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015918665s
    Feb  6 18:58:08.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:58:10.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01630612s
    Feb  6 18:58:10.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:58:12.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014492844s
    Feb  6 18:58:12.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:58:14.930: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01343645s
    Feb  6 18:58:14.930: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:58:16.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.017327186s
    Feb  6 18:58:16.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:58:18.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.015754919s
    Feb  6 18:58:18.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb  6 18:58:20.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014594419s
    Feb  6 18:58:20.931: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb  6 18:58:20.931: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb  6 18:58:20.934: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2378" to be "running and ready"
    Feb  6 18:58:20.938: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.576827ms
    Feb  6 18:58:20.938: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb  6 18:58:20.938: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb  6 18:58:20.941: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2378" to be "running and ready"
    Feb  6 18:58:20.945: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.367591ms
    Feb  6 18:58:20.945: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb  6 18:58:20.945: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/06/23 18:58:20.948
    Feb  6 18:58:20.963: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2378" to be "running"
    Feb  6 18:58:20.971: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.947865ms
    Feb  6 18:58:22.977: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013339153s
    Feb  6 18:58:24.976: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012806962s
    Feb  6 18:58:24.976: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb  6 18:58:24.980: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2378" to be "running"
    Feb  6 18:58:24.983: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.261758ms
    Feb  6 18:58:24.983: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb  6 18:58:24.987: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb  6 18:58:24.987: INFO: Going to poll 192.168.2.48 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb  6 18:58:24.990: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.2.48:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2378 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:58:24.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:58:24.991: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:58:24.991: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2378/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.2.48%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb  6 18:58:25.073: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb  6 18:58:25.073: INFO: Going to poll 192.168.3.84 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb  6 18:58:25.077: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.3.84:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2378 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:58:25.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:58:25.078: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:58:25.078: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2378/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.3.84%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb  6 18:58:25.153: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb  6 18:58:25.153: INFO: Going to poll 192.168.1.6 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb  6 18:58:25.159: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.1.6:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2378 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb  6 18:58:25.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 18:58:25.160: INFO: ExecWithOptions: Clientset creation
    Feb  6 18:58:25.160: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2378/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.1.6%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb  6 18:58:25.229: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb  6 18:58:25.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2378" for this suite. 02/06/23 18:58:25.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:58:25.246
Feb  6 18:58:25.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename cronjob 02/06/23 18:58:25.248
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:58:25.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:58:25.269
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 02/06/23 18:58:25.272
STEP: creating 02/06/23 18:58:25.272
STEP: getting 02/06/23 18:58:25.278
STEP: listing 02/06/23 18:58:25.282
STEP: watching 02/06/23 18:58:25.285
Feb  6 18:58:25.285: INFO: starting watch
STEP: cluster-wide listing 02/06/23 18:58:25.286
STEP: cluster-wide watching 02/06/23 18:58:25.29
Feb  6 18:58:25.290: INFO: starting watch
STEP: patching 02/06/23 18:58:25.291
STEP: updating 02/06/23 18:58:25.302
Feb  6 18:58:25.314: INFO: waiting for watch events with expected annotations
Feb  6 18:58:25.314: INFO: saw patched and updated annotations
STEP: patching /status 02/06/23 18:58:25.314
STEP: updating /status 02/06/23 18:58:25.323
STEP: get /status 02/06/23 18:58:25.332
STEP: deleting 02/06/23 18:58:25.336
STEP: deleting a collection 02/06/23 18:58:25.353
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb  6 18:58:25.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6305" for this suite. 02/06/23 18:58:25.373
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":337,"skipped":6370,"failed":0}
------------------------------
• [0.135 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:58:25.246
    Feb  6 18:58:25.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename cronjob 02/06/23 18:58:25.248
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:58:25.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:58:25.269
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 02/06/23 18:58:25.272
    STEP: creating 02/06/23 18:58:25.272
    STEP: getting 02/06/23 18:58:25.278
    STEP: listing 02/06/23 18:58:25.282
    STEP: watching 02/06/23 18:58:25.285
    Feb  6 18:58:25.285: INFO: starting watch
    STEP: cluster-wide listing 02/06/23 18:58:25.286
    STEP: cluster-wide watching 02/06/23 18:58:25.29
    Feb  6 18:58:25.290: INFO: starting watch
    STEP: patching 02/06/23 18:58:25.291
    STEP: updating 02/06/23 18:58:25.302
    Feb  6 18:58:25.314: INFO: waiting for watch events with expected annotations
    Feb  6 18:58:25.314: INFO: saw patched and updated annotations
    STEP: patching /status 02/06/23 18:58:25.314
    STEP: updating /status 02/06/23 18:58:25.323
    STEP: get /status 02/06/23 18:58:25.332
    STEP: deleting 02/06/23 18:58:25.336
    STEP: deleting a collection 02/06/23 18:58:25.353
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb  6 18:58:25.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6305" for this suite. 02/06/23 18:58:25.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:58:25.384
Feb  6 18:58:25.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 18:58:25.385
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:58:25.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:58:25.407
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-f883530a-82fe-463e-b321-ae89f0ab6f31 02/06/23 18:58:25.41
STEP: Creating a pod to test consume configMaps 02/06/23 18:58:25.417
Feb  6 18:58:25.429: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf" in namespace "projected-9746" to be "Succeeded or Failed"
Feb  6 18:58:25.435: INFO: Pod "pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110202ms
Feb  6 18:58:27.443: INFO: Pod "pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013349859s
Feb  6 18:58:29.442: INFO: Pod "pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012724935s
Feb  6 18:58:31.440: INFO: Pod "pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011096884s
STEP: Saw pod success 02/06/23 18:58:31.44
Feb  6 18:58:31.441: INFO: Pod "pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf" satisfied condition "Succeeded or Failed"
Feb  6 18:58:31.444: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf container agnhost-container: <nil>
STEP: delete the pod 02/06/23 18:58:31.75
Feb  6 18:58:31.768: INFO: Waiting for pod pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf to disappear
Feb  6 18:58:31.771: INFO: Pod pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb  6 18:58:31.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9746" for this suite. 02/06/23 18:58:31.776
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":338,"skipped":6389,"failed":0}
------------------------------
• [SLOW TEST] [6.400 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:58:25.384
    Feb  6 18:58:25.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 18:58:25.385
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:58:25.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:58:25.407
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-f883530a-82fe-463e-b321-ae89f0ab6f31 02/06/23 18:58:25.41
    STEP: Creating a pod to test consume configMaps 02/06/23 18:58:25.417
    Feb  6 18:58:25.429: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf" in namespace "projected-9746" to be "Succeeded or Failed"
    Feb  6 18:58:25.435: INFO: Pod "pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110202ms
    Feb  6 18:58:27.443: INFO: Pod "pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013349859s
    Feb  6 18:58:29.442: INFO: Pod "pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012724935s
    Feb  6 18:58:31.440: INFO: Pod "pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011096884s
    STEP: Saw pod success 02/06/23 18:58:31.44
    Feb  6 18:58:31.441: INFO: Pod "pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf" satisfied condition "Succeeded or Failed"
    Feb  6 18:58:31.444: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf container agnhost-container: <nil>
    STEP: delete the pod 02/06/23 18:58:31.75
    Feb  6 18:58:31.768: INFO: Waiting for pod pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf to disappear
    Feb  6 18:58:31.771: INFO: Pod pod-projected-configmaps-6dd8fd89-9b5a-4a18-8312-cbe76ef291cf no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb  6 18:58:31.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9746" for this suite. 02/06/23 18:58:31.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:58:31.785
Feb  6 18:58:31.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename webhook 02/06/23 18:58:31.786
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:58:31.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:58:31.81
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/06/23 18:58:31.828
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:58:32.08
STEP: Deploying the webhook pod 02/06/23 18:58:32.09
STEP: Wait for the deployment to be ready 02/06/23 18:58:32.108
Feb  6 18:58:32.117: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb  6 18:58:34.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  6 18:58:36.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/06/23 18:58:38.135
STEP: Verifying the service has paired with the endpoint 02/06/23 18:58:38.158
Feb  6 18:58:39.158: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 02/06/23 18:58:39.163
STEP: Creating a configMap that does not comply to the validation webhook rules 02/06/23 18:58:39.183
STEP: Updating a validating webhook configuration's rules to not include the create operation 02/06/23 18:58:39.192
STEP: Creating a configMap that does not comply to the validation webhook rules 02/06/23 18:58:39.208
STEP: Patching a validating webhook configuration's rules to include the create operation 02/06/23 18:58:39.222
STEP: Creating a configMap that does not comply to the validation webhook rules 02/06/23 18:58:39.231
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 18:58:39.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6472" for this suite. 02/06/23 18:58:39.249
STEP: Destroying namespace "webhook-6472-markers" for this suite. 02/06/23 18:58:39.258
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":339,"skipped":6405,"failed":0}
------------------------------
• [SLOW TEST] [7.569 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:58:31.785
    Feb  6 18:58:31.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename webhook 02/06/23 18:58:31.786
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:58:31.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:58:31.81
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/06/23 18:58:31.828
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/06/23 18:58:32.08
    STEP: Deploying the webhook pod 02/06/23 18:58:32.09
    STEP: Wait for the deployment to be ready 02/06/23 18:58:32.108
    Feb  6 18:58:32.117: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Feb  6 18:58:34.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb  6 18:58:36.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 6, 18, 58, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/06/23 18:58:38.135
    STEP: Verifying the service has paired with the endpoint 02/06/23 18:58:38.158
    Feb  6 18:58:39.158: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 02/06/23 18:58:39.163
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/06/23 18:58:39.183
    STEP: Updating a validating webhook configuration's rules to not include the create operation 02/06/23 18:58:39.192
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/06/23 18:58:39.208
    STEP: Patching a validating webhook configuration's rules to include the create operation 02/06/23 18:58:39.222
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/06/23 18:58:39.231
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 18:58:39.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6472" for this suite. 02/06/23 18:58:39.249
    STEP: Destroying namespace "webhook-6472-markers" for this suite. 02/06/23 18:58:39.258
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:58:39.355
Feb  6 18:58:39.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 18:58:39.356
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:58:39.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:58:39.383
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-9945 02/06/23 18:58:39.386
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9945 to expose endpoints map[] 02/06/23 18:58:39.407
Feb  6 18:58:39.421: INFO: successfully validated that service endpoint-test2 in namespace services-9945 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9945 02/06/23 18:58:39.421
Feb  6 18:58:39.432: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9945" to be "running and ready"
Feb  6 18:58:39.437: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.849955ms
Feb  6 18:58:39.437: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:58:41.443: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010881203s
Feb  6 18:58:41.443: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:58:43.449: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.016914199s
Feb  6 18:58:43.449: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb  6 18:58:43.449: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9945 to expose endpoints map[pod1:[80]] 02/06/23 18:58:43.46
Feb  6 18:58:43.482: INFO: successfully validated that service endpoint-test2 in namespace services-9945 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 02/06/23 18:58:43.482
Feb  6 18:58:43.482: INFO: Creating new exec pod
Feb  6 18:58:43.496: INFO: Waiting up to 5m0s for pod "execpodm79ch" in namespace "services-9945" to be "running"
Feb  6 18:58:43.507: INFO: Pod "execpodm79ch": Phase="Pending", Reason="", readiness=false. Elapsed: 10.971612ms
Feb  6 18:58:45.513: INFO: Pod "execpodm79ch": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016712003s
Feb  6 18:58:47.512: INFO: Pod "execpodm79ch": Phase="Running", Reason="", readiness=true. Elapsed: 4.016001863s
Feb  6 18:58:47.512: INFO: Pod "execpodm79ch" satisfied condition "running"
Feb  6 18:58:48.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-9945 exec execpodm79ch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb  6 18:58:48.691: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb  6 18:58:48.691: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:58:48.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-9945 exec execpodm79ch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.194.244 80'
Feb  6 18:58:48.859: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.194.244 80\nConnection to 10.102.194.244 80 port [tcp/http] succeeded!\n"
Feb  6 18:58:48.859: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-9945 02/06/23 18:58:48.859
Feb  6 18:58:48.866: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9945" to be "running and ready"
Feb  6 18:58:48.870: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679486ms
Feb  6 18:58:48.870: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:58:50.875: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008484272s
Feb  6 18:58:50.875: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb  6 18:58:52.875: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.00935141s
Feb  6 18:58:52.875: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb  6 18:58:52.875: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9945 to expose endpoints map[pod1:[80] pod2:[80]] 02/06/23 18:58:52.879
Feb  6 18:58:52.896: INFO: successfully validated that service endpoint-test2 in namespace services-9945 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 02/06/23 18:58:52.896
Feb  6 18:58:53.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-9945 exec execpodm79ch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb  6 18:58:54.069: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb  6 18:58:54.069: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:58:54.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-9945 exec execpodm79ch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.194.244 80'
Feb  6 18:58:54.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.194.244 80\nConnection to 10.102.194.244 80 port [tcp/http] succeeded!\n"
Feb  6 18:58:54.236: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9945 02/06/23 18:58:54.236
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9945 to expose endpoints map[pod2:[80]] 02/06/23 18:58:54.27
Feb  6 18:58:54.292: INFO: successfully validated that service endpoint-test2 in namespace services-9945 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 02/06/23 18:58:54.292
Feb  6 18:58:55.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-9945 exec execpodm79ch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb  6 18:58:55.454: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb  6 18:58:55.454: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb  6 18:58:55.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-9945 exec execpodm79ch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.194.244 80'
Feb  6 18:58:55.632: INFO: stderr: "+ nc -v -t -w 2 10.102.194.244 80\n+ echo hostName\nConnection to 10.102.194.244 80 port [tcp/http] succeeded!\n"
Feb  6 18:58:55.632: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-9945 02/06/23 18:58:55.632
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9945 to expose endpoints map[] 02/06/23 18:58:55.653
Feb  6 18:58:55.673: INFO: successfully validated that service endpoint-test2 in namespace services-9945 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 18:58:55.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9945" for this suite. 02/06/23 18:58:55.711
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":340,"skipped":6407,"failed":0}
------------------------------
• [SLOW TEST] [16.365 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:58:39.355
    Feb  6 18:58:39.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 18:58:39.356
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:58:39.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:58:39.383
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-9945 02/06/23 18:58:39.386
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9945 to expose endpoints map[] 02/06/23 18:58:39.407
    Feb  6 18:58:39.421: INFO: successfully validated that service endpoint-test2 in namespace services-9945 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9945 02/06/23 18:58:39.421
    Feb  6 18:58:39.432: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9945" to be "running and ready"
    Feb  6 18:58:39.437: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.849955ms
    Feb  6 18:58:39.437: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:58:41.443: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010881203s
    Feb  6 18:58:41.443: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:58:43.449: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.016914199s
    Feb  6 18:58:43.449: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb  6 18:58:43.449: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9945 to expose endpoints map[pod1:[80]] 02/06/23 18:58:43.46
    Feb  6 18:58:43.482: INFO: successfully validated that service endpoint-test2 in namespace services-9945 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 02/06/23 18:58:43.482
    Feb  6 18:58:43.482: INFO: Creating new exec pod
    Feb  6 18:58:43.496: INFO: Waiting up to 5m0s for pod "execpodm79ch" in namespace "services-9945" to be "running"
    Feb  6 18:58:43.507: INFO: Pod "execpodm79ch": Phase="Pending", Reason="", readiness=false. Elapsed: 10.971612ms
    Feb  6 18:58:45.513: INFO: Pod "execpodm79ch": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016712003s
    Feb  6 18:58:47.512: INFO: Pod "execpodm79ch": Phase="Running", Reason="", readiness=true. Elapsed: 4.016001863s
    Feb  6 18:58:47.512: INFO: Pod "execpodm79ch" satisfied condition "running"
    Feb  6 18:58:48.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-9945 exec execpodm79ch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb  6 18:58:48.691: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb  6 18:58:48.691: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:58:48.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-9945 exec execpodm79ch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.194.244 80'
    Feb  6 18:58:48.859: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.194.244 80\nConnection to 10.102.194.244 80 port [tcp/http] succeeded!\n"
    Feb  6 18:58:48.859: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-9945 02/06/23 18:58:48.859
    Feb  6 18:58:48.866: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9945" to be "running and ready"
    Feb  6 18:58:48.870: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679486ms
    Feb  6 18:58:48.870: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:58:50.875: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008484272s
    Feb  6 18:58:50.875: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 18:58:52.875: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.00935141s
    Feb  6 18:58:52.875: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb  6 18:58:52.875: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9945 to expose endpoints map[pod1:[80] pod2:[80]] 02/06/23 18:58:52.879
    Feb  6 18:58:52.896: INFO: successfully validated that service endpoint-test2 in namespace services-9945 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 02/06/23 18:58:52.896
    Feb  6 18:58:53.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-9945 exec execpodm79ch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb  6 18:58:54.069: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb  6 18:58:54.069: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:58:54.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-9945 exec execpodm79ch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.194.244 80'
    Feb  6 18:58:54.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.194.244 80\nConnection to 10.102.194.244 80 port [tcp/http] succeeded!\n"
    Feb  6 18:58:54.236: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-9945 02/06/23 18:58:54.236
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9945 to expose endpoints map[pod2:[80]] 02/06/23 18:58:54.27
    Feb  6 18:58:54.292: INFO: successfully validated that service endpoint-test2 in namespace services-9945 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 02/06/23 18:58:54.292
    Feb  6 18:58:55.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-9945 exec execpodm79ch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb  6 18:58:55.454: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb  6 18:58:55.454: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb  6 18:58:55.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-9945 exec execpodm79ch -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.194.244 80'
    Feb  6 18:58:55.632: INFO: stderr: "+ nc -v -t -w 2 10.102.194.244 80\n+ echo hostName\nConnection to 10.102.194.244 80 port [tcp/http] succeeded!\n"
    Feb  6 18:58:55.632: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-9945 02/06/23 18:58:55.632
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9945 to expose endpoints map[] 02/06/23 18:58:55.653
    Feb  6 18:58:55.673: INFO: successfully validated that service endpoint-test2 in namespace services-9945 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 18:58:55.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9945" for this suite. 02/06/23 18:58:55.711
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:58:55.72
Feb  6 18:58:55.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename namespaces 02/06/23 18:58:55.721
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:58:55.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:58:55.753
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 02/06/23 18:58:55.758
STEP: patching the Namespace 02/06/23 18:58:55.776
STEP: get the Namespace and ensuring it has the label 02/06/23 18:58:55.785
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb  6 18:58:55.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5746" for this suite. 02/06/23 18:58:55.794
STEP: Destroying namespace "nspatchtest-8dda5221-2689-487c-a669-00c620e16f20-3141" for this suite. 02/06/23 18:58:55.801
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":341,"skipped":6417,"failed":0}
------------------------------
• [0.089 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:58:55.72
    Feb  6 18:58:55.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename namespaces 02/06/23 18:58:55.721
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:58:55.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:58:55.753
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 02/06/23 18:58:55.758
    STEP: patching the Namespace 02/06/23 18:58:55.776
    STEP: get the Namespace and ensuring it has the label 02/06/23 18:58:55.785
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 18:58:55.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5746" for this suite. 02/06/23 18:58:55.794
    STEP: Destroying namespace "nspatchtest-8dda5221-2689-487c-a669-00c620e16f20-3141" for this suite. 02/06/23 18:58:55.801
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:58:55.809
Feb  6 18:58:55.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 18:58:55.81
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:58:55.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:58:55.835
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 02/06/23 18:58:55.842
Feb  6 18:58:55.853: INFO: Waiting up to 5m0s for pod "downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832" in namespace "downward-api-8538" to be "Succeeded or Failed"
Feb  6 18:58:55.857: INFO: Pod "downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832": Phase="Pending", Reason="", readiness=false. Elapsed: 3.753373ms
Feb  6 18:58:57.862: INFO: Pod "downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009544413s
Feb  6 18:58:59.862: INFO: Pod "downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00873646s
Feb  6 18:59:01.862: INFO: Pod "downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008613637s
STEP: Saw pod success 02/06/23 18:59:01.862
Feb  6 18:59:01.862: INFO: Pod "downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832" satisfied condition "Succeeded or Failed"
Feb  6 18:59:01.865: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832 container client-container: <nil>
STEP: delete the pod 02/06/23 18:59:01.874
Feb  6 18:59:01.890: INFO: Waiting for pod downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832 to disappear
Feb  6 18:59:01.893: INFO: Pod downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb  6 18:59:01.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8538" for this suite. 02/06/23 18:59:01.899
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":342,"skipped":6417,"failed":0}
------------------------------
• [SLOW TEST] [6.096 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:58:55.809
    Feb  6 18:58:55.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 18:58:55.81
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:58:55.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:58:55.835
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 02/06/23 18:58:55.842
    Feb  6 18:58:55.853: INFO: Waiting up to 5m0s for pod "downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832" in namespace "downward-api-8538" to be "Succeeded or Failed"
    Feb  6 18:58:55.857: INFO: Pod "downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832": Phase="Pending", Reason="", readiness=false. Elapsed: 3.753373ms
    Feb  6 18:58:57.862: INFO: Pod "downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009544413s
    Feb  6 18:58:59.862: INFO: Pod "downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00873646s
    Feb  6 18:59:01.862: INFO: Pod "downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008613637s
    STEP: Saw pod success 02/06/23 18:59:01.862
    Feb  6 18:59:01.862: INFO: Pod "downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832" satisfied condition "Succeeded or Failed"
    Feb  6 18:59:01.865: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832 container client-container: <nil>
    STEP: delete the pod 02/06/23 18:59:01.874
    Feb  6 18:59:01.890: INFO: Waiting for pod downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832 to disappear
    Feb  6 18:59:01.893: INFO: Pod downwardapi-volume-583ec62a-3f71-4a25-b337-46d0e03da832 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb  6 18:59:01.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8538" for this suite. 02/06/23 18:59:01.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 18:59:01.911
Feb  6 18:59:01.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename taint-single-pod 02/06/23 18:59:01.912
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:59:01.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:59:01.933
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Feb  6 18:59:01.936: INFO: Waiting up to 1m0s for all nodes to be ready
Feb  6 19:00:01.984: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Feb  6 19:00:01.988: INFO: Starting informer...
STEP: Starting pod... 02/06/23 19:00:01.988
Feb  6 19:00:02.207: INFO: Pod is running on tneyla25-md-0-68cbcb4798-5xxmw. Tainting Node
STEP: Trying to apply a taint on the Node 02/06/23 19:00:02.207
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/06/23 19:00:02.224
STEP: Waiting short time to make sure Pod is queued for deletion 02/06/23 19:00:02.233
Feb  6 19:00:02.233: INFO: Pod wasn't evicted. Proceeding
Feb  6 19:00:02.233: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/06/23 19:00:02.249
STEP: Waiting some time to make sure that toleration time passed. 02/06/23 19:00:02.255
Feb  6 19:01:17.259: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Feb  6 19:01:17.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6429" for this suite. 02/06/23 19:01:17.269
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":343,"skipped":6438,"failed":0}
------------------------------
• [SLOW TEST] [135.377 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 18:59:01.911
    Feb  6 18:59:01.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename taint-single-pod 02/06/23 18:59:01.912
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 18:59:01.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 18:59:01.933
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Feb  6 18:59:01.936: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb  6 19:00:01.984: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Feb  6 19:00:01.988: INFO: Starting informer...
    STEP: Starting pod... 02/06/23 19:00:01.988
    Feb  6 19:00:02.207: INFO: Pod is running on tneyla25-md-0-68cbcb4798-5xxmw. Tainting Node
    STEP: Trying to apply a taint on the Node 02/06/23 19:00:02.207
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/06/23 19:00:02.224
    STEP: Waiting short time to make sure Pod is queued for deletion 02/06/23 19:00:02.233
    Feb  6 19:00:02.233: INFO: Pod wasn't evicted. Proceeding
    Feb  6 19:00:02.233: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/06/23 19:00:02.249
    STEP: Waiting some time to make sure that toleration time passed. 02/06/23 19:00:02.255
    Feb  6 19:01:17.259: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 19:01:17.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-6429" for this suite. 02/06/23 19:01:17.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:01:17.289
Feb  6 19:01:17.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename runtimeclass 02/06/23 19:01:17.29
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:01:17.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:01:17.417
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-2415-delete-me 02/06/23 19:01:17.434
STEP: Waiting for the RuntimeClass to disappear 02/06/23 19:01:17.448
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb  6 19:01:17.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2415" for this suite. 02/06/23 19:01:17.489
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":344,"skipped":6454,"failed":0}
------------------------------
• [0.223 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:01:17.289
    Feb  6 19:01:17.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename runtimeclass 02/06/23 19:01:17.29
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:01:17.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:01:17.417
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-2415-delete-me 02/06/23 19:01:17.434
    STEP: Waiting for the RuntimeClass to disappear 02/06/23 19:01:17.448
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb  6 19:01:17.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2415" for this suite. 02/06/23 19:01:17.489
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:01:17.513
Feb  6 19:01:17.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename resourcequota 02/06/23 19:01:17.514
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:01:17.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:01:17.71
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 02/06/23 19:01:17.713
STEP: Ensuring ResourceQuota status is calculated 02/06/23 19:01:17.724
STEP: Creating a ResourceQuota with not best effort scope 02/06/23 19:01:19.73
STEP: Ensuring ResourceQuota status is calculated 02/06/23 19:01:19.737
STEP: Creating a best-effort pod 02/06/23 19:01:21.743
STEP: Ensuring resource quota with best effort scope captures the pod usage 02/06/23 19:01:21.761
STEP: Ensuring resource quota with not best effort ignored the pod usage 02/06/23 19:01:23.766
STEP: Deleting the pod 02/06/23 19:01:25.771
STEP: Ensuring resource quota status released the pod usage 02/06/23 19:01:25.786
STEP: Creating a not best-effort pod 02/06/23 19:01:27.791
STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/06/23 19:01:27.804
STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/06/23 19:01:29.808
STEP: Deleting the pod 02/06/23 19:01:31.814
STEP: Ensuring resource quota status released the pod usage 02/06/23 19:01:31.829
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb  6 19:01:33.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8203" for this suite. 02/06/23 19:01:33.841
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":345,"skipped":6457,"failed":0}
------------------------------
• [SLOW TEST] [16.335 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:01:17.513
    Feb  6 19:01:17.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename resourcequota 02/06/23 19:01:17.514
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:01:17.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:01:17.71
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 02/06/23 19:01:17.713
    STEP: Ensuring ResourceQuota status is calculated 02/06/23 19:01:17.724
    STEP: Creating a ResourceQuota with not best effort scope 02/06/23 19:01:19.73
    STEP: Ensuring ResourceQuota status is calculated 02/06/23 19:01:19.737
    STEP: Creating a best-effort pod 02/06/23 19:01:21.743
    STEP: Ensuring resource quota with best effort scope captures the pod usage 02/06/23 19:01:21.761
    STEP: Ensuring resource quota with not best effort ignored the pod usage 02/06/23 19:01:23.766
    STEP: Deleting the pod 02/06/23 19:01:25.771
    STEP: Ensuring resource quota status released the pod usage 02/06/23 19:01:25.786
    STEP: Creating a not best-effort pod 02/06/23 19:01:27.791
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/06/23 19:01:27.804
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/06/23 19:01:29.808
    STEP: Deleting the pod 02/06/23 19:01:31.814
    STEP: Ensuring resource quota status released the pod usage 02/06/23 19:01:31.829
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb  6 19:01:33.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8203" for this suite. 02/06/23 19:01:33.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:01:33.857
Feb  6 19:01:33.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 19:01:33.858
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:01:33.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:01:33.885
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/06/23 19:01:33.887
Feb  6 19:01:33.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
Feb  6 19:01:42.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb  6 19:02:12.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5554" for this suite. 02/06/23 19:02:12.294
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":346,"skipped":6509,"failed":0}
------------------------------
• [SLOW TEST] [38.444 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:01:33.857
    Feb  6 19:01:33.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename crd-publish-openapi 02/06/23 19:01:33.858
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:01:33.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:01:33.885
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/06/23 19:01:33.887
    Feb  6 19:01:33.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    Feb  6 19:01:42.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb  6 19:02:12.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5554" for this suite. 02/06/23 19:02:12.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:02:12.304
Feb  6 19:02:12.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename secrets 02/06/23 19:02:12.305
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:12.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:12.328
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-bf6a84e6-368d-44ac-9973-830bfc6d8765 02/06/23 19:02:12.336
STEP: Creating secret with name s-test-opt-upd-a3581486-c671-4961-8b19-1fe842249bab 02/06/23 19:02:12.342
STEP: Creating the pod 02/06/23 19:02:12.348
Feb  6 19:02:12.359: INFO: Waiting up to 5m0s for pod "pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e" in namespace "secrets-2928" to be "running and ready"
Feb  6 19:02:12.366: INFO: Pod "pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.4187ms
Feb  6 19:02:12.366: INFO: The phase of Pod pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e is Pending, waiting for it to be Running (with Ready = true)
Feb  6 19:02:14.373: INFO: Pod "pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013892432s
Feb  6 19:02:14.373: INFO: The phase of Pod pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e is Pending, waiting for it to be Running (with Ready = true)
Feb  6 19:02:16.370: INFO: Pod "pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e": Phase="Running", Reason="", readiness=true. Elapsed: 4.011602868s
Feb  6 19:02:16.371: INFO: The phase of Pod pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e is Running (Ready = true)
Feb  6 19:02:16.371: INFO: Pod "pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-bf6a84e6-368d-44ac-9973-830bfc6d8765 02/06/23 19:02:16.402
STEP: Updating secret s-test-opt-upd-a3581486-c671-4961-8b19-1fe842249bab 02/06/23 19:02:16.408
STEP: Creating secret with name s-test-opt-create-0f717578-c66b-4c1f-be83-ae852348c5aa 02/06/23 19:02:16.415
STEP: waiting to observe update in volume 02/06/23 19:02:16.421
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb  6 19:02:18.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2928" for this suite. 02/06/23 19:02:18.455
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":347,"skipped":6533,"failed":0}
------------------------------
• [SLOW TEST] [6.158 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:02:12.304
    Feb  6 19:02:12.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename secrets 02/06/23 19:02:12.305
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:12.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:12.328
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-bf6a84e6-368d-44ac-9973-830bfc6d8765 02/06/23 19:02:12.336
    STEP: Creating secret with name s-test-opt-upd-a3581486-c671-4961-8b19-1fe842249bab 02/06/23 19:02:12.342
    STEP: Creating the pod 02/06/23 19:02:12.348
    Feb  6 19:02:12.359: INFO: Waiting up to 5m0s for pod "pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e" in namespace "secrets-2928" to be "running and ready"
    Feb  6 19:02:12.366: INFO: Pod "pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.4187ms
    Feb  6 19:02:12.366: INFO: The phase of Pod pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 19:02:14.373: INFO: Pod "pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013892432s
    Feb  6 19:02:14.373: INFO: The phase of Pod pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e is Pending, waiting for it to be Running (with Ready = true)
    Feb  6 19:02:16.370: INFO: Pod "pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e": Phase="Running", Reason="", readiness=true. Elapsed: 4.011602868s
    Feb  6 19:02:16.371: INFO: The phase of Pod pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e is Running (Ready = true)
    Feb  6 19:02:16.371: INFO: Pod "pod-secrets-7c7429b5-b31f-4140-ab54-a3ad4984539e" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-bf6a84e6-368d-44ac-9973-830bfc6d8765 02/06/23 19:02:16.402
    STEP: Updating secret s-test-opt-upd-a3581486-c671-4961-8b19-1fe842249bab 02/06/23 19:02:16.408
    STEP: Creating secret with name s-test-opt-create-0f717578-c66b-4c1f-be83-ae852348c5aa 02/06/23 19:02:16.415
    STEP: waiting to observe update in volume 02/06/23 19:02:16.421
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb  6 19:02:18.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2928" for this suite. 02/06/23 19:02:18.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:02:18.464
Feb  6 19:02:18.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename watch 02/06/23 19:02:18.465
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:18.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:18.487
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 02/06/23 19:02:18.49
STEP: creating a watch on configmaps with label B 02/06/23 19:02:18.491
STEP: creating a watch on configmaps with label A or B 02/06/23 19:02:18.493
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/06/23 19:02:18.494
Feb  6 19:02:18.499: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89897 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 19:02:18.500: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89897 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/06/23 19:02:18.5
Feb  6 19:02:18.510: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89898 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 19:02:18.510: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89898 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/06/23 19:02:18.51
Feb  6 19:02:18.519: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89899 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 19:02:18.519: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89899 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/06/23 19:02:18.519
Feb  6 19:02:18.528: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89901 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 19:02:18.528: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89901 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/06/23 19:02:18.529
Feb  6 19:02:18.535: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6813  83746436-caef-400e-aeff-ffd25591f870 89903 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 19:02:18.535: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6813  83746436-caef-400e-aeff-ffd25591f870 89903 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/06/23 19:02:28.537
Feb  6 19:02:28.546: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6813  83746436-caef-400e-aeff-ffd25591f870 90022 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  6 19:02:28.547: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6813  83746436-caef-400e-aeff-ffd25591f870 90022 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb  6 19:02:38.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6813" for this suite. 02/06/23 19:02:38.554
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":348,"skipped":6547,"failed":0}
------------------------------
• [SLOW TEST] [20.098 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:02:18.464
    Feb  6 19:02:18.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename watch 02/06/23 19:02:18.465
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:18.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:18.487
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 02/06/23 19:02:18.49
    STEP: creating a watch on configmaps with label B 02/06/23 19:02:18.491
    STEP: creating a watch on configmaps with label A or B 02/06/23 19:02:18.493
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/06/23 19:02:18.494
    Feb  6 19:02:18.499: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89897 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 19:02:18.500: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89897 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/06/23 19:02:18.5
    Feb  6 19:02:18.510: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89898 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 19:02:18.510: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89898 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/06/23 19:02:18.51
    Feb  6 19:02:18.519: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89899 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 19:02:18.519: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89899 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/06/23 19:02:18.519
    Feb  6 19:02:18.528: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89901 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 19:02:18.528: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6813  a9d18c6b-7516-4706-b24b-b994b263a491 89901 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/06/23 19:02:18.529
    Feb  6 19:02:18.535: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6813  83746436-caef-400e-aeff-ffd25591f870 89903 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 19:02:18.535: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6813  83746436-caef-400e-aeff-ffd25591f870 89903 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/06/23 19:02:28.537
    Feb  6 19:02:28.546: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6813  83746436-caef-400e-aeff-ffd25591f870 90022 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb  6 19:02:28.547: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6813  83746436-caef-400e-aeff-ffd25591f870 90022 0 2023-02-06 19:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-06 19:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb  6 19:02:38.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6813" for this suite. 02/06/23 19:02:38.554
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:02:38.565
Feb  6 19:02:38.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 19:02:38.566
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:38.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:38.588
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-9517e97e-9028-4482-a984-313832f5039b 02/06/23 19:02:38.59
STEP: Creating a pod to test consume secrets 02/06/23 19:02:38.596
Feb  6 19:02:38.606: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72" in namespace "projected-6623" to be "Succeeded or Failed"
Feb  6 19:02:38.610: INFO: Pod "pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72": Phase="Pending", Reason="", readiness=false. Elapsed: 3.749042ms
Feb  6 19:02:40.616: INFO: Pod "pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009950863s
Feb  6 19:02:42.615: INFO: Pod "pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009337761s
Feb  6 19:02:44.614: INFO: Pod "pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008505344s
STEP: Saw pod success 02/06/23 19:02:44.614
Feb  6 19:02:44.615: INFO: Pod "pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72" satisfied condition "Succeeded or Failed"
Feb  6 19:02:44.618: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/06/23 19:02:44.625
Feb  6 19:02:44.638: INFO: Waiting for pod pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72 to disappear
Feb  6 19:02:44.641: INFO: Pod pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb  6 19:02:44.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6623" for this suite. 02/06/23 19:02:44.647
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":349,"skipped":6549,"failed":0}
------------------------------
• [SLOW TEST] [6.087 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:02:38.565
    Feb  6 19:02:38.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 19:02:38.566
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:38.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:38.588
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-9517e97e-9028-4482-a984-313832f5039b 02/06/23 19:02:38.59
    STEP: Creating a pod to test consume secrets 02/06/23 19:02:38.596
    Feb  6 19:02:38.606: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72" in namespace "projected-6623" to be "Succeeded or Failed"
    Feb  6 19:02:38.610: INFO: Pod "pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72": Phase="Pending", Reason="", readiness=false. Elapsed: 3.749042ms
    Feb  6 19:02:40.616: INFO: Pod "pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009950863s
    Feb  6 19:02:42.615: INFO: Pod "pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009337761s
    Feb  6 19:02:44.614: INFO: Pod "pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008505344s
    STEP: Saw pod success 02/06/23 19:02:44.614
    Feb  6 19:02:44.615: INFO: Pod "pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72" satisfied condition "Succeeded or Failed"
    Feb  6 19:02:44.618: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/06/23 19:02:44.625
    Feb  6 19:02:44.638: INFO: Waiting for pod pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72 to disappear
    Feb  6 19:02:44.641: INFO: Pod pod-projected-secrets-15929002-5f0e-48dc-8208-8613dbed8b72 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb  6 19:02:44.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6623" for this suite. 02/06/23 19:02:44.647
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:02:44.654
Feb  6 19:02:44.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename downward-api 02/06/23 19:02:44.656
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:44.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:44.676
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 02/06/23 19:02:44.681
Feb  6 19:02:44.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3" in namespace "downward-api-6777" to be "Succeeded or Failed"
Feb  6 19:02:44.697: INFO: Pod "downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.848594ms
Feb  6 19:02:46.760: INFO: Pod "downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068658727s
Feb  6 19:02:48.705: INFO: Pod "downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01350216s
Feb  6 19:02:50.704: INFO: Pod "downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012507575s
STEP: Saw pod success 02/06/23 19:02:50.704
Feb  6 19:02:50.704: INFO: Pod "downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3" satisfied condition "Succeeded or Failed"
Feb  6 19:02:50.708: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3 container client-container: <nil>
STEP: delete the pod 02/06/23 19:02:50.716
Feb  6 19:02:50.735: INFO: Waiting for pod downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3 to disappear
Feb  6 19:02:50.738: INFO: Pod downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb  6 19:02:50.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6777" for this suite. 02/06/23 19:02:50.744
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":350,"skipped":6551,"failed":0}
------------------------------
• [SLOW TEST] [6.097 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:02:44.654
    Feb  6 19:02:44.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename downward-api 02/06/23 19:02:44.656
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:44.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:44.676
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 02/06/23 19:02:44.681
    Feb  6 19:02:44.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3" in namespace "downward-api-6777" to be "Succeeded or Failed"
    Feb  6 19:02:44.697: INFO: Pod "downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.848594ms
    Feb  6 19:02:46.760: INFO: Pod "downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068658727s
    Feb  6 19:02:48.705: INFO: Pod "downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01350216s
    Feb  6 19:02:50.704: INFO: Pod "downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012507575s
    STEP: Saw pod success 02/06/23 19:02:50.704
    Feb  6 19:02:50.704: INFO: Pod "downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3" satisfied condition "Succeeded or Failed"
    Feb  6 19:02:50.708: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3 container client-container: <nil>
    STEP: delete the pod 02/06/23 19:02:50.716
    Feb  6 19:02:50.735: INFO: Waiting for pod downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3 to disappear
    Feb  6 19:02:50.738: INFO: Pod downwardapi-volume-742001b5-f793-4cfb-a38e-5f10c3f327e3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb  6 19:02:50.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6777" for this suite. 02/06/23 19:02:50.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:02:50.752
Feb  6 19:02:50.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename discovery 02/06/23 19:02:50.753
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:50.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:50.775
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 02/06/23 19:02:50.778
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Feb  6 19:02:51.056: INFO: Checking APIGroup: apiregistration.k8s.io
Feb  6 19:02:51.058: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb  6 19:02:51.058: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Feb  6 19:02:51.058: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb  6 19:02:51.058: INFO: Checking APIGroup: apps
Feb  6 19:02:51.058: INFO: PreferredVersion.GroupVersion: apps/v1
Feb  6 19:02:51.058: INFO: Versions found [{apps/v1 v1}]
Feb  6 19:02:51.058: INFO: apps/v1 matches apps/v1
Feb  6 19:02:51.058: INFO: Checking APIGroup: events.k8s.io
Feb  6 19:02:51.059: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb  6 19:02:51.059: INFO: Versions found [{events.k8s.io/v1 v1}]
Feb  6 19:02:51.059: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb  6 19:02:51.059: INFO: Checking APIGroup: authentication.k8s.io
Feb  6 19:02:51.060: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb  6 19:02:51.060: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Feb  6 19:02:51.060: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb  6 19:02:51.060: INFO: Checking APIGroup: authorization.k8s.io
Feb  6 19:02:51.061: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb  6 19:02:51.061: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Feb  6 19:02:51.061: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb  6 19:02:51.061: INFO: Checking APIGroup: autoscaling
Feb  6 19:02:51.062: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Feb  6 19:02:51.062: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Feb  6 19:02:51.062: INFO: autoscaling/v2 matches autoscaling/v2
Feb  6 19:02:51.062: INFO: Checking APIGroup: batch
Feb  6 19:02:51.063: INFO: PreferredVersion.GroupVersion: batch/v1
Feb  6 19:02:51.063: INFO: Versions found [{batch/v1 v1}]
Feb  6 19:02:51.063: INFO: batch/v1 matches batch/v1
Feb  6 19:02:51.063: INFO: Checking APIGroup: certificates.k8s.io
Feb  6 19:02:51.064: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb  6 19:02:51.064: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Feb  6 19:02:51.064: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb  6 19:02:51.064: INFO: Checking APIGroup: networking.k8s.io
Feb  6 19:02:51.065: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb  6 19:02:51.065: INFO: Versions found [{networking.k8s.io/v1 v1}]
Feb  6 19:02:51.065: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb  6 19:02:51.065: INFO: Checking APIGroup: policy
Feb  6 19:02:51.066: INFO: PreferredVersion.GroupVersion: policy/v1
Feb  6 19:02:51.066: INFO: Versions found [{policy/v1 v1}]
Feb  6 19:02:51.066: INFO: policy/v1 matches policy/v1
Feb  6 19:02:51.066: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb  6 19:02:51.067: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb  6 19:02:51.067: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Feb  6 19:02:51.067: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb  6 19:02:51.067: INFO: Checking APIGroup: storage.k8s.io
Feb  6 19:02:51.068: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb  6 19:02:51.068: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Feb  6 19:02:51.068: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb  6 19:02:51.068: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb  6 19:02:51.069: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb  6 19:02:51.069: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Feb  6 19:02:51.069: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb  6 19:02:51.069: INFO: Checking APIGroup: apiextensions.k8s.io
Feb  6 19:02:51.070: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb  6 19:02:51.070: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Feb  6 19:02:51.070: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb  6 19:02:51.070: INFO: Checking APIGroup: scheduling.k8s.io
Feb  6 19:02:51.071: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb  6 19:02:51.071: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Feb  6 19:02:51.071: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb  6 19:02:51.071: INFO: Checking APIGroup: coordination.k8s.io
Feb  6 19:02:51.071: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb  6 19:02:51.071: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Feb  6 19:02:51.071: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb  6 19:02:51.071: INFO: Checking APIGroup: node.k8s.io
Feb  6 19:02:51.072: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb  6 19:02:51.072: INFO: Versions found [{node.k8s.io/v1 v1}]
Feb  6 19:02:51.072: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb  6 19:02:51.072: INFO: Checking APIGroup: discovery.k8s.io
Feb  6 19:02:51.073: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Feb  6 19:02:51.073: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Feb  6 19:02:51.073: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Feb  6 19:02:51.073: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb  6 19:02:51.074: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Feb  6 19:02:51.074: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Feb  6 19:02:51.074: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Feb  6 19:02:51.074: INFO: Checking APIGroup: acme.cert-manager.io
Feb  6 19:02:51.075: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Feb  6 19:02:51.075: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Feb  6 19:02:51.075: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Feb  6 19:02:51.075: INFO: Checking APIGroup: cert-manager.io
Feb  6 19:02:51.076: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Feb  6 19:02:51.076: INFO: Versions found [{cert-manager.io/v1 v1}]
Feb  6 19:02:51.076: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Feb  6 19:02:51.076: INFO: Checking APIGroup: anywhere.eks.amazonaws.com
Feb  6 19:02:51.077: INFO: PreferredVersion.GroupVersion: anywhere.eks.amazonaws.com/v1alpha1
Feb  6 19:02:51.077: INFO: Versions found [{anywhere.eks.amazonaws.com/v1alpha1 v1alpha1}]
Feb  6 19:02:51.077: INFO: anywhere.eks.amazonaws.com/v1alpha1 matches anywhere.eks.amazonaws.com/v1alpha1
Feb  6 19:02:51.077: INFO: Checking APIGroup: distro.eks.amazonaws.com
Feb  6 19:02:51.078: INFO: PreferredVersion.GroupVersion: distro.eks.amazonaws.com/v1alpha1
Feb  6 19:02:51.078: INFO: Versions found [{distro.eks.amazonaws.com/v1alpha1 v1alpha1}]
Feb  6 19:02:51.078: INFO: distro.eks.amazonaws.com/v1alpha1 matches distro.eks.amazonaws.com/v1alpha1
Feb  6 19:02:51.078: INFO: Checking APIGroup: ipam.cluster.x-k8s.io
Feb  6 19:02:51.079: INFO: PreferredVersion.GroupVersion: ipam.cluster.x-k8s.io/v1alpha1
Feb  6 19:02:51.079: INFO: Versions found [{ipam.cluster.x-k8s.io/v1alpha1 v1alpha1}]
Feb  6 19:02:51.079: INFO: ipam.cluster.x-k8s.io/v1alpha1 matches ipam.cluster.x-k8s.io/v1alpha1
Feb  6 19:02:51.079: INFO: Checking APIGroup: packages.eks.amazonaws.com
Feb  6 19:02:51.080: INFO: PreferredVersion.GroupVersion: packages.eks.amazonaws.com/v1alpha1
Feb  6 19:02:51.080: INFO: Versions found [{packages.eks.amazonaws.com/v1alpha1 v1alpha1}]
Feb  6 19:02:51.080: INFO: packages.eks.amazonaws.com/v1alpha1 matches packages.eks.amazonaws.com/v1alpha1
Feb  6 19:02:51.080: INFO: Checking APIGroup: runtime.cluster.x-k8s.io
Feb  6 19:02:51.081: INFO: PreferredVersion.GroupVersion: runtime.cluster.x-k8s.io/v1alpha1
Feb  6 19:02:51.081: INFO: Versions found [{runtime.cluster.x-k8s.io/v1alpha1 v1alpha1}]
Feb  6 19:02:51.081: INFO: runtime.cluster.x-k8s.io/v1alpha1 matches runtime.cluster.x-k8s.io/v1alpha1
Feb  6 19:02:51.081: INFO: Checking APIGroup: addons.cluster.x-k8s.io
Feb  6 19:02:51.082: INFO: PreferredVersion.GroupVersion: addons.cluster.x-k8s.io/v1beta1
Feb  6 19:02:51.082: INFO: Versions found [{addons.cluster.x-k8s.io/v1beta1 v1beta1} {addons.cluster.x-k8s.io/v1alpha4 v1alpha4} {addons.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Feb  6 19:02:51.082: INFO: addons.cluster.x-k8s.io/v1beta1 matches addons.cluster.x-k8s.io/v1beta1
Feb  6 19:02:51.082: INFO: Checking APIGroup: bootstrap.cluster.x-k8s.io
Feb  6 19:02:51.083: INFO: PreferredVersion.GroupVersion: bootstrap.cluster.x-k8s.io/v1beta1
Feb  6 19:02:51.083: INFO: Versions found [{bootstrap.cluster.x-k8s.io/v1beta1 v1beta1} {bootstrap.cluster.x-k8s.io/v1alpha4 v1alpha4} {bootstrap.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Feb  6 19:02:51.083: INFO: bootstrap.cluster.x-k8s.io/v1beta1 matches bootstrap.cluster.x-k8s.io/v1beta1
Feb  6 19:02:51.083: INFO: Checking APIGroup: cluster.x-k8s.io
Feb  6 19:02:51.085: INFO: PreferredVersion.GroupVersion: cluster.x-k8s.io/v1beta1
Feb  6 19:02:51.085: INFO: Versions found [{cluster.x-k8s.io/v1beta1 v1beta1} {cluster.x-k8s.io/v1alpha4 v1alpha4} {cluster.x-k8s.io/v1alpha3 v1alpha3}]
Feb  6 19:02:51.085: INFO: cluster.x-k8s.io/v1beta1 matches cluster.x-k8s.io/v1beta1
Feb  6 19:02:51.085: INFO: Checking APIGroup: clusterctl.cluster.x-k8s.io
Feb  6 19:02:51.086: INFO: PreferredVersion.GroupVersion: clusterctl.cluster.x-k8s.io/v1alpha3
Feb  6 19:02:51.086: INFO: Versions found [{clusterctl.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Feb  6 19:02:51.086: INFO: clusterctl.cluster.x-k8s.io/v1alpha3 matches clusterctl.cluster.x-k8s.io/v1alpha3
Feb  6 19:02:51.086: INFO: Checking APIGroup: controlplane.cluster.x-k8s.io
Feb  6 19:02:51.087: INFO: PreferredVersion.GroupVersion: controlplane.cluster.x-k8s.io/v1beta1
Feb  6 19:02:51.087: INFO: Versions found [{controlplane.cluster.x-k8s.io/v1beta1 v1beta1} {controlplane.cluster.x-k8s.io/v1alpha4 v1alpha4} {controlplane.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Feb  6 19:02:51.087: INFO: controlplane.cluster.x-k8s.io/v1beta1 matches controlplane.cluster.x-k8s.io/v1beta1
Feb  6 19:02:51.087: INFO: Checking APIGroup: etcdcluster.cluster.x-k8s.io
Feb  6 19:02:51.088: INFO: PreferredVersion.GroupVersion: etcdcluster.cluster.x-k8s.io/v1beta1
Feb  6 19:02:51.088: INFO: Versions found [{etcdcluster.cluster.x-k8s.io/v1beta1 v1beta1} {etcdcluster.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Feb  6 19:02:51.088: INFO: etcdcluster.cluster.x-k8s.io/v1beta1 matches etcdcluster.cluster.x-k8s.io/v1beta1
Feb  6 19:02:51.088: INFO: Checking APIGroup: infrastructure.cluster.x-k8s.io
Feb  6 19:02:51.089: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.x-k8s.io/v1beta1
Feb  6 19:02:51.090: INFO: Versions found [{infrastructure.cluster.x-k8s.io/v1beta1 v1beta1} {infrastructure.cluster.x-k8s.io/v1alpha4 v1alpha4} {infrastructure.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Feb  6 19:02:51.090: INFO: infrastructure.cluster.x-k8s.io/v1beta1 matches infrastructure.cluster.x-k8s.io/v1beta1
Feb  6 19:02:51.090: INFO: Checking APIGroup: cilium.io
Feb  6 19:02:51.090: INFO: PreferredVersion.GroupVersion: cilium.io/v2
Feb  6 19:02:51.091: INFO: Versions found [{cilium.io/v2 v2}]
Feb  6 19:02:51.091: INFO: cilium.io/v2 matches cilium.io/v2
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Feb  6 19:02:51.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-95" for this suite. 02/06/23 19:02:51.096
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":351,"skipped":6561,"failed":0}
------------------------------
• [0.351 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:02:50.752
    Feb  6 19:02:50.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename discovery 02/06/23 19:02:50.753
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:50.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:50.775
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 02/06/23 19:02:50.778
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Feb  6 19:02:51.056: INFO: Checking APIGroup: apiregistration.k8s.io
    Feb  6 19:02:51.058: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Feb  6 19:02:51.058: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Feb  6 19:02:51.058: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Feb  6 19:02:51.058: INFO: Checking APIGroup: apps
    Feb  6 19:02:51.058: INFO: PreferredVersion.GroupVersion: apps/v1
    Feb  6 19:02:51.058: INFO: Versions found [{apps/v1 v1}]
    Feb  6 19:02:51.058: INFO: apps/v1 matches apps/v1
    Feb  6 19:02:51.058: INFO: Checking APIGroup: events.k8s.io
    Feb  6 19:02:51.059: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Feb  6 19:02:51.059: INFO: Versions found [{events.k8s.io/v1 v1}]
    Feb  6 19:02:51.059: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Feb  6 19:02:51.059: INFO: Checking APIGroup: authentication.k8s.io
    Feb  6 19:02:51.060: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Feb  6 19:02:51.060: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Feb  6 19:02:51.060: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Feb  6 19:02:51.060: INFO: Checking APIGroup: authorization.k8s.io
    Feb  6 19:02:51.061: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Feb  6 19:02:51.061: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Feb  6 19:02:51.061: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Feb  6 19:02:51.061: INFO: Checking APIGroup: autoscaling
    Feb  6 19:02:51.062: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Feb  6 19:02:51.062: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Feb  6 19:02:51.062: INFO: autoscaling/v2 matches autoscaling/v2
    Feb  6 19:02:51.062: INFO: Checking APIGroup: batch
    Feb  6 19:02:51.063: INFO: PreferredVersion.GroupVersion: batch/v1
    Feb  6 19:02:51.063: INFO: Versions found [{batch/v1 v1}]
    Feb  6 19:02:51.063: INFO: batch/v1 matches batch/v1
    Feb  6 19:02:51.063: INFO: Checking APIGroup: certificates.k8s.io
    Feb  6 19:02:51.064: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Feb  6 19:02:51.064: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Feb  6 19:02:51.064: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Feb  6 19:02:51.064: INFO: Checking APIGroup: networking.k8s.io
    Feb  6 19:02:51.065: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Feb  6 19:02:51.065: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Feb  6 19:02:51.065: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Feb  6 19:02:51.065: INFO: Checking APIGroup: policy
    Feb  6 19:02:51.066: INFO: PreferredVersion.GroupVersion: policy/v1
    Feb  6 19:02:51.066: INFO: Versions found [{policy/v1 v1}]
    Feb  6 19:02:51.066: INFO: policy/v1 matches policy/v1
    Feb  6 19:02:51.066: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Feb  6 19:02:51.067: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Feb  6 19:02:51.067: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Feb  6 19:02:51.067: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Feb  6 19:02:51.067: INFO: Checking APIGroup: storage.k8s.io
    Feb  6 19:02:51.068: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Feb  6 19:02:51.068: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Feb  6 19:02:51.068: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Feb  6 19:02:51.068: INFO: Checking APIGroup: admissionregistration.k8s.io
    Feb  6 19:02:51.069: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Feb  6 19:02:51.069: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Feb  6 19:02:51.069: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Feb  6 19:02:51.069: INFO: Checking APIGroup: apiextensions.k8s.io
    Feb  6 19:02:51.070: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Feb  6 19:02:51.070: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Feb  6 19:02:51.070: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Feb  6 19:02:51.070: INFO: Checking APIGroup: scheduling.k8s.io
    Feb  6 19:02:51.071: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Feb  6 19:02:51.071: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Feb  6 19:02:51.071: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Feb  6 19:02:51.071: INFO: Checking APIGroup: coordination.k8s.io
    Feb  6 19:02:51.071: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Feb  6 19:02:51.071: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Feb  6 19:02:51.071: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Feb  6 19:02:51.071: INFO: Checking APIGroup: node.k8s.io
    Feb  6 19:02:51.072: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Feb  6 19:02:51.072: INFO: Versions found [{node.k8s.io/v1 v1}]
    Feb  6 19:02:51.072: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Feb  6 19:02:51.072: INFO: Checking APIGroup: discovery.k8s.io
    Feb  6 19:02:51.073: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Feb  6 19:02:51.073: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Feb  6 19:02:51.073: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Feb  6 19:02:51.073: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Feb  6 19:02:51.074: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Feb  6 19:02:51.074: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Feb  6 19:02:51.074: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Feb  6 19:02:51.074: INFO: Checking APIGroup: acme.cert-manager.io
    Feb  6 19:02:51.075: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
    Feb  6 19:02:51.075: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
    Feb  6 19:02:51.075: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
    Feb  6 19:02:51.075: INFO: Checking APIGroup: cert-manager.io
    Feb  6 19:02:51.076: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
    Feb  6 19:02:51.076: INFO: Versions found [{cert-manager.io/v1 v1}]
    Feb  6 19:02:51.076: INFO: cert-manager.io/v1 matches cert-manager.io/v1
    Feb  6 19:02:51.076: INFO: Checking APIGroup: anywhere.eks.amazonaws.com
    Feb  6 19:02:51.077: INFO: PreferredVersion.GroupVersion: anywhere.eks.amazonaws.com/v1alpha1
    Feb  6 19:02:51.077: INFO: Versions found [{anywhere.eks.amazonaws.com/v1alpha1 v1alpha1}]
    Feb  6 19:02:51.077: INFO: anywhere.eks.amazonaws.com/v1alpha1 matches anywhere.eks.amazonaws.com/v1alpha1
    Feb  6 19:02:51.077: INFO: Checking APIGroup: distro.eks.amazonaws.com
    Feb  6 19:02:51.078: INFO: PreferredVersion.GroupVersion: distro.eks.amazonaws.com/v1alpha1
    Feb  6 19:02:51.078: INFO: Versions found [{distro.eks.amazonaws.com/v1alpha1 v1alpha1}]
    Feb  6 19:02:51.078: INFO: distro.eks.amazonaws.com/v1alpha1 matches distro.eks.amazonaws.com/v1alpha1
    Feb  6 19:02:51.078: INFO: Checking APIGroup: ipam.cluster.x-k8s.io
    Feb  6 19:02:51.079: INFO: PreferredVersion.GroupVersion: ipam.cluster.x-k8s.io/v1alpha1
    Feb  6 19:02:51.079: INFO: Versions found [{ipam.cluster.x-k8s.io/v1alpha1 v1alpha1}]
    Feb  6 19:02:51.079: INFO: ipam.cluster.x-k8s.io/v1alpha1 matches ipam.cluster.x-k8s.io/v1alpha1
    Feb  6 19:02:51.079: INFO: Checking APIGroup: packages.eks.amazonaws.com
    Feb  6 19:02:51.080: INFO: PreferredVersion.GroupVersion: packages.eks.amazonaws.com/v1alpha1
    Feb  6 19:02:51.080: INFO: Versions found [{packages.eks.amazonaws.com/v1alpha1 v1alpha1}]
    Feb  6 19:02:51.080: INFO: packages.eks.amazonaws.com/v1alpha1 matches packages.eks.amazonaws.com/v1alpha1
    Feb  6 19:02:51.080: INFO: Checking APIGroup: runtime.cluster.x-k8s.io
    Feb  6 19:02:51.081: INFO: PreferredVersion.GroupVersion: runtime.cluster.x-k8s.io/v1alpha1
    Feb  6 19:02:51.081: INFO: Versions found [{runtime.cluster.x-k8s.io/v1alpha1 v1alpha1}]
    Feb  6 19:02:51.081: INFO: runtime.cluster.x-k8s.io/v1alpha1 matches runtime.cluster.x-k8s.io/v1alpha1
    Feb  6 19:02:51.081: INFO: Checking APIGroup: addons.cluster.x-k8s.io
    Feb  6 19:02:51.082: INFO: PreferredVersion.GroupVersion: addons.cluster.x-k8s.io/v1beta1
    Feb  6 19:02:51.082: INFO: Versions found [{addons.cluster.x-k8s.io/v1beta1 v1beta1} {addons.cluster.x-k8s.io/v1alpha4 v1alpha4} {addons.cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Feb  6 19:02:51.082: INFO: addons.cluster.x-k8s.io/v1beta1 matches addons.cluster.x-k8s.io/v1beta1
    Feb  6 19:02:51.082: INFO: Checking APIGroup: bootstrap.cluster.x-k8s.io
    Feb  6 19:02:51.083: INFO: PreferredVersion.GroupVersion: bootstrap.cluster.x-k8s.io/v1beta1
    Feb  6 19:02:51.083: INFO: Versions found [{bootstrap.cluster.x-k8s.io/v1beta1 v1beta1} {bootstrap.cluster.x-k8s.io/v1alpha4 v1alpha4} {bootstrap.cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Feb  6 19:02:51.083: INFO: bootstrap.cluster.x-k8s.io/v1beta1 matches bootstrap.cluster.x-k8s.io/v1beta1
    Feb  6 19:02:51.083: INFO: Checking APIGroup: cluster.x-k8s.io
    Feb  6 19:02:51.085: INFO: PreferredVersion.GroupVersion: cluster.x-k8s.io/v1beta1
    Feb  6 19:02:51.085: INFO: Versions found [{cluster.x-k8s.io/v1beta1 v1beta1} {cluster.x-k8s.io/v1alpha4 v1alpha4} {cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Feb  6 19:02:51.085: INFO: cluster.x-k8s.io/v1beta1 matches cluster.x-k8s.io/v1beta1
    Feb  6 19:02:51.085: INFO: Checking APIGroup: clusterctl.cluster.x-k8s.io
    Feb  6 19:02:51.086: INFO: PreferredVersion.GroupVersion: clusterctl.cluster.x-k8s.io/v1alpha3
    Feb  6 19:02:51.086: INFO: Versions found [{clusterctl.cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Feb  6 19:02:51.086: INFO: clusterctl.cluster.x-k8s.io/v1alpha3 matches clusterctl.cluster.x-k8s.io/v1alpha3
    Feb  6 19:02:51.086: INFO: Checking APIGroup: controlplane.cluster.x-k8s.io
    Feb  6 19:02:51.087: INFO: PreferredVersion.GroupVersion: controlplane.cluster.x-k8s.io/v1beta1
    Feb  6 19:02:51.087: INFO: Versions found [{controlplane.cluster.x-k8s.io/v1beta1 v1beta1} {controlplane.cluster.x-k8s.io/v1alpha4 v1alpha4} {controlplane.cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Feb  6 19:02:51.087: INFO: controlplane.cluster.x-k8s.io/v1beta1 matches controlplane.cluster.x-k8s.io/v1beta1
    Feb  6 19:02:51.087: INFO: Checking APIGroup: etcdcluster.cluster.x-k8s.io
    Feb  6 19:02:51.088: INFO: PreferredVersion.GroupVersion: etcdcluster.cluster.x-k8s.io/v1beta1
    Feb  6 19:02:51.088: INFO: Versions found [{etcdcluster.cluster.x-k8s.io/v1beta1 v1beta1} {etcdcluster.cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Feb  6 19:02:51.088: INFO: etcdcluster.cluster.x-k8s.io/v1beta1 matches etcdcluster.cluster.x-k8s.io/v1beta1
    Feb  6 19:02:51.088: INFO: Checking APIGroup: infrastructure.cluster.x-k8s.io
    Feb  6 19:02:51.089: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.x-k8s.io/v1beta1
    Feb  6 19:02:51.090: INFO: Versions found [{infrastructure.cluster.x-k8s.io/v1beta1 v1beta1} {infrastructure.cluster.x-k8s.io/v1alpha4 v1alpha4} {infrastructure.cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Feb  6 19:02:51.090: INFO: infrastructure.cluster.x-k8s.io/v1beta1 matches infrastructure.cluster.x-k8s.io/v1beta1
    Feb  6 19:02:51.090: INFO: Checking APIGroup: cilium.io
    Feb  6 19:02:51.090: INFO: PreferredVersion.GroupVersion: cilium.io/v2
    Feb  6 19:02:51.091: INFO: Versions found [{cilium.io/v2 v2}]
    Feb  6 19:02:51.091: INFO: cilium.io/v2 matches cilium.io/v2
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Feb  6 19:02:51.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-95" for this suite. 02/06/23 19:02:51.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:02:51.105
Feb  6 19:02:51.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename projected 02/06/23 19:02:51.106
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:51.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:51.13
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 02/06/23 19:02:51.134
Feb  6 19:02:51.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5" in namespace "projected-72" to be "Succeeded or Failed"
Feb  6 19:02:51.151: INFO: Pod "downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.520226ms
Feb  6 19:02:53.157: INFO: Pod "downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012516385s
Feb  6 19:02:55.156: INFO: Pod "downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011690188s
Feb  6 19:02:57.155: INFO: Pod "downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010950921s
STEP: Saw pod success 02/06/23 19:02:57.155
Feb  6 19:02:57.155: INFO: Pod "downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5" satisfied condition "Succeeded or Failed"
Feb  6 19:02:57.159: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5 container client-container: <nil>
STEP: delete the pod 02/06/23 19:02:57.166
Feb  6 19:02:57.179: INFO: Waiting for pod downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5 to disappear
Feb  6 19:02:57.182: INFO: Pod downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb  6 19:02:57.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-72" for this suite. 02/06/23 19:02:57.188
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":352,"skipped":6585,"failed":0}
------------------------------
• [SLOW TEST] [6.090 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:02:51.105
    Feb  6 19:02:51.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename projected 02/06/23 19:02:51.106
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:51.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:51.13
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 02/06/23 19:02:51.134
    Feb  6 19:02:51.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5" in namespace "projected-72" to be "Succeeded or Failed"
    Feb  6 19:02:51.151: INFO: Pod "downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.520226ms
    Feb  6 19:02:53.157: INFO: Pod "downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012516385s
    Feb  6 19:02:55.156: INFO: Pod "downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011690188s
    Feb  6 19:02:57.155: INFO: Pod "downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010950921s
    STEP: Saw pod success 02/06/23 19:02:57.155
    Feb  6 19:02:57.155: INFO: Pod "downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5" satisfied condition "Succeeded or Failed"
    Feb  6 19:02:57.159: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5 container client-container: <nil>
    STEP: delete the pod 02/06/23 19:02:57.166
    Feb  6 19:02:57.179: INFO: Waiting for pod downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5 to disappear
    Feb  6 19:02:57.182: INFO: Pod downwardapi-volume-9915b64c-5f0d-4f7f-b10b-9ef2d1f274a5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb  6 19:02:57.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-72" for this suite. 02/06/23 19:02:57.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:02:57.197
Feb  6 19:02:57.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename svcaccounts 02/06/23 19:02:57.198
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:57.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:57.222
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 02/06/23 19:02:57.224
STEP: watching for the ServiceAccount to be added 02/06/23 19:02:57.232
STEP: patching the ServiceAccount 02/06/23 19:02:57.234
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/06/23 19:02:57.241
STEP: deleting the ServiceAccount 02/06/23 19:02:57.245
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb  6 19:02:57.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6633" for this suite. 02/06/23 19:02:57.266
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":353,"skipped":6611,"failed":0}
------------------------------
• [0.075 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:02:57.197
    Feb  6 19:02:57.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename svcaccounts 02/06/23 19:02:57.198
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:57.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:57.222
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 02/06/23 19:02:57.224
    STEP: watching for the ServiceAccount to be added 02/06/23 19:02:57.232
    STEP: patching the ServiceAccount 02/06/23 19:02:57.234
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/06/23 19:02:57.241
    STEP: deleting the ServiceAccount 02/06/23 19:02:57.245
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb  6 19:02:57.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6633" for this suite. 02/06/23 19:02:57.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:02:57.275
Feb  6 19:02:57.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename cronjob 02/06/23 19:02:57.276
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:57.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:57.295
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 02/06/23 19:02:57.298
STEP: Ensuring a job is scheduled 02/06/23 19:02:57.304
STEP: Ensuring exactly one is scheduled 02/06/23 19:03:01.309
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/06/23 19:03:01.313
STEP: Ensuring no more jobs are scheduled 02/06/23 19:03:01.317
STEP: Removing cronjob 02/06/23 19:08:01.326
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb  6 19:08:01.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1998" for this suite. 02/06/23 19:08:01.342
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":354,"skipped":6616,"failed":0}
------------------------------
• [SLOW TEST] [304.079 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:02:57.275
    Feb  6 19:02:57.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename cronjob 02/06/23 19:02:57.276
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:02:57.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:02:57.295
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 02/06/23 19:02:57.298
    STEP: Ensuring a job is scheduled 02/06/23 19:02:57.304
    STEP: Ensuring exactly one is scheduled 02/06/23 19:03:01.309
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/06/23 19:03:01.313
    STEP: Ensuring no more jobs are scheduled 02/06/23 19:03:01.317
    STEP: Removing cronjob 02/06/23 19:08:01.326
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb  6 19:08:01.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1998" for this suite. 02/06/23 19:08:01.342
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:08:01.359
Feb  6 19:08:01.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-probe 02/06/23 19:08:01.361
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:08:01.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:08:01.392
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-9dedc843-1734-4f92-8128-83d41af6ce0e in namespace container-probe-6045 02/06/23 19:08:01.395
Feb  6 19:08:01.405: INFO: Waiting up to 5m0s for pod "busybox-9dedc843-1734-4f92-8128-83d41af6ce0e" in namespace "container-probe-6045" to be "not pending"
Feb  6 19:08:01.411: INFO: Pod "busybox-9dedc843-1734-4f92-8128-83d41af6ce0e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.079281ms
Feb  6 19:08:03.415: INFO: Pod "busybox-9dedc843-1734-4f92-8128-83d41af6ce0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00997886s
Feb  6 19:08:05.417: INFO: Pod "busybox-9dedc843-1734-4f92-8128-83d41af6ce0e": Phase="Running", Reason="", readiness=true. Elapsed: 4.01117072s
Feb  6 19:08:05.417: INFO: Pod "busybox-9dedc843-1734-4f92-8128-83d41af6ce0e" satisfied condition "not pending"
Feb  6 19:08:05.417: INFO: Started pod busybox-9dedc843-1734-4f92-8128-83d41af6ce0e in namespace container-probe-6045
STEP: checking the pod's current state and verifying that restartCount is present 02/06/23 19:08:05.417
Feb  6 19:08:05.420: INFO: Initial restart count of pod busybox-9dedc843-1734-4f92-8128-83d41af6ce0e is 0
Feb  6 19:08:53.559: INFO: Restart count of pod container-probe-6045/busybox-9dedc843-1734-4f92-8128-83d41af6ce0e is now 1 (48.139311578s elapsed)
STEP: deleting the pod 02/06/23 19:08:53.56
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb  6 19:08:53.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6045" for this suite. 02/06/23 19:08:53.579
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":355,"skipped":6620,"failed":0}
------------------------------
• [SLOW TEST] [52.236 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:08:01.359
    Feb  6 19:08:01.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-probe 02/06/23 19:08:01.361
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:08:01.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:08:01.392
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-9dedc843-1734-4f92-8128-83d41af6ce0e in namespace container-probe-6045 02/06/23 19:08:01.395
    Feb  6 19:08:01.405: INFO: Waiting up to 5m0s for pod "busybox-9dedc843-1734-4f92-8128-83d41af6ce0e" in namespace "container-probe-6045" to be "not pending"
    Feb  6 19:08:01.411: INFO: Pod "busybox-9dedc843-1734-4f92-8128-83d41af6ce0e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.079281ms
    Feb  6 19:08:03.415: INFO: Pod "busybox-9dedc843-1734-4f92-8128-83d41af6ce0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00997886s
    Feb  6 19:08:05.417: INFO: Pod "busybox-9dedc843-1734-4f92-8128-83d41af6ce0e": Phase="Running", Reason="", readiness=true. Elapsed: 4.01117072s
    Feb  6 19:08:05.417: INFO: Pod "busybox-9dedc843-1734-4f92-8128-83d41af6ce0e" satisfied condition "not pending"
    Feb  6 19:08:05.417: INFO: Started pod busybox-9dedc843-1734-4f92-8128-83d41af6ce0e in namespace container-probe-6045
    STEP: checking the pod's current state and verifying that restartCount is present 02/06/23 19:08:05.417
    Feb  6 19:08:05.420: INFO: Initial restart count of pod busybox-9dedc843-1734-4f92-8128-83d41af6ce0e is 0
    Feb  6 19:08:53.559: INFO: Restart count of pod container-probe-6045/busybox-9dedc843-1734-4f92-8128-83d41af6ce0e is now 1 (48.139311578s elapsed)
    STEP: deleting the pod 02/06/23 19:08:53.56
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb  6 19:08:53.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6045" for this suite. 02/06/23 19:08:53.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:08:53.601
Feb  6 19:08:53.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 19:08:53.602
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:08:53.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:08:53.632
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 02/06/23 19:08:53.635
Feb  6 19:08:53.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8872 create -f -'
Feb  6 19:08:55.198: INFO: stderr: ""
Feb  6 19:08:55.198: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 02/06/23 19:08:55.198
Feb  6 19:08:55.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8872 diff -f -'
Feb  6 19:08:55.611: INFO: rc: 1
Feb  6 19:08:55.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8872 delete -f -'
Feb  6 19:08:55.703: INFO: stderr: ""
Feb  6 19:08:55.703: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 19:08:55.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8872" for this suite. 02/06/23 19:08:55.711
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":356,"skipped":6637,"failed":0}
------------------------------
• [2.134 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:08:53.601
    Feb  6 19:08:53.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 19:08:53.602
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:08:53.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:08:53.632
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 02/06/23 19:08:53.635
    Feb  6 19:08:53.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8872 create -f -'
    Feb  6 19:08:55.198: INFO: stderr: ""
    Feb  6 19:08:55.198: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 02/06/23 19:08:55.198
    Feb  6 19:08:55.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8872 diff -f -'
    Feb  6 19:08:55.611: INFO: rc: 1
    Feb  6 19:08:55.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-8872 delete -f -'
    Feb  6 19:08:55.703: INFO: stderr: ""
    Feb  6 19:08:55.703: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 19:08:55.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8872" for this suite. 02/06/23 19:08:55.711
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:08:55.735
Feb  6 19:08:55.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename services 02/06/23 19:08:55.736
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:08:55.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:08:55.764
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-8754 02/06/23 19:08:55.768
STEP: creating replication controller nodeport-test in namespace services-8754 02/06/23 19:08:55.799
I0206 19:08:55.812767      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8754, replica count: 2
I0206 19:08:58.863878      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0206 19:09:01.864699      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  6 19:09:01.864: INFO: Creating new exec pod
Feb  6 19:09:01.875: INFO: Waiting up to 5m0s for pod "execpodk72ls" in namespace "services-8754" to be "running"
Feb  6 19:09:01.884: INFO: Pod "execpodk72ls": Phase="Pending", Reason="", readiness=false. Elapsed: 8.874252ms
Feb  6 19:09:03.889: INFO: Pod "execpodk72ls": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014088423s
Feb  6 19:09:05.893: INFO: Pod "execpodk72ls": Phase="Running", Reason="", readiness=true. Elapsed: 4.018100957s
Feb  6 19:09:05.893: INFO: Pod "execpodk72ls" satisfied condition "running"
Feb  6 19:09:06.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb  6 19:09:07.076: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb  6 19:09:07.076: INFO: stdout: ""
Feb  6 19:09:08.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb  6 19:09:08.238: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb  6 19:09:08.238: INFO: stdout: ""
Feb  6 19:09:09.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb  6 19:09:09.254: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb  6 19:09:09.254: INFO: stdout: ""
Feb  6 19:09:10.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb  6 19:09:10.242: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb  6 19:09:10.242: INFO: stdout: "nodeport-test-c9pbb"
Feb  6 19:09:10.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.185.175 80'
Feb  6 19:09:10.397: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.185.175 80\nConnection to 10.109.185.175 80 port [tcp/http] succeeded!\n"
Feb  6 19:09:10.397: INFO: stdout: "nodeport-test-82lsv"
Feb  6 19:09:10.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.4.133 30979'
Feb  6 19:09:10.565: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.4.133 30979\nConnection to 195.17.4.133 30979 port [tcp/*] succeeded!\n"
Feb  6 19:09:10.565: INFO: stdout: "nodeport-test-c9pbb"
Feb  6 19:09:10.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.89 30979'
Feb  6 19:09:10.726: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.148.89 30979\nConnection to 195.17.148.89 30979 port [tcp/*] succeeded!\n"
Feb  6 19:09:10.726: INFO: stdout: "nodeport-test-82lsv"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb  6 19:09:10.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8754" for this suite. 02/06/23 19:09:10.735
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":357,"skipped":6638,"failed":0}
------------------------------
• [SLOW TEST] [15.009 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:08:55.735
    Feb  6 19:08:55.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename services 02/06/23 19:08:55.736
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:08:55.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:08:55.764
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-8754 02/06/23 19:08:55.768
    STEP: creating replication controller nodeport-test in namespace services-8754 02/06/23 19:08:55.799
    I0206 19:08:55.812767      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8754, replica count: 2
    I0206 19:08:58.863878      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0206 19:09:01.864699      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb  6 19:09:01.864: INFO: Creating new exec pod
    Feb  6 19:09:01.875: INFO: Waiting up to 5m0s for pod "execpodk72ls" in namespace "services-8754" to be "running"
    Feb  6 19:09:01.884: INFO: Pod "execpodk72ls": Phase="Pending", Reason="", readiness=false. Elapsed: 8.874252ms
    Feb  6 19:09:03.889: INFO: Pod "execpodk72ls": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014088423s
    Feb  6 19:09:05.893: INFO: Pod "execpodk72ls": Phase="Running", Reason="", readiness=true. Elapsed: 4.018100957s
    Feb  6 19:09:05.893: INFO: Pod "execpodk72ls" satisfied condition "running"
    Feb  6 19:09:06.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Feb  6 19:09:07.076: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb  6 19:09:07.076: INFO: stdout: ""
    Feb  6 19:09:08.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Feb  6 19:09:08.238: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb  6 19:09:08.238: INFO: stdout: ""
    Feb  6 19:09:09.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Feb  6 19:09:09.254: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb  6 19:09:09.254: INFO: stdout: ""
    Feb  6 19:09:10.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Feb  6 19:09:10.242: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb  6 19:09:10.242: INFO: stdout: "nodeport-test-c9pbb"
    Feb  6 19:09:10.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.185.175 80'
    Feb  6 19:09:10.397: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.185.175 80\nConnection to 10.109.185.175 80 port [tcp/http] succeeded!\n"
    Feb  6 19:09:10.397: INFO: stdout: "nodeport-test-82lsv"
    Feb  6 19:09:10.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.4.133 30979'
    Feb  6 19:09:10.565: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.4.133 30979\nConnection to 195.17.4.133 30979 port [tcp/*] succeeded!\n"
    Feb  6 19:09:10.565: INFO: stdout: "nodeport-test-c9pbb"
    Feb  6 19:09:10.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=services-8754 exec execpodk72ls -- /bin/sh -x -c echo hostName | nc -v -t -w 2 195.17.148.89 30979'
    Feb  6 19:09:10.726: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 195.17.148.89 30979\nConnection to 195.17.148.89 30979 port [tcp/*] succeeded!\n"
    Feb  6 19:09:10.726: INFO: stdout: "nodeport-test-82lsv"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb  6 19:09:10.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8754" for this suite. 02/06/23 19:09:10.735
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:09:10.745
Feb  6 19:09:10.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename ingressclass 02/06/23 19:09:10.746
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:09:10.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:09:10.772
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 02/06/23 19:09:10.776
STEP: getting /apis/networking.k8s.io 02/06/23 19:09:10.778
STEP: getting /apis/networking.k8s.iov1 02/06/23 19:09:10.78
STEP: creating 02/06/23 19:09:10.781
STEP: getting 02/06/23 19:09:10.799
STEP: listing 02/06/23 19:09:10.802
STEP: watching 02/06/23 19:09:10.806
Feb  6 19:09:10.806: INFO: starting watch
STEP: patching 02/06/23 19:09:10.807
STEP: updating 02/06/23 19:09:10.819
Feb  6 19:09:10.825: INFO: waiting for watch events with expected annotations
Feb  6 19:09:10.825: INFO: saw patched and updated annotations
STEP: deleting 02/06/23 19:09:10.826
STEP: deleting a collection 02/06/23 19:09:10.838
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Feb  6 19:09:10.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7116" for this suite. 02/06/23 19:09:10.864
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":358,"skipped":6645,"failed":0}
------------------------------
• [0.128 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:09:10.745
    Feb  6 19:09:10.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename ingressclass 02/06/23 19:09:10.746
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:09:10.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:09:10.772
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 02/06/23 19:09:10.776
    STEP: getting /apis/networking.k8s.io 02/06/23 19:09:10.778
    STEP: getting /apis/networking.k8s.iov1 02/06/23 19:09:10.78
    STEP: creating 02/06/23 19:09:10.781
    STEP: getting 02/06/23 19:09:10.799
    STEP: listing 02/06/23 19:09:10.802
    STEP: watching 02/06/23 19:09:10.806
    Feb  6 19:09:10.806: INFO: starting watch
    STEP: patching 02/06/23 19:09:10.807
    STEP: updating 02/06/23 19:09:10.819
    Feb  6 19:09:10.825: INFO: waiting for watch events with expected annotations
    Feb  6 19:09:10.825: INFO: saw patched and updated annotations
    STEP: deleting 02/06/23 19:09:10.826
    STEP: deleting a collection 02/06/23 19:09:10.838
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Feb  6 19:09:10.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-7116" for this suite. 02/06/23 19:09:10.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:09:10.879
Feb  6 19:09:10.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename sched-preemption 02/06/23 19:09:10.88
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:09:10.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:09:10.903
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb  6 19:09:10.928: INFO: Waiting up to 1m0s for all nodes to be ready
Feb  6 19:10:10.988: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 02/06/23 19:10:10.992
Feb  6 19:10:11.019: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb  6 19:10:11.030: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb  6 19:10:11.056: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb  6 19:10:11.069: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb  6 19:10:11.095: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb  6 19:10:11.102: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/06/23 19:10:11.103
Feb  6 19:10:11.103: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8673" to be "running"
Feb  6 19:10:11.108: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.085599ms
Feb  6 19:10:13.114: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010354754s
Feb  6 19:10:15.115: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011562468s
Feb  6 19:10:17.114: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.011189911s
Feb  6 19:10:17.114: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb  6 19:10:17.114: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8673" to be "running"
Feb  6 19:10:17.119: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.356342ms
Feb  6 19:10:17.119: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb  6 19:10:17.119: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8673" to be "running"
Feb  6 19:10:17.123: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.520021ms
Feb  6 19:10:19.129: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009839444s
Feb  6 19:10:19.129: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb  6 19:10:19.129: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8673" to be "running"
Feb  6 19:10:19.132: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.271178ms
Feb  6 19:10:19.132: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb  6 19:10:19.132: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8673" to be "running"
Feb  6 19:10:19.136: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976756ms
Feb  6 19:10:21.176: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043660884s
Feb  6 19:10:23.142: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009844434s
Feb  6 19:10:23.142: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb  6 19:10:23.142: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8673" to be "running"
Feb  6 19:10:23.147: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.915898ms
Feb  6 19:10:23.147: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 02/06/23 19:10:23.147
Feb  6 19:10:23.161: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Feb  6 19:10:23.165: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363369ms
Feb  6 19:10:25.170: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009372675s
Feb  6 19:10:27.172: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010877751s
Feb  6 19:10:29.174: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.013080398s
Feb  6 19:10:29.174: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb  6 19:10:29.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8673" for this suite. 02/06/23 19:10:29.225
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":359,"skipped":6669,"failed":0}
------------------------------
• [SLOW TEST] [78.418 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:09:10.879
    Feb  6 19:09:10.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename sched-preemption 02/06/23 19:09:10.88
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:09:10.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:09:10.903
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb  6 19:09:10.928: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb  6 19:10:10.988: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 02/06/23 19:10:10.992
    Feb  6 19:10:11.019: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb  6 19:10:11.030: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb  6 19:10:11.056: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb  6 19:10:11.069: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb  6 19:10:11.095: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb  6 19:10:11.102: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/06/23 19:10:11.103
    Feb  6 19:10:11.103: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8673" to be "running"
    Feb  6 19:10:11.108: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.085599ms
    Feb  6 19:10:13.114: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010354754s
    Feb  6 19:10:15.115: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011562468s
    Feb  6 19:10:17.114: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.011189911s
    Feb  6 19:10:17.114: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb  6 19:10:17.114: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8673" to be "running"
    Feb  6 19:10:17.119: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.356342ms
    Feb  6 19:10:17.119: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb  6 19:10:17.119: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8673" to be "running"
    Feb  6 19:10:17.123: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.520021ms
    Feb  6 19:10:19.129: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009839444s
    Feb  6 19:10:19.129: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb  6 19:10:19.129: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8673" to be "running"
    Feb  6 19:10:19.132: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.271178ms
    Feb  6 19:10:19.132: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb  6 19:10:19.132: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8673" to be "running"
    Feb  6 19:10:19.136: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976756ms
    Feb  6 19:10:21.176: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043660884s
    Feb  6 19:10:23.142: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009844434s
    Feb  6 19:10:23.142: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb  6 19:10:23.142: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8673" to be "running"
    Feb  6 19:10:23.147: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.915898ms
    Feb  6 19:10:23.147: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 02/06/23 19:10:23.147
    Feb  6 19:10:23.161: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Feb  6 19:10:23.165: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363369ms
    Feb  6 19:10:25.170: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009372675s
    Feb  6 19:10:27.172: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010877751s
    Feb  6 19:10:29.174: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.013080398s
    Feb  6 19:10:29.174: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb  6 19:10:29.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8673" for this suite. 02/06/23 19:10:29.225
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:10:29.298
Feb  6 19:10:29.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename var-expansion 02/06/23 19:10:29.299
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:10:29.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:10:29.373
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 02/06/23 19:10:29.375
Feb  6 19:10:29.384: INFO: Waiting up to 5m0s for pod "var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be" in namespace "var-expansion-6824" to be "Succeeded or Failed"
Feb  6 19:10:29.392: INFO: Pod "var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be": Phase="Pending", Reason="", readiness=false. Elapsed: 7.57575ms
Feb  6 19:10:31.397: INFO: Pod "var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012784465s
Feb  6 19:10:33.397: INFO: Pod "var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013289507s
Feb  6 19:10:35.397: INFO: Pod "var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012585667s
STEP: Saw pod success 02/06/23 19:10:35.397
Feb  6 19:10:35.397: INFO: Pod "var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be" satisfied condition "Succeeded or Failed"
Feb  6 19:10:35.401: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be container dapi-container: <nil>
STEP: delete the pod 02/06/23 19:10:35.418
Feb  6 19:10:35.437: INFO: Waiting for pod var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be to disappear
Feb  6 19:10:35.442: INFO: Pod var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb  6 19:10:35.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6824" for this suite. 02/06/23 19:10:35.449
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":360,"skipped":6670,"failed":0}
------------------------------
• [SLOW TEST] [6.159 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:10:29.298
    Feb  6 19:10:29.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename var-expansion 02/06/23 19:10:29.299
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:10:29.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:10:29.373
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 02/06/23 19:10:29.375
    Feb  6 19:10:29.384: INFO: Waiting up to 5m0s for pod "var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be" in namespace "var-expansion-6824" to be "Succeeded or Failed"
    Feb  6 19:10:29.392: INFO: Pod "var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be": Phase="Pending", Reason="", readiness=false. Elapsed: 7.57575ms
    Feb  6 19:10:31.397: INFO: Pod "var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012784465s
    Feb  6 19:10:33.397: INFO: Pod "var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013289507s
    Feb  6 19:10:35.397: INFO: Pod "var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012585667s
    STEP: Saw pod success 02/06/23 19:10:35.397
    Feb  6 19:10:35.397: INFO: Pod "var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be" satisfied condition "Succeeded or Failed"
    Feb  6 19:10:35.401: INFO: Trying to get logs from node tneyla25-md-0-68cbcb4798-5xxmw pod var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be container dapi-container: <nil>
    STEP: delete the pod 02/06/23 19:10:35.418
    Feb  6 19:10:35.437: INFO: Waiting for pod var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be to disappear
    Feb  6 19:10:35.442: INFO: Pod var-expansion-79b93f5b-28ce-4f2b-b77a-972cffdc26be no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb  6 19:10:35.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6824" for this suite. 02/06/23 19:10:35.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:10:35.463
Feb  6 19:10:35.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename container-runtime 02/06/23 19:10:35.464
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:10:35.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:10:35.499
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 02/06/23 19:10:35.501
STEP: wait for the container to reach Failed 02/06/23 19:10:35.517
STEP: get the container status 02/06/23 19:10:40.557
STEP: the container should be terminated 02/06/23 19:10:40.561
STEP: the termination message should be set 02/06/23 19:10:40.561
Feb  6 19:10:40.561: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/06/23 19:10:40.561
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb  6 19:10:40.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5045" for this suite. 02/06/23 19:10:40.59
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":361,"skipped":6677,"failed":0}
------------------------------
• [SLOW TEST] [5.135 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:10:35.463
    Feb  6 19:10:35.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename container-runtime 02/06/23 19:10:35.464
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:10:35.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:10:35.499
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 02/06/23 19:10:35.501
    STEP: wait for the container to reach Failed 02/06/23 19:10:35.517
    STEP: get the container status 02/06/23 19:10:40.557
    STEP: the container should be terminated 02/06/23 19:10:40.561
    STEP: the termination message should be set 02/06/23 19:10:40.561
    Feb  6 19:10:40.561: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/06/23 19:10:40.561
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb  6 19:10:40.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5045" for this suite. 02/06/23 19:10:40.59
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/06/23 19:10:40.599
Feb  6 19:10:40.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
STEP: Building a namespace api object, basename kubectl 02/06/23 19:10:40.6
STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:10:40.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:10:40.62
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 02/06/23 19:10:40.623
Feb  6 19:10:40.623: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7351 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 02/06/23 19:10:40.684
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb  6 19:10:40.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7351" for this suite. 02/06/23 19:10:40.698
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":362,"skipped":6679,"failed":0}
------------------------------
• [0.109 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/06/23 19:10:40.599
    Feb  6 19:10:40.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2221263174
    STEP: Building a namespace api object, basename kubectl 02/06/23 19:10:40.6
    STEP: Waiting for a default service account to be provisioned in namespace 02/06/23 19:10:40.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/06/23 19:10:40.62
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 02/06/23 19:10:40.623
    Feb  6 19:10:40.623: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2221263174 --namespace=kubectl-7351 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 02/06/23 19:10:40.684
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb  6 19:10:40.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7351" for this suite. 02/06/23 19:10:40.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Feb  6 19:10:40.711: INFO: Running AfterSuite actions on all nodes
Feb  6 19:10:40.711: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Feb  6 19:10:40.712: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Feb  6 19:10:40.712: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Feb  6 19:10:40.712: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Feb  6 19:10:40.712: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Feb  6 19:10:40.712: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Feb  6 19:10:40.712: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Feb  6 19:10:40.712: INFO: Running AfterSuite actions on node 1
Feb  6 19:10:40.712: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.002 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Feb  6 19:10:40.711: INFO: Running AfterSuite actions on all nodes
    Feb  6 19:10:40.711: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Feb  6 19:10:40.712: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Feb  6 19:10:40.712: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Feb  6 19:10:40.712: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Feb  6 19:10:40.712: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Feb  6 19:10:40.712: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Feb  6 19:10:40.712: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Feb  6 19:10:40.712: INFO: Running AfterSuite actions on node 1
    Feb  6 19:10:40.712: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.088 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 6658.252 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h50m58.70421944s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

